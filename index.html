
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 PINT: Physics-Informed Neural Time Series Models with Applications to
  Long-term Inference on WeatherBench 2m-Temperature Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keonvin Park, Jisu Kim, Jaemin Seo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces PINT (Physics-Informed Neural Time Series Models), a framework that integrates physical constraints into neural time series models to improve their ability to capture complex dynamics. We apply PINT to the ERA5 WeatherBench dataset, focusing on long-term forecasting of 2m-temperature data. PINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed prior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures. This equation's analytical solutions (sine and cosine functions) facilitate rigorous evaluation of the benefits of incorporating physics-informed constraints. By benchmarking against a linear regression baseline derived from its exact solutions, we quantify the impact of embedding physical principles in data-driven models. Unlike traditional time series models that rely on future observations, PINT is designed for practical forecasting. Using only the first 90 days of observed data, it iteratively predicts the next two years, addressing challenges posed by limited real-time updates. Experiments on the WeatherBench dataset demonstrate PINT's ability to generalize, capture periodic trends, and align with physical principles. This study highlights the potential of physics-informed neural models in bridging machine learning and interpretable climate applications.   Our models and datasets are publicly available on GitHub: https://github.com/KV-Park.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:44:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.04018v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.04018v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Integrating nano- and micrometer-scale energy deposition models for
  mechanistic prediction of radiation-induced DNA damage and cell survival</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giulio Bordieri, Marta Missiaggia, Gianluca Lattanzi, Carmen Villagrasa, Yann Perrot, Francesco G. Cordoni
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present an integrated modeling framework that combines the Generalized Stochastic Microdosimetric Model (GSM2), used to predict cell survival fractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for simulating radiation-induced DNA damage in cell populations. This approach enables the generation of spatially and structurally resolved double-strand break (DSB) distributions, capturing key features such as damage complexity and chromosome specificity. A novel application of the DBSCAN clustering algorithm is introduced to group DSBs at the micrometer scale. This allows the identification of physical aggregates of DNA damage and their association with subnuclear domains, providing a direct link to the cell survival probability as predicted by \gsm.   The model was validated using experimental data from HUVEC cells irradiated with 220 kV X-rays and H460 cells exposed to protons over a wide linear energy transfer (LET) range, from approximately 4 keV/{\mu}m to over 20 keV/{\mu}m. Results show excellent agreement between simulations and experimental survival probabilities, making this one of the first consistent multi-scale models to bridge nanodosimetric and microdosimetric representations of radiation with biological outcomes such as cell survival.   By incorporating the inherent stochastic nature of radiation-matter interactions, this framework effectively connects the physical properties of the radiation field to the biological response at the cellular level. Its accuracy across various radiation types and energies supports its potential for use in biologically optimized radiotherapy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:55:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.bio-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00929v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00929v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 IAM: Efficient Inference through Attention Mapping between
  Different-scale LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Zhao, Zuchao Li, Hai Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs encounter significant challenges in resource consumption nowadays, especially with long contexts. Despite extensive efforts dedicate to enhancing inference efficiency, these methods primarily exploit internal sparsity within the models, without leveraging external information for optimization. We identify the high similarity of attention matrices across different-scale LLMs, which offers a novel perspective for optimization. We first conduct a comprehensive analysis of how to measure similarity, how to select mapping Layers and whether mapping is consistency. Based on these insights, we introduce the IAM framework, which achieves dual benefits of accelerated attention computation and reduced KV cache usage by performing attention mapping between small and large LLMs. Our experimental results demonstrate that IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without appreciably sacrificing performance. Experiments on different series of models show the generalizability of IAM. Importantly, it is also orthogonal to many existing KV cache optimization methods, making it a versatile addition to the current toolkit for enhancing LLM efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-16T06:39:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11953v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11953v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Streaming 4D Visual Geometry Transformer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, Jiwen Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-15T17:59:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11539v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11539v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 MIRAGE: KV Cache Optimization through Parameter Remapping for
  Multi-tenant LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-15T17:23:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11507v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11507v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of real-world conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-15T12:59:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22791v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22791v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware
  Rotary Positional Embedding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luohe Shi, Zuchao Li, Lefei Zhang, Guoming Liu, Baoyuan Qi, Hai Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) based on Transformer Decoders have become the preferred choice for conversational generative AI. Despite the overall superiority of the Decoder architecture, the gradually increasing Key-Value (KV) cache during inference has emerged as a primary efficiency bottleneck, both in aspects of memory consumption and data transfer bandwidth limitations. To address these challenges, we propose a paradigm called KV-Latent. By down-sampling the Key-Value vector dimensions into a latent space, we can significantly reduce the KV Cache footprint and improve inference speed, only with a small amount of extra training, less than 1\% of pre-training takes. Besides, we enhanced the stability of Rotary Positional Embedding applied on lower-dimensional vectors by modifying its frequency sampling mechanism, avoiding noise introduced by higher frequencies while retaining position attenuation. Our experiments, including both models with Grouped Query Attention and those without, have yielded satisfactory results. Finally, we conducted comparative experiments to study the impact of separately reducing Key and Value components on model's performance. Our approach allows for the construction of more efficient language model systems, and opens the new possibility on KV Cache saving and efficient LLMs. Our code is available at https://github.com/ShiLuohe/KV-Latent.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-15T12:52:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11273v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11273v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 VSAG: An Optimized Search Framework for Graph-based Approximate Nearest
  Neighbor Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoyao Zhong, Haotian Li, Jiabao Jin, Mingyu Yang, Deming Chu, Xiangyu Wang, Zhitao Shen, Wei Jia, George Gu, Yi Xie, Xuemin Lin, Heng Tao Shen, Jingkuan Song, Peng Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Approximate nearest neighbor search (ANNS) is a fundamental problem in vector databases and AI infrastructures. Recent graph-based ANNS algorithms have achieved high search accuracy with practical efficiency. Despite the advancements, these algorithms still face performance bottlenecks in production, due to the random memory access patterns of graph-based search and the high computational overheads of vector distance. In addition, the performance of a graph-based ANNS algorithm is highly sensitive to parameters, while selecting the optimal parameters is cost-prohibitive, e.g., manual tuning requires repeatedly re-building the index. This paper introduces VSAG, an open-source framework that aims to enhance the in production performance of graph-based ANNS algorithms. VSAG has been deployed at scale in the services of Ant Group, and it incorporates three key optimizations: (i) efficient memory access: it reduces L3 cache misses with pre-fetching and cache-friendly vector organization; (ii) automated parameter tuning: it automatically selects performance-optimal parameters without requiring index rebuilding; (iii) efficient distance computation: it leverages modern hardware, scalar quantization, and smartly switches to low-precision representation to dramatically reduce the distance computation costs. We evaluate VSAG on real-world datasets. The experimental results show that VSAG achieves the state-of-the-art performance and provides up to 4x speedup over HNSWlib (an industry-standard library) while ensuring the same accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-15T11:31:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.17911v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.17911v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Two-dimensional single-crystal photonic scintillator for enhanced X-ray
  imaging</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tatsunori Shibuya, Eichi Terasawa, Hiromi Kimura, Takeshi Fujiwara
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The evolution of X-ray detection technology has significantly enhanced sensitivity and spatial resolution in non-destructive imaging of internal structure. However, the problem of low luminescence and transparency of scintillator materials restricts imaging with lower radiation doses and thicker materials. Here, we propose a two-dimensional photonic scintillator for single crystal and demonstrate that the optical guiding effect emerging from the structure reduces luminescence leakage and increases the signal intensity by around a factor of 2 from 200 to 450 kV. This approach has the potential to enhance the output rate by an order of magnitude. The photonic structure features a fine array pitch and large-scale detection area with fast fabrication time. Our scheme paves the way for high sensitivity X-ray imaging.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-15T09:15:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.optics</span><span>physics.ins-det</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11121v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11121v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix
  Unit</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yinuo Wang, Tianqi Mao, Lin Gan, Wubing Wan, Zeyu Song, Jiayu Fu, Lanke He, Wenqiang Wang, Zekun Yin, Wei Xue, Guangwen Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Matrix-accelerated stencil computation is a hot research topic, yet its application to three-dimensional (3D) high-order stencils and HPC remains underexplored. With the emergence of matrix units on multicore CPUs, we analyze matrix-based acceleration strategies and tailor an optimal approach for 3D high-order stencils. We introduce algorithmic optimizations based on SIMD and matrix units to address strided memory accesses, alignment conflicts, and redundant accesses. We propose memory optimizations to boost on-package memory efficiency, and a novel multi-thread parallelism paradigm to overcome data-sharing challenges caused by the absence of shared data caches. MMStencil sustains consistently high hardware utilization across diverse stencil shapes and dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA effects and MPI limitations in hybrid parallelism. Combining all the innovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100 GPGPU by up to 2.1x. Moreover, the performance improvements translate directly to real-world HPC applications and enable RTM applications to yield 1.8x speedup versus a highly optimized industrial Nvidia A100 GPGPU version.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-15T08:00:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11067v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11067v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aaron Jarmusch, Nathan Graddon, Sunita Chandrasekaran
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid development in scientific research provides a need for more compute power, which is partly being solved by GPUs. This paper presents a microarchitectural analysis of the modern NVIDIA Blackwell architecture by studying GPU performance   features with thought through microbenchmarks. We unveil key subsystems, including the memory hierarchy, SM execution   pipelines, and the SM sub-core units, including the 5th generation tensor cores supporting FP4 and FP6 precisions.   To understand the different key features of the NVIDIA GPU, we study latency, throughput, cache behavior, and scheduling   details, revealing subtle tuning metrics in the design of Blackwell. To develop a comprehensive analysis, we compare the   Blackwell architecture with the previous Hopper architecture by using the GeForce RTX 5080 and H100 PCIe, respectively. We   evaluate and compare results, presenting both generational improvements and performance regressions. Additionally, we   investigate the role of power efficiency and energy consumption under varied workloads. Our findings provide actionable insights   for application developers, compiler writers, and performance engineers to optimize workloads on Blackwell-based platforms,   and contribute new data to the growing research on GPU architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T20:38:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10789v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10789v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Enabling the Write-Back Page Cache with Strong Consistency in
  Distributed Userspace File Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoyu Li, Jingkai Fu, Qing Li, Windsor Hsu, Asaf Cidon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cloud platforms host thousands of tenants that demand POSIX semantics, high throughput, and rapid evolution from their storage layer. Kernel-native distributed file systems supply raw speed, but their privileged code base couples every release to the kernel, widens the blast radius of crashes, and slows innovation. FUSE-based distributed file systems flip those trade-offs: they run in user space for fast deployment and strong fault isolation, yet the FUSE interface disables the kernel's write-back page cache whenever strong consistency is required. Practitioners must therefore choose between (i) weak consistency with fast write-back caching or (ii) strong consistency with slow write-through I/O, an limitation that has kept FUSE distributed file systems out of write-intensive cloud workloads.   To this end, We present DistFUSE, the first distributed FUSE file system that delivers write-back kernel caching and strong consistency. DistFUSE achieves this by offloading userspace consistency control to the kernel driver, allowing coordinated access to the kernel's page cache across nodes. This design eliminates blind local cache updates and ensures cluster-wide strong consistency without compromising performance. In our evaluation, DistFUSE achieves up to 68.0% higher throughput and 40.4% lower latency than the existing write-through design of FUSE-based distributed file system.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T19:51:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.18191v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.18191v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 FAFO: Over 1 million TPS on a single node running EVM while still
  Merkleizing every block</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ryan Zarick, Isaac Zhang, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current blockchain execution throughput is limited by data contention, reducing execution layer parallelism. Fast Ahead-of-Formation Optimization (FAFO) is the first blockchain transaction scheduler to address this problem by reordering transactions before block formation for maximum concurrency. FAFO uses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts and schedule parallel transaction execution at high throughput and low overhead.   We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1 million native ETH transfers per second and over half a million ERC20 transfers per second on a single node (Table 1), with 91% lower cost compared to state-of-the-art sharded execution. Unlike many other existing high throughput blockchain execution clients, FAFO uses QMDB to Merkleize world state after every block, enabling light clients and stateless validation for ZK-based vApps. FAFO scales with minimal synchronization overhead, scaling linearly with additional CPU resources until it fully exploits the maximum parallelism of the underlying transaction flow. FAFO proves that the high throughput necessary to support future decentralized applications can be achieved with a streamlined execution layer and innovations in blockchain transaction scheduler design. FAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T19:31:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10757v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10757v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM
  Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhan Liu, Yuyang Huang, Jiayi Yao, Shaoting Feng, Zhuohan Gu, Kuntai Du, Hanchen Li, Yihua Cheng, Junchen Jiang, Shan Lu, Madan Musuvathi, Esha Choukse
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Compound AI systems, such as agentic systems, are an emerging trend in large-scale enterprise settings, with multiple LLMs specialized for different users, tasks, and/or roles working together. In these scenarios, different models often process inputs that share the same context prefix. Although much work was done in the past to enable the reuse of prefix KV caches across inputs for a single model, how to enable one model to reuse the prefix KV caches of a different model remains an open question.   We introduce DroidSpeak, the first distributed LLM inference system that enables KV cache reuse across distributed nodes running inference of different LLMs, so long as the LLMs have the same architecture. We present the first study that aims at understanding the impact of sharing KV caches across different LLMs, and if/when such sharing affects quality. Inspired by the findings, we present DroidSpeak, which selectively recomputes a few layers of the KV cache produced by another LLM and reuses the remaining layers, with negligible quality loss. Moreover, carefully pipelining the layer-wise re-computation and the loading of reused KV cache further improves the inference performance. Experiments on diverse datasets and model pairs demonstrate that DroidSpeak achieves up to 4x throughput improvement and about 3.1x faster prefill (time to first token), with negligible loss of quality in F1 scores, Rouge-L or code similarity score, compared to the baseline which does not allow any sharing across models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T18:22:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.02820v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.02820v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive
  Token-Level Computation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T17:49:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10524v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10524v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following
  Models Need for Efficient Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ao Wang, Hui Chen, Jiaxin Li, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Zijia Lin, Jungong Han, Guiguang Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs. However, these models incur significant computational and memory overhead during inference, which greatly hinders the efficient deployment in practical scenarios. The extensive key-value (KV) cache, necessitated by the lengthy input and output sequences, notably contributes to the high inference cost. Based on this, recent works have investigated ways to reduce the KV cache size for higher efficiency. Although effective, they generally overlook the distinct importance distributions of KV vectors across layers and maintain the same cache size for each layer during the next token prediction. This results in the significant contextual information loss for certain layers, leading to notable performance decline. To address this, we present PrefixKV. It reframes the challenge of determining KV cache sizes for all layers into the task of searching for the optimal global prefix configuration. With an adaptive layer-wise KV retention recipe based on binary search, the maximum contextual information can thus be preserved in each layer, facilitating the generation. Extensive experiments demonstrate that our method achieves the state-of-the-art performance compared with others. It exhibits superior inference efficiency and generation quality trade-offs, showing promising potential for practical applications. Code is available at https://github.com/THU-MIG/PrefixKV.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T16:14:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.03409v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.03409v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu, Haibo Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\times$ throughput for small file read/write and up to 12.81$\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T15:09:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10367v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10367v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Pruning the Tree: Rethinking RPKI Architecture From The Ground Up</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haya Schulmann, Niklas Vogel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Resource Public Key Infrastructure (RPKI) is a critical security mechanism for BGP, but the complexity of its architecture is a growing concern as its adoption scales. Current RPKI design heavily reuses legacy PKI components, such as X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols, which introduce excessive cryptographic validation, redundant metadata, and inefficiencies in both storage and processing. We show that these design choices, although based on established standards, create significant performance bottlenecks, increase the vulnerability surface, and hinder scalability for wide-scale Internet deployment.   In this paper, we perform the first systematic analysis of the root causes of complexity in RPKI's design and experimentally quantify their real-world impact. We show that over 70\% of validation time in RPKI relying parties is spent on certificate parsing and signature verification, much of it unnecessary. Building on this insight, we introduce the improved RPKI (iRPKI), a backwards-compatible redesign that preserves all security guarantees while substantially reducing protocol overhead. iRPKI eliminates EE-certificates and ROA signatures, merges revocation and integrity objects, replaces verbose encodings with Protobuf, and restructures repository metadata for more efficient access. We experimentally demonstrate that our implementation of iRPKI in the Routinator validator achieves a 20x speed-up of processing time, 18x improvement of bandwidth requirements and 8x reduction in cache memory footprint, while also eliminating classes of vulnerabilities that have led to at least 10 vulnerabilities in RPKI software. iRPKI significantly increases the feasibility of deploying RPKI at scale in the Internet, and especially in constrained environments. Our design may be deployed incrementally without impacting existing operations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T09:45:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01465v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01465v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal
  Parallelism</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zedong Liu, Shenggan Cheng, Guangming Tan, Yang You, Dingwen Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we propose Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T08:53:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10069v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10069v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Not all tokens are created equal: Perplexity Attention Weighted Networks
  for AI generated text detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T07:05:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.03940v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.03940v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 The Hitchhiker's Guide to Programming and Optimizing Cache Coherent
  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zixuan Wang, Suyash Mahar, Luyi Li, Jangseon Park, Jinpyo Kim, Theodore Michailidis, Yue Pan, Mingyao Shen, Tajana Rosing, Dean Tullsen, Steven Swanson, Jishen Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a thorough analysis of the use of modern heterogeneous systems interconnected by various cachecoherent links, including CXL, NVLink-C2C, and Infinity Fabric. We studied a wide range of server systems that combined CPUs from different vendors and various types of coherent memory devices, including CXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a HBM. For this study, we developed a heterogeneous memory benchmark suite, Heimdall, to profile the performance of such heterogeneous systems and present a detailed performance comparison across systems. By leveraging H E I M DA L L , we unveiled the detailed architecture design in these systems, drew observations on optimizing performance for workloads, and pointed out directions for future development of cache coherent heterogeneous systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T07:03:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span><span>cs.AR</span><span>cs.DC</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.02814v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.02814v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 InstCache: A Predictive Cache for LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Longwei Zou, Yan Liu, Jiamu Kang, Tingfeng Liu, Jiangang Kong, Yangdong Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The revolutionary capabilities of Large Language Models (LLMs) are attracting rapidly growing popularity and leading to soaring user requests to inference serving systems. Caching techniques, which leverage data reuse to reduce computation, offer opportunities to optimize the performance of LLM inference engines. On the one hand, the low-level key-value (KV) cache working at the token level is widely adopted, albeit it incurs significant overhead as request volume grows. On the other hand, instruction-level caching, which stores full instruction-response pairs, is expected to play an increasingly crucial role. However, the high variability in the content and length of instructions make it rare for identical instructions to recur within a short time window, presenting challenges for effective caching instruction-response pairs. To address this challenge, we propose InstCache, a predictive caching mechanism for LLM serving systems. Leveraging the capability of LLMs, we can effectively reorder the representation space of instruction texts and develop a sufficient level of spatial locality. Such spatial locality enables us to predict potential instructions located in a compact region in the space, resulting in an effective caching system at runtime. Experimental results demonstrate that InstCache achieves a 2.3x higher hit rate compared to the upper bound of traditional caching mechanisms on WildChat dataset and reduces the time per output token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-14T02:22:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.13820v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.13820v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Advancing Reliable Test-Time Adaptation of Vision-Language Models under
  Visual Variations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiwen Liang, Hui Chen, Yizhe Xiong, Zihan Zhou, Mengyao Lyu, Zijia Lin, Shuaicheng Niu, Sicheng Zhao, Jungong Han, Guiguang Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but struggle with distribution shifts in downstream tasks when labeled data is unavailable, which has motivated the development of Test-Time Adaptation (TTA) to improve VLMs' performance during inference without annotations. Among various TTA approaches, cache-based methods show promise by preserving historical knowledge from low-entropy samples in a dynamic cache and fostering efficient adaptation. However, these methods face two critical reliability challenges: (1) entropy often becomes unreliable under distribution shifts, causing error accumulation in the cache and degradation in adaptation performance; (2) the final predictions may be unreliable due to inflexible decision boundaries that fail to accommodate large downstream shifts. To address these challenges, we propose a Reliable Test-time Adaptation (ReTA) method that integrates two complementary strategies to enhance reliability from two perspectives. First, to mitigate the unreliability of entropy as a sample selection criterion for cache construction, we introduce Consistency-aware Entropy Reweighting (CER), which incorporates consistency constraints to weight entropy during cache updating. While conventional approaches rely solely on low entropy for cache prioritization and risk introducing noise, our method leverages predictive consistency to maintain a high-quality cache and facilitate more robust adaptation. Second, we present Diversity-driven Distribution Calibration (DDC), which models class-wise text embeddings as multivariate Gaussian distributions, enabling adaptive decision boundaries for more accurate predictions across visually diverse content. Extensive experiments demonstrate that ReTA consistently outperforms state-of-the-art methods, particularly under challenging real-world distribution shifts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-13T05:37:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.09500v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.09500v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Auditing Prompt Caching in Language Model APIs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenchen Gu, Xiang Lisa Li, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-13T04:42:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.07776v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.07776v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Temporal Feature Matters: A Framework for Diffusion Model Quantization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yushi Huang, Ruihao Gong, Xianglong Liu, Jing Liu, Yuhang Li, Jiwen Lu, Dacheng Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Diffusion models, widely used for image generation, face significant challenges related to their broad applicability due to prolonged inference times and high memory demands. Efficient Post-Training Quantization (PTQ) is crucial to address these issues. However, unlike traditional models, diffusion models critically rely on the time-step for the multi-round denoising. Typically, each time-step is encoded into a hypersensitive temporal feature by several modules. Despite this, existing PTQ methods do not optimize these modules individually. Instead, they employ unsuitable reconstruction objectives and complex calibration methods, leading to significant disturbances in the temporal feature and denoising trajectory, as well as reduced compression efficiency. To address these challenges, we introduce a novel quantization framework that includes three strategies: 1) TIB-based Maintenance: Based on our innovative Temporal Information Block (TIB) definition, Temporal Information-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are developed to efficiently align original temporal features. 2) Cache-based Maintenance: Instead of indirect and complex optimization for the related modules, pre-computing and caching quantized counterparts of temporal features are developed to minimize errors. 3) Disturbance-aware Selection: Employ temporal feature errors to guide a fine-grained selection between the two maintenance strategies for further disturbance reduction. This framework preserves most of the temporal information and ensures high-quality end-to-end generation. Extensive testing on various datasets, diffusion models and hardware confirms our superior performance and acceleration.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T22:14:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19547v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19547v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Lizard: An Efficient Linearization Framework for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chien Van Nguyen, Ruiyi Zhang, Hanieh Deilamsalehy, Puneet Mathur, Viet Dac Lai, Haoliang Wang, Jayakumar Subramanian, Ryan A. Rossi, Trung Bui, Nikos Vlassis, Franck Dernoncourt, Thien Huu Nguyen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T21:19:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.09025v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.09025v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 HotSwap: Enabling Live Dependency Sharing in Serverless Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Li, Devesh Tiwari, Gene Cooperman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work presents HotSwap, a novel provider-side cold-start optimization for serverless computing. This optimization reduces cold-start time when booting and loading dependencies at runtime inside a function container. Previous research has extensively focused on reducing cold-start latency for specific functions. However, little attention has been given to skewed production workloads. In such cases, cross-function optimization becomes essential. Without cross-function optimization, a cloud provider is left with two equally poor options: (i) Either the cloud provider gives up optimization for each function in the long tail (which is slow); or (ii) the cloud provider applies function-specific optimizations (e.g., cache function images) to every function in the long tail (which violates the vendor's cache constraints). HotSwap demonstrates cross-function optimization using a novel pre-warming strategy. In this strategy, a pre-initialized live dependency image is migrated to the new function instance. At the same time, HotSwap respects the provider's cache constraints, because a single pre-warmed dependency image in the cache can be shared among all serverless functions that require that image. HotSwap has been tested on seven representative functions from FunctionBench. In those tests, HotSwap accelerates dependency loading for those serverless functions with large dependency requirements by a factor ranging from 2.2 to 3.2. Simulation experiments using Azure traces indicate that HotSwap can save 88\% of space, compared with a previous function-specific method, PreBaking, when sharing a dependency image among ten different functions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T19:57:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09202v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09202v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 KV Cache Steering for Inducing Reasoning in Small Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb Mirza, Cees G. M. Snoek, Yuki M. Asano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T17:59:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08799v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08799v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Knowledge Graph-Based approach for Sustainable 6G End-to-End System
  Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Akshay Jain, Sylvaine Kerboeuf, Sokratis Barmpounakis, Cristóbal Vinagre Z., Stefan Wendt, Dinh Thai Bui, Pol Alemany, Riccardo Nicolicchia, José María Jorquera Valero, Dani Korpi, Mohammad Hossein Moghaddam, Mikko A. Uusitalo, Patrik Rugeland, Abdelkader Outtagarts, Karthik Upadhya, Panagiotis Demestichas, Raul Muñoz, Manuel Gil Pérez, Daniel Adanza, Ricard Vilalta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Previous generations of cellular communication, such as 5G, have been designed with the objective of improving key performance indicators (KPIs) such as throughput, latency, etc. However, to meet the evolving KPI demands as well as the ambitious sustainability targets for the ICT industry, 6G will need to be designed differently. Concretely, 6G will need to consider both the performance and sustainability targets for the various use cases it will serve. Moreover, like previous generations, 6G will have various candidate technological enablers, making the design space of the system even more complex. Furthermore, given the subjective nature of the sustainability indicators, in particular social sustainability, there is a significant gap in literature on how technical enablers and 6G System design can be linked to them. Hence, in this article a novel method for 6G end-to-end (E2E) system design based on Knowledge graphs (KG) has been introduced. It considers as its input: the use case KPIs, use case sustainability requirements expressed as Key Values (KV) and KV Indicators (KVIs), the ability of the technological enablers to satisfy these KPIs and KVIs, the 6G system design principles defined in Hexa-X-II project, the maturity of a technological enabler and the dependencies between the various enablers. As part of the KG method, a novel approach for determining the key values a technological enabler addresses, has also been introduced. The effectiveness of the KG method was demonstrated by its application in designing the 6G E2E system for the cooperating mobile robot use case defined in the Hexa-X-II project, where 82 enablers were selected. Lastly, results from proof-of-concept demonstrations for a subset of the selected enablers have also been provided, which reinforce the efficacy of the KG method for designing a sustainable 6G system.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T16:17:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>00</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08717v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08717v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Reciprocating Locks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dave Dice, Alex Kogan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T14:27:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>D.4.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02380v9' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02380v9' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language
  Models via Gaussian Discriminant Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuang Cui, Jinglin Xu, Yi Li, Xiongxin Tang, Jiangmeng Li, Jiahuan Zhou, Fanjiang Xu, Fuchun Sun, Hui Xiong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition but degrade significantly under \textit{temporally evolving distribution shifts} common in real-world scenarios (e.g., gradual illumination or seasonal changes). Existing continual test-time adaptation (CTTA) methods are typically built around sudden and severe distribution shifts and neglect temporal continuity, leading to three core defects: limited memory cache restricts long-range distribution modeling, causing catastrophic forgetting; entropy-based confidence becomes unreliable under temporal drift, worsening error accumulation; and static visual representations misalign with evolving inputs. We formalize this practical problem as \textit{Continual-Temporal Test-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over time. To address it, we propose \textit{BayesTTA}, a Bayesian adaptation framework that enforces temporally consistent predictions and dynamically aligns visual representations. Specifically, BayesTTA incrementally estimates class-conditional Gaussian mixture distributions without storing raw data, adaptively selects covariance structures through statistical hypothesis testing, and performs calibrated inference using Gaussian discriminant analysis (GDA). These calibrated predictions supervise self-paced adaptation of normalization layers, ensuring efficient and stable representation alignment. We establish a comprehensive CT-TTA benchmark across four temporally evolving datasets and further evaluate generalization on ten standard TTA datasets. Extensive experiments show that BayesTTA consistently outperforms state-of-the-art methods, achieving significant gains while maintaining efficiency. Code is available at \href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T14:02:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08607v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08607v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 InferLog: Accelerating LLM Inference for Online Log Parsing via
  ICL-oriented Prefix Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilun Wang, Pengfei Chen, Haiyu Huang, Zilong He, Gou Tan, Chuanfu Zhang, Jingkai He, Zibin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern software systems generate massive volumes of runtime logs, necessitating efficient and accurate log parsing to enable critical downstream tasks such as anomaly detection and root cause analysis. Recently, large language models (LLMs) have achieved advanced accuracy on log parsing, but their deployment in production environments faces two major limitations: (1) the privacy risks associated with commercial LLMs, driving the adoption of local deployment, and (2) the stringent latency and throughput requirements imposed by high-volume log streams, which existing LLM-based parsers fail to meet. Although recent efforts have reduced the number of LLM queries, they overlook the high latency of the LLM invocations, where concurrent log parsing requests can cause serve performance degradation of LLM inference system.   In this study, we present InferLog, the first LLM inference optimization method for online log parsing. Our key insight is that the inference efficiency emerges as the vital bottleneck in LLM-based online log parsing, rather than parsing accuracy. InferLog accelerates inference by designing (1) A Prefix-aware ICL Refinement policy to refine the examples and permutation of in-context learning to improve the prefix caching efficiency. (2) A rapid and task-specific configuration tuning pipeline based on meta-learning to find the optimal LLM scheduling-related configuration for dynamic log parsing workloads. The experimental results based on Loghub dataset and vLLM demonstrate that InferLog significantly outperforms existing inference optimization methods and markedly accelerates the state-of-the-art LLM-based log parser without compromising parsing accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T12:21:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08523v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08523v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment
  of AI-powered Tutors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ekaterina Kochmar, Kaushal Kumar Maurya, Kseniia Petukhova, KV Aditya Srivatsa, Anaïs Tack, Justin Vasselli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This shared task has aimed to assess pedagogical abilities of AI tutors powered by large language models (LLMs), focusing on evaluating the quality of tutor responses aimed at student's mistake remediation within educational dialogues. The task consisted of five tracks designed to automatically evaluate the AI tutor's performance across key dimensions of mistake identification, precise location of the mistake, providing guidance, and feedback actionability, grounded in learning science principles that define good and effective tutor responses, as well as the track focusing on detection of the tutor identity. The task attracted over 50 international teams across all tracks. The submitted models were evaluated against gold-standard human annotations, and the results, while promising, show that there is still significant room for improvement in this domain: the best results for the four pedagogical ability assessment tracks range between macro F1 scores of 58.34 (for providing guidance) and 71.81 (for mistake identification) on three-class problems, with the best F1 score in the tutor identification track reaching 96.98 on a 9-class task. In this paper, we overview the main findings of the shared task, discuss the approaches taken by the teams, and analyze their performance. All resources associated with this task are made publicly available to support future research in this critical domain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T10:57:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10579v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10579v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 xpSHACL: Explainable SHACL Validation using Retrieval-Augmented
  Generation and Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gustavo Correa Publio, José Emilio Labra Gayo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Shapes Constraint Language (SHACL) is a powerful language for validating RDF data. Given the recent industry attention to Knowledge Graphs (KGs), more users need to validate linked data properly. However, traditional SHACL validation engines often provide terse reports in English that are difficult for non-technical users to interpret and act upon. This paper presents xpSHACL, an explainable SHACL validation system that addresses this issue by combining rule-based justification trees with retrieval-augmented generation (RAG) and large language models (LLMs) to produce detailed, multilanguage, human-readable explanations for constraint violations. A key feature of xpSHACL is its usage of a Violation KG to cache and reuse explanations, improving efficiency and consistency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T09:18:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08432v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08432v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated
  Diffusion Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T09:07:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08422v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08422v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Observation of the electric Breit-Rabi Effect</h2>
                <div class="authors">
                    <strong>Authors:</strong> S. -Z. Wang, S. -B. Wang, Z. -J. Tao, T. Xia, Z. -T. Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The response of an atom to external electric and magnetic fields can reveal fundamental atomic properties. It has long been verified that, in a static magnetic field, those atomic energy levels with hyperfine interactions shift according to the Breit-Rabi formula, which introduces nonlinear dependence on the magnetic field. On the other hand, the corresponding Breit-Rabi dependence on a static electric field has not been observed before due to a combination of experimental challenges. Here we precisely measure the Stark shift of the $6s^2\ ^1S_0\ \leftrightarrow\ 6s6p\ ^1P_1$ transition of $^{171}$Yb ($I$ = 1/2) with cold atoms held by an optical dipole trap in a static electric field up to 120 kV/cm. We observe the electric Breit-Rabi effect displaying high-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the influence of the strong electric field on hyperfine interactions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T02:57:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.atom-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1073/pnas.2423902122' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.08278v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08278v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and
  Reading Comprehension?</h2>
                <div class="authors">
                    <strong>Authors:</strong> KV Aditya Srivatsa, Kaushal Kumar Maurya, Ekaterina Kochmar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly used as proxy students in the development of Intelligent Tutoring Systems (ITSs) and in piloting test questions. However, to what extent these proxy students accurately emulate the behavior and characteristics of real students remains an open question. To investigate this, we collected a dataset of 489 items from the National Assessment of Educational Progress (NAEP), covering mathematics and reading comprehension in grades 4, 8, and 12. We then apply an Item Response Theory (IRT) model to position 11 diverse and state-of-the-art LLMs on the same ability scale as real student populations. Our findings reveal that, without guidance, strong general-purpose models consistently outperform the average student at every grade, while weaker or domain-mismatched models may align incidentally. Using grade-enforcement prompts changes models' performance, but whether they align with the average grade-level student remains highly model- and prompt-specific: no evaluated model-prompt pair fits the bill across subjects and grades, underscoring the need for new training and evaluation strategies. We conclude by providing guidelines for the selection of viable proxies based on our findings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-11T00:36:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08232v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08232v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Compactor: Calibrated Query-Agnostic KV Cache Compression with
  Approximate Leverage Scores</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vivek Chari, Benjamin Van Durme
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. Unfortunately the ability to use long contexts in generation is complicated by the large memory requirement of the KV cache, which scales linearly with the context length. This memory footprint is often the dominant resource bottleneck in real-world deployments, limiting throughput and increasing serving cost. One way to address this is by compressing the KV cache, which can be done either with knowledge of the question being asked (query-aware) or without knowledge of the query (query-agnostic). We present Compactor, a parameter-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 1/2 the tokens in both synthetic and real-world context tasks, with minimal computational overhead. We further introduce a procedure for context-calibrated compression, which allows one to infer the maximum compression ratio a given context can support. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 63%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-10T20:03:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08143v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08143v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Multi-Granular Spatio-Temporal Token Merging for Training-Free
  Acceleration of Video LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose a training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves a 2$\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and a 3$\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is query-agnostic, allowing KV cache reuse across different questions for the same video. The project page is available at https://www.jshyun.me/projects/sttm.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-10T17:59:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.07990v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.07990v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Scaling RL to Long Videos</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-10T17:47:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.07966v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.07966v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiakun Fan, Yanglin Zhang, Xiangchen Li, Dimitrios S. Nikolopoulos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deploying large language models (LLMs) for online inference is often constrained by limited GPU memory, particularly due to the growing KV cache during auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a promising solution by offloading KV cache management and parts of attention computation to the CPU. However, a key bottleneck remains: existing schedulers fail to effectively overlap CPU-offloaded tasks with GPU execution during the latency-critical, bandwidth-bound decode phase. This particularly penalizes real-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning) which are currently underserved by existing systems, especially under memory pressure typical of edge or low-cost deployments.   We present APEX, a novel, profiling-informed scheduling strategy that maximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems relying on static rules or purely heuristic approaches, APEX dynamically dispatches compute across heterogeneous resources by predicting execution times of CPU and GPU subtasks to maximize overlap while avoiding scheduling overheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA T4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only schedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89% on A10 GPUs, while preserving latency. Against the best existing hybrid schedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in long-output settings. APEX significantly advances hybrid LLM inference efficiency on such memory-constrained hardware and provides a blueprint for scheduling in heterogeneous AI systems, filling a critical gap for efficient real-time LLM applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-10T17:10:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.03296v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.03296v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent
  Workflows</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zaifeng Pan, Ajjkumar Patel, Zhengding Hu, Yipeng Shen, Yue Guan, Wan-Lu Li, Lianhui Qin, Yida Wang, Yufei Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) based agentic workflows have become a popular paradigm for coordinating multiple specialized agents to solve complex tasks. To improve serving efficiency, existing LLM systems employ prefix caching to reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby avoiding redundant computation across repeated invocations. However, current systems typically evict KV caches using a Least Recently Used (LRU) policy, which fails to anticipate future agent usage and often discards KV caches shortly before their reuse. This leads to frequent cache misses and substantial recomputation or swapping overhead. We present KVFlow, a workflow-aware KV cache management framework tailored for agentic workloads. KVFlow abstracts the agent execution schedule as an Agent Step Graph and assigns each agent a steps-to-execution value that estimates its temporal proximity to future activation. These values guide a fine-grained eviction policy at the KV node level, allowing KVFlow to preserve entries likely to be reused and efficiently manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a fully overlapped KV prefetching mechanism, which proactively loads required tensors from CPU to GPU in background threads for agents scheduled in the next step, thereby avoiding cache miss stalls during generation. Compared to SGLang with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for single workflows with large prompts, and up to 2.19$\times$ speedup for scenarios with many concurrent workflows.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-10T03:39:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.07400v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.07400v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Krul: Efficient State Restoration for Multi-turn Conversations with
  Dynamic Cross-layer KV Sharing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junyi Wen, Junyuan Liang, Zicong Hong, Wuhui Chen, Zibin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge, primarily due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. To address this, existing approaches compress KV caches across adjacent layers with highly similar attention patterns. However, these methods often apply a fixed compression scheme across all conversations, selecting the same layer pairs for compression without considering conversation-specific attention dynamics. This static strategy overlooks variability in attention pattern similarity across different conversations, which can lead to noticeable accuracy degradation.   We present Krul, a multi-turn LLM inference system that enables accurate and efficient KV cache restoration. Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: 1) a preemptive compression strategy selector to preserve critical context for future conversation turns and selects a customized strategy for the conversation; 2) a token-wise heterogeneous attention similarity estimator to mitigate the attention similarity computation and storage overhead during model generation; 3) a bubble-free restoration scheduler to reduce potential bubbles brought by the imbalance of recomputing and loading stream due to compressed KV caches. Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-10T01:51:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08045v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08045v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Stabilization of the first-order phase transition character and
  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$
  ceramics</h2>
                <div class="authors">
                    <strong>Authors:</strong> M. Karakaya, I. Gurbuz, L. Fulanovic, U. Adem
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric materials have been widely investigated. One approach to achieving a large electrocaloric response is to exploit the substantial polarization change associated with the first-order phase transition at the Curie temperature. Following this strategy, we investigated the electrocaloric response of (1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05, 0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is established that increasing the NBT content enhances the tetragonality of BaTiO$_3$. We show that this increase in tetragonality helps maintain the first-order nature of the phase transition and enables a correspondingly large electrocaloric response, despite the simultaneous enhancement of relaxor ferroelectric character with NBT substitution. A significantly large effective electrocaloric temperature change ($\Delta T_{\mathrm{eff}}$) of ~1.65 K was obtained for the x = 0.20 composition under an applied field of 40 kV/cm using direct electrocaloric measurements, in reasonable agreement with the indirect results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-09T21:18:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1039/D4TC01735H' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.07290v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.07290v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 PromptTea: Let Prompts Tell TeaCache the Optimal Threshold</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zishen Huang, Chunyu Yang, Mengyuan Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite recent progress in video generation, inference speed remains a major bottleneck. A common acceleration strategy involves reusing model outputs via caching mechanisms at fixed intervals. However, we find that such fixed-frequency reuse significantly degrades quality in complex scenes, while manually tuning reuse thresholds is inefficient and lacks robustness. To address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that automatically adjusts reuse thresholds based on scene complexity estimated directly from the input prompt. By incorporating prompt-derived semantic cues, PCA enables more adaptive and informed reuse decisions than conventional caching methods. We also revisit the assumptions behind TeaCache and identify a key limitation: it suffers from poor input-output relationship modeling due to an oversimplified prior. To overcome this, we decouple the noisy input, enhance the contribution of meaningful textual information, and improve the model's predictive accuracy through multivariate polynomial feature expansion. To further reduce computational cost, we replace the static CFGCache with DynCFGCache, a dynamic mechanism that selectively reuses classifier-free guidance (CFG) outputs based on estimated output variations. This allows for more flexible reuse without compromising output quality. Extensive experiments demonstrate that our approach achieves significant acceleration-for example, 2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across a range of scenes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-09T10:53:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.06739v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.06739v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Saffron-1: Safety Inference Scaling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruizhong Qiu, Gaotang Li, Tianxin Wei, Jingrui He, Hanghang Tong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-09T07:47:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.06444v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.06444v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 SlimCaching: Edge Caching of Mixture-of-Experts for Distributed
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qian Chen, Xianhao Chen, Kaibin Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed within an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K\geq1$, expert co-activation within the same MoE layer introduces non-submodularity, causing greedy methods to be ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-09T05:43:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.06567v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.06567v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 FiRST: Finetuning Router-Selective Transformers for Input-Adaptive
  Latency Reduction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across different domains such as vision and language processing. However, due to sequential processing through a stack of transformer layers, autoregressive decoding faces significant computation/latency challenges, particularly in resource-constrained environments like mobile and edge devices. Existing approaches in literature that aim to improve latency via skipping layers have two distinct flavors - 1) Early exit, and 2) Input-agnostic heuristics where tokens exit at pre-determined layers irrespective of input sequence. Both the above strategies have limitations - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework and the latter does not capture the variation in layer importance across tasks or more generally, across input sequences. To address both limitations, we propose FiRST, an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence - the prompt (during the prefill stage) decides which layers will be skipped during decoding. FiRST preserves compatibility with KV caching enabling faster inference while being quality-aware. FiRST is model-agnostic and can be easily enabled on any pre-trained LLM. Our approach reveals that input adaptivity is critical - indeed, different task-specific middle layers play a crucial role in evolving hidden representations depending on tasks. Extensive experiments show that FiRST significantly reduces latency while outperforming other layer selection strategies in quality metics. It retains competitive performance to base model (without layer skipping) and in some cases, even improves upon it. FiRST is thus a promising and efficient solution for LLM deployment in low-resource environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-09T04:43:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.12513v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.12513v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and
  Deep Layers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-09T03:33:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.06517v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.06517v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tong Wu, Junzhe Shen, Zixia Jia, Yuxuan Wang, Zilong Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at https://github.com/bigai-nlco/TokenSwift.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-09T02:35:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.18890v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.18890v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Optomechanical resource for fault-tolerant quantum computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Margaret Pavlovich, Peter Rakich, Shruti Puri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fusion-based quantum computing with dual-rail qubits is a leading candidate for scalable quantum computing using linear optics. This paradigm requires single photons which are entangled into small resource states before being fed into a fusion network. The most common sources for single optical photons and for small entangled states are probabilistic and heralded. The realization of a single reliable deterministic source requires many redundant probabilistic sources and a complex optical network for rerouting and retiming probabilistic outputs. In this work, we show how optomechanics enables reliable production of resources for photonic quantum computing without the redundancy of the all-optical approach. This is achieved by using acoustic modes as caches of quantum resources, ranging from single-particle states to small entangled states, with on-demand read-out. The advantages of acoustic modes as optical quantum memories, compared to other technologies, include their intrinsically long lifetimes and that they are solid state, highly tailorable, and insensitive to electromagnetic noise. We show how the resource states can be prepared directly in the acoustic modes using optical controls. This is still probabilistic and heralded, as in the all-optical approach, but the acoustic modes act as a quantum memory which is integrated into the production of the states. The quantum states may be deterministically transferred from acoustic modes to optical modes, on demand, with another optical drive.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T21:23:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.00768v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.00768v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Multi-Queue SSD I/O Modeling & Its Implications for Data Structure
  Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Erin Ransom, Andrew Lim, Michael Mitzenmacher
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the performance profiles of storage devices and how best to utilize them has always been non-trivial due to factors such as seek times, caching, scheduling, concurrent access, flash wear-out, and garbage collection. However, analytical frameworks that provide simplified abstractions of storage performance can still be accurate enough to evaluate external memory algorithms and data structures at the design stage. For example, the Disk Access Machine (DAM) model assumes that a storage device transfers data in fixed-size blocks of size B and that all transfers have unit latency. This abstraction is already sufficient to explain some of the benefits of data structures such as B-trees and Log-Structured Merge trees (LSM trees); however, storage technology advances have significantly reduced current models' accuracy and utility.   This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new storage abstraction. This model builds upon previous models and aims to more accurately represent the performance characteristics of modern storage hardware. We identify key performance-critical aspects of modern multi-queue solid-state drives on which we base our model and demonstrate these characteristics on actual hardware. We then show how our model can be applied to LSM-tree-based storage engines to optimize them for modern storage hardware. We highlight that leveraging concurrent access is crucial for fully utilizing the high throughput of multi-queue SSDs, enabling designs that may appear counterintuitive under traditional paradigms We then validate these insights through experiments using Facebook's LSM-tree-based key-value store, RocksDB. We conclude that the MQSSD model offers a more accurate abstraction of modern hardware than previous models, allowing for greater insight and optimization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T19:20:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DS</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.06349v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.06349v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hang Guo, Yawei Li, Taolin Zhang, Jiangshan Wang, Tao Dai, Shu-Tao Xia, Luca Benini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual Autoregressive (VAR) modeling has gained popularity for its shift towards next-scale prediction. However, existing VAR paradigms process the entire token map at each scale step, leading to the complexity and runtime scaling dramatically with image resolution. To address this challenge, we propose FastVAR, a post-training acceleration method for efficient resolution scaling with VARs. Our key finding is that the majority of latency arises from the large-scale step where most tokens have already converged. Leveraging this observation, we develop the cached token pruning strategy that only forwards pivotal tokens for scale-specific modeling while using cached tokens from previous scale steps to restore the pruned slots. This significantly reduces the number of forwarded tokens and improves the efficiency at larger resolutions. Experiments show the proposed FastVAR can further speedup FlashAttention-accelerated VAR by 2.7$\times$ with negligible performance drop of <1%. We further extend FastVAR to zero-shot generation of higher resolution images. In particular, FastVAR can generate one 2K image with 15GB memory footprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at https://github.com/csguoh/FastVAR.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T12:34:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.23367v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.23367v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 An Ensemble Embedding Approach for Improving Semantic Caching
  Performance in LLM-based Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shervin Ghaffari, Zohre Bahranifard, Mohammad Akbari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Semantic caching enhances the efficiency of large language model (LLM) systems by identifying semantically similar queries, storing responses once, and serving them for subsequent equivalent requests. However, existing semantic caching frameworks rely on single embedding models for query representation, which limits their ability to capture the diverse semantic relationships present in real-world query distributions. This paper presents an ensemble embedding approach that combines multiple embedding models through a trained meta-encoder to improve semantic similarity detection in LLM caching systems. We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring cache hit ratios, cache miss ratios, token savings, and response times. Our ensemble approach achieves a 92\% cache hit ratio for semantically equivalent queries while maintaining an 85\% accuracy in correctly rejecting non-equivalent queries as cache misses. These results demonstrate that ensemble embedding methods significantly outperform single-model approaches in distinguishing between semantically similar and dissimilar queries, leading to more effective caching performance and reduced computational overhead in LLM-based systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T09:20:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>68T50</span><span>I.2.7; H.3.3; I.5.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.07061v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.07061v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Towards Stabilized and Efficient Diffusion Transformers through
  Long-Skip-Connections with Spectral Constraints</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Xiaoye Qu, Tianlong Chen, Yu Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiT) have emerged as a powerful architecture for image and video generation, offering superior quality and scalability. However, their practical application suffers from inherent dynamic feature instability, leading to error amplification during cached inference. Through systematic analysis, we identify the absence of long-range feature preservation mechanisms as the root cause of unstable feature propagation and perturbation sensitivity. To this end, we propose Skip-DiT, an image and video generative DiT variant enhanced with Long-Skip-Connections (LSCs) - the key efficiency component in U-Nets. Theoretical spectral norm and visualization analysis demonstrate how LSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic feature enable an efficient statical caching mechanism that reuses deep features across timesteps while updating shallow components. Extensive experiments across the image and video generation tasks demonstrate that Skip-DiT achieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2 times inference acceleration with negligible quality loss and high fidelity to the original output, outperforming existing DiT caching methods across various quantitative metrics. Our findings establish Long-Skip-Connections as critical architectural components for stable and efficient diffusion transformers. Codes are provided in the https://github.com/OpenSparseLLMs/Skip-DiT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T07:10:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.17616v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.17616v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Torpor: GPU-Enabled Serverless Computing for Low-Latency,
  Resource-Efficient Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minchen Yu, Ao Wang, Dong Chen, Haoxuan Yu, Xiaonan Luo, Zhuohao Li, Wei Wang, Ruichuan Chen, Dapeng Nie, Haoran Yang, Yu Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serverless computing offers a compelling cloud model for online inference services. However, existing serverless platforms lack efficient support for GPUs, hindering their ability to deliver high-performance inference. In this paper, we present Torpor, a serverless platform for GPU-efficient, low-latency inference. To enable efficient sharing of a node's GPUs among numerous inference functions, Torpor maintains models in main memory and dynamically swaps them onto GPUs upon request arrivals (i.e., late binding with model swapping). Torpor uses various techniques, including asynchronous API redirection, GPU runtime sharing, pipelined model execution, and efficient GPU memory management, to minimize latency overhead caused by model swapping. Additionally, we design an interference-aware request scheduling algorithm that utilizes high-speed GPU interconnects to meet latency service-level objectives (SLOs) for individual inference functions. We have implemented Torpor and evaluated its performance in a production environment. Utilizing late binding and model swapping, Torpor can concurrently serve hundreds of inference functions on a worker node with 4 GPUs, while achieving latency performance comparable to native execution, where each model is cached exclusively on a GPU. Pilot deployment in a leading commercial serverless cloud shows that Torpor reduces the GPU provisioning cost by 70% and 65% for users and the platform, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T02:15:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2306.03622v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2306.03622v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 RandAR: Decoder-only Autoregressive Visual Generation in Random Orders</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, Yu-Xiong Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generating images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enables random order by inserting a "position instruction token" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to its conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner. We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. Our project page is at https://rand-ar.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T00:51:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.01827v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.01827v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Helix Parallelism: Rethinking Sharding Strategies for Interactive
  Multi-Million-Token LLM Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nidhi Bhatia, Ankit More, Ritika Borkar, Tiyasa Mitra, Ramon Matas, Ritchie Zhao, Maximilian Golub, Dheevatsa Mudigere, Brian Pharris, Bita Darvish Rouhani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As LLMs scale to multi-million-token KV histories, real-time autoregressive decoding under tight Token-to-Token Latency (TTL) constraints faces growing pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN) weights and reading long KV caches. While Tensor Parallelism (TP) helps mitigate the cost of FFN weight reads, it does not scale well for attention. When TP width exceeds the number of KV heads, it leads to inefficient KV duplication, limits parallelism, and constrains batch size. Simultaneously, DRAM reads for long KV histories scale linearly with batch size, further capping efficiency.   We introduce Helix Parallelism, a hybrid execution strategy that applies KV parallelism during attention to shard KV caches across GPUs, then reuses the same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN computation. To preserve exact attention behavior, Helix includes a lightweight communication step. To minimize the exposed communication cost, we introduce Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through batchwise overlap, preserving low TTL while improving GPU efficiency. Compared to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at fixed batch sizes and supports up to 32x larger batches under the same latency budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on Blackwell and making real-time inference with ultra-long-sequence practical.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T19:47:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.07120v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.07120v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context
  Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{https://streamvln.github.io/}{https://streamvln.github.io/}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:49:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05240v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05240v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 The Case for Instance-Optimized LLMs in OLAP Databases</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bardia Mohammadi, Laurent Bindschaedler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) can enhance analytics systems with powerful data summarization, cleaning, and semantic transformation capabilities. However, deploying LLMs at scale -- processing millions to billions of rows -- remains prohibitively expensive in computation and memory. We present IOLM-DB, a novel system that makes LLM-enhanced database queries practical through query-specific model optimization. Instead of using general-purpose LLMs, IOLM-DB generates lightweight, specialized models tailored to each query's specific needs using representative data samples. IOLM-DB reduces model footprints by up to 76% and increases throughput by up to 3.31$\times$ while maintaining accuracy through aggressive compression techniques, including quantization, sparsification, and structural pruning. We further show how our approach enables higher parallelism on existing hardware and seamlessly supports caching and batching strategies to reduce overheads. Our prototype demonstrates that leveraging LLM queries inside analytics systems is feasible at scale, opening new possibilities for future OLAP applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:10:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04967v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04967v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated
  in a coupled reactive transport HPC simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Max Lübke, Marco De Lucia, Stefan Petri, Bettina Schnor
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Surrogate models can play a pivotal role in enhancing performance in contemporary High-Performance Computing applications. Cache-based surrogates use already calculated simulation results to interpolate or extrapolate further simulation output values. But this approach only pays off if the access time to retrieve the needed values is much faster than the actual simulation. While the most existing key-value stores use a Client-Server architecture with dedicated storage nodes, this is not the most suitable architecture for HPC applications. Instead, we propose a distributed architecture where the parallel processes offer a part of their available memory to build a shared distributed hash table based on MPI. This paper presents three DHT approaches with the special requirements of HPC applications in mind. The presented lock-free design outperforms both DHT versions which use explicit synchronization by coarse-grained resp. fine-grained locking. The lock-free DHT shows very good scaling regarding read and write performance. The runtime of a coupled reactive transport simulation was improved between 14% and 42% using the lock-free DHT as a surrogate model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:25:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/978-3-031-97635-3_28' target='_blank'>doi</a><a href='http://arxiv.org/abs/2504.14374v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14374v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Performance Evaluation of General Purpose Large Language Models for
  Basic Linear Algebra Subprograms Code Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daichi Mukunoki, Shun-ichiro Hayashi, Tetsuya Hoshino, Takahiro Katagiri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative AI technology based on Large Language Models (LLM) has been developed and applied to assist or automatically generate program codes. In this paper, we evaluate the capability of existing general LLMs for Basic Linear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs provided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model, and o4-mini, one of the o-series of Reasoning models. Both have been released in April 2025. For the routines from level-1 to 3 BLAS, we tried to generate (1) C code without optimization from routine name only, (2) C code with basic performance optimizations (thread parallelization, SIMD vectorization, and cache blocking) from routine name only, and (3) C code with basic performance optimizations based on Fortran reference code. As a result, we found that correct code can be generated in many cases even when only routine name are given. We also confirmed that thread parallelization with OpenMP, SIMD vectorization, and cache blocking can be implemented to some extent, and that the code is faster than the reference code.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:33:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span><span>cs.MS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04697v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04697v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiuying Wei, Anunay Yadav, Razvan Pascanu, Caglar Gulcehre
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformers have become the cornerstone of modern large-scale language models; however, their dependence on softmax attention poses a major computational bottleneck, particularly in long-context settings. In this work, rather than following prevalent approaches such as linear attention (or SSMs) and local attention, we introduce an intermediate design called \rat between recurrence and attention mechanisms. It partitions the input into chunks, applies a simple linear recurrence within each chunk to capture local dependencies, and then performs softmax attention across chunks to model long-range interactions. By adjusting the size of the chunk, \rat enables flexible trade-offs, combining the strengths of RNN and attention. Empirically, with a chunk size of 16, the \rat layer achieves a \(7\times\) improvement in training speed with 100K token sequences and \(9\times\) in generation at 4K sequence length, while maintaining similar or sometimes even better accuracy compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves \rat with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage compared to attention, but also consistently enhances performance, for example, achieving an average 1 point gain in commonsense reasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase in a summarization SFT task. Code is available at https://github.com/CLAIRE-Labo/RAT
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-06T15:08:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04416v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04416v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale
  Reconstruction with External Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Felix Windisch, Lukas Radl, Thomas Köhler, Michael Steiner, Dieter Schmalstieg, Markus Steinberger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T15:51:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01110v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01110v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T15:40:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.05344v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.05344v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Heterogeneous Memory Benchmarking Toolkit</h2>
                <div class="authors">
                    <strong>Authors:</strong> Golsana Ghaemi, Gabriel Franco, Kazem Taram, Renato Mancuso
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents an open-source kernel-level heterogeneous memory characterization framework (MemScope) for embedded systems. MemScope enables precise characterization of the temporal behavior of available memory modules under configurable contention stress scenarios. MemScope leverages kernel-level control over physical memory allocation, cache maintenance, CPU state, interrupts, and I/O device activity to accurately benchmark heterogeneous memory subsystems. This gives us the privilege to directly map pieces of contiguous physical memory and instantiate allocators, allowing us to finely control cores to create and eliminate interference. Additionally, we can minimize noise and interruptions, guaranteeing more consistent and precise results compared to equivalent user-space solutions. Running our Framework on a Xilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to precisely benchmark bandwidth and latency across various memory types, including PL-side DRAM and BRAM, in a multi-core system.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T13:37:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.00901v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.00901v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Combination generators with optimal cache utilization and communication
  free parallel execution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xi He, Max. A. Little
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce an efficient and elegant combination generator for producing all combinations of size less than or equal to K, designed for exhaustive generation and combinatorial optimization tasks. This generator can be implemented to achieve what we define as optimal efficiency: constant amortized time, optimal cache utilization, embarrassingly parallel execution, and a recursive structure compatible with pruning-based search. These properties are difficult to satisfy simultaneously in existing generators. For example, classical Gray code or lexicographic generators are typically list-based and sequentially defined, making them difficult to vectorized, inefficient in cache usage, and inherently hard to parallelize. Generators based on unranking methods, while easy to parallelize, are non-recursive. These limitations reduce their applicability in our target applications, where both computational efficiency and recursion are crucial. We adapt Bird's algebra of programming-style calculation to derive our algorithms, a formalism for developing correct-by-construction programs from specifications. As a result, all generators in this paper are first formulated in their clearest specification, and efficient definitions are derived constructively through equational reasoning, resulting in concise and elegant divide-and-conquer definitions. Beyond presenting a combination generator, we extend our approach to construct generators for K-permutations, nested combinations of combinations, and nested permutation-combination structures. To the best of our knowledge, the literature has not previously reported generators for these nested structures. We also develop sequential variants that produce configurations in Gray code-compatible orders -- such as the revolving door ordering -- which are particularly useful for constructing nested generators.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T10:11:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DM</span><span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03980v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03980v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 PFCS: Prime Factorization Cache System for Deterministic Data
  Relationship Discovery</h2>
                <div class="authors">
                    <strong>Authors:</strong> Duy Le
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cache systems fundamentally limit modern computing performance due to their inability to precisely capture data relationships. While achieving 85-92% hit rates, traditional systems rely on statistical heuristics that cannot guarantee relationship discovery, leading to suboptimal prefetching and resource waste. We present PFCS (Prime Factorization Cache System), which leverages the mathematical uniqueness of prime factorization to achieve deterministic relationship discovery with zero false positives. PFCS assigns unique primes to data elements and represents relationships as composite numbers, enabling the recovery of perfect relationships through factorization. A comprehensive evaluation across database, ML, and HPC workloads demonstrates an average performance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction compared to state-of-the-art systems. The mathematical foundation provides formal guarantees impossible with approximation-based approaches, establishing a new paradigm for cache system design
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T06:55:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.CC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03919v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03919v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 A Taxonomy and Comparative Analysis of IPv4 Identifier Selection
  Correctness, Security, and Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joshua J. Daymude, Antonio M. Espinoza, Sean Bergen, Benjamin Mixon-Baca, Jeffrey Knockel, Jedidiah R. Crandall
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T01:08:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.06483v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.06483v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Memory- and compute-optimized geometric multigrid GMGPolar for
  curvilinear coordinate representations -- Applications to fusion plasma</h2>
                <div class="authors">
                    <strong>Authors:</strong> Julian Litz, Philippe Leleux, Carola Kruse, Joscha Gedicke, Martin J. Kühn
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tokamak fusion reactors are actively studied as a means of realizing energy production from plasma fusion. However, due to the substantial cost and time required to construct fusion reactors and run physical experiments, numerical experiments are indispensable for understanding plasma physics inside tokamaks, supporting the design and engineering phase, and optimizing future reactor designs. Geometric multigrid methods are optimal solvers for many problems that arise from the discretization of partial differential equations. It has been shown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson equation in linear complexity and with only small memory requirements compared to other state-of-the-art solvers. In this paper, we present a completely refactored and object-oriented version of GMGPolar which offers two different matrix-free implementations. Among other things, we leverage the Sherman-Morrison formula to solve cyclic tridiagonal systems from circular line solvers without additional fill-in and we apply reordering to optimize cache access of circular and radial smoothing operations. With the Give approach, memory requirements are further reduced and speedups of four to seven are obtained for usual test cases. For the Take approach, speedups of 16 to 18 can be attained.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T21:09:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MS</span><span>physics.plasm-ph</span><span>68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03812v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03812v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Quantum Algorithm for the Fixed-Radius Neighbor Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luca Cappelli, Claudio Sanavio, Alessandro Andrea Zecchi, Giuseppe Murante, Sauro Succi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The neighbor search is a computationally demanding problem, usually both time- and memory-consuming. The main problem of this kind of algorithms is the long execution time due to cache misses. In this work, we propose a quantum algorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the fixed-point version of Grover's algorithm. We derive an efficient circuit for solving the FRANS with linear query complexity with the number of particles $N$. The quantum circuit returns the list of all the neighbors' pairs within the fixed radius, together with their distance, avoiding the slow down given by cache miss. We explicitly write the Grover's operator and analyze its gate complexity. The whole algorithm has complexity of $\mathcal{O}(M^{\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is the number of neighboring pairs, and uses $\mathcal{O}(\log N)$ number of qubits. By employing extra ancilla qubits the depth of the circuit can be brought down to $\mathcal{O}(N\log N)$ at the cost of $\mathcal{O}(N)$ qubits for unstructured dataset, or $\mathcal{O}(\text{poly}(\log N))$ qubits for structured datasets. Finally we assess the resilience of the model to the readout error, suggesting an error correction-free strategy to check the accuracy of the results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T10:01:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03445v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03445v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Numerical investigation of the effect of high voltage frequency on the
  density of RONS species in the air atmospheric pressure gas discharge</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fariborz Momtazzadeh, Farshad Sohbatzadeh, Hamed Soltani Ahmadi, Ramin Mehrabifard
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the last few decades, studies in various fields of plasma technology have expanded and its application in different processes has increased. Therefore, the achievement of a desirable and practical plasma with specific characteristics is of particular importance. The frequency of the applied voltage is one of the important factors that play a role in the physical and chemical characteristics. In this research, changes in the density of active species produced in an electrical discharge using a dielectric barrier and air working gas have been investigated from a frequency of 500 Hz to 500 kHz, and by applying a constant voltage of 2 kV, have been investigated. For this purpose, 87 different reactions with specific collision cross-sections were defined in COMSOL Multiphysics. Other parameters, including current-voltage waveform, electric field, and species densitywere evaluated. The results show that under completely identical conditions, the electron temperature distribution changes with increasing applied frequency, and the density of reactive oxygen and nitrogen species RONS decreases, but O shows an increasing trend. It should be noted that the simulation results are in good agreement with previous experimental and simulation reports. These results offer valuable insights into optimizing plasma parameters for different applications, potentially resulting in better treatment outcomes across a range of therapeutic domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T09:03:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.plasm-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/S40042-025-01392-9.' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.03396v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03396v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token
  Selection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T06:49:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.14051v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14051v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Hunyuan-TurboS: Advancing Large Language Models through
  Mamba-Transformer Synergy and Adaptive Chain-of-Thought</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tencent Hunyuan Team, Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Zhen Yang, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang, Cheng Zhang, Chengjun Liu, Chengxu Yang, Chengzhong Xu, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with Transformer's superior contextual understanding. Hunyuan-TurboS features an adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching between rapid responses for simple queries and deep "thinking" modes for complex problems, optimizing computational resources. Architecturally, this 56B activated (560B total) parameter model employs 128 layers (Mamba2, Attention, FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE structure. Pre-trained on 16T high-quality tokens, it supports a 256K context length and is the first industry-deployed large-scale Mamba model. Our comprehensive post-training strategy enhances capabilities via Supervised Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method, Multi-round Deliberation Learning for iterative improvement, and a two-stage Large-scale Reinforcement Learning process targeting STEM and general instruction-following. Evaluations show strong performance: overall top 7 rank on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances high performance and efficiency, offering substantial capabilities at lower inference costs than many reasoning models, establishing a new paradigm for efficient large-scale pre-trained models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T06:36:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.15431v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.15431v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Robust and Efficient Embedded Convex Optimization through First-Order
  Adaptive Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ishaan Mahajan, Brian Plancher
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in Model Predictive Control (MPC) leveraging a combination of first-order methods, such as the Alternating Direction Method of Multipliers (ADMM), and offline precomputation and caching of select operations, have excitingly enabled real-time MPC on microcontrollers. Unfortunately, these approaches require the use of fixed hyperparameters, limiting their adaptability and overall performance. In this work, we introduce First-Order Adaptive Caching, which precomputes not only select matrix operations but also their sensitivities to hyperparameter variations, enabling online hyperparameter updates without full recomputation of the cache. We demonstrate the effectiveness of our approach on a number of dynamic quadrotor tasks, achieving up to a 63.4% reduction in ADMM iterations over the use of optimized fixed hyperparameters and approaching 70% of the performance of a full cache recomputation, while reducing the computational cost from O(n^3) to O(n^2) complexity. This performance enables us to perform figure-eight trajectories on a 27g tiny quadrotor under wind disturbances. We release our implementation open-source for the benefit of the wider robotics community.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T00:16:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>math.OC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03231v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03231v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weishu Deng, Yujie Yang, Peiran Du, Lingfeng Xiang, Zhen Lin, Chen Zhong, Song Jiang, Hui Lu, Jia Rao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling inference for large language models (LLMs) is increasingly constrained by limited GPU memory, especially due to growing key-value (KV) caches required for long-context generation. While existing approaches offload KV caches to CPU memory or apply sparse attention to reduce GPU load, they often underutilize CPU compute resources and compromise accuracy. We present HGCA, a hybrid CPU-GPU attention mechanism that enables scalable, high-throughput LLM inference with near-full attention quality. HGCA performs dense attention on recently generated KV entries retained in GPU memory and parallel sparse attention on selected, salient KV entries in CPU memory. The attention outputs are efficiently merged using log-sum-exp fusion, minimizing PCIe transfer overhead. HGCA also introduces a finegrained, per-head sparsification strategy optimized for CPU execution, preserving contextual relevance while reducing computation. Our implementation seamlessly integrates into existing LLM frameworks without requiring model retraining. Experiments across diverse models and workloads show that HGCA achieves superior scalability, supports longer sequences and larger batch sizes, and outperforms existing sparse attention baselines in both performance and accuracy -- all on commodity GPU hardware.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T20:20:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03153v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03153v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Less is Enough: Training-Free Video Diffusion Acceleration via
  Runtime-Adaptive Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Zhou, Dingkang Liang, Kaijin Chen, Tianrui Feng, Xiwu Chen, Hongkai Lin, Yikang Ding, Feiyang Tan, Hengshuang Zhao, Xiang Bai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T17:59:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02860v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02860v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kevin Song, Jiacheng Yang, Zixuan Wang, Jishen Zhao, Sihang Liu, Gennady Pekhimenko
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern workloads are demanding increasingly larger memory capacity. Compute Express Link (CXL)-based memory tiering has emerged as a promising solution for addressing this problem by utilizing traditional DRAM alongside slow-tier CXL memory devices. We analyze prior tiering systems and observe two challenges for high-performance memory tiering: adapting to skewed but dynamically varying data hotness distributions while minimizing memory and cache overhead due to tiering.   To address these challenges, we propose HybridTier, an adaptive and lightweight tiering system for CXL memory. HybridTier tracks both long-term data access frequency and short-term access momentum \emph{simultaneously} to accurately capture and adapt to shifting hotness distributions. HybridTier reduces the metadata memory overhead by tracking data accesses \emph{probabilistically}, obtaining higher memory efficiency by trading off a small amount of tracking inaccuracy that has a negligible impact on application performance. To reduce cache overhead, HybridTier uses lightweight data structures that optimize for data locality to track data hotness. Our evaluations show that HybridTier outperforms prior systems by up to $91\%$ ($19\%$ geomean), incurring $2.0-7.8\times$ less memory overhead and $1.7-3.5\times$ less cache misses.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T17:11:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3676642.3736119' target='_blank'>doi</a><a href='http://arxiv.org/abs/2312.04789v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.04789v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache
  Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T16:06:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.05693v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.05693v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device
  Speculative Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T14:20:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02659v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02659v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Skip-Vision: Efficient and Scalable Acceleration of Vision-Language
  Models via Adaptive Token Skipping</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based models have driven significant advancements in Multimodal Large Language Models (MLLMs), yet their computational costs surge drastically when scaling resolution, training data, and model parameters. A key bottleneck stems from the proliferation of visual tokens required for fine-grained image understanding. We propose Skip-Vision, a unified framework addressing both training and inference inefficiencies in vision-language models. On top of conventional token compression approaches, our method introduces two complementary acceleration strategies. For training acceleration, we observe that Feed-Forward Network (FFN) computations on visual tokens induce marginal feature updates. This motivates our Skip-FFN strategy, which bypasses FFN layers for redundant visual tokens. For inference acceleration, we design a selective KV-cache removal mechanism that prunes the skipped key-value pairs during decoding while preserving model performance. Experimental results demonstrate that Skip-Vision reduces training time by up to 35\%, inference FLOPs by 75\%, and latency by 45\%, while achieving comparable or superior performance to existing methods. Our work provides a practical solution for scaling high-performance MLLMs with enhanced efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T08:22:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.21817v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.21817v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Direct Reconstruction of Terahertz-driven Subcycle Electron Emission
  Dynamics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiakang Mao, Yushan Zeng, Hongyang Li, Liwei Song, Ye Tian, Ruxin Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At the opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 71.2% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the Fowler-Nordheim model under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T07:49:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.optics</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02397v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02397v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV
  Cache and Parallel Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, Enze Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$ throughput} improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T04:51:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.22618v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.22618v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 PhysicsCorrect: A Training-Free Approach for Stable Neural PDE
  Simulations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinquan Huang, Paris Perdikaris
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Neural networks have emerged as powerful surrogates for solving partial differential equations (PDEs), offering significant computational speedups over traditional methods. However, these models suffer from a critical limitation: error accumulation during long-term rollouts, where small inaccuracies compound exponentially, eventually causing complete divergence from physically valid solutions. We present PhysicsCorrect, a training-free correction framework that enforces PDE consistency at each prediction step by formulating correction as a linearized inverse problem based on PDE residuals. Our key innovation is an efficient caching strategy that precomputes the Jacobian and its pseudoinverse during an offline warm-up phase, reducing computational overhead by two orders of magnitude compared to standard correction approaches. Across three representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction errors by up to 100x while adding negligible inference time (under 5\%). The framework integrates seamlessly with diverse architectures including Fourier Neural Operators, UNets, and Vision Transformers, effectively transforming unstable neural surrogates into reliable simulation tools that bridge the gap between deep learning's computational efficiency and the physical fidelity demanded by practical scientific applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T01:22:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02227v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02227v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Autoregressive Image Generation with Linear Complexity: A Spatial-Aware
  Decay Perspective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxin Mao, Zhen Qin, Jinxing Zhou, Hui Deng, Xuyang Shen, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoregressive (AR) models have garnered significant attention in image generation for their ability to effectively capture both local and global structures within visual data. However, prevalent AR models predominantly rely on the transformer architectures, which are beset by quadratic computational complexity concerning input sequence length and substantial memory overhead due to the necessity of maintaining key-value caches. Although linear attention mechanisms have successfully reduced this burden in language models, our initial experiments reveal that they significantly degrade image generation quality because of their inability to capture critical long-range dependencies in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a novel attention mechanism that explicitly preserves genuine 2D spatial relationships within the flattened image sequences by computing position-dependent decay factors based on true 2D spatial location rather than 1D sequence positions. Based on this mechanism, we present LASADGen, an autoregressive image generator that enables selective attention to relevant spatial contexts with linear complexity. Experiments on ImageNet show LASADGen achieves state-of-the-art image generation performance and computational efficiency, bridging the gap between linear attention's efficiency and spatial understanding needed for high-quality generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T12:27:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01652v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01652v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Learned-Database Systems Security</h2>
                <div class="authors">
                    <strong>Authors:</strong> Roei Schuster, Jin Peng Zhou, Thorsten Eisenhofer, Paul Grubbs, Nicolas Papernot
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A learned database system uses machine learning (ML) internally to improve performance. We can expect such systems to be vulnerable to some adversarial-ML attacks. Often, the learned component is shared between mutually-distrusting users or processes, much like microarchitectural resources such as caches, potentially giving rise to highly-realistic attacker models. However, compared to attacks on other ML-based systems, attackers face a level of indirection as they cannot interact directly with the learned model. Additionally, the difference between the attack surface of learned and non-learned versions of the same system is often subtle. These factors obfuscate the de-facto risks that the incorporation of ML carries. We analyze the root causes of potentially-increased attack surface in learned database systems and develop a framework for identifying vulnerabilities that stem from the use of ML. We apply our framework to a broad set of learned components currently being explored in the database community. To empirically validate the vulnerabilities surfaced by our framework, we choose 3 of them and implement and evaluate exploits against these. We show that the use of ML cause leakage of past queries in a database, enable a poisoning attack that causes exponential memory blowup in an index structure and crashes it in seconds, and enable index users to snoop on each others' key distributions by timing queries over their own keys. We find that adversarial ML is an universal threat against learned components in database systems, point to open research gaps in our understanding of learned-systems security, and conclude by discussing mitigations, while noting that data leakage is inherent in systems whose learned component is shared between multiple parties.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T10:16:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2212.10318v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2212.10318v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheyu Shen, Yexiao He, Ziyao Wang, Yuning Zhang, Guoheng Sun, Wanghao Ye, Ang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have gained significant attention due to their versatility across a wide array of applications. Fine-tuning LLMs with parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these models to efficiently adapt to downstream tasks without extensive retraining. Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial benefits, such as reduced latency, enhanced privacy, and personalized responses. However, serving LLMs efficiently on resource-constrained edge devices presents critical challenges, including the complexity of adapter selection for different tasks and memory overhead from frequent adapter swapping. Moreover, given the multiple requests in multi-tenant settings, processing requests sequentially results in underutilization of computational resources and increased latency. This paper introduces EdgeLoRA, an efficient system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA incorporates three key innovations: (1) an adaptive adapter selection mechanism to streamline the adapter configuration process; (2) heterogeneous memory management, leveraging intelligent adapter caching and pooling to mitigate memory operation overhead; and (3) batch LoRA inference, enabling efficient batch processing to significantly reduce computational latency. Comprehensive evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly outperforms the status quo (i.e., llama.cpp) in terms of both latency and throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times boost in throughput. Even more impressively, it can serve several orders of magnitude more adapters simultaneously. These results highlight EdgeLoRA's potential to transform edge deployment of LLMs in multi-tenant scenarios, offering a scalable and efficient solution for resource-constrained environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T07:47:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3711875.3729141' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.01438v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01438v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV
  Management on a Single Commodity GPU</h2>
                <div class="authors">
                    <strong>Authors:</strong> He Sun, Li Li, Mingjun Xiao, Chengzhong Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Advanced Large Language Models (LLMs) have achieved impressive performance across a wide range of complex and long-context natural language tasks. However, performing long-context LLM inference locally on a commodity GPU (a PC) with privacy concerns remains challenging due to the increasing memory demands of the key-value (KV) cache. Existing systems typically identify important tokens and selectively offload their KV data to GPU and CPU memory. The KV data needs to be offloaded to disk due to the limited memory on a commodity GPU, but the process is bottlenecked by token importance evaluation overhead and the disk's low bandwidth. In this paper, we present LeoAM, the first efficient importance-aware long-context LLM inference system for a single commodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system employs an adaptive KV management strategy that partitions KV data into variable-sized chunks based on the skewed distribution of attention weights across different layers to reduce computational and additional transmission overheads. Moreover, we propose a lightweight KV abstract method, which minimizes transmission latency by storing and extracting the KV abstract of each chunk on disk instead of the full KV data. LeoAM also leverages the dynamic compression and pipeline techniques to further accelerate inference. Experimental results demonstrate that LongInfer achieves an average inference latency speedup of 3.46x, while maintaining comparable LLM response quality. In scenarios with larger batch sizes, it achieves up to a 5.47x speedup.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T05:12:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.CR</span><span>68M20</span><span>C.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.20187v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.20187v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shakya Jayakody, Youpeng Zhao, Jun Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph convolutional networks (GCNs) are fundamental in various scientific applications, ranging from biomedical protein-protein interactions (PPI) to large-scale recommendation systems. An essential component for modeling graph structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As the size of graph data continues to scale up, SpGEMMs are often conducted in an out-of-core fashion due to limited GPU memory space in resource-constrained systems. Albeit recent efforts that aim to alleviate the memory constraints of out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory layout, or performing the computation in sparse format, current systems suffer from both high I/O latency and GPU under-utilization issues.   In this paper, we first identify the problems of existing systems, where sparse format data alignment and memory allocation are the main performance bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the algorithm angle, AIRES proposes to alleviate the data alignment issues on the block level for matrices in sparse formats and develops a tiling algorithm to facilitate row block-wise alignment. On the system level, AIRES employs a three-phase dynamic scheduling that features a dual-way data transfer strategy utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage (GDS), and host memory to reduce I/O latency and improve throughput. Evaluations show that AIRES significantly outperforms the state-of-the-art methods, achieving up to 1.8x lower latency in real-world graph processing benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T00:35:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02006v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02006v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile
  Device via Additive Side-Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingke Yang, Liang Li, Zhiyi Wan, Sicong Li, Hao Wang, Xiaoqi Qi, Jiang Liu, Tomoaki Ohtsuki, Xin Fu, Miao Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> There is a huge gap between numerous intriguing applications fostered by on-device large language model (LLM) fine-tuning (FT) from fresh mobile data and the limited resources of a mobile device. While existing server-assisted methods (e.g., split learning or side-tuning) may enable LLM FT on the local mobile device, they suffer from heavy communication burdens of activation transmissions, and may disclose data, labels or fine-tuned models to the server. To address those issues, we develop PAE MobiLLM, a privacy-aware and efficient LLM FT method which can be deployed on the mobile device via server-assisted additive side-tuning. To further accelerate FT convergence and improve computing efficiency, PAE MobiLLM integrates activation caching on the server side, which allows the server to reuse historical activations and saves the mobile device from repeatedly computing forward passes for the recurring data samples. Besides, to reduce communication cost, PAE MobiLLM develops a one-token (i.e., ``pivot'' token) activation shortcut that transmits only a single activation dimension instead of full activation matrices to guide the side network tuning. Last but not least, PAE MobiLLM introduces the additive adapter side-network design which makes the server train the adapter modules based on device-defined prediction differences rather than raw ground-truth labels. In this way, the server can only assist device-defined side-network computing, and learn nothing about data, labels or fine-tuned models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T22:27:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01216v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01216v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anirud Aggarwal, Abhinav Shrivastava, Matthew Gwilliam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T21:27:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.15682v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.15682v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 FlashDP: Private Training Large Language Models with Efficient DP-SGD</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, Di Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) increasingly underpin technological advancements, the privacy of their training data emerges as a critical concern. Differential Privacy (DP) serves as a rigorous mechanism to protect this data, yet its integration via Differentially Private Stochastic Gradient Descent (DP-SGD) introduces substantial challenges, primarily due to the complexities of per-sample gradient clipping. Current explicit methods, such as Opacus, necessitate extensive storage for per-sample gradients, significantly inflating memory requirements. Conversely, implicit methods like GhostClip reduce storage needs by recalculating gradients multiple times, which leads to inefficiencies due to redundant computations. This paper introduces FlashDP, an innovative cache-friendly per-layer DP-SGD that consolidates necessary operations into a single task, calculating gradients only once in a fused manner. This approach not only diminishes memory movement by up to \textbf{50\%} but also cuts down redundant computations by \textbf{20\%}, compared to previous methods. Consequently, FlashDP does not increase memory demands and achieves a \textbf{90\%} throughput compared to the Non-DP method on a four-A100 system during the pre-training of the Llama-13B model, while maintaining parity with standard per-layer clipped DP-SGD in terms of accuracy. These advancements establish FlashDP as a pivotal development for efficient and privacy-preserving training of LLMs. FlashDP's code has been open-sourced in https://github.com/kaustpradalab/flashdp.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T19:28:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01154v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01154v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction
  and Dataflow-flexible Accelerator</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhican Wang, Hongxiang Fan, Haroon Waris, Gang Wang, Zhenyu Li, Jianfei Jiang, Yanan Sun, Guanghui He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) excel in natural language processing tasks but pose significant computational and memory challenges for edge deployment due to their intensive resource demands. This work addresses the efficiency of LLM inference by algorithm-hardware-dataflow tri-optimizations. We propose a novel voting-based KV cache eviction algorithm, balancing hardware efficiency and algorithm accuracy by adaptively identifying unimportant kv vectors. From a dataflow perspective, we introduce a flexible-product dataflow and a runtime reconfigurable PE array for matrix-vector multiplication. The proposed approach effectively handles the diverse dimensional requirements and solves the challenges of incrementally varying sequence lengths. Additionally, an element-serial scheduling scheme is proposed for nonlinear operations, such as softmax and layer normalization (layernorm). Results demonstrate a substantial reduction in latency, accompanied by a significant decrease in hardware complexity, from O(N) to O(1). The proposed solution is realized in a custom-designed accelerator, VEDA, which outperforms existing hardware platforms. This research represents a significant advancement in LLM inference on resource-constrained edge devices, facilitating real-time processing, enhancing data privacy, and enabling model customization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T14:30:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00797v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00797v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 On Hierarchical Coded Caching with Offline Users</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rashid Ummer N. T., B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper studies a two-layer hierarchical network in which some users are offline during the content delivery phase. A two-layer hierarchical network consists of a single server connected to multiple cache-aided mirror sites, and each mirror site is connected to a distinct set of cache-aided users. A scheme for such a hierarchical system with offline users has been proposed recently but considered a special case where all mirror caches have zero memory, which is a significant limitation. We propose an array known as a hierarchical hotplug placement delivery array (HHPDA), which describes the placement and delivery phases of a coded caching scheme for a general two-layer hierarchical network with offline users. Further, we construct a class of HHPDAs using combinatorial t-designs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T13:17:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00727v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00727v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Accelerating Loading WebGraphs in ParaGrapher</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohsen Koohi Esfahani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> ParaGrapher is a graph loading API and library that enables graph processing frameworks to load large-scale compressed graphs with minimal overhead. This capability accelerates the design and implementation of new high-performance graph algorithms and their evaluation on a wide range of graphs and across different frameworks. However, our previous study identified two major limitations in ParaGrapher: inefficient utilization of high-bandwidth storage and reduced decompression bandwidth due to increased compression ratios. To address these limitations, we present two optimizations for ParaGrapher in this paper. To improve storage utilization, particularly for high-bandwidth storage, we introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE (Filesystem in User Space). PG-Fuse optimizes storage access by increasing the size of requested blocks, reducing the number of calls to the underlying filesystem, and caching the received blocks in memory for future calls. To improve the decompression bandwidth, we introduce CompBin, a compact binary representation of the CSR format. CompBin facilitates direct accesses to neighbors while preventing storage usage for unused bytes. Our evaluation on 12 real-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse and CompBin achieve up to 7.6 and 21.8 times speedup, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T12:51:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00716v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00716v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 EARN: Efficient Inference Acceleration for LLM-based Generative
  Recommendation by Register Tokens</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chaoqun Yang, Xinyu Lin, Wenjie Wang, Yongqi Li, Teng Sun, Xianjing Han, Tat-Seng Chua
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Model-based generative recommendation (LLMRec) has achieved notable success, but it suffers from high inference latency due to massive computational overhead and memory pressure of KV Cache. Existing KV Cache reduction methods face critical limitations: cache compression offers marginal acceleration given recommendation tasks' short decoding steps, while prompt compression risks discarding vital interaction history. Through systematic analysis of attention patterns in LLMRec, we uncover two pivotal insights: 1) layer-wise attention sparsity inversion where early layers retain dense informative patterns while later layers exhibit high redundancy, and 2) dual attention sinks phenomenon where attention scores concentrate on both head and tail tokens of input sequences. Motivated by these insights, we propose EARN, an efficient inference framework that leverages the early layers to compress information into register tokens placed at the input sequence boundaries, then focuses solely on these tokens in the subsequent layers. Extensive experiments on three datasets, two LLMRec methods and two LLM architectures demonstrate EARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction with better accuracy than the general finetuning approach. Our work bridges the efficiency-effectiveness gap in LLMRec, offering practical deployment advantages for industrial scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T12:42:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00715v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00715v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Structural, dielectric, and ferroelectric characteristics of the
  low-temperature sintered 65PMN-35PT sample for electroceramic applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> B. Ramachandran, N. Sudarshan, G. Mangamma, M. S. Ramachandra Rao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A single-phase 65PMN-35PT ceramic was synthesized at a relatively low temperature (875 oC) using a modified columbite method. X-ray diffraction analysis confirmed the single-phase formation of perovskite 65PMN-35PT with a tetragonal structure. Morphological studies indicated that the sample consisted of small grains with a size of about 2 micro-m. The dielectric properties of the material demonstrate its relaxor behavior near the ferroelectric transition temperature, TC = 457 K. The saturation and remnant polarization values of approximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically poled sample. Additionally, the poling induced a negative internal electric field of about -0.2 kV cm-1 was detected due to the presence of ferroelectric nano-grains in this bulk 65PMN-35PT sample. These observed characteristics of the pyrochlore-free 65PMN-35PT ceramic are similar to those of its single-crystal counterpart.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T09:47:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/s10832-025-00423-y' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.00614v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00614v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Unleashing the Potential of All Test Samples: Mean-Shift Guided
  Test-Time Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jizhou Han, Chenhao Ding, SongLin Dong, Yuhang He, Xinyuan Gao, Yihong Gong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual-language models (VLMs) like CLIP exhibit strong generalization but struggle with distribution shifts at test time. Existing training-free test-time adaptation (TTA) methods operate strictly within CLIP's original feature space, relying on high-confidence samples while overlooking the potential of low-confidence ones. We propose MS-TTA, a training-free approach that enhances feature representations beyond CLIP's space using a single-step k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA improves feature compactness and class separability, leading to more stable adaptation. Additionally, a cache of refined embeddings further enhances inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms state-of-the-art training-free TTA methods, achieving robust adaptation without requiring additional training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T06:22:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00462v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00462v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanting Miao, William Loh, Pacal Poupart, Suraj Kothawade
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the "golden noise" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T05:46:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.12036v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.12036v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache
  Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-30T19:01:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.14051v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.14051v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 VideoITG: Multimodal Video Understanding with Instructed Temporal
  Grounding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shihao Wang, Guo Chen, De-an Huang, Zhiqi Li, Minghan Li, Guilin Li, Jose M. Alvarez, Lei Zhang, Zhiding Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:59:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13353v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13353v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 VisionThink: Smart and Efficient Vision Language Model via Reinforcement
  Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:59:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13348v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13348v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Taming Diffusion Transformer for Real-Time Mobile Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yushu Wu, Yanyu Li, Anil Kag, Ivan Skorokhodov, Willi Menapace, Ke Ma, Arpit Sahni, Ju Hu, Aliaksandr Siarohin, Dhritiman Sagar, Yanzhi Wang, Sergey Tulyakov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiT) have shown strong performance in video generation tasks, but their high computational cost makes them impractical for resource-constrained devices like smartphones, and real-time generation is even more challenging. In this work, we propose a series of novel optimizations to significantly accelerate video generation and enable real-time performance on mobile platforms. First, we employ a highly compressed variational autoencoder (VAE) to reduce the dimensionality of the input data without sacrificing visual quality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning strategy to shrink the model size to suit mobile platform while preserving critical performance characteristics. Third, we develop an adversarial step distillation technique tailored for DiT, which allows us to reduce the number of inference steps to four. Combined, these optimizations enable our model to achieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max, demonstrating the feasibility of real-time, high-quality video generation on mobile devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:59:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13343v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13343v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 DeFine: Decision-Making with Analogical Reasoning over Factor Profiles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yebowen Hu, Xiaoyang Wang, Wenlin Yao, Yiming Lu, Daoan Zhang, Hassan Foroosh, Dong Yu, Fei Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs are ideal for decision-making thanks to their ability to reason over long contexts. However, challenges arise when processing speech transcripts that describe complex scenarios, as they are verbose and include repetition, hedging, and vagueness. E.g., during a company's earnings call, an executive might project a positive revenue outlook to reassure investors, despite uncertainty regarding future earnings. It is crucial for LLMs to incorporate this uncertainty systematically when making decisions. In this paper, we introduce \textsc{DeFine}, a modular framework that constructs probabilistic factor profiles from complex scenarios. It then integrates these profiles with analogical reasoning, leveraging insights from similar past experiences to guide LLMs in making critical decisions in new situations. Our framework separates the tasks of quantifying uncertainty and incorporating it into LLM decision-making. This approach is particularly useful in areas such as consulting and financial deliberation, where making decisions under uncertainty is vital.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:58:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.01772v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.01772v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 SpectraLift: Physics-Guided Spectral-Inversion Network for
  Self-Supervised Hyperspectral Image Super-Resolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ritik Shah, Marco F. Duarte
                </div>
                <div class="summary">
                    <strong>Summary:</strong> High-spatial-resolution hyperspectral images (HSI) are essential for applications such as remote sensing and medical imaging, yet HSI sensors inherently trade spatial detail for spectral richness. Fusing high-spatial-resolution multispectral images (HR-MSI) with low-spatial-resolution hyperspectral images (LR-HSI) is a promising route to recover fine spatial structures without sacrificing spectral fidelity. Most state-of-the-art methods for HSI-MSI fusion demand point spread function (PSF) calibration or ground truth high resolution HSI (HR-HSI), both of which are impractical to obtain in real world settings. We present SpectraLift, a fully self-supervised framework that fuses LR-HSI and HR-MSI inputs using only the MSI's Spectral Response Function (SRF). SpectraLift trains a lightweight per-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic low-spatial-resolution multispectral image (LR-MSI) obtained by applying the SRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an $\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as the optimization objective. At inference, SpectraLift uses the trained network to map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in minutes, is agnostic to spatial blur and resolution, and outperforms state-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:57:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.IV</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13339v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13339v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour
  Understanding from Traditional Puns to Topical Jokes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tyler Loakman, William Thorne, Chenghua Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Humour, as a complex language form, is derived from myriad aspects of life, whilst existing work on computational humour has focussed almost exclusively on short pun-based jokes. In this work, we investigate whether the ability of Large Language Models (LLMs) to explain humour depends on the particular humour form. We compare models on simple puns and more complex topical humour that requires knowledge of real-world entities and events. In doing so, we curate a dataset of 600 jokes split across 4 joke types and manually write high-quality explanations. These jokes include heterographic and homographic puns, contemporary internet humour, and topical jokes, where understanding relies on reasoning beyond "common sense", rooted instead in world knowledge regarding news events and pop culture. Using this dataset, we compare the zero-shot abilities of a range of LLMs to accurately and comprehensively explain jokes of different types, identifying key research gaps in the task of humour explanation. We find that none of the tested models (inc. reasoning models) are capable of reliably generating adequate explanations of all joke types, further highlighting the narrow focus of most works in computational humour on overly simple joke forms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:51:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13335v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13335v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 A Survey of Context Engineering for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, Shenghua Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:50:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13334v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13334v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 The Imitation Game: Turing Machine Imitator is Length Generalizable
  Reasoner</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhouqi Hua, Wenwei Zhang, Chengqi Lyu, Yuzhe Gu, Songyang Gao, Kuikun Liu, Kai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:50:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13332v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13332v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 A Real-Time System for Egocentric Hand-Object Interaction Detection in
  Industrial Domains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Antonio Finocchiaro, Alessandro Sebastiano Catinello, Michele Mazzamuto, Rosario Leonardi, Antonino Furnari, Giovanni Maria Farinella
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Hand-object interaction detection remains an open challenge in real-time applications, where intuitive user experiences depend on fast and accurate detection of interactions with surrounding objects. We propose an efficient approach for detecting hand-objects interactions from streaming egocentric vision that operates in real time. Our approach consists of an action recognition module and an object detection module for identifying active objects upon confirmed interaction. Our Mamba model with EfficientNetV2 as backbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark at 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object. We implement our models in a cascaded architecture where the action recognition and object detection modules operate sequentially. When the action recognition predicts a contact state, it activates the object detection module, which in turn performs inference on the relevant frame to detect and classify the active object.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:45:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13326v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13326v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic
  Estimation using LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kyeongjin Ahn, Sungwon Han, Seungeon Lee, Donghyun Ahn, Hyoshin Kim, Jungwon Kim, Jihee Kim, Sangyoon Park, Meeyoung Cha
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Socio-economic indicators like regional GDP, population, and education levels, are crucial to shaping policy decisions and fostering sustainable development. This research introduces GeoReg a regression model that integrates diverse data sources, including satellite imagery and web-based geospatial information, to estimate these indicators even for data-scarce regions such as developing countries. Our approach leverages the prior knowledge of large language model (LLM) to address the scarcity of labeled data, with the LLM functioning as a data engineer by extracting informative features to enable effective estimation in few-shot settings. Specifically, our model obtains contextual relationships between data features and the target indicator, categorizing their correlations as positive, negative, mixed, or irrelevant. These features are then fed into the linear estimator with tailored weight constraints for each category. To capture nonlinear patterns, the model also identifies meaningful feature interactions and integrates them, along with nonlinear transformations. Experiments across three countries at different stages of development demonstrate that our model outperforms baselines in estimating socio-economic indicators, even for low-income countries with limited data availability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:42:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13323v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13323v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Testing halo models for constraining astrophysical feedback with
  multi-probe modeling: I. 3D Power spectra and mass fractions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pranjal R. S., Shivam Pandey, Dhayaa Anbajagane, Elisabeth Krause, Klaus Dolag
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Upcoming Stage-IV surveys will deliver measurements of distribution of matter with unprecedented precision, demanding highly accurate theoretical models for cosmological parameter inference. A major source of modeling uncertainty lies in astrophysical processes associated with galaxy formation and evolution, which remain poorly understood. Probes such as the thermal and kinematic Sunyaev-Zel'dovich effects, X-rays, and dispersion measure from fast radio bursts offer a promising avenue for mapping the distribution and thermal properties of cosmic baryons. A unified analytical framework capable of jointly modeling these observables is essential for fully harnessing the complementary information while mitigating probe-specific systematics. In this work, we present a detailed assessment of existing analytical models, which differ in their assumptions and prescriptions for simultaneously describing the distribution of matter and baryons in the universe. Using the Magneticum hydrodynamical simulation, we test these models by jointly analyzing the 3D auto- and cross-power spectra of the matter and baryonic fields that underpin the above probes. We find that all models can reproduce the power spectra at sub-percent to few-percent accuracy, depending on the tracer combination and number of free parameters. Their ability to recover underlying halo properties, such as the evolution of gas abundance and thermodynamic profiles with halo mass, varies considerably. Our results suggest that these models require further refinement and testing for reliable interpretation of multi-wavelength datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:36:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13317v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13317v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 GPU Performance Portability needs Autotuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Burkhard Ringlein, Thomas Parnell, Radu Stoica
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As LLMs grow in complexity, achieving state-of-the-art performance requires tight co-design across algorithms, software, and hardware. Today's reliance on a single dominant platform limits portability, creates vendor lock-in, and raises barriers for new AI hardware. In this work, we make the case for combining just-in-time (JIT) compilation with comprehensive kernel parameter autotuning to enable portable LLM inference with state-of-the-art performance without code changes. Focusing on performance-critical LLM kernels, we demonstrate that this approach explores up to 15x more kernel parameter configurations, produces significantly more diverse code across multiple dimensions, and even outperforms vendor-optimized implementations by up to 230%, all while reducing kernel code size by 70x and eliminating manual code optimizations. Our results highlight autotuning as a promising path to unlocking model portability across GPU vendors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:31:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.03780v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.03780v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Boosting Team Modeling through Tempo-Relational Representation Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vincenzo Marco De Luca, Giovanna Varni, Andrea Passerini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Team modeling remains a fundamental challenge at the intersection of Artificial Intelligence and the Social Sciences. Social Science research emphasizes the need to jointly model dynamics and relations, while practical applications demand unified models capable of inferring multiple team constructs simultaneously, providing interpretable insights and actionable recommendations to enhance team performance. However, existing works do not meet these practical demands. To bridge this gap, we present TRENN, a novel tempo-relational architecture that integrates: (i) an automatic temporal graph extractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct prediction, and (iv) two complementary explainability modules. TRENN jointly captures relational and temporal team dynamics, providing a solid foundation for MT-TRENN, which extends TReNN by replacing the decoder with a multi-task head, enabling the model to learn shared Social Embeddings and simultaneously predict multiple team constructs, including Emergent Leadership, Leadership Style, and Teamwork components. Experimental results demonstrate that our approach significantly outperforms approaches that rely exclusively on temporal or relational information. Additionally, experimental evaluation has shown that the explainability modules integrated in MT-TRENN yield interpretable insights and actionable suggestions to support team improvement. These capabilities make our approach particularly well-suited for Human-Centered AI applications, such as intelligent decision-support systems in high-stakes collaborative environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:22:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13305v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13305v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Retraining-Free Merging of Sparse MoE via Hierarchical Clustering</h2>
                <div class="authors">
                    <strong>Authors:</strong> I-Chun Chen, Hsu-Shen Liu, Wei-Fang Sun, Chen-Hao Chao, Yen-Chang Hsu, Chun-Yi Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse Mixture-of-Experts (SMoE) models represent a significant advancement in large language model (LLM) development through their efficient parameter utilization. These models achieve substantial performance improvements at reduced inference costs. However, the deployment of SMoE models faces constraints from extensive memory requirements of expert components in resource-limited environments. To address these limitations, this paper introduces Hierarchical Clustering for Sparsely activated Mixture of Experts (HC-SMoE), a task-agnostic expert merging framework for parameter reduction without retraining. HC-SMoE introduces a novel hierarchical clustering approach based on expert outputs to ensure merging robustness independent of routing decisions. The proposed output-based clustering method enables effective capture of functional relationships between experts for large-scale architectures. We provide theoretical analysis and comprehensive evaluations across multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness in state-of-the-art models including Qwen and Mixtral. The experimental results validate HC-SMoE's superior performance and practical applicability for real-world deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:20:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.08589v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.08589v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 The Generative Energy Arena (GEA): Incorporating Energy Awareness in
  Large Language Model (LLM) Human Evaluations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Carlos Arriaga, Gonzalo Martínez, Eneko Sendin, Javier Conde, Pedro Reviriego
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The evaluation of large language models is a complex task, in which several approaches have been proposed. The most common is the use of automated benchmarks in which LLMs have to answer multiple-choice questions of different topics. However, this method has certain limitations, being the most concerning, the poor correlation with the humans. An alternative approach, is to have humans evaluate the LLMs. This poses scalability issues as there is a large and growing number of models to evaluate making it impractical (and costly) to run traditional studies based on recruiting a number of evaluators and having them rank the responses of the models. An alternative approach is the use of public arenas, such as the popular LM arena, on which any user can freely evaluate models on any question and rank the responses of two models. The results are then elaborated into a model ranking. An increasingly important aspect of LLMs is their energy consumption and, therefore, evaluating how energy awareness influences the decisions of humans in selecting a model is of interest. In this paper, we present GEA, the Generative Energy Arena, an arena that incorporates information on the energy consumption of the model in the evaluation process. Preliminary results obtained with GEA are also presented, showing that for most questions, when users are aware of the energy consumption, they favor smaller and more energy efficient models. This suggests that for most user interactions, the extra cost and energy incurred by the more complex and top-performing models do not provide an increase in the perceived quality of the responses that justifies their use.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:11:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13302v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13302v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 AbGen: Evaluating Large Language Models in Ablation Study Design and
  Evaluation for Scientific Research</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilun Zhao, Weiyuan Chen, Zhijian Xu, Manasi Patwardhan, Yixin Liu, Chengye Wang, Lovekesh Vig, Arman Cohan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:09:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13300v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13300v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Towards Formal Verification of LLM-Generated Code from Natural Language
  Prompts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aaron Councilman, David Fu, Aryan Gupta, Chengxiao Wang, David Grove, Yu-Xiong Wang, Vikram Adve
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the past few years LLMs have emerged as a tool that can aid programmers by taking natural language descriptions and generating code based on it. However, LLMs often generate incorrect code that users need to fix and the literature suggests users often struggle to detect these errors. In this work we seek to offer formal guarantees of correctness to LLM generated code; such guarantees could improve the experience of using AI Code Assistants and potentially enable natural language programming for users with little or no programming knowledge. To address this challenge we propose to incorporate a formal query language that can represent a user's intent in a formally defined but natural language-like manner that a user can confirm matches their intent. Then, using such a query we propose to verify LLM generated code to ensure it matches the user's intent. We implement these ideas in our system, Astrogator, for the Ansible programming language which includes such a formal query language, a calculus for representing the behavior of Ansible programs, and a symbolic interpreter which is used for the verification. On a benchmark suite of 21 code-generation tasks, our verifier is able to verify correct code in 83% of cases and identify incorrect code in 92%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:54:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13290v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13290v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 GPU-Accelerated Gravitational Lensing & Dynamical (GLaD) Modeling for
  Cosmology and Galaxies</h2>
                <div class="authors">
                    <strong>Authors:</strong> Han Wang, Sherry H. Suyu, Aymeric Galan, Aleksi Halkola, Michele Cappellari, Anowar J. Shajib, Miha Cernetic
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Time-delay distance measurements from strongly lensed quasars provide a robust, independent method for determining the Hubble constant ($H_0$). This approach cross-checks $H_0$ estimates from the distance ladder in the late universe and the cosmic microwave background in the early universe. However, the mass-sheet degeneracy in lensing models introduces systematic uncertainty, limiting precision. Dynamical modeling complements strong lensing by constraining the mass distribution with independent observational data. We develop a methodology and software framework for joint modeling of stellar kinematics and lensing data. Using simulated data for the lensed quasar RXJ1131$-$1131, we demonstrate that high-quality kinematic data can achieve $\sim$4% precision on $H_0$. Through extensive modeling, we examine the impact of the presence of a supermassive black hole in the lens galaxy and potential systematic biases in kinematic data on $H_0$ measurements. Our results show that imposing priors on black hole mass and orbital anisotropy, or excluding central kinematic bins, mitigates biases in $H_0$ estimates. By testing on mock kinematic data with systematic biases, we highlight the need for sub-percent control of kinematic systematics, which is achievable with current technology. Additionally, we leverage GPU parallelization to accelerate Bayesian inference, reducing a previously month-long process by an order of magnitude. This pipeline offers significant potential for advancing cosmological and galaxy evolution studies with large datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:44:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.01302v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.01302v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 A fast and accurate method for inferring solid-state diffusivity in
  lithium-ion battery active materials: improving upon the classical GITT
  approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> A. Emir Gumrukcuoglu, James Burridge, Kieran O'Regan, Emma Kendrick, Jamie M. Foster
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Data collected using the galvanostatic intermittent titration technique (GITT) and application of the Sand equation is a ubiquitous method for inferring the solid-state diffusivity in lithium-ion battery active materials. However, the experiment is notoriously time-consuming and the Sand equation relies on assumptions whose applicability can be questionable. We propose a novel methodology, termed Inference from a Consistent Model (ICM), which enables inference of solid-state diffusivity using the same physical model employed for prediction, and is applicable to more general and quick-to-measure data. We infer the diffusivity (as a function of inserted lithium concentration) by minimising the residual sum of squares between data and solutions to a spherically-symmetric nonlinear diffusion model in a single representative active material particle. Using data harvested from the NMC cathode of a commercial LG M50 cell we demonstrate that the ICM is robust, and yields more accurate diffusivity estimates, while relying on data that are five times faster to collect than that required by the classical approach. Moreover, there is good reason to believe that further speed ups could be achieved when other types of data are available. This work contributes towards developing faster and more reliable techniques in parameter inference for lithium-ion batteries, and the code required to deploy ICM is provided to facilitate its adoption in future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:35:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.app-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.16658v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.16658v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with
  Security Filtering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Grace Byun, Shinsun Lee, Nayoung Choi, Jinho D. Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing Retrieval-Augmented Generation (RAG) systems face challenges in enterprise settings due to limited retrieval scope and data security risks. When relevant internal documents are unavailable, the system struggles to generate accurate and complete responses. Additionally, using closed-source Large Language Models (LLMs) raises concerns about exposing proprietary information. To address these issues, we propose the Secure Multifaceted-RAG (SecMulti-RAG) framework, which retrieves not only from internal documents but also from two supplementary sources: pre-generated expert knowledge for anticipated queries and on-demand external LLM-generated knowledge. To mitigate security risks, we adopt a local open-source generator and selectively utilize external LLMs only when prompts are deemed safe by a filtering mechanism. This approach enhances completeness, prevents data leakage, and reduces costs. In our evaluation on a report generation task in the automotive industry, SecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9 percent win rates across correctness, richness, and helpfulness in LLM-based evaluation, and 56.3 to 70.4 percent in human evaluation. This highlights SecMulti-RAG as a practical and secure solution for enterprise RAG.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:32:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.13425v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.13425v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiazheng Li, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Hongzhou Lin, Yi Wu, Jingzhao Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning (RL) has become a key component in training large language reasoning models (LLMs). However, recent studies questions its effectiveness in improving multi-step reasoning-particularly on hard problems. To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%) on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical explanations that QuestA improves sample efficiency, offering a practical and generalizable pathway for expanding reasoning capability through RL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:21:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>68T50</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13266v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13266v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Automating Steering for Safe Multimodal Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lyucheng Wu, Mengru Wang, Ziwen Xu, Tri Cao, Nay Oo, Bryan Hooi, Shumin Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:04:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span><span>cs.LG</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13255v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13255v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Comparing astrophysical models to gravitational-wave data in the
  observable space</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alexandre Toubiana, Davide Gerosa, Matthew Mould, Stefano Rinaldi, Manuel Arca Sedda, Tristan Bruel, Riccardo Buscicchio, Jonathan Gair, Lavinia Paiella, Filippo Santoliquido, Rodrigo Tenorio, Cristiano Ugolini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Comparing population-synthesis models to the results of hierarchical Bayesian inference in gravitational-wave astronomy requires a careful understanding of the domain of validity of the models fitted to data. This comparison is usually done using the inferred astrophysical distribution: from the data that were collected, one deconvolves selection effects to reconstruct the generating population distribution. In this letter, we demonstrate the benefits of instead comparing observable populations directly. In this approach, the domain of validity of the models is trivially respected, such that only the relevant parameter space regions as predicted by the astrophysical models of interest contribute to the comparison. We clarify that unbiased inference of the observable compact-binary population is indeed possible. Crucially, this approach still requires incorporating selection effects, but in a manner that differs from the standard implementation. We apply our observable-space reconstruction to LIGO-Virgo-KAGRA data from their third observing run and illustrate its potential by comparing the results to the predictions of a fiducial population-synthesis model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:58:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span><span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13249v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13249v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 ConTextual: Improving Clinical Text Summarization in LLMs with
  Context-preserving Token Filtering and Knowledge Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fahmida Liza Piya, Rahmatollah Beheshti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:50:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.16394v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.16394v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ashray Gupta, Rohan Joseph, Sunny Rai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Analogies test a model's ability to infer implicit relationships between concepts, making them a key benchmark for evaluating reasoning capabilities. While large language models (LLMs) are widely evaluated for reasoning in English, their abilities in Indic languages remain understudied, limiting our understanding of whether these models generalize across languages. To address this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405 multiple-choice questions sourced from Indian government exams. We benchmark state-of-the-art multilingual LLMs using various prompting strategies and introduce a grounded Chain of Thought approach that leverages cognitive theories of analogical reasoning. This approach improves model performance on Hindi analogy questions. Our experiments show that models perform best with English prompts, irrespective of the prompting strategy. Our test set addresses the lack of a critical resource to evaluate LLM reasoning capabilities in Hindi.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:47:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13238v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13238v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Enhancing Cross-task Transfer of Large Language Models via Activation
  Steering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyu Tang, Zhihao Lv, Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Zujie Wen, Zhiqiang Zhang, Jun Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have shown impressive abilities in leveraging pretrained knowledge through prompting, but they often struggle with unseen tasks, particularly in data-scarce scenarios. While cross-task in-context learning offers a direct solution for transferring knowledge across tasks, it still faces critical challenges in terms of robustness, scalability, and efficiency. In this paper, we investigate whether cross-task transfer can be achieved via latent space steering without parameter updates or input expansion. Through an analysis of activation patterns in the latent space of LLMs, we observe that the enhanced activations induced by in-context examples have consistent patterns across different tasks. Inspired by these findings, we propose CAST, a novel Cross-task Activation Steering Transfer framework that enables effective transfer by manipulating the model's internal activation states. Our approach first selects influential and diverse samples from high-resource tasks, then utilizes their contrastive representation-enhanced activations to adapt LLMs to low-resource tasks. Extensive experiments across both cross-domain and cross-lingual transfer settings show that our method outperforms competitive baselines and demonstrates superior scalability and lower computational costs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:47:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13236v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13236v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Unraveling the Feedback-Regulated Star Formation Activities around the
  Expanding Galactic MIR Bubble [HKS2019] E71</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aayushi Verma, Saurabh Sharma, Lokesh K. Dewangan, Tarak Chand, Ariful Hoque, Devendra K. Ojha, Harmeen Kaur, Ram Kesh Yadav, Mamta, Manojit Chakraborty, Archana Gupta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We explore the physical environment of the Galactic mid-infrared (MIR) bubble [HKS2019] E71 (hereafter E71) through a multi-wavelength approach. E71 is located at the edge of a filamentary structure, as traced in Herschel images (250-500 $\mu$m), Herschel column density map, and molecular maps in the velocity range [-20,-14] km/s. It hosts a stellar cluster (radius~1.26 pc, distance~1.81+/-0.15 kpc) associated with radio continuum emission, including a centrally positioned B1.5-type massive star (hereafter 'm2'), along with an enhanced population of evolved low-mass stars and young stellar objects. MIR images and molecular line maps reveal a PDR surrounding 'm2', exhibiting an arc-like structure along the edges of E71. Regularly spaced molecular and dust condensations are identified along this structure. The position-velocity map of 12CO emission suggests an expansion of molecular gas concentrated at the periphery of E71. Near-infrared spectroscopic observations with TANSPEC confirm the presence of the accretion process in a massive young stellar object (MYSO) located near the edge of the bubble. High-resolution uGMRT radio continuum maps uncover substructures in the ionized emission, both toward the MYSO and the center of E71. These findings support that 'm2' has shaped an arc-like morphology through its feedback processes. The pressure exerted by 'm2' and the velocity structure of the 12/13CO(1-0) emission suggest that the stellar feedback has likely driven out molecular material, leading to the formation of the expanding E71 bubble. Our overall investigation infers that the "collect and collapse" process might be a possible mechanism that can describe the ongoing star formation activities around the E71 bubble.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:42:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13232v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13232v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 VITA: Vision-to-Action Flow Matching Policy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dechen Gao, Boqi Zhao, Andrew Lee, Ian Chuang, Hanchu Zhou, Hang Wang, Zhe Zhao, Junshan Zhang, Iman Soltani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present VITA, a Vision-To-Action flow matching policy that evolves latent visual representations into latent actions for visuomotor control. Traditional flow matching and diffusion policies sample from standard source distributions (e.g., Gaussian noise) and require additional conditioning mechanisms like cross-attention to condition action generation on visual information, creating time and space overheads. VITA proposes a novel paradigm that treats latent images as the flow source, learning an inherent mapping from vision to action while eliminating separate conditioning modules and preserving generative modeling capabilities. Learning flows between fundamentally different modalities like vision and action is challenging due to sparse action data lacking semantic structures and dimensional mismatches between high-dimensional visual representations and raw actions. We address this by creating a structured action latent space via an autoencoder as the flow matching target, up-sampling raw actions to match visual representation shapes. Crucially, we supervise flow matching with both encoder targets and final action outputs through flow latent decoding, which backpropagates action reconstruction loss through sequential flow matching ODE solving steps for effective end-to-end learning. Implemented as simple MLP layers, VITA is evaluated on challenging bi-manual manipulation tasks on the ALOHA platform, including 5 simulation and 2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or matches state-of-the-art generative policies while reducing inference latency by 50-130% compared to conventional flow matching policies requiring different conditioning mechanisms or complex architectures. To our knowledge, VITA is the first MLP-only flow matching policy capable of solving complex bi-manual manipulation tasks like those in ALOHA benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:41:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13231v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13231v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,
  Multi-Generator and Multi-Domain Settings</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniil Orel, Dilshod Azizov, Preslav Nakov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have revolutionized code generation, automating programming with remarkable efficiency. However, these advancements challenge programming skills, ethics, and assessment integrity, making the detection of LLM-generated code essential for maintaining accountability and standards. While, there has been some research on this problem, it generally lacks domain coverage and robustness, and only covers a small number of programming languages. To this end, we propose a framework capable of distinguishing between human- and LLM-written code across multiple programming languages, code generators, and domains. We use a large-scale dataset from renowned platforms and LLM-based code generators, alongside applying rigorous data quality checks, feature engineering, and comparative analysis using evaluation of traditional machine learning models, pre-trained language models (PLMs), and LLMs for code detection. We perform an evaluation on out-of-domain scenarios, such as detecting the authorship and hybrid authorship of generated code and generalizing to unseen models, domains, and programming languages. Moreover, our extensive experiments show that our framework effectively distinguishes human- from LLM-written code and sets a new benchmark for this task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:31:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.13733v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.13733v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 A Comparative Approach to Assessing Linguistic Creativity of Large
  Language Models and Humans</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anca Dinu, Andra-Maria Florescu, Alina Resceanu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The following paper introduces a general linguistic creativity test for humans and Large Language Models (LLMs). The test consists of various tasks aimed at assessing their ability to generate new original words and phrases based on word formation processes (derivation and compounding) and on metaphorical language use. We administered the test to 24 humans and to an equal number of LLMs, and we automatically evaluated their answers using OCSAI tool for three criteria: Originality, Elaboration, and Flexibility. The results show that LLMs not only outperformed humans in all the assessed criteria, but did better in six out of the eight test tasks. We then computed the uniqueness of the individual answers, which showed some minor differences between humans and LLMs. Finally, we performed a short manual analysis of the dataset, which revealed that humans are more inclined towards E(extending)-creativity, while LLMs favor F(ixed)-creativity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:27:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12039v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12039v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 MoTM: Towards a Foundation Model for Time Series Imputation based on
  Continuous Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Etienne Le Naour, Tahar Nabil, Ghislain Agoua
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent years have witnessed a growing interest for time series foundation models, with a strong emphasis on the forecasting task. Yet, the crucial task of out-of-domain imputation of missing values remains largely underexplored. We propose a first step to fill this gap by leveraging implicit neural representations (INRs). INRs model time series as continuous functions and naturally handle various missing data scenarios and sampling rates. While they have shown strong performance within specific distributions, they struggle under distribution shifts. To address this, we introduce MoTM (Mixture of Timeflow Models), a step toward a foundation model for time series imputation. Building on the idea that a new time series is a mixture of previously seen patterns, MoTM combines a basis of INRs, each trained independently on a distinct family of time series, with a ridge regressor that adapts to the observed context at inference. We demonstrate robust in-domain and out-of-domain generalization across diverse imputation scenarios (e.g., block and pointwise missingness, variable sampling rates), paving the way for adaptable foundation imputation models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:16:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13207v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13207v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Automatically assessing oral narratives of Afrikaans and isiXhosa
  children</h2>
                <div class="authors">
                    <strong>Authors:</strong> R. Louw, E. Sharratt, F. de Wet, C. Jacobs, A. Smith, H. Kamper
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Developing narrative and comprehension skills in early childhood is critical for later literacy. However, teachers in large preschool classrooms struggle to accurately identify students who require intervention. We present a system for automatically assessing oral narratives of preschool children in Afrikaans and isiXhosa. The system uses automatic speech recognition followed by a machine learning scoring model to predict narrative and comprehension scores. For scoring predicted transcripts, we compare a linear model to a large language model (LLM). The LLM-based system outperforms the linear model in most cases, but the linear system is competitive despite its simplicity. The LLM-based system is comparable to a human expert in flagging children who require intervention. We lay the foundation for automatic oral assessments in classrooms, giving teachers extra capacity to focus on personalised support for children's learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:15:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13205v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13205v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge
  Expansion for Dense Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sangam Lee, Ryang Heo, SeongKu Kang, Dongha Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing dense retrieval models struggle with reasoning-intensive retrieval task as they fail to capture implicit relevance that requires reasoning beyond surface-level semantic information. To address these challenges, we propose Scenario-Profiled Indexing with Knowledge Expansion (SPIKE), a dense retrieval framework that explicitly indexes implicit relevance by decomposing documents into scenario-based retrieval units. SPIKE organizes documents into scenario, which encapsulates the reasoning process necessary to uncover implicit relationships between hypothetical information needs and document content. SPIKE constructs a scenario-augmented dataset using a powerful teacher large language model (LLM), then distills these reasoning capabilities into a smaller, efficient scenario generator. During inference, SPIKE incorporates scenario-level relevance alongside document-level relevance, enabling reasoning-aware retrieval. Extensive experiments demonstrate that SPIKE consistently enhances retrieval performance across various query types and dense retrievers. It also enhances the retrieval experience for users through scenario and offers valuable contextual information for LLMs in retrieval-augmented generation (RAG).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:06:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.23033v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.23033v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Branching Stein Variational Gradient Descent for sampling multimodal
  distributions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Isaías Bañales, Arturo Jaramillo, Joshué Helí Ricalde-Guerrero
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose a novel particle-based variational inference method designed to work with multimodal distributions. Our approach, referred to as Branched Stein Variational Gradient Descent (BSVGD), extends the classical Stein Variational Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism that encourages the exploration of the state space. In this work, a theoretical guarantee for the convergence in distribution is presented, as well as numerical experiments to validate the suitability of our algorithm. Performance comparisons between the BSVGD and the SVGD are presented using the Wasserstein distance between samples and the corresponding computational times.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:05:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.CO</span><span>stat.ML</span><span>62F15, 65C05, 65C35</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.13916v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.13916v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday
  Storytelling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiangnan Xu, Haeseul Cha, Gosu Choi, Gyu-cheol Lee, Yeo-Jin Yoon, Zucheul Lee, Konstantinos Papangelis, Dae Hyun Kim, Juho Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> An interactive vignette is a popular and immersive visual storytelling approach that invites viewers to role-play a character and influences the narrative in an interactive environment. However, it has not been widely used by everyday storytellers yet due to authoring complexity, which conflicts with the immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted authoring system for interactive vignette creation in everyday storytelling. It takes a natural language story as input and extracts the three core elements of an interactive vignette (environment, characters, and events), enabling authors to focus on refining these elements instead of constructing them from scratch. Then, it automatically transforms the single-branch story input into a branch-and-bottleneck structure using an LLM-powered narrative planner, which enables flexible viewer interactions while freeing the author from multi-branching. A technical evaluation (N=16) shows that DiaryPlay-generated character activities are on par with human-authored ones regarding believability. A user study (N=16) shows that DiaryPlay effectively supports authors in creating interactive vignette elements, maintains authorial intent while reacting to viewer interactions, and provides engaging viewing experiences.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:01:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11628v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11628v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyan Wang, Yingpeng Du, Zhu Sun, Jieyi Bi, Haoyan Chua, Tianjun Wei, Jie Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, much effort has been devoted to modeling users' multi-interests based on their behaviors or auxiliary signals. However, existing methods often rely on heuristic assumptions, e.g., co-occurring items indicate the same interest of users, failing to capture user multi-interests aligning with real-world scenarios. While large language models (LLMs) show significant potential for multi-interest analysis due to their extensive knowledge and powerful reasoning capabilities, two key challenges remain. First, the granularity of LLM-driven multi-interests is agnostic, possibly leading to overly fine or coarse interest grouping. Second, individual user analysis provides limited insights due to the data sparsity issue. In this paper, we propose an LLM-driven dual-level multi-interest modeling framework for more effective recommendation. At the user-individual level, we exploit LLMs to flexibly allocate items engaged by users into different semantic clusters, indicating their diverse and distinct interests. To alleviate the agnostic generation of LLMs, we adaptively assign these semantic clusters to users' collaborative multi-interests learned from global user-item interactions, allowing the granularity to be automatically adjusted according to the user's behaviors using an alignment module. To alleviate the limited insights derived from individual users' behaviors, at the user-crowd level, we propose aggregating user cliques into synthesized users with rich behaviors for more comprehensive LLM-driven multi-interest analysis. We formulate a max covering problem to ensure the compactness and representativeness of synthesized users' behaviors, and then conduct contrastive learning based on their LLM-driven multi-interests to disentangle item representations among different interests. Experiments on real-world datasets show the superiority of our approach against state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:01:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10917v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10917v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Recent Advances in Simulation-based Inference for Gravitational Wave
  Data Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bo Liang, He Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration has ushered in a new era of observational astronomy, emphasizing the need for rapid and detailed parameter estimation and population-level analyses. Traditional Bayesian inference methods, particularly Markov chain Monte Carlo, face significant computational challenges when dealing with the high-dimensional parameter spaces and complex noise characteristics inherent in gravitational wave data. This review examines the emerging role of simulation-based inference methods in gravitational wave astronomy, with a focus on approaches that leverage machine-learning techniques such as normalizing flows and neural posterior estimation. We provide a comprehensive overview of the theoretical foundations underlying various simulation-based inference methods, including neural posterior estimation, neural ratio estimation, neural likelihood estimation, flow matching, and consistency models. We explore the applications of these methods across diverse gravitational wave data processing scenarios, from single-source parameter estimation and overlapping signal analysis to testing general relativity and conducting population studies. Although these techniques demonstrate speed improvements over traditional methods in controlled studies, their model-dependent nature and sensitivity to prior assumptions are barriers to their widespread adoption. Their accuracy, which is similar to that of conventional methods, requires further validation across broader parameter spaces and noise conditions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:00:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span><span>astro-ph.HE</span><span>astro-ph.IM</span><span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.61977/ati2025020' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.11192v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11192v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction
  with an Agentic Robot</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luca Garello, Francesca Cocchella, Alessandra Sciutti, Manuel Catalano, Francesco Rea
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous robots are increasingly being tested into public spaces to enhance user experiences, particularly in cultural and educational settings. This paper presents the design, implementation, and evaluation of the autonomous museum guide robot Alter-Ego equipped with advanced navigation and interactive capabilities. The robot leverages state-of-the-art Large Language Models (LLMs) to provide real-time, context aware question-and-answer (Q&A) interactions, allowing visitors to engage in conversations about exhibits. It also employs robust simultaneous localization and mapping (SLAM) techniques, enabling seamless navigation through museum spaces and route adaptation based on user requests. The system was tested in a real museum environment with 34 participants, combining qualitative analysis of visitor-robot conversations and quantitative analysis of pre and post interaction surveys. Results showed that the robot was generally well-received and contributed to an engaging museum experience, despite some limitations in comprehension and responsiveness. This study sheds light on HRI in cultural spaces, highlighting not only the potential of AI-driven robotics to support accessibility and knowledge acquisition, but also the current limitations and challenges of deploying such technologies in complex, real-world environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:54:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12273v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12273v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Flexible Nonparametric Inference for Causal Effects under the Front-Door
  Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anna Guo, David Benkeser, Razieh Nabi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Evaluating causal treatment effects in observational studies requires addressing confounding. While the back-door criterion enables identification through adjustment for observed covariates, it fails in the presence of unmeasured confounding. The front-door criterion offers an alternative by leveraging variables that fully mediate the treatment effect and are unaffected by unmeasured confounders of the treatment-outcome pair. We develop novel one-step and targeted minimum loss-based estimators for both the average treatment effect and the average treatment effect on the treated under front-door assumptions. Our estimators are built on multiple parameterizations of the observed data distribution, including approaches that avoid modeling the mediator density entirely, and are compatible with flexible, machine learning-based nuisance estimation. We establish conditions for root-$n$ consistency and asymptotic linearity by deriving second-order remainder bounds. We also develop flexible tests for assessing identification assumptions, including a doubly robust testing procedure, within a semiparametric extension of the front-door model that encodes generalized (Verma) independence constraints. We further show how these constraints can be leveraged to improve the efficiency of causal effect estimators. Simulation studies confirm favorable finite-sample performance, and real-data applications in education and emergency medicine illustrate the practical utility of our methods. An accompanying R package, fdcausal, implements all proposed procedures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:45:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.10234v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.10234v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Black Box Deployed -- Functional Criteria for Artificial Moral Agents in
  the LLM Era</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matthew E. Brophy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The advancement of powerful yet opaque large language models (LLMs) necessitates a fundamental revision of the philosophical criteria used to evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the assumption of transparent architectures, which LLMs defy due to their stochastic outputs and opaque internal states. This paper argues that traditional ethical criteria are pragmatically obsolete for LLMs due to this mismatch. Engaging with core themes in the philosophy of technology, this paper proffers a revised set of ten functional criteria to evaluate LLM-based artificial moral agents: moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy, and moral imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating Moral Agency through Large Language Systems), aim to steer AMAs toward greater alignment and beneficial societal integration in the coming years. We illustrate these criteria using hypothetical scenarios involving an autonomous public bus (APB) to demonstrate their practical applicability in morally salient contexts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:39:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>68T27, 03B42 68T27, 03B4268T27, 03B42 68T27, 03B42 68T27, 03B42
  68T27, 03B42 68T27, 03B42 68T27, 03B4268T27, 03B42</span><span>I.2.0; I.2.9; K.4.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13175v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13175v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Prompt Injection 2.0: Hybrid AI Threats</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeremy McHugh, Kristina Šekrst, Jon Cefalu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prompt injection attacks, where malicious input is designed to manipulate AI systems into ignoring their original instructions and following unauthorized commands instead, were first discovered by Preamble, Inc. in May 2022 and responsibly disclosed to OpenAI. Over the last three years, these attacks have continued to pose a critical security threat to LLM-integrated systems. The emergence of agentic AI systems, where LLMs autonomously perform multistep tasks through tools and coordination with other agents, has fundamentally transformed the threat landscape. Modern prompt injection attacks can now combine with traditional cybersecurity exploits to create hybrid threats that systematically evade traditional security controls. This paper presents a comprehensive analysis of Prompt Injection 2.0, examining how prompt injections integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and other web security vulnerabilities to bypass traditional security measures. We build upon Preamble's foundational research and mitigation technologies, evaluating them against contemporary threats, including AI worms, multi-agent infections, and hybrid cyber-AI attacks. Our analysis incorporates recent benchmarks that demonstrate how traditional web application firewalls, XSS filters, and CSRF tokens fail against AI-enhanced attacks. We also present architectural solutions that combine prompt isolation, runtime security, and privilege separation with novel threat detection capabilities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:33:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13169v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13169v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Inverse Reinforcement Learning Meets Large Language Model Post-Training:
  Basics, Advances, and Opportunities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Sun, Mihaela van der Schaar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:22:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13158v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13158v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on
  Multimodal Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangyu Dong, Haoran Zhao, Jiang Gao, Haozhou Li, Xiaoguang Ma, Yaoming Zhou, Fuhai Chen, Juan Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:13:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13152v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13152v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow
  Generation & Instruct-Masking Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fucai Ke, Vijay Kumar B G, Xingjian Leng, Zhixi Cai, Zaid Khan, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi, Manmohan Chandraker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual reasoning (VR), which is crucial in many fields for enabling human-like visual understanding, remains highly challenging. Recently, compositional visual reasoning approaches, which leverage the reasoning abilities of large language models (LLMs) with integrated tools to solve problems, have shown promise as more effective strategies than end-to-end VR methods. However, these approaches face limitations, as frozen LLMs lack tool awareness in VR, leading to performance bottlenecks. While leveraging LLMs for reasoning is widely used in other domains, they are not directly applicable to VR due to limited training data, imperfect tools that introduce errors and reduce data collection efficiency in VR, and challenging in fine-tuning on noisy workflows. To address these challenges, we propose DWIM: i) Discrepancy-aware training Workflow generation, which assesses tool usage and extracts more viable workflows for training; and ii) Instruct-Masking fine-tuning, which guides the model to only clone effective actions, enabling the generation of more practical solutions. Our experiments demonstrate that DWIM achieves state-of-the-art performance across various VR tasks, exhibiting strong generalization on multiple widely-used datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:10:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.19263v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.19263v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language
  Models on Software Engineering Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pavel Adamenko, Mikhail Ivanov, Aidar Valeev, Rodion Levichev, Pavel Zadorozhny, Ivan Lopatin, Dmitry Babayev, Alena Fenogenova, Valentin Malykh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of Large Language Models (LLMs) in software engineering has revealed critical limitations in existing benchmarks, particularly the widely used SWE-bench dataset. Recent studies have uncovered severe data contamination issues, e.g. SWE-bench reports 32.67% of successful patches involve direct solution leakage and 31.08% pass due to inadequate test cases. We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to address these fundamental challenges through an automated collection of real-world GitHub issues and rigorous quality validation. Our approach implements a reliable pipeline that ensures quality while minimizing contamination risks, resulting in approximately 10,000 potential tasks with 300 samples currently available. Evaluation using the Aider coding agent demonstrates strong discriminative power in state-of-the-art models. We report performance across a dozen recent LLMs evaluated on tasks collected between September 2024 and June 2025.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:04:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11059v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11059v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and
  Intention-Driven Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kuiyuan Ding, Caili Guo, Yang Yang, Jianzhang Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sixth generation (6G) networks demand tight integration of artificial intelligence (AI) into radio access networks (RANs) to meet stringent quality of service (QoS) and resource efficiency requirements. Existing solutions struggle to bridge the gap between high level user intents and the low level, parameterized configurations required for optimal performance. To address this challenge, we propose RIDAS, a multi agent framework composed of representation driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open interface with tunable control parameters (rank and quantization bits, enabling explicit trade) offs between distortion and transmission rate. The IDA employs a two stage planning scheme (bandwidth pre allocation and reallocation) driven by a large language model (LLM) to map user intents and system state into optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\% more users than WirelessAgent under equivalent QoS constraints. These results validate ability of RIDAS to capture user intent and allocate resources more efficiently in AI RAN environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:02:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13140v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13140v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Assessing the Reliability of LLMs Annotations in the Context of
  Demographic Bias and Model Explanation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hadi Mohammadi, Tina Shahedi, Pablo Mosteiro, Massimo Poesio, Ayoub Bagheri, Anastasia Giachanou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the sources of variability in annotations is crucial for developing fair NLP systems, especially for tasks like sexism detection where demographic bias is a concern. This study investigates the extent to which annotator demographic features influence labeling decisions compared to text content. Using a Generalized Linear Mixed Model, we quantify this inf luence, finding that while statistically present, demographic factors account for a minor fraction ( 8%) of the observed variance, with tweet content being the dominant factor. We then assess the reliability of Generative AI (GenAI) models as annotators, specifically evaluating if guiding them with demographic personas improves alignment with human judgments. Our results indicate that simplistic persona prompting often fails to enhance, and sometimes degrades, performance compared to baseline models. Furthermore, explainable AI (XAI) techniques reveal that model predictions rely heavily on content-specific tokens related to sexism, rather than correlates of demographic characteristics. We argue that focusing on content-driven explanations and robust annotation protocols offers a more reliable path towards fairness than potentially persona simulation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:00:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13138v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13138v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 PINT: Physics-Informed Neural Time Series Models with Applications to
  Long-term Inference on WeatherBench 2m-Temperature Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keonvin Park, Jisu Kim, Jaemin Seo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces PINT (Physics-Informed Neural Time Series Models), a framework that integrates physical constraints into neural time series models to improve their ability to capture complex dynamics. We apply PINT to the ERA5 WeatherBench dataset, focusing on long-term forecasting of 2m-temperature data. PINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed prior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures. This equation's analytical solutions (sine and cosine functions) facilitate rigorous evaluation of the benefits of incorporating physics-informed constraints. By benchmarking against a linear regression baseline derived from its exact solutions, we quantify the impact of embedding physical principles in data-driven models. Unlike traditional time series models that rely on future observations, PINT is designed for practical forecasting. Using only the first 90 days of observed data, it iteratively predicts the next two years, addressing challenges posed by limited real-time updates. Experiments on the WeatherBench dataset demonstrate PINT's ability to generalize, capture periodic trends, and align with physical principles. This study highlights the potential of physics-informed neural models in bridging machine learning and interpretable climate applications.   Our models and datasets are publicly available on GitHub: https://github.com/KV-Park.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:44:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.04018v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.04018v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Detecting LLM-generated Code with Subtle Modification by Adversarial
  Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Yin, Xinrui Li, Chao Ni, Xiaodan Xu, Xiaohu Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapid development of Large Language Models (LLMs), their powerful code-generation capabilities have been widely applied in tasks like code completion and automated development, demonstrating the value of improving coding efficiency. However, the extensive use of LLM-generated code also raises several new challenges. On the one hand, issues such as the regulation of code provenance, copyright disputes, and code quality have become increasingly concerning. How to effectively detect LLM-generated code and ensure its compliant and responsible use has become a critical and urgent issue. On the other hand, in practical applications, LLM-generated code is often subject to manual modifications, such as variable renaming or structural adjustments. Although some recent studies have proposed training-based and zero-shot methods for detecting LLM-generated code, these approaches show insufficient robustness when facing modified LLM-generated code, and there is a lack of an effective solution. To address the real-world scenario where LLM-generated code may undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of CodeGPTSensor, which employs adversarial training to improve robustness against input perturbations. CodeGPTSensor+ integrates an adversarial sample generation module, Multi-objective Identifier and Structure Transformation (MIST), which systematically generates both high-quality and representative adversarial samples. This module effectively enhances the model's resistance against diverse adversarial attacks. Experimental results on the HMCorp dataset demonstrate that CodeGPTSensor+ significantly improves detection accuracy on the adversarial test set while maintaining high accuracy on the original test set, showcasing superior robustness compared to CodeGPTSensor.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:38:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13123v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13123v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Inferring Attributed Grammars from Parser Implementations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andreas Pointner, Josef Pichler, Herbert Prähofer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Software systems that process structured inputs often lack complete and up-to-date specifications, which specify the input syntax and the semantics of input processing. While grammar mining techniques have focused on recovering syntactic structures, the semantics of input processing remains largely unexplored. In this work, we introduce a novel approach for inferring attributed grammars from parser implementations. Given an input grammar, our technique dynamically analyzes the implementation of recursive descent parsers to reconstruct the semantic aspects of input handling, resulting in specifications in the form of attributed grammars. By observing program executions and mapping the program's runtime behavior to the grammar, we systematically extract and embed semantic actions into the grammar rules. This enables comprehensive specification recovery. We demonstrate the feasibility of our approach using an initial set of programs, showing that it can accurately reproduce program behavior through the generated attributed grammars.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:32:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13117v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13117v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Task-Circuit Quantization: Leveraging Knowledge Localization and
  Interpretability for Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanqi Xiao, Yi-Lin Sung, Elias Stengel-Eskin, Mohit Bansal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Post-training quantization (PTQ) reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low 2- to 3-bit settings. We develop a new mixed-precision PTQ approach, Task-Circuit Quantization (TaCQ), that draws parallels to automated circuit discovery, directly conditioning the quantization process on specific weight circuits -- which we define as sets of weights associated with downstream task performance. These weights are kept as 16-bit weights, while others are quantized, maintaining performance while only adding a marginal memory cost. Specifically, TaCQ contrasts unquantized model weights with a uniformly-quantized model to estimate the expected change in weights due to quantization and uses gradient information to predict the resulting impact on task performance, allowing us to preserve task-specific weights. We compare TaCQ-based quantization to existing mixed-precision quantization methods when conditioning both on general-purpose and task-specific data. Across QA, math reasoning, and text-to-SQL tasks for both Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the same calibration data and a lower weight budget, achieving major improvements in the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of Llama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25% absolute improvement over SPQR. We also observe consistently large gains over existing methods in the 2-bit regime, with an average gain of 14.74% over the strongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without conditioning on specific tasks, showing TaCQ's ability to identify important weights is not limited to task-conditioned settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:24:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.07389v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.07389v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept
  Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaohan Guo, Yusong Cai, Zejia Liu, Zhengning Wang, Lili Pan, Hongliang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Enabling large-scale generative models to continuously learn new visual concepts is essential for personalizing pre-trained models to meet individual user preferences. Existing approaches for continual visual concept learning are constrained by two fundamental challenges: catastrophic forgetting and parameter expansion. In this paper, we propose Redundancy-Removal Mixture of Experts (R^2MoE), a parameter-efficient framework for lifelong visual concept learning that effectively learns new concepts while incurring minimal parameter overhead. Our framework includes three key innovative contributions: First, we propose a mixture-of-experts framework with a routing distillation mechanism that enables experts to acquire concept-specific knowledge while preserving the gating network's routing capability, thereby effectively mitigating catastrophic forgetting. Second, we propose a strategy for eliminating redundant layer-wise experts that reduces the number of expert parameters by fully utilizing previously learned experts. Third, we employ a hierarchical local attention-guided inference approach to mitigate interference between generated visual concepts. Extensive experiments have demonstrated that our method generates images with superior conceptual fidelity compared to the state-of-the-art (SOTA) method, achieving an impressive 87.8\% reduction in forgetting rates and 63.3\% fewer parameters on the CustomConcept 101 dataset. Our code is available at {https://github.com/learninginvision/R2MoE}
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:22:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13107v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13107v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated
  Summaries For Scientific Abstracts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Marc Brinner, Sina Zarriess
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce SemCSE, an unsupervised method for learning semantic embeddings of scientific texts. Building on recent advances in contrastive learning for text embeddings, our approach leverages LLM-generated summaries of scientific abstracts to train a model that positions semantically related summaries closer together in the embedding space. This resulting objective ensures that the model captures the true semantic content of a text, in contrast to traditional citation-based approaches that do not necessarily reflect semantic similarity. To validate this, we propose a novel benchmark designed to assess a model's ability to understand and encode the semantic content of scientific texts, demonstrating that our method enforces a stronger semantic separation within the embedding space. Additionally, we evaluate SemCSE on the comprehensive SciRepEval benchmark for scientific text embeddings, where it achieves state-of-the-art performance among models of its size, thus highlighting the benefits of a semantically focused training approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:19:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13105v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13105v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level
  Length Control</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingyang He, Xiao Ling, Jie Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large reasoning models (LRMs) have exhibited remarkable reasoning capabilities through inference-time scaling, but this progress has also introduced considerable redundancy and inefficiency into their reasoning processes, resulting in substantial computational waste. Previous work has attempted to mitigate this issue by penalizing the overall length of generated samples during reinforcement learning (RL), with the goal of encouraging a more concise chains of thought. However, we observe that such global length penalty often lead to excessive compression of critical reasoning steps while preserving unnecessary details in simpler ones, yielding a suboptimal trade-off between accuracy and efficiency. To address this issue, we propose SmartThinker, a two-stage learnable framework designed to enable fine-grained control over the length of reasoning chains based on the importance of each individual step. In the first stage, SmartThinker adapts a reasoning model to a short-form reasoning mode through rejection sampling combined with supervised fine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length Control Policy Optimization (SCPO) to refine the model output distribution, which increases the proportion of length allocated to critical steps while reducing redundancy in less important ones. SCPO consists of four core components: an online importance estimator, a step-level length control reward function, a step-level generalized advantage estimation (S-GAE) and a difficulty-adaptive clipping strategy. Working in concert, these components enable SCPO to implement differentiated length control across reasoning steps. Empirical results across multiple reasoning benchmarks and various backbone models demonstrate that SmartThinker significantly reduces redundant reasoning while achieving comparable or even superior performance to existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:07:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04348v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04348v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in
  Dynamic Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheng Jia, Shengbin Yue, Wei Chen, Siyuan Wang, Yidong Liu, Yun Song, Zhongyu Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The gap between static benchmarks and the dynamic nature of real-world legal practice poses a key barrier to advancing legal intelligence. To this end, we introduce J1-ENVS, the first interactive and dynamic legal environment tailored for LLM-based agents. Guided by legal experts, it comprises six representative scenarios from Chinese legal practices across three levels of environmental complexity. We further introduce J1-EVAL, a fine-grained evaluation framework, designed to assess both task performance and procedural compliance across varying levels of legal proficiency. Extensive experiments on 17 LLM agents reveal that, while many models demonstrate solid legal knowledge, they struggle with procedural execution in dynamic settings. Even the SOTA model, GPT-4o, falls short of 60% overall performance. These findings highlight persistent challenges in achieving dynamic legal intelligence and offer valuable insights to guide future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:02:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04037v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04037v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 MERA Code: A Unified Framework for Evaluating Code Generation Across
  Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Artem Chervyakov, Alexander Kharitonov, Pavel Zadorozhny, Adamenko Pavel, Rodion Levichev, Dmitrii Vorobev, Dmitrii Salikhov, Aidar Valeev, Alena Pestova, Maria Dziuba, Ilseyar Alimova, Artem Zavgorodnev, Aleksandr Medvedev, Stanislav Moiseev, Elena Bruches, Daniil Grebenkin, Roman Derunets, Vikulov Vladimir, Anton Emelyanov, Dmitrii Babaev, Vladimir V. Ivanov, Valentin Malykh, Alena Fenogenova
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Advancements in LLMs have enhanced task automation in software engineering; however, current evaluations primarily focus on natural language tasks, overlooking code quality. Most benchmarks prioritize high-level reasoning over executable code and real-world performance, leaving gaps in understanding true capabilities and risks associated with these models in production. To address this issue, we propose MERA Code, a new addition to the MERA benchmark family, specifically focused on evaluating code for the latest code generation LLMs in Russian. This benchmark includes 11 evaluation tasks that span 8 programming languages. Our proposed evaluation methodology features a taxonomy that outlines the practical coding skills necessary for models to complete these tasks. The benchmark comprises an open-source codebase for users to conduct MERA assessments, a scoring system compatible with various programming environments, and a platform featuring a leaderboard and submission system. We evaluate open LLMs and frontier API models, analyzing their limitations in terms of practical coding tasks in non-English languages. We are publicly releasing MERA to guide future research, anticipate groundbreaking features in model development, and standardize evaluation procedures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:55:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12284v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12284v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Human Demonstrations are Generalizable Knowledge for Robots</h2>
                <div class="authors">
                    <strong>Authors:</strong> Te Cui, Tianxing Zhou, Zicai Peng, Mengxiao Hu, Haoyang Lu, Haizhou Li, Guangyan Chen, Meiling Wang, Yufeng Yue
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning from human demonstrations is an emerging trend for designing intelligent robotic systems. However, previous methods typically regard videos as instructions, simply dividing them into action sequences for robotic repetition, which poses obstacles to generalization to diverse tasks or object instances. In this paper, we propose a different perspective, considering human demonstration videos not as mere instructions, but as a source of knowledge for robots. Motivated by this perspective and the remarkable comprehension and generalization capabilities exhibited by large language models (LLMs), we propose DigKnow, a method that DIstills Generalizable KNOWledge with a hierarchical structure. Specifically, DigKnow begins by converting human demonstration video frames into observation knowledge. This knowledge is then subjected to analysis to extract human action knowledge and further distilled into pattern knowledge compassing task and object instances, resulting in the acquisition of generalizable knowledge with a hierarchical structure. In settings with different tasks or object instances, DigKnow retrieves relevant knowledge for the current task and object instances. Subsequently, the LLM-based planner conducts planning based on the retrieved knowledge, and the policy executes actions in line with the plan to achieve the designated task. Utilizing the retrieved knowledge, we validate and rectify planning and execution outcomes, resulting in a substantial enhancement of the success rate. Experimental results across a range of tasks and scenes demonstrate the effectiveness of this approach in facilitating real-world robots to accomplish tasks with the knowledge derived from human demonstrations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:34:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.02419v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.02419v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Write Your Own CodeChecker: An Automated Test-Driven Checker Development
  Approach with LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jun Liu, Yuanyuan Xie, Jiwei Yan, Jinhao Huang, Jun Yan, Jian Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rising demand for code quality assurance, developers are not only utilizing existing static code checkers but also seeking custom checkers to satisfy their specific needs. Nowadays, various code-checking frameworks provide extensive checker customization interfaces to meet this need. However, both the abstract checking logic and the complex API usage of large-scale checker frameworks make this task challenging. To this end, automated code checker generation is anticipated to ease the burden of checker development. In this paper, we propose AutoChecker, an innovative LLM-powered approach that can write code checkers automatically based on only a rule description and a test suite. To achieve comprehensive checking logic, AutoChecker incrementally updates the checker's logic by focusing on solving one selected case each time. To obtain precise API knowledge, during each iteration, it leverages fine-grained logic-guided API-context retrieval, where it first decomposes the checking logic into a series of sub-operations and then retrieves checker-related API-contexts for each sub-operation. For evaluation, we apply AutoChecker, five baselines, and three ablation methods using multiple LLMs to generate checkers for 20 randomly selected PMD rules. Experimental results show that AutoChecker significantly outperforms others across all effectiveness metrics, with an average test pass rate of 82.28%. Additionally, the checkers generated by AutoChecker can be successfully applied to real-world projects, matching the performance of official checkers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:33:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.06796v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.06796v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient
  Communication</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianyu Song, Feng Li, Yuan Bi, Angelos Karlas, Amir Yousefi, Daniela Branzan, Zhongliang Jiang, Ulrich Eck, Nassir Navab
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:25:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13052v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13052v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Gravitational Lensing by Black Holes in Einstein-nonlinear
  Electrodynamic Theories with Multiple Photon Spheres</h2>
                <div class="authors">
                    <strong>Authors:</strong> Siyuan Hui, Benrong Mu, Peng Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we study the gravitational lensing effects of non-linear electrodynamic black holes. Non-linear electrodynamic black holes serve as typical models for multi-event horizon black holes. Depending on the choice of metric parameters, these black holes can possess more than five event horizons. Consequently, within certain parameter ranges, black holes can have more than three photon spheres of varying sizes outside the event horizon. Specifically, we focus on the strong gravitational lensing effects near the triple photon spheres, particularly the formation of higher-order images of point sources and celestial spheres. The presence of one, two, or three or more photon spheres significantly increases the number of higher-order images of a point source. When a black hole is illuminated by a celestial sphere, the three photon spheres generate three critical curves in the black hole image, with the smallest critical curve coinciding with the shadow's edge. Additionally, since non-linear electrodynamic black holes are models of multi-event horizon black holes, we can infer the gravitational lensing effects and the changes in celestial images for black holes with more than three photon spheres by analyzing the distinctions and patterns between the gravitational lensing effects of one, two, and three photon spheres.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:21:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13048v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13048v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent
  Debate Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu Cui, Hongyang Du
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-agent debate (MAD) systems leverage collaborative interactions among large language models (LLMs) agents to improve reasoning capabilities. While recent studies have focused on increasing the accuracy and scalability of MAD systems, their security vulnerabilities have received limited attention. In this work, we introduce MAD-Spear, a targeted prompt injection attack that compromises a small subset of agents but significantly disrupts the overall MAD process. Manipulated agents produce multiple plausible yet incorrect responses, exploiting LLMs' conformity tendencies to propagate misinformation and degrade consensus quality. Furthermore, the attack can be composed with other strategies, such as communication attacks, to further amplify its impact by increasing the exposure of agents to incorrect responses. To assess MAD's resilience under attack, we propose a formal definition of MAD fault-tolerance and develop a comprehensive evaluation framework that jointly considers accuracy, consensus efficiency, and scalability. Extensive experiments on five benchmark datasets with varying difficulty levels demonstrate that MAD-Spear consistently outperforms the baseline attack in degrading system performance. Additionally, we observe that agent diversity substantially improves MAD performance in mathematical reasoning tasks, which challenges prior work suggesting that agent diversity has minimal impact on performance. These findings highlight the urgent need to improve the security in MAD design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:09:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13038v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13038v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image
  Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Xin, Le Zhuo, Qi Qin, Siqi Luo, Yuewen Cao, Bin Fu, Yangfan He, Hongsheng Li, Guangtao Zhai, Xiaohong Liu, Peng Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AutoRegressive (AR) models have made notable progress in image generation, with Masked AutoRegressive (MAR) models gaining attention for their efficient parallel decoding. However, MAR models have traditionally underperformed when compared to standard AR models. This study refines the MAR architecture to improve image generation quality. We begin by evaluating various image tokenizers to identify the most effective one. Subsequently, we introduce an improved Bidirectional LLaMA architecture by replacing causal attention with bidirectional attention and incorporating 2D RoPE, which together form our advanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves a FID score of 3.71, matching state-of-the-art AR models in the ImageNet 256x256 benchmark, while requiring only 8 inference steps compared to the 256 steps of AR models. Furthermore, we develop a text-driven MaskGIL model with 775M parameters for generating images from text at various resolutions. Beyond image generation, MaskGIL extends to accelerate AR-based generation and enable real-time speech-to-image conversion. Our codes and models are available at https://github.com/synbol/MaskGIL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:02:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13032v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13032v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 When Pattern-by-Pattern Works: Theoretical and Empirical Insights for
  Logistic Models with Missing Values</h2>
                <div class="authors">
                    <strong>Authors:</strong> Christophe Muller, Erwan Scornet, Julie Josse
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Predicting a response with partially missing inputs remains a challenging task even in parametric models, since parameter estimation in itself is not sufficient to predict on partially observed inputs. Several works study prediction in linear models. In this paper, we focus on logistic models, which present their own difficulties. From a theoretical perspective, we prove that a Pattern-by-Pattern strategy (PbP), which learns one logistic model per missingness pattern, accurately approximates Bayes probabilities in various missing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare various methods (constant and iterative imputations, complete case analysis, PbP, and an EM algorithm) across classification, probability estimation, calibration, and parameter inference. Our analysis provides a comprehensive view on the logistic regression with missing values. It reveals that mean imputation can be used as baseline for low sample sizes, and improved performance is obtained via nonlinear multiple iterative imputation techniques with the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for Gaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear features.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T11:52:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13024v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13024v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Rethinking the Embodied Gap in Vision-and-Language Navigation: A
  Holistic Study of Physical and Visual Disparities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen, Jiangmiao Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T11:46:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13019v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13019v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Eigen-inference by Marchenko-Pastur inversion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ben Deitmar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A new method of estimating population linear spectral statistics from high-dimensional data is introduced. When the dimension $d$ grows with the sample size $n$ such that $\frac{d}{n} \rightarrow c>0$, the introduced method is the first to provably achieve eigen-inference with fast convergence rates of $\mathcal{O}(n^{\varepsilon-1})$ for any $\varepsilon > 0$ in the general non-parametric setting. This is achieved though a novel Marchenko-Pastur inversion formula, which may also be formulated as a semi-explicit solution to the Marchenko-Pastur equation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T11:36:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.03390v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.03390v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Scalable Bernoulli factories for Bayesian inference with intractable
  likelihoods</h2>
                <div class="authors">
                    <strong>Authors:</strong> Timothée Stumpf-Fétizon, Flávio B. Gonçalves
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Bernoulli factory MCMC algorithms implement accept-reject Markov chains without explicit computation of acceptance probabilities, and are used to target posterior distributions associated with intractable likelihood models. Intractable likelihoods naturally arise in continuous-time models and mixture distributions, or from the marginalisation of a tractable augmented model. Bernoulli factory MCMC algorithms often mix better than alternatives that target a tractable augmented posterior. However, for a likelihood that factorizes over observations, we show that their computational performance typically deteriorates exponentially with data size. To address this, we propose a simple divide-and-conquer Bernoulli factory MCMC algorithm and prove that it has polynomial complexity of degree between 1 and 2, with the exact degree depending on the existence of efficient unbiased estimators of the intractable likelihood ratio. We demonstrate the effectiveness of our approach with applications to Bayesian inference in two intractable likelihood models, and observe respective polynomial cost of degree 1.2 and 1 in the data size.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T11:22:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.CO</span><span>62-08</span><span>G.3</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.05438v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.05438v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Simultaneously Constraining the Neutron Star Equation of State and Mass
  Distribution through Multimessenger Observations and Nuclear Benchmarks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bhaskar Biswas, Stephan Rosswog
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With ongoing advancements in nuclear theory and experimentation, together with a growing body of neutron star (NS) observations, a wealth of information on the equation of state (EOS) for matter at extreme densities has become accessible. Here, we utilize a hybrid EOS formulation that combines an empirical parameterization centered around the nuclear saturation density with a generic three-segment piecewise polytrope model at higher densities. We incorporate data derived from chiral effective field theory ($\chi$EFT), perturbative quantum chromodynamics (pQCD), and from experiments such as PREX-II and CREX. Furthermore, we examine the influence of a total of 129 NS mass measurements up to April 2023, as well as simultaneous mass and radius measurements derived from the X-ray emission from surface hot spots on NSs. Additionally, we consider constraints on tidal properties inferred from the gravitational waves emitted by coalescing NS binaries. To integrate this extensive and varied array of constraints, we utilize a hierarchical Bayesian statistical framework to simultaneously deduce the EOS and the distribution of NS masses. We find that incorporating data from $\chi$EFT significantly tightens the constraints on the EOS of NSs near or below the nuclear saturation density. However, constraints derived from pQCD computations and nuclear experiments such as PREX-II and CREX have minimal impact.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T11:07:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span><span>gr-qc</span><span>nucl-th</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.15192v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.15192v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Teach Old SAEs New Domain Tricks with Boosting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nikita Koriagin, Yaroslav Aksenov, Daniil Laptev, Gleb Gerasimov, Nikita Balagansky, Daniil Gavrilov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T10:57:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12990v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12990v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Robustness Requirement Coverage using a Situation Coverage Approach for
  Vision-based AI Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sepeedeh Shahbeigi, Nawshin Mannan Proma, Victoria Hodge, Richard Hawkins, Boda Li, Valentina Donzella
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI-based robots and vehicles are expected to operate safely in complex and dynamic environments, even in the presence of component degradation. In such systems, perception relies on sensors such as cameras to capture environmental data, which is then processed by AI models to support decision-making. However, degradation in sensor performance directly impacts input data quality and can impair AI inference. Specifying safety requirements for all possible sensor degradation scenarios leads to unmanageable complexity and inevitable gaps. In this position paper, we present a novel framework that integrates camera noise factor identification with situation coverage analysis to systematically elicit robustness-related safety requirements for AI-based perception systems. We focus specifically on camera degradation in the automotive domain. Building on an existing framework for identifying degradation modes, we propose involving domain, sensor, and safety experts, and incorporating Operational Design Domain specifications to extend the degradation model by incorporating noise factors relevant to AI performance. Situation coverage analysis is then applied to identify representative operational contexts. This work marks an initial step toward integrating noise factor analysis and situational coverage to support principled formulation and completeness assessment of robustness requirements for camera-based AI perception.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T10:46:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12986v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12986v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with
  Multiple Steps</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maximiliano Hormazábal Lagos, Álvaro Bueno Sáez, Héctor Cerezo-Costas, Pedro Alonso Doval, Jorge Alcalde Vesteiro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in Spanish). Our solution obtains answers to the questions by implementing Python code generation with LLMs that is used to filter and process the table. This solution evolves from the MRT implementation for the Semeval 2025 related task. The process consists of multiple steps: analyzing and understanding the content of the table, selecting the useful columns, generating instructions in natural language, translating these instructions to code, running it, and handling potential errors or exceptions. These steps use open-source LLMs and fine-grained optimized prompts for each step. With this approach, we achieved an accuracy score of 85\% in the task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T10:33:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12981v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12981v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Can Large Language Models Predict Audio Effects Parameters from Natural
  Language?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seungheon Doh, Junghyun Koo, Marco A. Martínez-Ramírez, Wei-Hsiang Liao, Juhan Nam, Yuki Mitsufuji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In music production, manipulating audio effects (Fx) parameters through natural language has the potential to reduce technical barriers for non-experts. We present LLM2Fx, a framework leveraging Large Language Models (LLMs) to predict Fx parameters directly from textual descriptions without requiring task-specific training or fine-tuning. Our approach address the text-to-effect parameter prediction (Text2Fx) task by mapping natural language descriptions to the corresponding Fx parameters for equalization and reverberation. We demonstrate that LLMs can generate Fx parameters in a zero-shot manner that elucidates the relationship between timbre semantics and audio effects in music production. To enhance performance, we introduce three types of in-context examples: audio Digital Signal Processing (DSP) features, DSP function code, and few-shot examples. Our results demonstrate that LLM-based Fx parameter generation outperforms previous optimization approaches, offering competitive performance in translating natural language descriptions to appropriate Fx settings. Furthermore, LLMs can serve as text-driven interfaces for audio production, paving the way for more intuitive and accessible music production tools.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T10:10:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.MM</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.20770v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.20770v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 New {\em ab initio} constrained extended Skyrme equations of state for
  simulations of neutron stars, supernovae and binary mergers: I. Subsaturation
  density domain</h2>
                <div class="authors">
                    <strong>Authors:</strong> Adriana R. Raduta, Mikhail V. Beznogov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In numerical simulations of core-collapse supernova and binary neutron stars mergers, information about the energetics and composition of matter is implemented via external tables covering the huge ranges of thermodynamic conditions explored during the astrophysical evolution. More than 120 general-purpose equation of state (EOS) tables have been contributed so far. Unfortunately, not all of them comply with current constraints from theoretical and experimental nuclear physics and astrophysical observations of neutron stars. Systematic investigations of the role that dense matter properties play in the evolution of these astrophysical phenomena require that more EOS tables are provided. We build a set of general-purpose EOS tables. At zero temperature, they comply with all currently accepted constraints, including {\em ab initio} chiral effective field theory calculations of pure neutron matter. This set is designed to explore a wide variety of the behaviors of the effective masses as functions of density, which is reflected into a wide range of thermal behaviors. We employ Brussels extended Skyrme interactions generated by means of Bayesian inference techniques. An extended nuclear statistical equilibrium model is developed for modeling sub-saturated inhomogeneous nuclear matter (NM). Here, we study the properties of sub-saturated inhomogeneous NM over wide ranges of density, temperature, and proton fraction. We analyze in detail the mechanisms of transition to homogeneous matter and estimate the transition density. Our key results include a thick layer of neutron rich isotopes of He or H in the inner crusts of neo-neutron stars, significant abundance of exotic isotopes of H and He in warm and neutron-rich matter and a detailed study of the thermodynamic stability of cold stellar matter. The EOS tables are publicly available in the \textsc{CompOSE} online database.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:56:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>nucl-th</span><span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.21725v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.21725v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Probabilistic Soundness Guarantees in LLM Reasoning Chains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiqiu You, Anton Xue, Shreya Havaldar, Delip Rao, Helen Jin, Chris Callison-Burch, Eric Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In reasoning chains generated by large language models (LLMs), initial errors often propagate and undermine the reliability of the final conclusion. Current LLM-based error detection methods often fail to detect propagated errors because they do not properly account for how earlier errors might corrupt judgments of downstream reasoning. To better detect such propagated errors, we introduce Autoregressive Reasoning Entailment Stability (ARES), a novel probabilistic framework that prevents error propagation by judging each claim based only on previously-assessed sound premises. This inductive method yields a nuanced score for each step and provides certified statistical guarantees of its soundness, rather than a brittle binary label. ARES achieves state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2 points) and demonstrates superior robustness on very long synthetic reasoning chains, where it excels at detecting propagated errors (90.3% F1, +27.6 points).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:40:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12948v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12948v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 OASIS: Order-Augmented Strategy for Improved Code Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zuchen Gao, Zizheng Zhan, Xianming Li, Erxin Yu, Ziqi Zhan, Haotian Zhang, Bin Chen, Yuqun Zhang, Jing Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Code embeddings capture the semantic representations of code and are crucial for various code-related large language model (LLM) applications, such as code search. Previous training primarily relies on optimizing the InfoNCE loss by comparing positive natural language (NL)-code pairs with in-batch negatives. However, due to the sparse nature of code contexts, training solely by comparing the major differences between positive and negative pairs may fail to capture deeper semantic nuances. To address this issue, we propose a novel order-augmented strategy for improved code search (OASIS). It leverages order-based similarity labels to train models to capture subtle differences in similarity among negative pairs. Extensive benchmark evaluations demonstrate that our OASIS model significantly outperforms previous state-of-the-art models focusing solely on major positive-negative differences. It underscores the value of exploiting subtle differences among negative pairs with order labels for effective code embedding training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:34:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.08161v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.08161v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Disorder-induced spin excitation continuum and spin-glass ground state
  in the inverse spinel CuGa$_2$O$_4$</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhentao Huang, Zhijun Xu, Shuaiwei Li, Qingchen Duan, Junbo Liao, Song Bao, Yanyan Shangguan, Bo Zhang, Hao Xu, Shufan Cheng, Zihang Song, Shuai Dong, Maofeng Wu, M. B. Stone, Yiming Qiu, Ruidan Zhong, Guangyong Xu, Zhen Ma, G. D. Gu, J. M. Tranquada, Jinsheng Wen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Spinel-structured compounds serve as prototypical examples of highly frustrated systems, and are promising candidates for realizing the long-sought quantum spin liquid (QSL) state. However, structural disorder is inevitable in many real QSL candidates and its impact remains a topic of intense debate. In this work, we conduct comprehensive investigations on CuGa$_2$O$_4$, a spinel compound with significant structural disorder, focusing on its thermodynamic properties and spectroscopic behaviors. No long-range magnetic order is observed down to $\sim$80 mK, as evidenced by magnetic susceptibility, specific heat and elastic neutron scattering measurements. More intriguingly, inelastic neutron scattering experiments reveal a broad gapless continuum of magnetic excitations around the Brillouin zone boundary, resembling the magnetic excitation spectra expected for a QSL. Nevertheless, a spin-freezing transition at $T_{\rm{f}} \approx $ 0.88 K is identified from the cusp in the dc susceptibility curves, where a bifurcation between zero-field-cooling and field-cooling curves occurs. Furthermore, ac susceptibility measurements show a peak close to $T_{\rm{f}}$ at low frequency, which shifts to higher temperature with increasing frequency. These results are evident that CuGa$_2$O$_4$ has spin-glass ground state, consistent with the establishment of short-range order inferred from the specific heat measurements. Collectively, these results illustrate the crucial role of disorder in defining the excitation spectrum out of the disordered ground state. Our findings shed light onto the broader class of AB$_2$O$_4$ spinels and advance our understanding of the spin dynamics in magnetically disordered systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:32:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.str-el</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1103/1xtf-wg3q' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.12943v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12943v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Theoretical Radio Signals from Radio-Band Gravitational Waves Converted
  from the Neutron Star Magnetic Field</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Hong, Zhen-Zhao Tao, Peng He, Tong-Jie Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Gravitational waves (GWs) can convert into electromagnetic waves in the presence of a magnetic field via the Gertsenshtein-Zeldovich (GZ) effect. The characteristics of the magnetic field substantially affect this conversion probability. This paper confirms that strong magnetic fields in neutron stars significantly enhance the conversion probability, facilitating detectable radio signatures of very high-frequency (VHF, $\left(10^6-10^{11}\mathrm{~Hz}\right)$) gravitational waves. We theoretically identify two distinct signatures using single-dish telescopes (FAST, TMRT, QTT, GBT) and interferometers (SKA1/2-MID): transient signals from burst-like gravitational wave sources and persistent signals from cosmological background gravitational wave sources. These signatures are mapped to graviton spectral lines derived from quantum field theory by incorporating spin-2 and mass constraints, resulting in smooth, featureless profiles that are critical for distinguishing gravitational wave signals from astrophysical foregrounds. FAST attains a characteristic strain bound of $h_c<10^{-23}$, approaching $10^{-24}$ in the frequency range of $1-3\mathrm{~GHz}$ with a 6-hour observation period. This performance exceeds the $5 \sigma$ detection thresholds for GWs originating from primordial black holes (PBHs) and nears the limits set by Big Bang nucleosynthesis. Additionally, projections for SKA2-MID indicate even greater sensitivity. Detecting such gravitational waves would improve our comprehension of cosmological models, refine the parameter spaces for primordial black holes, and function as a test for quantum field theory. This approach addresses significant deficiencies in VHF GW research, improving detection sensitivity and facilitating the advancement of next-generation radio telescopes such as FASTA and SKA, which feature larger fields of view and enhanced gain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:22:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span><span>astro-ph.CO</span><span>gr-qc</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.05338v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.05338v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov
  Chain Monte Carlo Acceleration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shirui Zhao, Jun Yin, Lingyun Yao, Martin Andraud, Wannes Meert, Marian Verhelst
                </div>
                <div class="summary">
                    <strong>Summary:</strong> An increasing number of applications are exploiting sampling-based algorithms for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC) algorithms form the computational backbone of this emerging branch of machine learning. Unfortunately, the high computational cost limits their feasibility for large-scale problems and real-world applications, and the existing MCMC acceleration solutions are either limited in hardware flexibility or fail to maintain efficiency at the system level across a variety of end-to-end applications. This paper introduces \textbf{MC$^2$A}, an algorithm-hardware co-design framework, enabling efficient and flexible optimization for MCMC acceleration. Firstly, \textbf{MC$^2$A} analyzes the MCMC workload diversity through an extension of the processor performance roofline model with a 3rd dimension to derive the optimal balance between the compute, sampling and memory parameters. Secondly, \textbf{MC$^2$A} proposes a parametrized hardware accelerator architecture with flexible and efficient support of MCMC kernels with a pipeline of ISA-programmable tree-structured processing units, reconfigurable samplers and a crossbar interconnect to support irregular access. Thirdly, the core of \textbf{MC$^2$A} is powered by a novel Gumbel sampler that eliminates exponential and normalization operations. In the end-to-end case study, \textbf{MC$^2$A} achieves an overall {$307.6\times$, $1.4\times$, $2.0\times$, $84.2\times$} speedup compared to the CPU, GPU, TPU and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC workloads, this work demonstrates and exploits the feasibility of general hardware acceleration to popularize MCMC-based solutions in diverse application domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:20:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12935v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12935v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding
  With Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Xu, Chao Zhang, Hanqi Jiang, Xiaoyan Wang, Ruifei Ma, Yiwei Li, Zihao Wu, Zeju Li, Xiangde Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Advancements in foundation models have made it possible to conduct applications in various downstream tasks. Especially, the new era has witnessed a remarkable capability to extend Large Language Models (LLMs) for tackling tasks of 3D scene understanding. Current methods rely heavily on 3D point clouds, but the 3D point cloud reconstruction of an indoor scene often results in information loss. Some textureless planes or repetitive patterns are prone to omission and manifest as voids within the reconstructed 3D point clouds. Besides, objects with complex structures tend to introduce distortion of details caused by misalignments between the captured images and the dense reconstructed point clouds. 2D multi-view images present visual consistency with 3D point clouds and provide more detailed representations of scene components, which can naturally compensate for these deficiencies. Based on these insights, we propose Argus, a novel 3D multimodal framework that leverages multi-view images for enhanced 3D scene understanding with LLMs. In general, Argus can be treated as a 3D Large Multimodal Foundation Model (3D-LMM) since it takes various modalities as input(text instructions, 2D multi-view images, and 3D point clouds) and expands the capability of LLMs to tackle 3D tasks. Argus involves fusing and integrating multi-view images and camera poses into view-as-scene features, which interact with the 3D features to create comprehensive and detailed 3D-aware scene embeddings. Our approach compensates for the information loss while reconstructing 3D point clouds and helps LLMs better understand the 3D world. Extensive experiments demonstrate that our method outperforms existing 3D-LMMs in various downstream tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:02:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/TNNLS.2025.3581411' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.12916v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12916v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 MEM1: Learning to Synergize Memory and Reasoning for Efficient
  Long-Horizon Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, Paul Pu Liang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:53:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.15841v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.15841v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General
  Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual-language Chain-of-Thought (CoT) data resources are relatively scarce compared to text-only counterparts, limiting the improvement of reasoning capabilities in Vision Language Models (VLMs). However, high-quality vision-language reasoning data is expensive and labor-intensive to annotate. To address this issue, we leverage a promising resource: game code, which naturally contains logical structures and state transition processes. Therefore, we propose Code2Logic, a novel game-code-driven approach for multimodal reasoning data synthesis. Our approach leverages Large Language Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning processes and results through code execution. Using the Code2Logic approach, we developed the GameQA dataset to train and evaluate VLMs. GameQA is cost-effective and scalable, offers controllable difficulty gradation and is diverse with 30 games and 158 tasks. Surprisingly, despite training solely on game data, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language benchmarks. Our code, dataset and models are available at https://github.com/tongjingqi/Code2Logic.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:46:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>I.2.7; I.2.10</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13886v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13886v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic
  Chain-of-Thought Synthesis Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoke Zhao, Zhaowen Zhou, Lin Chen, Lihong Wang, Zhiyi Huang, Kaiyuan Zheng, Yanjun Zheng, Xiyang Du, Longfei Liao, Jiawei Liu, Xiang Qi, Bo Zhang, Peng Zhang, Zhe Li, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models (LLMs) have demonstrated remarkable general reasoning capabilities, holding significant potential for applications in the financial domain, a field that requires robust and reliable reasoning. It has been demonstrated that distilling high-quality chain-of-thought (CoT) rationales from advanced general reasoning models offers a promising and efficient path to the financial reasoning model. However, existing CoT synthesis methods suffer from shallow CoT sampling, leaving the question of how to construct a well-designed knowledge space for finance reasoning unexplored. In this paper, we present \textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset characterized by its systematic CoT synthesis optimization. We first introduce a comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and deep financial reasoning trajectories. Furthermore, a systematic investigation, termed CoT Cube, is conducted to analyze critical factors that influence CoT effectiveness, such as necessity, length and synthesizer, yielding valuable insights for high-quality financial CoT construction. Experiments demonstrate that models trained on our Agentar-DeepFinance-300K achieve significant improvements on financial benchmarks. We publicly release Agentar-DeepFinance-300K , hoping to advance the research in financial reasoning models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:40:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12901v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12901v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 IOPO: Empowering LLMs with Complex Instruction Following via
  Input-Output Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, Yongbin Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:39:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.06208v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.06208v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 On the Limitations of Large Language Models (LLMs): False Attribution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tosin Adewumi, Nudrat Habib, Lama Alkhaled, Elisa Barney
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:20:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.04631v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.04631v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Camera-based implicit mind reading by capturing higher-order semantic
  dynamics of human gaze within environmental context</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengke Song, Yuge Xie, Qi Cui, Luming Li, Xinyu Liu, Guotao Wang, Chenglizhao Chen, Shanchen Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emotion recognition,as a step toward mind reading,seeks to infer internal states from external cues.Most existing methods rely on explicit signals-such as facial expressions,speech,or gestures-that reflect only bodily responses and overlook the influence of environmental context.These cues are often voluntary,easy to mask,and insufficient for capturing deeper,implicit emotions. Physiological signal-based approaches offer more direct access to internal states but require complex sensors that compromise natural behavior and limit scalability.Gaze-based methods typically rely on static fixation analysis and fail to capture the rich,dynamic interactions between gaze and the environment,and thus cannot uncover the deep connection between emotion and implicit behavior.To address these limitations,we propose a novel camera-based,user-unaware emotion recognition approach that integrates gaze fixation patterns with environmental semantics and temporal dynamics.Leveraging standard HD cameras,our method unobtrusively captures users'eye appearance and head movements in natural settings-without the need for specialized hardware or active user participation.From these visual cues,the system estimates gaze trajectories over time and space, providing the basis for modeling the spatial, semantic,and temporal dimensions of gaze behavior. This allows us to capture the dynamic interplay between visual attention and the surrounding environment,revealing that emotions are not merely physiological responses but complex outcomes of human-environment interactions.The proposed approach enables user-unaware,real-time,and continuous emotion recognition,offering high generalizability and low deployment cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:17:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12889v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12889v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 A Logically Consistent Chain-of-Thought Approach for Stance Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bowen Zhang, Daijun Ding, Liwen Jing, Hu Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Zero-shot stance detection (ZSSD) aims to detect stances toward unseen targets. Incorporating background knowledge to enhance transferability between seen and unseen targets constitutes the primary approach of ZSSD. However, these methods often struggle with a knowledge-task disconnect and lack logical consistency in their predictions. To address these issues, we introduce a novel approach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which improves stance detection by ensuring relevant and logically sound knowledge extraction. LC-CoT employs a three-step process. Initially, it assesses whether supplementary external knowledge is necessary. Subsequently, it uses API calls to retrieve this knowledge, which can be processed by a separate LLM. Finally, a manual exemplar guides the LLM to infer stance categories, using an if-then logical structure to maintain relevance and logical coherence. This structured approach to eliciting background knowledge enhances the model's capability, outperforming traditional supervised methods without relying on labeled data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:13:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.16054v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.16054v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 VAR-MATH: Probing True Mathematical Reasoning in Large Language Models
  via Symbolic Multi-Instance Benchmarks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jian Yao, Ran Cheng, Kay Chen Tan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of large language models (LLMs), as measured by standard benchmarks. However, these gains often persist even when models are trained with flawed signals, such as random or inverted rewards, raising a fundamental question: do such improvements reflect true reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To address this question, we take an evaluation-centric perspective and identify two critical shortcomings in existing protocols. First, \emph{benchmark contamination} arises from the public availability of test problems, increasing the risk of data leakage. Second, \emph{evaluation fragility} stems from the reliance on single-instance assessments, which are highly sensitive to stochastic outputs and fail to capture reasoning consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic evaluation framework designed to probe genuine reasoning ability. By converting fixed numerical problems into symbolic templates and requiring models to solve multiple instantiations of each, VAR-MATH enforces consistent reasoning across structurally equivalent variants, thereby mitigating contamination and improving evaluation robustness. We apply VAR-MATH to transform two popular benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and VAR-AIME24. Experimental results reveal substantial performance drops for RL-trained models on the variabilized versions, especially for smaller models, with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings suggest that many existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms. Overall, VAR-MATH offers a principled, contamination-resistant evaluation paradigm for mathematical reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:10:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12885v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12885v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced
  Knowledge Boundary Awareness</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junsheng Huang, Zhitao He, Yucheng Huang, Sandeep Polisetty, Qingyun Wang, May Fung
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:03:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.21773v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.21773v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 SEALGuard: Safeguarding the Multilingual Conversations in Southeast
  Asian Languages for LLM Software Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenliang Shan, Michael Fu, Rui Yang, Chakkrit Tantithamthavorn
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''), they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia. This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems. We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases. We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on this benchmark. Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score. Our ablation study further reveals the contributions of adaptation strategies and model size to the overall performance of SEALGuard. We release our pre-trained model and benchmark at https://github.com/awsm-research/SEALGuard to support further research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:01:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08898v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08898v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Bayesian Modeling and Estimation of Linear Time-Variant Systems using
  Neural Networks and Gaussian Processes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yaniv Shulman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The identification of Linear Time-Variant (LTV) systems from input-output data is a fundamental yet challenging ill-posed inverse problem. This work introduces a unified Bayesian framework that models the system's impulse response, $h(t, \tau)$, as a stochastic process. We decompose the response into a posterior mean and a random fluctuation term, a formulation that provides a principled approach for quantifying uncertainty and naturally defines a new, useful system class we term Linear Time-Invariant in Expectation (LTIE). To perform inference, we leverage modern machine learning techniques, including Bayesian neural networks and Gaussian Processes, using scalable variational inference. We demonstrate through a series of experiments that our framework can robustly infer the properties of an LTI system from a single noisy observation, show superior data efficiency compared to classical methods in a simulated ambient noise tomography problem, and successfully track a continuously varying LTV impulse response by using a structured Gaussian Process prior. This work provides a flexible and robust methodology for uncertainty-aware system identification in dynamic environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T07:55:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12878v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12878v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 LLM-Enhanced User-Item Interactions: Leveraging Edge Information for
  Optimized Recommendations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyuan Wang, Liang Wu, Liangjie Hong, Hao Liu, Yanjie Fu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph recommendation methods, representing a connected interaction perspective, reformulate user-item interactions as graphs to leverage graph structure and topology to recommend and have proved practical effectiveness at scale. Large language models, representing a textual generative perspective, excel at modeling user languages, understanding behavioral contexts, capturing user-item semantic relationships, analyzing textual sentiments, and generating coherent and contextually relevant texts as recommendations. However, there is a gap between the connected graph perspective and the text generation perspective as the task formulations are different. A research question arises: how can we effectively integrate the two perspectives for more personalized recsys? To fill this gap, we propose to incorporate graph-edge information into LLMs via prompt and attention innovations. We reformulate recommendations as a probabilistic generative problem using prompts. We develop a framework to incorporate graph edge information from the prompt and attention mechanisms for graph-structured LLM recommendations. We develop a new prompt design that brings in both first-order and second-order graph relationships; we devise an improved LLM attention mechanism to embed direct the spatial and connectivity information of edges. Our evaluation of real-world datasets demonstrates the framework's ability to understand connectivity information in graph data and to improve the relevance and quality of recommendation results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T07:51:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.09617v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.09617v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task
  Demonstration Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rahel Rickenbach, Bruce Lee, René Zurbrügg, Carmen Amo Alonso, Melanie N. Zeilinger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of large language models (LLMs) with control systems has demonstrated significant potential in various settings, such as task completion with a robotic manipulator. A main reason for this success is the ability of LLMs to perform in-context learning, which, however, strongly relies on the design of task examples, closely related to the target tasks. Consequently, employing LLMs to formulate optimal control problems often requires task examples that contain explicit mathematical expressions, designed by trained engineers. Furthermore, there is often no principled way to evaluate for hallucination before task execution. To address these challenges, we propose DEMONSTRATE, a novel methodology that avoids the use of LLMs for complex optimization problem generations, and instead only relies on the embedding representations of task descriptions. To do this, we leverage tools from inverse optimal control to replace in-context prompt examples with task demonstrations, as well as the concept of multitask learning, which ensures target and example task similarity by construction. Given the fact that hardware demonstrations can easily be collected using teleoperation or guidance of the robot, our approach significantly reduces the reliance on engineering expertise for designing in-context examples. Furthermore, the enforced multitask structure enables learning from few demonstrations and assessment of hallucinations prior to task execution. We demonstrate the effectiveness of our method through simulation and hardware experiments involving a robotic arm tasked with tabletop manipulation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T07:26:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12855v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12855v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Formalising causal inference as prediction on a target population</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benedikt Höltgen, Robert C. Williamson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The standard approach to causal modelling especially in social and health sciences is the potential outcomes framework due to Neyman and Rubin. In this framework, observations are thought to be drawn from a distribution over variables of interest, and the goal is to identify parameters of this distribution. Even though the stated goal is often to inform decision making on some target population, there is no straightforward way to include these target populations in the framework. Instead of modelling the relationship between the observed sample and the target population, the inductive assumptions in this framework take the form of abstract sampling and independence assumptions. In this paper, we develop a version of this framework that construes causal inference as treatment-wise predictions for finite populations where all assumptions are testable in retrospect; this means that one can not only test predictions themselves (without any fundamental problem) but also investigate sources of error when they fail. Due to close connections to the original framework, established methods can still be be analysed under the new framework.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T07:10:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>cs.LG</span><span>econ.EM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.17385v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.17385v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better
  Understand Public Concerns about Vaccines</h2>
                <div class="authors">
                    <strong>Authors:</strong> Muhammad Javed, Sedigh Khademi Habibabadi, Christopher Palmer, Hazel Clothier, Jim Buttery, Gerardo Luis Dimaguila
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vaccine hesitancy threatens public health, leading to delayed or rejected vaccines. Social media is a vital source for understanding public concerns, and traditional methods like topic modelling often struggle to capture nuanced opinions. Though trained for query answering, large Language Models (LLMs) often miss current events and community concerns. Additionally, hallucinations in LLMs can compromise public health communication. To address these limitations, we developed a tool (VaxPulse Query Corner) using the Retrieval Augmented Generation technique. It addresses complex queries about public vaccine concerns on various online platforms, aiding public health administrators and stakeholders in understanding public concerns and implementing targeted interventions to boost vaccine confidence. Analysing 35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and relevance (0.94).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T06:59:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.LG</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12840v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12840v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Does $K$-fold CV based penalty perform variable selection or does it
  lead to $n^{1/2}$-consistency in Lasso?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mayukh Choudhury, Debraj Das
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Least absolute shrinkage and selection operator or Lasso, introduced by Tibshirani (1996), is one of the widely used regularization methods in regression. It is observed that the properties of Lasso vary wildly depending on the choice of the penalty parameter. The recent results of Lahiri (2021) suggest that, depending on the nature of the penalty parameter, Lasso can either be variable selection consistent or be $n^{1/2}-$consistent. However, practitioners generally implement Lasso by choosing the penalty parameter in a data-dependent way, the most popular being the $K$-fold cross-validation. In this paper, we explore the variable selection consistency and $n^{1/2}-$consistency of Lasso when the penalty is chosen based on $K$-fold cross-validation with $K$ being fixed. We consider the fixed-dimensional heteroscedastic linear regression model and show that Lasso with $K$-fold cross-validation based penalty is $n^{1/2}-$consistent, but not variable selection consistent. We also establish the $n^{1/2}-$consistency of the $K$-fold cross-validation based penalty as an intermediate result. Additionally, as a consequence of $n^{1/2}-$consistency, we establish the validity of Bootstrap to approximate the distribution of the Lasso estimator based on $K-$fold cross-validation. We validate the Bootstrap approximation in finite samples based on a moderate simulation study. Thus, our results essentially justify the use of $K$-fold cross-validation in practice to draw inferences based on $n^{1/2}-$scaled pivotal quantities in Lasso regression.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T06:38:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12457v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12457v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Emotional Support with LLM-based Empathetic Dialogue Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shiquan Wang, Ruiyu Fang, Zhongjiang He, Shuangyong Song, Yongxiang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emotional Support Conversation (ESC) aims to provide empathetic and effective emotional assistance through dialogue, addressing the growing demand for mental health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC evaluation, where we leverage large-scale language models enhanced by prompt engineering and finetuning techniques. We explore both parameter-efficient Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the model's ability to generate supportive and contextually appropriate responses. Our best model ranked second in the competition, highlighting the potential of combining LLMs with effective adaptation methods for ESC tasks. Future work will focus on further enhancing emotional understanding and response personalization to build more practical and reliable emotional support systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T06:24:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12820v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12820v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free
  Zero-Shot Composed Image Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeong-Woo Park, Seong-Whan Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Composed Image Retrieval (CIR) is the task of retrieving a target image from a gallery using a composed query consisting of a reference image and a modification text. Among various CIR approaches, training-free zero-shot methods based on pre-trained models are cost-effective but still face notable limitations. For example, sequential VLM-LLM pipelines process each modality independently, which often results in information loss and limits cross-modal interaction. In contrast, methods based on multimodal large language models (MLLMs) often focus exclusively on applying changes indicated by the text, without fully utilizing the contextual visual information from the reference image. To address these issues, we propose multi-faceted Chain-of-Thought with re-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes multi-faceted Chain-of-Thought to guide the MLLM to balance explicit modifications and contextual visual cues, generating two distinct captions: one focused on modification and the other integrating comprehensive visual-textual context. The first caption is used to filter candidate images. Subsequently, we combine these two captions and the reference image to perform multi-grained re-ranking. This two-stage approach facilitates precise retrieval by aligning with the textual modification instructions while preserving the visual context of the reference image. Through extensive experiments, MCoT-RE achieves state-of-the-art results among training-free methods, yielding improvements of up to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T06:22:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12819v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12819v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Large Language Models' Internal Perception of Symbolic Music</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrew Shin, Kunitake Kaneko
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) excel at modeling relationships between strings in natural language and have shown promise in extending to other symbolic domains like coding or mathematics. However, the extent to which they implicitly model symbolic music remains underexplored. This paper investigates how LLMs represent musical concepts by generating symbolic music data from textual prompts describing combinations of genres and styles, and evaluating their utility through recognition and generation tasks. We produce a dataset of LLM-generated MIDI files without relying on explicit musical training. We then train neural networks entirely on this LLM-generated MIDI dataset and perform genre and style classification as well as melody completion, benchmarking their performance against established models. Our results demonstrate that LLMs can infer rudimentary musical structures and temporal relationships from text, highlighting both their potential to implicitly encode musical patterns and their limitations due to a lack of explicit musical context, shedding light on their generative capabilities for symbolic music.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:48:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12808v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12808v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 THOR: Transformer Heuristics for On-Demand Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Isaac Shi, Zeyuan Li, Fan Liu, Wenli Wang, Lewei He, Yang Yang, Tianyu Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce the THOR (Transformer Heuristics for On-Demand Retrieval) Module, designed and implemented by eSapiens, a secure, scalable engine that transforms natural-language questions into verified, read-only SQL analytics for enterprise databases. The Text-to-SQL module follows a decoupled orchestration/execution architecture: a Supervisor Agent routes queries, Schema Retrieval dynamically injects table and column metadata, and a SQL Generation Agent emits single-statement SELECT queries protected by a read-only guardrail. An integrated Self-Correction & Rating loop captures empty results, execution errors, or low-quality outputs and triggers up to five LLM-driven regeneration attempts. Finally, a Result Interpretation Agent produces concise, human-readable insights and hands raw rows to the Insight & Intelligence engine for visualization or forecasting.   Smoke tests across finance, sales, and operations scenarios demonstrate reliable ad-hoc querying and automated periodic reporting. By embedding schema awareness, fault-tolerant execution, and compliance guardrails, the THOR Module empowers non-technical users to access live data with zero-SQL simplicity and enterprise-grade safety.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:47:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.09592v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.09592v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Huan Wang, Shelby Heinecke, Silvio Savarese, Caiming Xiong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce \oursystemname, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:46:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12806v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12806v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Semantic Structure-Aware Generative Attacks for Enhanced Adversarial
  Transferability</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jongoh Jeong, Hunmin Yang, Jaeseok Jeong, Kuk-Jin Yoon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative adversarial attacks train a perturbation generator on a white-box surrogate model and subsequently apply the crafted perturbations to unseen black-box victim models. In contrast to iterative attacks, these methods deliver superior inference-time efficiency, scalability, and transferability; however, up until now, existing studies have not fully exploited the representational capacity of generative models to preserve and harness semantic information. Specifically, the intermediate activations of the generator encode rich semantic features--object boundaries and coarse shapes--that remain under-exploited, thereby limiting the alignment of perturbations with object-salient regions which are critical for adversarial transferability. To remedy this, we introduce a semantic structure-aware attack framework based on the Mean Teacher, which serves as a temporally smoothed feature reference. With this smoothed reference, we further direct semantic consistency between the early-layer activations in the student and those of the semantically rich teacher by feature distillation. By anchoring perturbation synthesis to the semantically salient early intermediate blocks within the generator based on empirical findings, our method guides progressive adversarial perturbation on regions that substantially enhance adversarial transferability. We conduct extensive experiments over diverse models, domains and tasks to demonstrate consistent improvements relative to state-of-the-art generative attacks, comprehensively evaluated using conventional metrics and our newly proposed Accidental Correction Rate (ACR).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:35:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.18248v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.18248v3' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 VideoITG: Multimodal Video Understanding with Instructed Temporal
  Grounding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shihao Wang, Guo Chen, De-an Huang, Zhiqi Li, Minghan Li, Guilin Li, Jose M. Alvarez, Lei Zhang, Zhiding Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:59:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13353v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13353v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 VisionThink: Smart and Efficient Vision Language Model via Reinforcement
  Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:59:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13348v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13348v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 DeFine: Decision-Making with Analogical Reasoning over Factor Profiles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yebowen Hu, Xiaoyang Wang, Wenlin Yao, Yiming Lu, Daoan Zhang, Hassan Foroosh, Dong Yu, Fei Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs are ideal for decision-making thanks to their ability to reason over long contexts. However, challenges arise when processing speech transcripts that describe complex scenarios, as they are verbose and include repetition, hedging, and vagueness. E.g., during a company's earnings call, an executive might project a positive revenue outlook to reassure investors, despite uncertainty regarding future earnings. It is crucial for LLMs to incorporate this uncertainty systematically when making decisions. In this paper, we introduce \textsc{DeFine}, a modular framework that constructs probabilistic factor profiles from complex scenarios. It then integrates these profiles with analogical reasoning, leveraging insights from similar past experiences to guide LLMs in making critical decisions in new situations. Our framework separates the tasks of quantifying uncertainty and incorporating it into LLM decision-making. This approach is particularly useful in areas such as consulting and financial deliberation, where making decisions under uncertainty is vital.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:58:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.01772v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.01772v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour
  Understanding from Traditional Puns to Topical Jokes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tyler Loakman, William Thorne, Chenghua Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Humour, as a complex language form, is derived from myriad aspects of life, whilst existing work on computational humour has focussed almost exclusively on short pun-based jokes. In this work, we investigate whether the ability of Large Language Models (LLMs) to explain humour depends on the particular humour form. We compare models on simple puns and more complex topical humour that requires knowledge of real-world entities and events. In doing so, we curate a dataset of 600 jokes split across 4 joke types and manually write high-quality explanations. These jokes include heterographic and homographic puns, contemporary internet humour, and topical jokes, where understanding relies on reasoning beyond "common sense", rooted instead in world knowledge regarding news events and pop culture. Using this dataset, we compare the zero-shot abilities of a range of LLMs to accurately and comprehensively explain jokes of different types, identifying key research gaps in the task of humour explanation. We find that none of the tested models (inc. reasoning models) are capable of reliably generating adequate explanations of all joke types, further highlighting the narrow focus of most works in computational humour on overly simple joke forms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:51:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13335v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13335v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 A Survey of Context Engineering for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, Shenghua Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:50:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13334v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13334v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 The Imitation Game: Turing Machine Imitator is Length Generalizable
  Reasoner</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhouqi Hua, Wenwei Zhang, Chengqi Lyu, Yuzhe Gu, Songyang Gao, Kuikun Liu, Kai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:50:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13332v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13332v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does
  Not Fundamentally Alter It</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yulu Qin, Dheeraj Varghese, Adam Dahlgren Lindström, Lucia Donatelli, Kanishka Misra, Najoung Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? Most results in the literature have shown inconsistent or marginal differences, both behaviorally and representationally. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:47:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13328v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13328v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic
  Estimation using LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kyeongjin Ahn, Sungwon Han, Seungeon Lee, Donghyun Ahn, Hyoshin Kim, Jungwon Kim, Jihee Kim, Sangyoon Park, Meeyoung Cha
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Socio-economic indicators like regional GDP, population, and education levels, are crucial to shaping policy decisions and fostering sustainable development. This research introduces GeoReg a regression model that integrates diverse data sources, including satellite imagery and web-based geospatial information, to estimate these indicators even for data-scarce regions such as developing countries. Our approach leverages the prior knowledge of large language model (LLM) to address the scarcity of labeled data, with the LLM functioning as a data engineer by extracting informative features to enable effective estimation in few-shot settings. Specifically, our model obtains contextual relationships between data features and the target indicator, categorizing their correlations as positive, negative, mixed, or irrelevant. These features are then fed into the linear estimator with tailored weight constraints for each category. To capture nonlinear patterns, the model also identifies meaningful feature interactions and integrates them, along with nonlinear transformations. Experiments across three countries at different stages of development demonstrate that our model outperforms baselines in estimating socio-economic indicators, even for low-income countries with limited data availability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:42:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13323v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13323v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 GPU Performance Portability needs Autotuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Burkhard Ringlein, Thomas Parnell, Radu Stoica
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As LLMs grow in complexity, achieving state-of-the-art performance requires tight co-design across algorithms, software, and hardware. Today's reliance on a single dominant platform limits portability, creates vendor lock-in, and raises barriers for new AI hardware. In this work, we make the case for combining just-in-time (JIT) compilation with comprehensive kernel parameter autotuning to enable portable LLM inference with state-of-the-art performance without code changes. Focusing on performance-critical LLM kernels, we demonstrate that this approach explores up to 15x more kernel parameter configurations, produces significantly more diverse code across multiple dimensions, and even outperforms vendor-optimized implementations by up to 230%, all while reducing kernel code size by 70x and eliminating manual code optimizations. Our results highlight autotuning as a promising path to unlocking model portability across GPU vendors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:31:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.03780v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.03780v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Retraining-Free Merging of Sparse MoE via Hierarchical Clustering</h2>
                <div class="authors">
                    <strong>Authors:</strong> I-Chun Chen, Hsu-Shen Liu, Wei-Fang Sun, Chen-Hao Chao, Yen-Chang Hsu, Chun-Yi Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse Mixture-of-Experts (SMoE) models represent a significant advancement in large language model (LLM) development through their efficient parameter utilization. These models achieve substantial performance improvements at reduced inference costs. However, the deployment of SMoE models faces constraints from extensive memory requirements of expert components in resource-limited environments. To address these limitations, this paper introduces Hierarchical Clustering for Sparsely activated Mixture of Experts (HC-SMoE), a task-agnostic expert merging framework for parameter reduction without retraining. HC-SMoE introduces a novel hierarchical clustering approach based on expert outputs to ensure merging robustness independent of routing decisions. The proposed output-based clustering method enables effective capture of functional relationships between experts for large-scale architectures. We provide theoretical analysis and comprehensive evaluations across multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness in state-of-the-art models including Qwen and Mixtral. The experimental results validate HC-SMoE's superior performance and practical applicability for real-world deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:20:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.08589v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.08589v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 The Generative Energy Arena (GEA): Incorporating Energy Awareness in
  Large Language Model (LLM) Human Evaluations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Carlos Arriaga, Gonzalo Martínez, Eneko Sendin, Javier Conde, Pedro Reviriego
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The evaluation of large language models is a complex task, in which several approaches have been proposed. The most common is the use of automated benchmarks in which LLMs have to answer multiple-choice questions of different topics. However, this method has certain limitations, being the most concerning, the poor correlation with the humans. An alternative approach, is to have humans evaluate the LLMs. This poses scalability issues as there is a large and growing number of models to evaluate making it impractical (and costly) to run traditional studies based on recruiting a number of evaluators and having them rank the responses of the models. An alternative approach is the use of public arenas, such as the popular LM arena, on which any user can freely evaluate models on any question and rank the responses of two models. The results are then elaborated into a model ranking. An increasingly important aspect of LLMs is their energy consumption and, therefore, evaluating how energy awareness influences the decisions of humans in selecting a model is of interest. In this paper, we present GEA, the Generative Energy Arena, an arena that incorporates information on the energy consumption of the model in the evaluation process. Preliminary results obtained with GEA are also presented, showing that for most questions, when users are aware of the energy consumption, they favor smaller and more energy efficient models. This suggests that for most user interactions, the extra cost and energy incurred by the more complex and top-performing models do not provide an increase in the perceived quality of the responses that justifies their use.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:11:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13302v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13302v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 AbGen: Evaluating Large Language Models in Ablation Study Design and
  Evaluation for Scientific Research</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilun Zhao, Weiyuan Chen, Zhijian Xu, Manasi Patwardhan, Yixin Liu, Chengye Wang, Lovekesh Vig, Arman Cohan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T17:09:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13300v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13300v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Towards Formal Verification of LLM-Generated Code from Natural Language
  Prompts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aaron Councilman, David Fu, Aryan Gupta, Chengxiao Wang, David Grove, Yu-Xiong Wang, Vikram Adve
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the past few years LLMs have emerged as a tool that can aid programmers by taking natural language descriptions and generating code based on it. However, LLMs often generate incorrect code that users need to fix and the literature suggests users often struggle to detect these errors. In this work we seek to offer formal guarantees of correctness to LLM generated code; such guarantees could improve the experience of using AI Code Assistants and potentially enable natural language programming for users with little or no programming knowledge. To address this challenge we propose to incorporate a formal query language that can represent a user's intent in a formally defined but natural language-like manner that a user can confirm matches their intent. Then, using such a query we propose to verify LLM generated code to ensure it matches the user's intent. We implement these ideas in our system, Astrogator, for the Ansible programming language which includes such a formal query language, a calculus for representing the behavior of Ansible programs, and a symbolic interpreter which is used for the verification. On a benchmark suite of 21 code-generation tasks, our verifier is able to verify correct code in 83% of cases and identify incorrect code in 92%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:54:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13290v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13290v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with
  Security Filtering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Grace Byun, Shinsun Lee, Nayoung Choi, Jinho D. Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing Retrieval-Augmented Generation (RAG) systems face challenges in enterprise settings due to limited retrieval scope and data security risks. When relevant internal documents are unavailable, the system struggles to generate accurate and complete responses. Additionally, using closed-source Large Language Models (LLMs) raises concerns about exposing proprietary information. To address these issues, we propose the Secure Multifaceted-RAG (SecMulti-RAG) framework, which retrieves not only from internal documents but also from two supplementary sources: pre-generated expert knowledge for anticipated queries and on-demand external LLM-generated knowledge. To mitigate security risks, we adopt a local open-source generator and selectively utilize external LLMs only when prompts are deemed safe by a filtering mechanism. This approach enhances completeness, prevents data leakage, and reduces costs. In our evaluation on a report generation task in the automotive industry, SecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9 percent win rates across correctness, richness, and helpfulness in LLM-based evaluation, and 56.3 to 70.4 percent in human evaluation. This highlights SecMulti-RAG as a practical and secure solution for enterprise RAG.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:32:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.13425v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.13425v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiazheng Li, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Hongzhou Lin, Yi Wu, Jingzhao Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning (RL) has become a key component in training large language reasoning models (LLMs). However, recent studies questions its effectiveness in improving multi-step reasoning-particularly on hard problems. To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%) on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical explanations that QuestA improves sample efficiency, offering a practical and generalizable pathway for expanding reasoning capability through RL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:21:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>68T50</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13266v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13266v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Automating Steering for Safe Multimodal Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lyucheng Wu, Mengru Wang, Ziwen Xu, Tri Cao, Nay Oo, Bryan Hooi, Shumin Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T16:04:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span><span>cs.LG</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13255v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13255v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 ConTextual: Improving Clinical Text Summarization in LLMs with
  Context-preserving Token Filtering and Knowledge Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fahmida Liza Piya, Rahmatollah Beheshti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:50:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.16394v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.16394v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ashray Gupta, Rohan Joseph, Sunny Rai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Analogies test a model's ability to infer implicit relationships between concepts, making them a key benchmark for evaluating reasoning capabilities. While large language models (LLMs) are widely evaluated for reasoning in English, their abilities in Indic languages remain understudied, limiting our understanding of whether these models generalize across languages. To address this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405 multiple-choice questions sourced from Indian government exams. We benchmark state-of-the-art multilingual LLMs using various prompting strategies and introduce a grounded Chain of Thought approach that leverages cognitive theories of analogical reasoning. This approach improves model performance on Hindi analogy questions. Our experiments show that models perform best with English prompts, irrespective of the prompting strategy. Our test set addresses the lack of a critical resource to evaluate LLM reasoning capabilities in Hindi.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:47:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13238v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13238v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Enhancing Cross-task Transfer of Large Language Models via Activation
  Steering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyu Tang, Zhihao Lv, Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Zujie Wen, Zhiqiang Zhang, Jun Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have shown impressive abilities in leveraging pretrained knowledge through prompting, but they often struggle with unseen tasks, particularly in data-scarce scenarios. While cross-task in-context learning offers a direct solution for transferring knowledge across tasks, it still faces critical challenges in terms of robustness, scalability, and efficiency. In this paper, we investigate whether cross-task transfer can be achieved via latent space steering without parameter updates or input expansion. Through an analysis of activation patterns in the latent space of LLMs, we observe that the enhanced activations induced by in-context examples have consistent patterns across different tasks. Inspired by these findings, we propose CAST, a novel Cross-task Activation Steering Transfer framework that enables effective transfer by manipulating the model's internal activation states. Our approach first selects influential and diverse samples from high-resource tasks, then utilizes their contrastive representation-enhanced activations to adapt LLMs to low-resource tasks. Extensive experiments across both cross-domain and cross-lingual transfer settings show that our method outperforms competitive baselines and demonstrates superior scalability and lower computational costs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:47:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13236v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13236v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,
  Multi-Generator and Multi-Domain Settings</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniil Orel, Dilshod Azizov, Preslav Nakov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have revolutionized code generation, automating programming with remarkable efficiency. However, these advancements challenge programming skills, ethics, and assessment integrity, making the detection of LLM-generated code essential for maintaining accountability and standards. While, there has been some research on this problem, it generally lacks domain coverage and robustness, and only covers a small number of programming languages. To this end, we propose a framework capable of distinguishing between human- and LLM-written code across multiple programming languages, code generators, and domains. We use a large-scale dataset from renowned platforms and LLM-based code generators, alongside applying rigorous data quality checks, feature engineering, and comparative analysis using evaluation of traditional machine learning models, pre-trained language models (PLMs), and LLMs for code detection. We perform an evaluation on out-of-domain scenarios, such as detecting the authorship and hybrid authorship of generated code and generalizing to unseen models, domains, and programming languages. Moreover, our extensive experiments show that our framework effectively distinguishes human- from LLM-written code and sets a new benchmark for this task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:31:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.13733v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.13733v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 A Comparative Approach to Assessing Linguistic Creativity of Large
  Language Models and Humans</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anca Dinu, Andra-Maria Florescu, Alina Resceanu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The following paper introduces a general linguistic creativity test for humans and Large Language Models (LLMs). The test consists of various tasks aimed at assessing their ability to generate new original words and phrases based on word formation processes (derivation and compounding) and on metaphorical language use. We administered the test to 24 humans and to an equal number of LLMs, and we automatically evaluated their answers using OCSAI tool for three criteria: Originality, Elaboration, and Flexibility. The results show that LLMs not only outperformed humans in all the assessed criteria, but did better in six out of the eight test tasks. We then computed the uniqueness of the individual answers, which showed some minor differences between humans and LLMs. Finally, we performed a short manual analysis of the dataset, which revealed that humans are more inclined towards E(extending)-creativity, while LLMs favor F(ixed)-creativity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:27:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12039v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12039v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Automatically assessing oral narratives of Afrikaans and isiXhosa
  children</h2>
                <div class="authors">
                    <strong>Authors:</strong> R. Louw, E. Sharratt, F. de Wet, C. Jacobs, A. Smith, H. Kamper
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Developing narrative and comprehension skills in early childhood is critical for later literacy. However, teachers in large preschool classrooms struggle to accurately identify students who require intervention. We present a system for automatically assessing oral narratives of preschool children in Afrikaans and isiXhosa. The system uses automatic speech recognition followed by a machine learning scoring model to predict narrative and comprehension scores. For scoring predicted transcripts, we compare a linear model to a large language model (LLM). The LLM-based system outperforms the linear model in most cases, but the linear system is competitive despite its simplicity. The LLM-based system is comparable to a human expert in flagging children who require intervention. We lay the foundation for automatic oral assessments in classrooms, giving teachers extra capacity to focus on personalised support for children's learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:15:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13205v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13205v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge
  Expansion for Dense Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sangam Lee, Ryang Heo, SeongKu Kang, Dongha Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing dense retrieval models struggle with reasoning-intensive retrieval task as they fail to capture implicit relevance that requires reasoning beyond surface-level semantic information. To address these challenges, we propose Scenario-Profiled Indexing with Knowledge Expansion (SPIKE), a dense retrieval framework that explicitly indexes implicit relevance by decomposing documents into scenario-based retrieval units. SPIKE organizes documents into scenario, which encapsulates the reasoning process necessary to uncover implicit relationships between hypothetical information needs and document content. SPIKE constructs a scenario-augmented dataset using a powerful teacher large language model (LLM), then distills these reasoning capabilities into a smaller, efficient scenario generator. During inference, SPIKE incorporates scenario-level relevance alongside document-level relevance, enabling reasoning-aware retrieval. Extensive experiments demonstrate that SPIKE consistently enhances retrieval performance across various query types and dense retrievers. It also enhances the retrieval experience for users through scenario and offers valuable contextual information for LLMs in retrieval-augmented generation (RAG).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:06:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.23033v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.23033v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday
  Storytelling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiangnan Xu, Haeseul Cha, Gosu Choi, Gyu-cheol Lee, Yeo-Jin Yoon, Zucheul Lee, Konstantinos Papangelis, Dae Hyun Kim, Juho Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> An interactive vignette is a popular and immersive visual storytelling approach that invites viewers to role-play a character and influences the narrative in an interactive environment. However, it has not been widely used by everyday storytellers yet due to authoring complexity, which conflicts with the immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted authoring system for interactive vignette creation in everyday storytelling. It takes a natural language story as input and extracts the three core elements of an interactive vignette (environment, characters, and events), enabling authors to focus on refining these elements instead of constructing them from scratch. Then, it automatically transforms the single-branch story input into a branch-and-bottleneck structure using an LLM-powered narrative planner, which enables flexible viewer interactions while freeing the author from multi-branching. A technical evaluation (N=16) shows that DiaryPlay-generated character activities are on par with human-authored ones regarding believability. A user study (N=16) shows that DiaryPlay effectively supports authors in creating interactive vignette elements, maintains authorial intent while reacting to viewer interactions, and provides engaging viewing experiences.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:01:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11628v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11628v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyan Wang, Yingpeng Du, Zhu Sun, Jieyi Bi, Haoyan Chua, Tianjun Wei, Jie Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, much effort has been devoted to modeling users' multi-interests based on their behaviors or auxiliary signals. However, existing methods often rely on heuristic assumptions, e.g., co-occurring items indicate the same interest of users, failing to capture user multi-interests aligning with real-world scenarios. While large language models (LLMs) show significant potential for multi-interest analysis due to their extensive knowledge and powerful reasoning capabilities, two key challenges remain. First, the granularity of LLM-driven multi-interests is agnostic, possibly leading to overly fine or coarse interest grouping. Second, individual user analysis provides limited insights due to the data sparsity issue. In this paper, we propose an LLM-driven dual-level multi-interest modeling framework for more effective recommendation. At the user-individual level, we exploit LLMs to flexibly allocate items engaged by users into different semantic clusters, indicating their diverse and distinct interests. To alleviate the agnostic generation of LLMs, we adaptively assign these semantic clusters to users' collaborative multi-interests learned from global user-item interactions, allowing the granularity to be automatically adjusted according to the user's behaviors using an alignment module. To alleviate the limited insights derived from individual users' behaviors, at the user-crowd level, we propose aggregating user cliques into synthesized users with rich behaviors for more comprehensive LLM-driven multi-interest analysis. We formulate a max covering problem to ensure the compactness and representativeness of synthesized users' behaviors, and then conduct contrastive learning based on their LLM-driven multi-interests to disentangle item representations among different interests. Experiments on real-world datasets show the superiority of our approach against state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T15:01:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10917v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10917v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction
  with an Agentic Robot</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luca Garello, Francesca Cocchella, Alessandra Sciutti, Manuel Catalano, Francesco Rea
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous robots are increasingly being tested into public spaces to enhance user experiences, particularly in cultural and educational settings. This paper presents the design, implementation, and evaluation of the autonomous museum guide robot Alter-Ego equipped with advanced navigation and interactive capabilities. The robot leverages state-of-the-art Large Language Models (LLMs) to provide real-time, context aware question-and-answer (Q&A) interactions, allowing visitors to engage in conversations about exhibits. It also employs robust simultaneous localization and mapping (SLAM) techniques, enabling seamless navigation through museum spaces and route adaptation based on user requests. The system was tested in a real museum environment with 34 participants, combining qualitative analysis of visitor-robot conversations and quantitative analysis of pre and post interaction surveys. Results showed that the robot was generally well-received and contributed to an engaging museum experience, despite some limitations in comprehension and responsiveness. This study sheds light on HRI in cultural spaces, highlighting not only the potential of AI-driven robotics to support accessibility and knowledge acquisition, but also the current limitations and challenges of deploying such technologies in complex, real-world environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:54:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12273v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12273v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Black Box Deployed -- Functional Criteria for Artificial Moral Agents in
  the LLM Era</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matthew E. Brophy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The advancement of powerful yet opaque large language models (LLMs) necessitates a fundamental revision of the philosophical criteria used to evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the assumption of transparent architectures, which LLMs defy due to their stochastic outputs and opaque internal states. This paper argues that traditional ethical criteria are pragmatically obsolete for LLMs due to this mismatch. Engaging with core themes in the philosophy of technology, this paper proffers a revised set of ten functional criteria to evaluate LLM-based artificial moral agents: moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy, and moral imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating Moral Agency through Large Language Systems), aim to steer AMAs toward greater alignment and beneficial societal integration in the coming years. We illustrate these criteria using hypothetical scenarios involving an autonomous public bus (APB) to demonstrate their practical applicability in morally salient contexts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:39:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>68T27, 03B42 68T27, 03B4268T27, 03B42 68T27, 03B42 68T27, 03B42
  68T27, 03B42 68T27, 03B42 68T27, 03B4268T27, 03B42</span><span>I.2.0; I.2.9; K.4.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13175v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13175v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Prompt Injection 2.0: Hybrid AI Threats</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeremy McHugh, Kristina Šekrst, Jon Cefalu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prompt injection attacks, where malicious input is designed to manipulate AI systems into ignoring their original instructions and following unauthorized commands instead, were first discovered by Preamble, Inc. in May 2022 and responsibly disclosed to OpenAI. Over the last three years, these attacks have continued to pose a critical security threat to LLM-integrated systems. The emergence of agentic AI systems, where LLMs autonomously perform multistep tasks through tools and coordination with other agents, has fundamentally transformed the threat landscape. Modern prompt injection attacks can now combine with traditional cybersecurity exploits to create hybrid threats that systematically evade traditional security controls. This paper presents a comprehensive analysis of Prompt Injection 2.0, examining how prompt injections integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and other web security vulnerabilities to bypass traditional security measures. We build upon Preamble's foundational research and mitigation technologies, evaluating them against contemporary threats, including AI worms, multi-agent infections, and hybrid cyber-AI attacks. Our analysis incorporates recent benchmarks that demonstrate how traditional web application firewalls, XSS filters, and CSRF tokens fail against AI-enhanced attacks. We also present architectural solutions that combine prompt isolation, runtime security, and privilege separation with novel threat detection capabilities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:33:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13169v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13169v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Inverse Reinforcement Learning Meets Large Language Model Post-Training:
  Basics, Advances, and Opportunities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Sun, Mihaela van der Schaar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:22:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13158v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13158v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on
  Multimodal Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangyu Dong, Haoran Zhao, Jiang Gao, Haozhou Li, Xiaoguang Ma, Yaoming Zhou, Fuhai Chen, Juan Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:13:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13152v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13152v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow
  Generation & Instruct-Masking Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fucai Ke, Vijay Kumar B G, Xingjian Leng, Zhixi Cai, Zaid Khan, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi, Manmohan Chandraker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual reasoning (VR), which is crucial in many fields for enabling human-like visual understanding, remains highly challenging. Recently, compositional visual reasoning approaches, which leverage the reasoning abilities of large language models (LLMs) with integrated tools to solve problems, have shown promise as more effective strategies than end-to-end VR methods. However, these approaches face limitations, as frozen LLMs lack tool awareness in VR, leading to performance bottlenecks. While leveraging LLMs for reasoning is widely used in other domains, they are not directly applicable to VR due to limited training data, imperfect tools that introduce errors and reduce data collection efficiency in VR, and challenging in fine-tuning on noisy workflows. To address these challenges, we propose DWIM: i) Discrepancy-aware training Workflow generation, which assesses tool usage and extracts more viable workflows for training; and ii) Instruct-Masking fine-tuning, which guides the model to only clone effective actions, enabling the generation of more practical solutions. Our experiments demonstrate that DWIM achieves state-of-the-art performance across various VR tasks, exhibiting strong generalization on multiple widely-used datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:10:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.19263v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.19263v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language
  Models on Software Engineering Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pavel Adamenko, Mikhail Ivanov, Aidar Valeev, Rodion Levichev, Pavel Zadorozhny, Ivan Lopatin, Dmitry Babayev, Alena Fenogenova, Valentin Malykh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of Large Language Models (LLMs) in software engineering has revealed critical limitations in existing benchmarks, particularly the widely used SWE-bench dataset. Recent studies have uncovered severe data contamination issues, e.g. SWE-bench reports 32.67% of successful patches involve direct solution leakage and 31.08% pass due to inadequate test cases. We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to address these fundamental challenges through an automated collection of real-world GitHub issues and rigorous quality validation. Our approach implements a reliable pipeline that ensures quality while minimizing contamination risks, resulting in approximately 10,000 potential tasks with 300 samples currently available. Evaluation using the Aider coding agent demonstrates strong discriminative power in state-of-the-art models. We report performance across a dozen recent LLMs evaluated on tasks collected between September 2024 and June 2025.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:04:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11059v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11059v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and
  Intention-Driven Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kuiyuan Ding, Caili Guo, Yang Yang, Jianzhang Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sixth generation (6G) networks demand tight integration of artificial intelligence (AI) into radio access networks (RANs) to meet stringent quality of service (QoS) and resource efficiency requirements. Existing solutions struggle to bridge the gap between high level user intents and the low level, parameterized configurations required for optimal performance. To address this challenge, we propose RIDAS, a multi agent framework composed of representation driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open interface with tunable control parameters (rank and quantization bits, enabling explicit trade) offs between distortion and transmission rate. The IDA employs a two stage planning scheme (bandwidth pre allocation and reallocation) driven by a large language model (LLM) to map user intents and system state into optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\% more users than WirelessAgent under equivalent QoS constraints. These results validate ability of RIDAS to capture user intent and allocate resources more efficiently in AI RAN environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:02:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13140v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13140v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Assessing the Reliability of LLMs Annotations in the Context of
  Demographic Bias and Model Explanation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hadi Mohammadi, Tina Shahedi, Pablo Mosteiro, Massimo Poesio, Ayoub Bagheri, Anastasia Giachanou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the sources of variability in annotations is crucial for developing fair NLP systems, especially for tasks like sexism detection where demographic bias is a concern. This study investigates the extent to which annotator demographic features influence labeling decisions compared to text content. Using a Generalized Linear Mixed Model, we quantify this inf luence, finding that while statistically present, demographic factors account for a minor fraction ( 8%) of the observed variance, with tweet content being the dominant factor. We then assess the reliability of Generative AI (GenAI) models as annotators, specifically evaluating if guiding them with demographic personas improves alignment with human judgments. Our results indicate that simplistic persona prompting often fails to enhance, and sometimes degrades, performance compared to baseline models. Furthermore, explainable AI (XAI) techniques reveal that model predictions rely heavily on content-specific tokens related to sexism, rather than correlates of demographic characteristics. We argue that focusing on content-driven explanations and robust annotation protocols offers a more reliable path towards fairness than potentially persona simulation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T14:00:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13138v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13138v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Detecting LLM-generated Code with Subtle Modification by Adversarial
  Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Yin, Xinrui Li, Chao Ni, Xiaodan Xu, Xiaohu Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapid development of Large Language Models (LLMs), their powerful code-generation capabilities have been widely applied in tasks like code completion and automated development, demonstrating the value of improving coding efficiency. However, the extensive use of LLM-generated code also raises several new challenges. On the one hand, issues such as the regulation of code provenance, copyright disputes, and code quality have become increasingly concerning. How to effectively detect LLM-generated code and ensure its compliant and responsible use has become a critical and urgent issue. On the other hand, in practical applications, LLM-generated code is often subject to manual modifications, such as variable renaming or structural adjustments. Although some recent studies have proposed training-based and zero-shot methods for detecting LLM-generated code, these approaches show insufficient robustness when facing modified LLM-generated code, and there is a lack of an effective solution. To address the real-world scenario where LLM-generated code may undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of CodeGPTSensor, which employs adversarial training to improve robustness against input perturbations. CodeGPTSensor+ integrates an adversarial sample generation module, Multi-objective Identifier and Structure Transformation (MIST), which systematically generates both high-quality and representative adversarial samples. This module effectively enhances the model's resistance against diverse adversarial attacks. Experimental results on the HMCorp dataset demonstrate that CodeGPTSensor+ significantly improves detection accuracy on the adversarial test set while maintaining high accuracy on the original test set, showcasing superior robustness compared to CodeGPTSensor.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:38:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13123v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13123v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Task-Circuit Quantization: Leveraging Knowledge Localization and
  Interpretability for Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanqi Xiao, Yi-Lin Sung, Elias Stengel-Eskin, Mohit Bansal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Post-training quantization (PTQ) reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low 2- to 3-bit settings. We develop a new mixed-precision PTQ approach, Task-Circuit Quantization (TaCQ), that draws parallels to automated circuit discovery, directly conditioning the quantization process on specific weight circuits -- which we define as sets of weights associated with downstream task performance. These weights are kept as 16-bit weights, while others are quantized, maintaining performance while only adding a marginal memory cost. Specifically, TaCQ contrasts unquantized model weights with a uniformly-quantized model to estimate the expected change in weights due to quantization and uses gradient information to predict the resulting impact on task performance, allowing us to preserve task-specific weights. We compare TaCQ-based quantization to existing mixed-precision quantization methods when conditioning both on general-purpose and task-specific data. Across QA, math reasoning, and text-to-SQL tasks for both Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the same calibration data and a lower weight budget, achieving major improvements in the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of Llama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25% absolute improvement over SPQR. We also observe consistently large gains over existing methods in the 2-bit regime, with an average gain of 14.74% over the strongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without conditioning on specific tasks, showing TaCQ's ability to identify important weights is not limited to task-conditioned settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:24:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.07389v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.07389v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated
  Summaries For Scientific Abstracts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Marc Brinner, Sina Zarriess
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce SemCSE, an unsupervised method for learning semantic embeddings of scientific texts. Building on recent advances in contrastive learning for text embeddings, our approach leverages LLM-generated summaries of scientific abstracts to train a model that positions semantically related summaries closer together in the embedding space. This resulting objective ensures that the model captures the true semantic content of a text, in contrast to traditional citation-based approaches that do not necessarily reflect semantic similarity. To validate this, we propose a novel benchmark designed to assess a model's ability to understand and encode the semantic content of scientific texts, demonstrating that our method enforces a stronger semantic separation within the embedding space. Additionally, we evaluate SemCSE on the comprehensive SciRepEval benchmark for scientific text embeddings, where it achieves state-of-the-art performance among models of its size, thus highlighting the benefits of a semantically focused training approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:19:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13105v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13105v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in
  Dynamic Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheng Jia, Shengbin Yue, Wei Chen, Siyuan Wang, Yidong Liu, Yun Song, Zhongyu Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The gap between static benchmarks and the dynamic nature of real-world legal practice poses a key barrier to advancing legal intelligence. To this end, we introduce J1-ENVS, the first interactive and dynamic legal environment tailored for LLM-based agents. Guided by legal experts, it comprises six representative scenarios from Chinese legal practices across three levels of environmental complexity. We further introduce J1-EVAL, a fine-grained evaluation framework, designed to assess both task performance and procedural compliance across varying levels of legal proficiency. Extensive experiments on 17 LLM agents reveal that, while many models demonstrate solid legal knowledge, they struggle with procedural execution in dynamic settings. Even the SOTA model, GPT-4o, falls short of 60% overall performance. These findings highlight persistent challenges in achieving dynamic legal intelligence and offer valuable insights to guide future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T13:02:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04037v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04037v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 MERA Code: A Unified Framework for Evaluating Code Generation Across
  Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Artem Chervyakov, Alexander Kharitonov, Pavel Zadorozhny, Adamenko Pavel, Rodion Levichev, Dmitrii Vorobev, Dmitrii Salikhov, Aidar Valeev, Alena Pestova, Maria Dziuba, Ilseyar Alimova, Artem Zavgorodnev, Aleksandr Medvedev, Stanislav Moiseev, Elena Bruches, Daniil Grebenkin, Roman Derunets, Vikulov Vladimir, Anton Emelyanov, Dmitrii Babaev, Vladimir V. Ivanov, Valentin Malykh, Alena Fenogenova
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Advancements in LLMs have enhanced task automation in software engineering; however, current evaluations primarily focus on natural language tasks, overlooking code quality. Most benchmarks prioritize high-level reasoning over executable code and real-world performance, leaving gaps in understanding true capabilities and risks associated with these models in production. To address this issue, we propose MERA Code, a new addition to the MERA benchmark family, specifically focused on evaluating code for the latest code generation LLMs in Russian. This benchmark includes 11 evaluation tasks that span 8 programming languages. Our proposed evaluation methodology features a taxonomy that outlines the practical coding skills necessary for models to complete these tasks. The benchmark comprises an open-source codebase for users to conduct MERA assessments, a scoring system compatible with various programming environments, and a platform featuring a leaderboard and submission system. We evaluate open LLMs and frontier API models, analyzing their limitations in terms of practical coding tasks in non-English languages. We are publicly releasing MERA to guide future research, anticipate groundbreaking features in model development, and standardize evaluation procedures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:55:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12284v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12284v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Unmodulated Visible Light Positioning: A Deep Dive into Techniques,
  Studies, and Future Prospects</h2>
                <div class="authors">
                    <strong>Authors:</strong> Morteza Alijani, Wout Joseph, David Plets
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visible Light Positioning (VLP) has emerged as a promising technology for next-generation indoor positioning systems (IPS), particularly within the scope of sixth-generation (6G) wireless networks. Its attractiveness stems from leveraging existing lighting infrastructures equipped with light-emitting diodes (LEDs), enabling cost-efficient deployments and achieving high-precision positioning accuracy in the centimeter-todecimeter range. However, widespread adoption of traditional VLP solutions faces significant barriers due to the increased costs and operational complexity associated with modulating LEDs, which consequently reduces illumination efficiency by lowering their radiant flux. To address these limitations, recent research has introduced the concept of unmodulated Visible Light Positioning (uVLP), which exploits Light Signals of Opportunity (LSOOP) emitted by unmodulated illumination sources such as conventional LEDs. This paradigm offers a cost-effective, lowinfrastructure alternative for indoor positioning by eliminating the need for modulation hardware and maintaining lighting efficiency. This paper delineates the fundamental principles of uVLP, provides a comparative analysis of uVLP versus conventional VLP methods, and classifies existing uVLP techniques according to receiver technologies into intensity-based methods (e.g., photodiodes, solar cells, etc.) and imaging-based methods. Additionally, we propose a comprehensive taxonomy categorizing techniques into demultiplexed and undemultiplexed approaches. Within this structured framework, we critically review current advancements in uVLP, discuss prevailing challenges, and outline promising research directions essential for developing robust, scalable, and widely deployable uVLP solutions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:50:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13080v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13080v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Dual LiDAR-Based Traffic Movement Count Estimation at a Signalized
  Intersection: Deployment, Data Collection, and Preliminary Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Saswat Priyadarshi Nayak, Guoyuan Wu, Kanok Boriboonsomsin, Matthew Barth
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traffic Movement Count (TMC) at intersections is crucial for optimizing signal timings, assessing the performance of existing traffic control measures, and proposing efficient lane configurations to minimize delays, reduce congestion, and promote safety. Traditionally, methods such as manual counting, loop detectors, pneumatic road tubes, and camera-based recognition have been used for TMC estimation. Although generally reliable, camera-based TMC estimation is prone to inaccuracies under poor lighting conditions during harsh weather and nighttime. In contrast, Light Detection and Ranging (LiDAR) technology is gaining popularity in recent times due to reduced costs and its expanding use in 3D object detection, tracking, and related applications. This paper presents the authors' endeavor to develop, deploy and evaluate a dual-LiDAR system at an intersection in the city of Rialto, California, for TMC estimation. The 3D bounding box detections from the two LiDARs are used to classify vehicle counts based on traffic directions, vehicle movements, and vehicle classes. This work discusses the estimated TMC results and provides insights into the observed trends and irregularities. Potential improvements are also discussed that could enhance not only TMC estimation, but also trajectory forecasting and intent prediction at intersections.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:42:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.CV</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13073v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13073v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Human Demonstrations are Generalizable Knowledge for Robots</h2>
                <div class="authors">
                    <strong>Authors:</strong> Te Cui, Tianxing Zhou, Zicai Peng, Mengxiao Hu, Haoyang Lu, Haizhou Li, Guangyan Chen, Meiling Wang, Yufeng Yue
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning from human demonstrations is an emerging trend for designing intelligent robotic systems. However, previous methods typically regard videos as instructions, simply dividing them into action sequences for robotic repetition, which poses obstacles to generalization to diverse tasks or object instances. In this paper, we propose a different perspective, considering human demonstration videos not as mere instructions, but as a source of knowledge for robots. Motivated by this perspective and the remarkable comprehension and generalization capabilities exhibited by large language models (LLMs), we propose DigKnow, a method that DIstills Generalizable KNOWledge with a hierarchical structure. Specifically, DigKnow begins by converting human demonstration video frames into observation knowledge. This knowledge is then subjected to analysis to extract human action knowledge and further distilled into pattern knowledge compassing task and object instances, resulting in the acquisition of generalizable knowledge with a hierarchical structure. In settings with different tasks or object instances, DigKnow retrieves relevant knowledge for the current task and object instances. Subsequently, the LLM-based planner conducts planning based on the retrieved knowledge, and the policy executes actions in line with the plan to achieve the designated task. Utilizing the retrieved knowledge, we validate and rectify planning and execution outcomes, resulting in a substantial enhancement of the success rate. Experimental results across a range of tasks and scenes demonstrate the effectiveness of this approach in facilitating real-world robots to accomplish tasks with the knowledge derived from human demonstrations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:34:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.02419v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.02419v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Write Your Own CodeChecker: An Automated Test-Driven Checker Development
  Approach with LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jun Liu, Yuanyuan Xie, Jiwei Yan, Jinhao Huang, Jun Yan, Jian Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rising demand for code quality assurance, developers are not only utilizing existing static code checkers but also seeking custom checkers to satisfy their specific needs. Nowadays, various code-checking frameworks provide extensive checker customization interfaces to meet this need. However, both the abstract checking logic and the complex API usage of large-scale checker frameworks make this task challenging. To this end, automated code checker generation is anticipated to ease the burden of checker development. In this paper, we propose AutoChecker, an innovative LLM-powered approach that can write code checkers automatically based on only a rule description and a test suite. To achieve comprehensive checking logic, AutoChecker incrementally updates the checker's logic by focusing on solving one selected case each time. To obtain precise API knowledge, during each iteration, it leverages fine-grained logic-guided API-context retrieval, where it first decomposes the checking logic into a series of sub-operations and then retrieves checker-related API-contexts for each sub-operation. For evaluation, we apply AutoChecker, five baselines, and three ablation methods using multiple LLMs to generate checkers for 20 randomly selected PMD rules. Experimental results show that AutoChecker significantly outperforms others across all effectiveness metrics, with an average test pass rate of 82.28%. Additionally, the checkers generated by AutoChecker can be successfully applied to real-world projects, matching the performance of official checkers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:33:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.06796v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.06796v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient
  Communication</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianyu Song, Feng Li, Yuan Bi, Angelos Karlas, Amir Yousefi, Daniela Branzan, Zhongliang Jiang, Ulrich Eck, Nassir Navab
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:25:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13052v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13052v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Backscattering-Based Security in Wireless Power Transfer Applied to
  Battery-Free BLE Sensors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Taki Eddine Djidjekh, Gaël Loubet, Alexandru Takacs
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of security and energy efficiency in Internet of Things systems remains a critical challenge, particularly for battery-free and resource-constrained devices. This paper explores the scalability and protocol-agnostic nature of a backscattering-based security mechanism by integrating it into Bluetooth Low Energy battery-free Wireless Sensor Network. The proposed approach leverages the Wireless Power Transfer link, traditionally used for energy harvesting, to generate additional identification signals without increasing energy consumption or computational demands. Experimental validation demonstrates the solution's functionality using compact, low-gain antenna, ensuring compatibility with size-constrained applications such as Structural Health Monitoring and smart transport. Furthermore, this work addresses the challenges associated with backscattering dynamic range and multi-node Wireless Sensor Network scenarios, discussing potential collisions between identification signals and proposing future improvements to enhance generalizability and scalability. The findings underscore the potential of the backscattering-based security mechanism for creating secure, sustainable, and scalable IoT deployments across diverse protocols and applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:15:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13042v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13042v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent
  Debate Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu Cui, Hongyang Du
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-agent debate (MAD) systems leverage collaborative interactions among large language models (LLMs) agents to improve reasoning capabilities. While recent studies have focused on increasing the accuracy and scalability of MAD systems, their security vulnerabilities have received limited attention. In this work, we introduce MAD-Spear, a targeted prompt injection attack that compromises a small subset of agents but significantly disrupts the overall MAD process. Manipulated agents produce multiple plausible yet incorrect responses, exploiting LLMs' conformity tendencies to propagate misinformation and degrade consensus quality. Furthermore, the attack can be composed with other strategies, such as communication attacks, to further amplify its impact by increasing the exposure of agents to incorrect responses. To assess MAD's resilience under attack, we propose a formal definition of MAD fault-tolerance and develop a comprehensive evaluation framework that jointly considers accuracy, consensus efficiency, and scalability. Extensive experiments on five benchmark datasets with varying difficulty levels demonstrate that MAD-Spear consistently outperforms the baseline attack in degrading system performance. Additionally, we observe that agent diversity substantially improves MAD performance in mathematical reasoning tasks, which challenges prior work suggesting that agent diversity has minimal impact on performance. These findings highlight the urgent need to improve the security in MAD design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T12:09:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13038v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13038v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 From Paranoia to Compliance: The Bumpy Road of System Hardening
  Practices on Stack Exchange</h2>
                <div class="authors">
                    <strong>Authors:</strong> Niklas Busch, Philip Klostermeyer, Jan H. Klemmer, Yasemin Acar, Sascha Fahl
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Hardening computer systems against cyberattacks is crucial for security. However, past incidents illustrated, that many system operators struggle with effective system hardening. Hence, many computer systems and applications remain insecure. So far, the research community lacks an in-depth understanding of system operators motivation, practices, and challenges around system hardening. With a focus on practices and challenges, we qualitatively analyzed 316 Stack Exchange (SE) posts related to system hardening. We find that access control and deployment-related issues are the most challenging, and system operators suffer from misconceptions and unrealistic expectations. Most frequently, posts focused on operating systems and server applications. System operators were driven by the fear of their systems getting attacked or by compliance reasons. Finally, we discuss our research questions, make recommendations for future system hardening, and illustrate the implications of our work.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T11:57:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13028v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13028v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Rethinking the Embodied Gap in Vision-and-Language Navigation: A
  Holistic Study of Physical and Visual Disparities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen, Jiangmiao Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T11:46:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13019v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13019v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Teach Old SAEs New Domain Tricks with Boosting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nikita Koriagin, Yaroslav Aksenov, Daniil Laptev, Gleb Gerasimov, Nikita Balagansky, Daniil Gavrilov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T10:57:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12990v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12990v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Variance-Based Pruning for Accelerating and Compressing Trained Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Uranik Berisha, Jens Mehnert, Alexandru Paul Condurache
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Increasingly expensive training of ever larger models such as Vision Transfomers motivate reusing the vast library of already trained state-of-the-art networks. However, their latency, high computational costs and memory demands pose significant challenges for deployment, especially on resource-constrained hardware. While structured pruning methods can reduce these factors, they often require costly retraining, sometimes for up to hundreds of epochs, or even training from scratch to recover the lost accuracy resulting from the structural modifications. Maintaining the provided performance of trained models after structured pruning and thereby avoiding extensive retraining remains a challenge. To solve this, we introduce Variance-Based Pruning, a simple and structured one-shot pruning technique for efficiently compressing networks, with minimal finetuning. Our approach first gathers activation statistics, which are used to select neurons for pruning. Simultaneously the mean activations are integrated back into the model to preserve a high degree of performance. On ImageNet-1k recognition tasks, we demonstrate that directly after pruning DeiT-Base retains over 70% of its original performance and requires only 10 epochs of fine-tuning to regain 99% of the original accuracy while simultaneously reducing MACs by 35% and model size by 36%, thus speeding up the model by 1.44x.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T10:54:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12988v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12988v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with
  Multiple Steps</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maximiliano Hormazábal Lagos, Álvaro Bueno Sáez, Héctor Cerezo-Costas, Pedro Alonso Doval, Jorge Alcalde Vesteiro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in Spanish). Our solution obtains answers to the questions by implementing Python code generation with LLMs that is used to filter and process the table. This solution evolves from the MRT implementation for the Semeval 2025 related task. The process consists of multiple steps: analyzing and understanding the content of the table, selecting the useful columns, generating instructions in natural language, translating these instructions to code, running it, and handling potential errors or exceptions. These steps use open-source LLMs and fine-grained optimized prompts for each step. With this approach, we achieved an accuracy score of 85\% in the task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T10:33:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12981v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12981v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Can Large Language Models Predict Audio Effects Parameters from Natural
  Language?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seungheon Doh, Junghyun Koo, Marco A. Martínez-Ramírez, Wei-Hsiang Liao, Juhan Nam, Yuki Mitsufuji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In music production, manipulating audio effects (Fx) parameters through natural language has the potential to reduce technical barriers for non-experts. We present LLM2Fx, a framework leveraging Large Language Models (LLMs) to predict Fx parameters directly from textual descriptions without requiring task-specific training or fine-tuning. Our approach address the text-to-effect parameter prediction (Text2Fx) task by mapping natural language descriptions to the corresponding Fx parameters for equalization and reverberation. We demonstrate that LLMs can generate Fx parameters in a zero-shot manner that elucidates the relationship between timbre semantics and audio effects in music production. To enhance performance, we introduce three types of in-context examples: audio Digital Signal Processing (DSP) features, DSP function code, and few-shot examples. Our results demonstrate that LLM-based Fx parameter generation outperforms previous optimization approaches, offering competitive performance in translating natural language descriptions to appropriate Fx settings. Furthermore, LLMs can serve as text-driven interfaces for audio production, paving the way for more intuitive and accessible music production tools.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T10:10:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.MM</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.20770v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.20770v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Probabilistic Soundness Guarantees in LLM Reasoning Chains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiqiu You, Anton Xue, Shreya Havaldar, Delip Rao, Helen Jin, Chris Callison-Burch, Eric Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In reasoning chains generated by large language models (LLMs), initial errors often propagate and undermine the reliability of the final conclusion. Current LLM-based error detection methods often fail to detect propagated errors because they do not properly account for how earlier errors might corrupt judgments of downstream reasoning. To better detect such propagated errors, we introduce Autoregressive Reasoning Entailment Stability (ARES), a novel probabilistic framework that prevents error propagation by judging each claim based only on previously-assessed sound premises. This inductive method yields a nuanced score for each step and provides certified statistical guarantees of its soundness, rather than a brittle binary label. ARES achieves state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2 points) and demonstrates superior robustness on very long synthetic reasoning chains, where it excels at detecting propagated errors (90.3% F1, +27.6 points).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:40:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12948v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12948v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 OASIS: Order-Augmented Strategy for Improved Code Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zuchen Gao, Zizheng Zhan, Xianming Li, Erxin Yu, Ziqi Zhan, Haotian Zhang, Bin Chen, Yuqun Zhang, Jing Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Code embeddings capture the semantic representations of code and are crucial for various code-related large language model (LLM) applications, such as code search. Previous training primarily relies on optimizing the InfoNCE loss by comparing positive natural language (NL)-code pairs with in-batch negatives. However, due to the sparse nature of code contexts, training solely by comparing the major differences between positive and negative pairs may fail to capture deeper semantic nuances. To address this issue, we propose a novel order-augmented strategy for improved code search (OASIS). It leverages order-based similarity labels to train models to capture subtle differences in similarity among negative pairs. Extensive benchmark evaluations demonstrate that our OASIS model significantly outperforms previous state-of-the-art models focusing solely on major positive-negative differences. It underscores the value of exploiting subtle differences among negative pairs with order labels for effective code embedding training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:34:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.08161v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.08161v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 DMQ: Dissecting Outliers of Diffusion Models for Post-Training
  Quantization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dongyeun Lee, Jiwan Hur, Hyounguk Shon, Jae Young Lee, Junmo Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have achieved remarkable success in image generation but come with significant computational costs, posing challenges for deployment in resource-constrained environments. Recent post-training quantization (PTQ) methods have attempted to mitigate this issue by focusing on the iterative nature of diffusion models. However, these approaches often overlook outliers, leading to degraded performance at low bit-widths. In this paper, we propose a DMQ which combines Learned Equivalent Scaling (LES) and channel-wise Power-of-Two Scaling (PTS) to effectively address these challenges. Learned Equivalent Scaling optimizes channel-wise scaling factors to redistribute quantization difficulty between weights and activations, reducing overall quantization error. Recognizing that early denoising steps, despite having small quantization errors, crucially impact the final output due to error accumulation, we incorporate an adaptive timestep weighting scheme to prioritize these critical steps during learning. Furthermore, identifying that layers such as skip connections exhibit high inter-channel variance, we introduce channel-wise Power-of-Two Scaling for activations. To ensure robust selection of PTS factors even with small calibration set, we introduce a voting algorithm that enhances reliability. Extensive experiments demonstrate that our method significantly outperforms existing works, especially at low bit-widths such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image generation quality and model stability. The code is available at https://github.com/LeeDongYeun/dmq.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:15:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12933v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12933v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding
  With Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Xu, Chao Zhang, Hanqi Jiang, Xiaoyan Wang, Ruifei Ma, Yiwei Li, Zihao Wu, Zeju Li, Xiangde Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Advancements in foundation models have made it possible to conduct applications in various downstream tasks. Especially, the new era has witnessed a remarkable capability to extend Large Language Models (LLMs) for tackling tasks of 3D scene understanding. Current methods rely heavily on 3D point clouds, but the 3D point cloud reconstruction of an indoor scene often results in information loss. Some textureless planes or repetitive patterns are prone to omission and manifest as voids within the reconstructed 3D point clouds. Besides, objects with complex structures tend to introduce distortion of details caused by misalignments between the captured images and the dense reconstructed point clouds. 2D multi-view images present visual consistency with 3D point clouds and provide more detailed representations of scene components, which can naturally compensate for these deficiencies. Based on these insights, we propose Argus, a novel 3D multimodal framework that leverages multi-view images for enhanced 3D scene understanding with LLMs. In general, Argus can be treated as a 3D Large Multimodal Foundation Model (3D-LMM) since it takes various modalities as input(text instructions, 2D multi-view images, and 3D point clouds) and expands the capability of LLMs to tackle 3D tasks. Argus involves fusing and integrating multi-view images and camera poses into view-as-scene features, which interact with the 3D features to create comprehensive and detailed 3D-aware scene embeddings. Our approach compensates for the information loss while reconstructing 3D point clouds and helps LLMs better understand the 3D world. Extensive experiments demonstrate that our method outperforms existing 3D-LMMs in various downstream tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T09:02:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/TNNLS.2025.3581411' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.12916v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12916v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 MEM1: Learning to Synergize Memory and Reasoning for Efficient
  Long-Horizon Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, Paul Pu Liang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:53:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.15841v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.15841v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General
  Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual-language Chain-of-Thought (CoT) data resources are relatively scarce compared to text-only counterparts, limiting the improvement of reasoning capabilities in Vision Language Models (VLMs). However, high-quality vision-language reasoning data is expensive and labor-intensive to annotate. To address this issue, we leverage a promising resource: game code, which naturally contains logical structures and state transition processes. Therefore, we propose Code2Logic, a novel game-code-driven approach for multimodal reasoning data synthesis. Our approach leverages Large Language Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning processes and results through code execution. Using the Code2Logic approach, we developed the GameQA dataset to train and evaluate VLMs. GameQA is cost-effective and scalable, offers controllable difficulty gradation and is diverse with 30 games and 158 tasks. Surprisingly, despite training solely on game data, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language benchmarks. Our code, dataset and models are available at https://github.com/tongjingqi/Code2Logic.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:46:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>I.2.7; I.2.10</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13886v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13886v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic
  Chain-of-Thought Synthesis Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoke Zhao, Zhaowen Zhou, Lin Chen, Lihong Wang, Zhiyi Huang, Kaiyuan Zheng, Yanjun Zheng, Xiyang Du, Longfei Liao, Jiawei Liu, Xiang Qi, Bo Zhang, Peng Zhang, Zhe Li, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models (LLMs) have demonstrated remarkable general reasoning capabilities, holding significant potential for applications in the financial domain, a field that requires robust and reliable reasoning. It has been demonstrated that distilling high-quality chain-of-thought (CoT) rationales from advanced general reasoning models offers a promising and efficient path to the financial reasoning model. However, existing CoT synthesis methods suffer from shallow CoT sampling, leaving the question of how to construct a well-designed knowledge space for finance reasoning unexplored. In this paper, we present \textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset characterized by its systematic CoT synthesis optimization. We first introduce a comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and deep financial reasoning trajectories. Furthermore, a systematic investigation, termed CoT Cube, is conducted to analyze critical factors that influence CoT effectiveness, such as necessity, length and synthesizer, yielding valuable insights for high-quality financial CoT construction. Experiments demonstrate that models trained on our Agentar-DeepFinance-300K achieve significant improvements on financial benchmarks. We publicly release Agentar-DeepFinance-300K , hoping to advance the research in financial reasoning models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:40:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12901v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12901v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 IOPO: Empowering LLMs with Complex Instruction Following via
  Input-Output Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, Yongbin Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:39:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.06208v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.06208v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 On the Limitations of Large Language Models (LLMs): False Attribution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tosin Adewumi, Nudrat Habib, Lama Alkhaled, Elisa Barney
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:20:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.04631v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.04631v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Camera-based implicit mind reading by capturing higher-order semantic
  dynamics of human gaze within environmental context</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengke Song, Yuge Xie, Qi Cui, Luming Li, Xinyu Liu, Guotao Wang, Chenglizhao Chen, Shanchen Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emotion recognition,as a step toward mind reading,seeks to infer internal states from external cues.Most existing methods rely on explicit signals-such as facial expressions,speech,or gestures-that reflect only bodily responses and overlook the influence of environmental context.These cues are often voluntary,easy to mask,and insufficient for capturing deeper,implicit emotions. Physiological signal-based approaches offer more direct access to internal states but require complex sensors that compromise natural behavior and limit scalability.Gaze-based methods typically rely on static fixation analysis and fail to capture the rich,dynamic interactions between gaze and the environment,and thus cannot uncover the deep connection between emotion and implicit behavior.To address these limitations,we propose a novel camera-based,user-unaware emotion recognition approach that integrates gaze fixation patterns with environmental semantics and temporal dynamics.Leveraging standard HD cameras,our method unobtrusively captures users'eye appearance and head movements in natural settings-without the need for specialized hardware or active user participation.From these visual cues,the system estimates gaze trajectories over time and space, providing the basis for modeling the spatial, semantic,and temporal dimensions of gaze behavior. This allows us to capture the dynamic interplay between visual attention and the surrounding environment,revealing that emotions are not merely physiological responses but complex outcomes of human-environment interactions.The proposed approach enables user-unaware,real-time,and continuous emotion recognition,offering high generalizability and low deployment cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:17:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12889v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12889v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 A Logically Consistent Chain-of-Thought Approach for Stance Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bowen Zhang, Daijun Ding, Liwen Jing, Hu Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Zero-shot stance detection (ZSSD) aims to detect stances toward unseen targets. Incorporating background knowledge to enhance transferability between seen and unseen targets constitutes the primary approach of ZSSD. However, these methods often struggle with a knowledge-task disconnect and lack logical consistency in their predictions. To address these issues, we introduce a novel approach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which improves stance detection by ensuring relevant and logically sound knowledge extraction. LC-CoT employs a three-step process. Initially, it assesses whether supplementary external knowledge is necessary. Subsequently, it uses API calls to retrieve this knowledge, which can be processed by a separate LLM. Finally, a manual exemplar guides the LLM to infer stance categories, using an if-then logical structure to maintain relevance and logical coherence. This structured approach to eliciting background knowledge enhances the model's capability, outperforming traditional supervised methods without relying on labeled data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:13:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.16054v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.16054v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 VAR-MATH: Probing True Mathematical Reasoning in Large Language Models
  via Symbolic Multi-Instance Benchmarks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jian Yao, Ran Cheng, Kay Chen Tan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of large language models (LLMs), as measured by standard benchmarks. However, these gains often persist even when models are trained with flawed signals, such as random or inverted rewards, raising a fundamental question: do such improvements reflect true reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To address this question, we take an evaluation-centric perspective and identify two critical shortcomings in existing protocols. First, \emph{benchmark contamination} arises from the public availability of test problems, increasing the risk of data leakage. Second, \emph{evaluation fragility} stems from the reliance on single-instance assessments, which are highly sensitive to stochastic outputs and fail to capture reasoning consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic evaluation framework designed to probe genuine reasoning ability. By converting fixed numerical problems into symbolic templates and requiring models to solve multiple instantiations of each, VAR-MATH enforces consistent reasoning across structurally equivalent variants, thereby mitigating contamination and improving evaluation robustness. We apply VAR-MATH to transform two popular benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and VAR-AIME24. Experimental results reveal substantial performance drops for RL-trained models on the variabilized versions, especially for smaller models, with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings suggest that many existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms. Overall, VAR-MATH offers a principled, contamination-resistant evaluation paradigm for mathematical reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:10:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12885v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12885v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced
  Knowledge Boundary Awareness</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junsheng Huang, Zhitao He, Yucheng Huang, Sandeep Polisetty, Qingyun Wang, May Fung
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:03:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.21773v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.21773v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 SEALGuard: Safeguarding the Multilingual Conversations in Southeast
  Asian Languages for LLM Software Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenliang Shan, Michael Fu, Rui Yang, Chakkrit Tantithamthavorn
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''), they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia. This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems. We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases. We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on this benchmark. Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score. Our ablation study further reveals the contributions of adaptation strategies and model size to the overall performance of SEALGuard. We release our pre-trained model and benchmark at https://github.com/awsm-research/SEALGuard to support further research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T08:01:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08898v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08898v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 LLM-Enhanced User-Item Interactions: Leveraging Edge Information for
  Optimized Recommendations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyuan Wang, Liang Wu, Liangjie Hong, Hao Liu, Yanjie Fu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph recommendation methods, representing a connected interaction perspective, reformulate user-item interactions as graphs to leverage graph structure and topology to recommend and have proved practical effectiveness at scale. Large language models, representing a textual generative perspective, excel at modeling user languages, understanding behavioral contexts, capturing user-item semantic relationships, analyzing textual sentiments, and generating coherent and contextually relevant texts as recommendations. However, there is a gap between the connected graph perspective and the text generation perspective as the task formulations are different. A research question arises: how can we effectively integrate the two perspectives for more personalized recsys? To fill this gap, we propose to incorporate graph-edge information into LLMs via prompt and attention innovations. We reformulate recommendations as a probabilistic generative problem using prompts. We develop a framework to incorporate graph edge information from the prompt and attention mechanisms for graph-structured LLM recommendations. We develop a new prompt design that brings in both first-order and second-order graph relationships; we devise an improved LLM attention mechanism to embed direct the spatial and connectivity information of edges. Our evaluation of real-world datasets demonstrates the framework's ability to understand connectivity information in graph data and to improve the relevance and quality of recommendation results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T07:51:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.09617v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.09617v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 An Investigation of Ear-EEG Signals for a Novel Biometric Authentication
  System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Danilo Avola, Giancarlo Crocetti, Gian Luca Foresti, Daniele Pannone, Claudio Piciarelli, Amedeo Ranaldi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work explores the feasibility of biometric authentication using EEG signals acquired through in-ear devices, commonly referred to as ear-EEG. Traditional EEG-based biometric systems, while secure, often suffer from low usability due to cumbersome scalp-based electrode setups. In this study, we propose a novel and practical framework leveraging ear-EEG signals as a user-friendly alternative for everyday biometric authentication. The system extracts an original combination of temporal and spectral features from ear-EEG signals and feeds them into a fully connected deep neural network for subject identification. Experimental results on the only currently available ear-EEG dataset suitable for different purposes, including biometric authentication, demonstrate promising performance, with an average accuracy of 82\% in a subject identification scenario. These findings confirm the potential of ear-EEG as a viable and deployable direction for next-generation real-world biometric systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T07:48:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12873v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12873v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case
  Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rishane Dassanayake, Mario Demetroudi, James Walpole, Lindley Lentati, Jason R. Brown, Edward James Young
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Frontier AI systems are rapidly advancing in their capabilities to persuade, deceive, and influence human behaviour, with current models already demonstrating human-level persuasion and strategic deception in specific contexts. Humans are often the weakest link in cybersecurity systems, and a misaligned AI system deployed internally within a frontier company may seek to undermine human oversight by manipulating employees. Despite this growing threat, manipulation attacks have received little attention, and no systematic framework exists for assessing and mitigating these risks. To address this, we provide a detailed explanation of why manipulation attacks are a significant threat and could lead to catastrophic outcomes. Additionally, we present a safety case framework for manipulation risk, structured around three core lines of argument: inability, control, and trustworthiness. For each argument, we specify evidence requirements, evaluation methodologies, and implementation considerations for direct application by AI companies. This paper provides the first systematic methodology for integrating manipulation risk into AI safety governance, offering AI companies a concrete foundation to assess and mitigate these threats before deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T07:45:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CR</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12872v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12872v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task
  Demonstration Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rahel Rickenbach, Bruce Lee, René Zurbrügg, Carmen Amo Alonso, Melanie N. Zeilinger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of large language models (LLMs) with control systems has demonstrated significant potential in various settings, such as task completion with a robotic manipulator. A main reason for this success is the ability of LLMs to perform in-context learning, which, however, strongly relies on the design of task examples, closely related to the target tasks. Consequently, employing LLMs to formulate optimal control problems often requires task examples that contain explicit mathematical expressions, designed by trained engineers. Furthermore, there is often no principled way to evaluate for hallucination before task execution. To address these challenges, we propose DEMONSTRATE, a novel methodology that avoids the use of LLMs for complex optimization problem generations, and instead only relies on the embedding representations of task descriptions. To do this, we leverage tools from inverse optimal control to replace in-context prompt examples with task demonstrations, as well as the concept of multitask learning, which ensures target and example task similarity by construction. Given the fact that hardware demonstrations can easily be collected using teleoperation or guidance of the robot, our approach significantly reduces the reliance on engineering expertise for designing in-context examples. Furthermore, the enforced multitask structure enables learning from few demonstrations and assessment of hallucinations prior to task execution. We demonstrate the effectiveness of our method through simulation and hardware experiments involving a robotic arm tasked with tabletop manipulation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T07:26:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12855v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12855v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust
  Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in
  Extremely Preterm Infants</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sybelle Goedicke-Fritz, Michelle Bous, Annika Engel, Matthias Flotho, Pascal Hirsch, Hannah Wittig, Dino Milanovic, Dominik Mohr, Mathias Kaspar, Sogand Nemat, Dorothea Kerner, Arno Bücker, Andreas Keller, Sascha Meyer, Michael Zemlin, Philipp Flotho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD prognosis and prediction of BPD outcome is crucial to avoid unnecessary toxicity in low risk infants. Admission radiographs of extremely preterm infants are routinely acquired within 24h of life and could serve as a non-invasive prognostic tool. In this work, we developed and investigated a deep learning approach using chest X-rays from 163 extremely low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within 24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult chest radiographs, employing progressive layer freezing with discriminative learning rates to prevent overfitting and evaluated a CutMix augmentation and linear probing. For moderate/severe BPD outcome prediction, our best performing model with progressive freezing, linear probing and CutMix achieved an AUROC of 0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67 $\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet initialization (p = 0.031) which confirms domain-specific pretraining to be important for BPD outcome prediction. Routine IRDS grades showed limited prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned markers. Our approach demonstrates that domain-specific pretraining enables accurate BPD prediction from routine day-1 radiographs. Through progressive freezing and linear probing, the method remains computationally feasible for site-level implementation and future federated learning deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T07:11:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12269v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12269v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better
  Understand Public Concerns about Vaccines</h2>
                <div class="authors">
                    <strong>Authors:</strong> Muhammad Javed, Sedigh Khademi Habibabadi, Christopher Palmer, Hazel Clothier, Jim Buttery, Gerardo Luis Dimaguila
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vaccine hesitancy threatens public health, leading to delayed or rejected vaccines. Social media is a vital source for understanding public concerns, and traditional methods like topic modelling often struggle to capture nuanced opinions. Though trained for query answering, large Language Models (LLMs) often miss current events and community concerns. Additionally, hallucinations in LLMs can compromise public health communication. To address these limitations, we developed a tool (VaxPulse Query Corner) using the Retrieval Augmented Generation technique. It addresses complex queries about public vaccine concerns on various online platforms, aiding public health administrators and stakeholders in understanding public concerns and implementing targeted interventions to boost vaccine confidence. Analysing 35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and relevance (0.94).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T06:59:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.LG</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12840v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12840v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 AI Governance InternationaL Evaluation Index (AGILE Index) 2024</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Zeng, Enmeng Lu, Xin Guan, Cunqing Huangfu, Zizhe Ruan, Ammar Younas, Kang Sun, Xuan Tang, Yuwei Wang, Hongjie Suo, Dongqi Liang, Zhengqiang Han, Aorigele Bao, Xiaoyang Guo, Jin Wang, Jiawei Xie, Yao Liang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of Artificial Intelligence (AI) technology is profoundly transforming human society and concurrently presenting a series of ethical, legal, and social issues. The effective governance of AI has become a crucial global concern. Since 2022, the extensive deployment of generative AI, particularly large language models, marked a new phase in AI governance. Continuous efforts are being made by the international community in actively addressing the novel challenges posed by these AI developments. As consensus on international governance continues to be established and put into action, the practical importance of conducting a global assessment of the state of AI governance is progressively coming to light. In this context, we initiated the development of the AI Governance InternationaL Evaluation Index (AGILE Index). Adhering to the design principle, "the level of governance should match the level of development," the inaugural evaluation of the AGILE Index commences with an exploration of four foundational pillars: the development level of AI, the AI governance environment, the AI governance instruments, and the AI governance effectiveness. It covers 39 indicators across 18 dimensions to comprehensively assess the AI governance level of 14 representative countries globally. The index is utilized to delve into the status of AI governance to date in 14 countries for the first batch of evaluation. The aim is to depict the current state of AI governance in these countries through data scoring, assist them in identifying their governance stage and uncovering governance issues, and ultimately offer insights for the enhancement of their AI governance systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T06:34:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span><span>68T01</span><span>A.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.15859v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.15859v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Emotional Support with LLM-based Empathetic Dialogue Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shiquan Wang, Ruiyu Fang, Zhongjiang He, Shuangyong Song, Yongxiang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emotional Support Conversation (ESC) aims to provide empathetic and effective emotional assistance through dialogue, addressing the growing demand for mental health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC evaluation, where we leverage large-scale language models enhanced by prompt engineering and finetuning techniques. We explore both parameter-efficient Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the model's ability to generate supportive and contextually appropriate responses. Our best model ranked second in the competition, highlighting the potential of combining LLMs with effective adaptation methods for ESC tasks. Future work will focus on further enhancing emotional understanding and response personalization to build more practical and reliable emotional support systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T06:24:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12820v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12820v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free
  Zero-Shot Composed Image Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeong-Woo Park, Seong-Whan Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Composed Image Retrieval (CIR) is the task of retrieving a target image from a gallery using a composed query consisting of a reference image and a modification text. Among various CIR approaches, training-free zero-shot methods based on pre-trained models are cost-effective but still face notable limitations. For example, sequential VLM-LLM pipelines process each modality independently, which often results in information loss and limits cross-modal interaction. In contrast, methods based on multimodal large language models (MLLMs) often focus exclusively on applying changes indicated by the text, without fully utilizing the contextual visual information from the reference image. To address these issues, we propose multi-faceted Chain-of-Thought with re-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes multi-faceted Chain-of-Thought to guide the MLLM to balance explicit modifications and contextual visual cues, generating two distinct captions: one focused on modification and the other integrating comprehensive visual-textual context. The first caption is used to filter candidate images. Subsequently, we combine these two captions and the reference image to perform multi-grained re-ranking. This two-stage approach facilitates precise retrieval by aligning with the textual modification instructions while preserving the visual context of the reference image. Through extensive experiments, MCoT-RE achieves state-of-the-art results among training-free methods, yielding improvements of up to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T06:22:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12819v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12819v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Large Language Models' Internal Perception of Symbolic Music</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrew Shin, Kunitake Kaneko
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) excel at modeling relationships between strings in natural language and have shown promise in extending to other symbolic domains like coding or mathematics. However, the extent to which they implicitly model symbolic music remains underexplored. This paper investigates how LLMs represent musical concepts by generating symbolic music data from textual prompts describing combinations of genres and styles, and evaluating their utility through recognition and generation tasks. We produce a dataset of LLM-generated MIDI files without relying on explicit musical training. We then train neural networks entirely on this LLM-generated MIDI dataset and perform genre and style classification as well as melody completion, benchmarking their performance against established models. Our results demonstrate that LLMs can infer rudimentary musical structures and temporal relationships from text, highlighting both their potential to implicitly encode musical patterns and their limitations due to a lack of explicit musical context, shedding light on their generative capabilities for symbolic music.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:48:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12808v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12808v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 THOR: Transformer Heuristics for On-Demand Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Isaac Shi, Zeyuan Li, Fan Liu, Wenli Wang, Lewei He, Yang Yang, Tianyu Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce the THOR (Transformer Heuristics for On-Demand Retrieval) Module, designed and implemented by eSapiens, a secure, scalable engine that transforms natural-language questions into verified, read-only SQL analytics for enterprise databases. The Text-to-SQL module follows a decoupled orchestration/execution architecture: a Supervisor Agent routes queries, Schema Retrieval dynamically injects table and column metadata, and a SQL Generation Agent emits single-statement SELECT queries protected by a read-only guardrail. An integrated Self-Correction & Rating loop captures empty results, execution errors, or low-quality outputs and triggers up to five LLM-driven regeneration attempts. Finally, a Result Interpretation Agent produces concise, human-readable insights and hands raw rows to the Insight & Intelligence engine for visualization or forecasting.   Smoke tests across finance, sales, and operations scenarios demonstrate reliable ad-hoc querying and automated periodic reporting. By embedding schema awareness, fault-tolerant execution, and compliance guardrails, the THOR Module empowers non-technical users to access live data with zero-SQL simplicity and enterprise-grade safety.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:47:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.09592v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.09592v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Huan Wang, Shelby Heinecke, Silvio Savarese, Caiming Xiong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce \oursystemname, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:46:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12806v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12806v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 A comparative study of physics capabilities of a liquid argon and a
  water based liquid scintillator at DUNE</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nishat Fiza, Suhyeon Kim, Emar Masaku, Mehedi Masud, Hokyeong Nam, Juseong Park, Yujin Park, Kim Siyeon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a comprehensive comparison of the physics sensitivities of a Liquid Argon Time Projection Chamber (LArTPC) and a Water-based Liquid Scintillator (WbLS) detector, considering their potential deployment as the fourth far detector module in the DUNE facility. Using GLoBES-based simulations, we evaluate their performance in measuring standard neutrino oscillation parameters ($\theta_{23}, \delta_{13}$ and $\Delta m^{2}_{31}$), both in standard 3-neutrino case, as well as in presence of new physics scenarios involving light sterile neutrinos and neutral-current non-standard interactions (NC NSI). Our findings show that THEIA (a WbLS-based detector) significantly outperforms LArTPC in resolving the CP phase $\delta_{13}$,- especially near maximal CP violation, and in lifting the octant degeneracy of $\theta_{23}$ due to its superior energy resolution and ability to clearly identify the second oscillation maximum. Furthermore, THEIA offers competitive reconstruction precision even with relatively moderate energy resolutions ($7-10\%/\sqrt{E}$) and demonstrates enhanced robustness under new physics scenarios. These results support the physics-driven case for a hybrid DUNE configuration utilizing both LArTPC and WbLS technologies for optimized sensitivity across the full spectrum of neutrino oscillation and physics beyond the standard model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:44:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>hep-ph</span><span>hep-ex</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.07476v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.07476v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 ReCode: Updating Code API Knowledge with Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:31:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span><span>cs.LG</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.20495v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.20495v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 MPO: An Efficient Post-Processing Framework for Mixing Diverse
  Preference Alignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, Linjun Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T05:29:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.18699v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.18699v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Learning Robust Negation Text Representations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thinh Hung Truong, Karin Verspoor, Trevor Cohn, Timothy Baldwin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite rapid adoption of autoregressive large language models, smaller text encoders still play an important role in text understanding tasks that require rich contextualized representations. Negation is an important semantic function that is still not properly captured by such methods, affecting many downstream applications relying on text embeddings. We propose a strategy to improve negation robustness of text encoders, by distilling data from large language models using diverse patterns of negation and hedging. We adopt a standard contrastive learning strategy to finetune a strong BERT-based model, and observe large improvement in negation understanding capabilities while maintaining competitive performance on general benchmarks. In addition, we also show that our method can be adapted to LLMs, leading to improved performance on negation benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T04:48:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12782v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12782v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 A Comprehensive Survey of Electronic Health Record Modeling: From Deep
  Learning Approaches to Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weijieying Ren, Jingxi Zhu, Zehao Liu, Tianxiang Zhao, Vasant Honavar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artificial intelligence (AI) has demonstrated significant potential in transforming healthcare through the analysis and modeling of electronic health records (EHRs). However, the inherent heterogeneity, temporal irregularity, and domain-specific nature of EHR data present unique challenges that differ fundamentally from those in vision and natural language tasks. This survey offers a comprehensive overview of recent advancements at the intersection of deep learning, large language models (LLMs), and EHR modeling. We introduce a unified taxonomy that spans five key design dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based modeling systems. Within each dimension, we review representative methods addressing data quality enhancement, structural and temporal representation, self-supervised learning, and integration with clinical knowledge. We further highlight emerging trends such as foundation models, LLM-driven clinical agents, and EHR-to-text translation for downstream reasoning. Finally, we discuss open challenges in benchmarking, explainability, clinical alignment, and generalization across diverse clinical settings. This survey aims to provide a structured roadmap for advancing AI-driven EHR modeling and clinical decision support. For a comprehensive list of EHR-related methods, kindly refer to https://survey-on-tabular-data.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T04:31:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12774v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12774v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Critique-GRPO: Advancing LLM Reasoning with Natural Language and
  Numerical Feedback</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, Helen Meng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of spontaneous self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided self-refinements simultaneously while maintaining exploration. Additionally, we employ a shaping function to amplify learning from correct, especially unfamiliar, refinements and penalize incorrect ones. Extensive experiments with Qwen2.5-7B-Base, Qwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently outperforms supervised learning and RL-based fine-tuning methods across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.4% and 3.8% on Qwen2.5-7B-Base and Qwen3-8B, respectively. Notably, Critique-GRPO enables effective self-improvement through self-critiquing and weak-to-strong generalization, achieving consistent gains over GRPO, such as 16.7% and 10.0% pass@1 improvements on AIME 2024, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T04:08:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.03106v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.03106v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation
  of Text Analytics with Intelligent Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sam Yu-Te Lee, Chengyang Ji, Shicheng Wen, Lifu Huang, Dongyu Liu, Kwan-Liu Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis (e.g., topic detection, summarization, information extraction, etc.). We introduce VIDEE, a system that supports entry-level data analysts to conduct advanced text analytics with intelligent agents. VIDEE instantiates a human-agent collaroration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results. We conduct two quantitative experiments to evaluate VIDEE's effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience -- from none to expert -- demonstrates the system's usability and reveals distinct user behavior patterns. The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T03:52:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.21582v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.21582v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Aime: Towards Fully-Autonomous Multi-Agent Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yexuan Shi, Mingyu Wang, Yunxiang Cao, Hongjie Lai, Junjian Lan, Xin Han, Yu Wang, Jie Geng, Zhenan Li, Zihao Xia, Xiang Chen, Chen Li, Jian Xu, Wenbo Duan, Yuanshuo Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are emerging as a powerful paradigm for solving complex, multifaceted problems. However, the potential of these systems is often constrained by the prevalent plan-and-execute framework, which suffers from critical limitations: rigid plan execution, static agent capabilities, and inefficient communication. These weaknesses hinder their adaptability and robustness in dynamic environments. This paper introduces Aime, a novel multi-agent framework designed to overcome these challenges through dynamic, reactive planning and execution. Aime replaces the conventional static workflow with a fluid and adaptive architecture. Its core innovations include: (1) a Dynamic Planner that continuously refines the overall strategy based on real-time execution feedback; (2) an Actor Factory that implements Dynamic Actor instantiation, assembling specialized agents on-demand with tailored tools and knowledge; and (3) a centralized Progress Management Module that serves as a single source of truth for coherent, system-wide state awareness. We empirically evaluated Aime on a diverse suite of benchmarks spanning general reasoning (GAIA), software engineering (SWE-bench Verified), and live web navigation (WebVoyager). The results demonstrate that Aime consistently outperforms even highly specialized state-of-the-art agents in their respective domains. Its superior adaptability and task success rate establish Aime as a more resilient and effective foundation for multi-agent collaboration.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T03:34:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11988v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11988v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 A Comprehensive Survey of Synthetic Tabular Data Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruxue Shi, Yili Wang, Mengnan Du, Xu Shen, Yi Chang, Xin Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tabular data is one of the most prevalent and important data formats in real-world applications such as healthcare, finance, and education. However, its effective use in machine learning is often constrained by data scarcity, privacy concerns, and class imbalance. Synthetic tabular data generation has emerged as a powerful solution, leveraging generative models to learn underlying data distributions and produce realistic, privacy-preserving samples. Although this area has seen growing attention, most existing surveys focus narrowly on specific methods (e.g., GANs or privacy-enhancing techniques), lacking a unified and comprehensive view that integrates recent advances such as diffusion models and large language models (LLMs).   In this survey, we present a structured and in-depth review of synthetic tabular data generation methods. Specifically, the survey is organized into three core components: (1) Background, which covers the overall generation pipeline, including problem definitions, synthetic tabular data generation methods, post processing, and evaluation; (2) Generation Methods, where we categorize existing approaches into traditional generation methods, diffusion model methods, and LLM-based methods, and compare them in terms of architecture, generation quality, and applicability; and (3) Applications and Challenges, which summarizes practical use cases, highlights common datasets, and discusses open challenges such as heterogeneity, data fidelity, and privacy protection.   This survey aims to provide researchers and practitioners with a holistic understanding of the field and to highlight key directions for future work in synthetic tabular data generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T03:22:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.16506v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.16506v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps
  and Large Language Models Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fujing Xie, Sören Schwertfeger, Hermann Blum
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features, achieving a high level of detail and guiding robots to find objects specified by open-vocabulary language queries. While the issue of scalability for such approaches has received some attention, another fundamental problem is that high-detail object mapping quickly becomes outdated, as objects get moved around a lot. In this work, we develop a mapping and navigation system for object-goal navigation that, from the ground up, considers the possibilities that a queried object can have moved, or may not be mapped at all. Instead of striving for high-fidelity mapping detail, we consider that the main purpose of a map is to provide environment grounding and context, which we combine with the semantic priors of LLMs to reason about object locations and deploy an active, online approach to navigate to the objects. Through simulated and real-world experiments we find that our approach tends to have higher retrieval success at shorter path lengths for static objects and by far outperforms prior approaches in cases of dynamic or unmapped object queries. We provide our code and dataset at: https://anonymous.4open.science/r/osmAG-LLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T03:14:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12753v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12753v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 ASC-SW: Atrous strip convolution network with sliding windows for
  visual-assisted map navigation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cheng Liu, Fan Zhu, Yaoyu Zhuang Zhinan Chen Jiefeng Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapid development of lightweight visual neural network architectures, traditional high-performance vision models have undergone significant compression, greatly improving their computational efficiency and energy consumption ratio. This makes them feasible for deployment on resource-constrained edge computing devices. We propose a visual-assisted navigation framework called Atrous Strip Convolution-Sliding Window (ASC-SW), which leverages a depth camera and a lightweight visual neural network to assist map-based mobile robot navigation. This framework compensates for the inability of traditional light detection and range (LiDAR) sensors to detect ground-level obstacles such as ground-level wires. We introduce a lightweight and efficient segmentation model, Atrous Strip Convolution Network (ASCnet), for detecting deformable linear objects (DLOs). MobileNetV2 is used as the backbone network, and Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP) is designed to extract DLO features more effectively. Atrous Strip Convolution is integrated into ASCSPP to accurately identify the linear structure of DLOs with low computational cost. Additionally, a Sliding Window (SW) post-processing module is proposed to denoise the output in complex environments, improving recognition accuracy. Our method strikes a balance between inference speed and segmentation performance. It achieves a mean Intersection over Union (Miou) score of 75.3% on a self-built dataset and reaches 9.3 FPS inference speed on the Jetson Orin Nano edge device. Overall, our approach outperforms existing DLO detection models and has been successfully validated on a physical robotic platform.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T02:59:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12744v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12744v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Learning Universal Human Mobility Patterns with a Foundation Model for
  Cross-domain Data Fusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoxuan Ma, Xishun Liao, Yifan Liu, Qinhua Jiang, Chris Stanford, Shangqing Cao, Jiaqi Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Human mobility modeling is critical for urban planning and transportation management, yet existing approaches often lack the integration capabilities needed to handle diverse data sources. We present a foundation model framework for universal human mobility patterns that leverages cross-domain data fusion and large language models to address these limitations. Our approach integrates multi-modal data of distinct nature and spatio-temporal resolution, including geographical, mobility, socio-demographic, and traffic information, to construct a privacy-preserving and semantically enriched human travel trajectory dataset. Our framework demonstrates adaptability through domain transfer techniques that ensure transferability across diverse urban contexts, as evidenced in case studies of Los Angeles (LA) and Egypt. The framework employs LLMs for semantic enrichment of trajectory data, enabling comprehensive understanding of mobility patterns. Quantitative evaluation shows that our generated synthetic dataset accurately reproduces mobility patterns observed in empirical data. The practical utility of this foundation model approach is demonstrated through large-scale traffic simulations for LA County, where results align well with observed traffic data. On California's I-405 corridor, the simulation yields a Mean Absolute Percentage Error of 5.85% for traffic volume and 4.36% for speed compared to Caltrans PeMS observations, illustrating the framework's potential for intelligent transportation systems and urban mobility applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T02:52:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.15779v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.15779v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Public Evaluation on Potential Social Impacts of Fully Autonomous
  Cybernetic Avatars for Physical Support in Daily-Life Environments:
  Large-Scale Demonstration and Survey at Avatar Land</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lotfi El Hafi, Kazuma Onishi, Shoichi Hasegawa, Akira Oyama, Tomochika Ishikawa, Masashi Osada, Carl Tornberg, Ryoma Kado, Kento Murata, Saki Hashimoto, Sebastian Carrera Villalobos, Akira Taniguchi, Gustavo Alfonso Garcia Ricardez, Yoshinobu Hagiwara, Tatsuya Aoki, Kensuke Iwata, Takato Horii, Yukiko Horikawa, Takahiro Miyashita, Tadahiro Taniguchi, Hiroshi Ishiguro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cybernetic avatars (CAs) are key components of an avatar-symbiotic society, enabling individuals to overcome physical limitations through virtual agents and robotic assistants. While semi-autonomous CAs intermittently require human teleoperation and supervision, the deployment of fully autonomous CAs remains a challenge. This study evaluates public perception and potential social impacts of fully autonomous CAs for physical support in daily life. To this end, we conducted a large-scale demonstration and survey during Avatar Land, a 19-day public event in Osaka, Japan, where fully autonomous robotic CAs, alongside semi-autonomous CAs, performed daily object retrieval tasks. Specifically, we analyzed responses from 2,285 visitors who engaged with various CAs, including a subset of 333 participants who interacted with fully autonomous CAs and shared their perceptions and concerns through a survey questionnaire. The survey results indicate interest in CAs for physical support in daily life and at work. However, concerns were raised regarding task execution reliability. In contrast, cost and human-like interaction were not dominant concerns. Project page: https://lotfielhafi.github.io/FACA-Survey/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T02:49:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12741v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12741v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 DSSD: Efficient Edge-Device LLM Deployment and Collaborative Inference
  via Distributed Split Speculative Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahong Ning, Ce Zheng, Tingting Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have transformed natural language processing but face critical deployment challenges in device-edge systems due to resource limitations and communication overhead. To address these issues, collaborative frameworks have emerged that combine small language models (SLMs) on devices with LLMs at the edge, using speculative decoding (SD) to improve efficiency. However, existing solutions often trade inference accuracy for latency or suffer from high uplink transmission costs when verifying candidate tokens. In this paper, we propose Distributed Split Speculative Decoding (DSSD), a novel architecture that not only preserves the SLM-LLM split but also partitions the verification phase between the device and edge. In this way, DSSD replaces the uplink transmission of multiple vocabulary distributions with a single downlink transmission, significantly reducing communication latency while maintaining inference quality. Experiments show that our solution outperforms current methods, and codes are at: https://github.com/JasonNing96/DSSD-Efficient-Edge-Computing
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T02:34:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12000v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12000v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 TransEvalnia: Reasoning-based Evaluation and Ranking of Translations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Richard Sproat, Tianyu Zhao, Llion Jones
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present TransEvalnia, a prompting-based translation evaluation and ranking system that uses reasoning in performing its evaluations and ranking. This system presents fine-grained evaluations based on a subset of the Multidimensional Quality Metrics (https://themqm.org/), returns an assessment of which translation it deems the best, and provides numerical scores for the various dimensions and for the overall translation. We show that TransEvalnia performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al. 2024) on our own English-Japanese data as well as several language pairs from various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations returned are deemed highly acceptable to human raters, and that the scores assigned to the translations by Sonnet, as well as other LLMs, correlate well with scores assigned by the human raters. We also note the sensitivity of our system -- as well as MT-Ranker -- to the order in which the translations are presented, and we propose methods to address this position bias. All data, including the system's evaluation and reasoning, human assessments, as well as code is released.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T02:02:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12724v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12724v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 MoistureMapper: An Autonomous Mobile Robot for High-Resolution Soil
  Moisture Mapping at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nathaniel Rose, Hannah Chuang, Manuel A Andrade-Rodriguez, Rishi Parashar, Dani Or, Parikshit Maini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Soil moisture is a quantity of interest in many application areas including agriculture and climate modeling. Existing methods are not suitable for scale applications due to large deployment costs in high-resolution sensing applications such as for variable irrigation. In this work, we design, build and field deploy an autonomous mobile robot, MoistureMapper, for soil moisture sensing. The robot is equipped with Time Domain Reflectometry (TDR) sensors and a direct push drill mechanism for deploying the sensor to measure volumetric water content in the soil. Additionally, we implement and evaluate multiple adaptive sampling strategies based on a Gaussian Process based modeling to build a spatial mapping of moisture distribution in the soil. We present results from large scale computational simulations and proof-of-concept deployment on the field. The adaptive sampling approach outperforms a greedy benchmark approach and results in up to 30\% reduction in travel distance and 5\% reduction in variance in the reconstructed moisture maps. Link to video showing field experiments: https://youtu.be/S4bJ4tRzObg
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T01:47:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.12716v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.12716v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Synthesizing Privacy-Preserving Text Data via Finetuning without
  Finetuning Billion-Scale LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bowen Tan, Zheng Xu, Eric Xing, Zhiting Hu, Shanshan Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Synthetic data offers a promising path to train models while preserving data privacy. Differentially private (DP) finetuning of large language models (LLMs) as data generator is effective, but is impractical when computation resources are limited. Meanwhile, prompt-based methods such as private evolution depend heavily on the manual prompts, and ineffectively use private information in their iterative data selection process. To overcome these limitations, we propose CTCL (Data Synthesis with ConTrollability and CLustering), a novel framework for generating privacy-preserving synthetic data without extensive prompt engineering or billion-scale LLM finetuning. CTCL pretrains a lightweight 140M conditional generator and a clustering-based topic model on large-scale public data. To further adapt to the private domain, the generator is DP finetuned on private data for fine-grained textual information, while the topic model extracts a DP histogram representing distributional information. The DP generator then samples according to the DP histogram to synthesize a desired number of data examples. Evaluation across five diverse domains demonstrates the effectiveness of our framework, particularly in the strong privacy regime. Systematic ablation validates the design of each framework component and highlights the scalability of our approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T01:39:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.12347v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.12347v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based
  Code Assistance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Myeongsoo Kim, Shweta Garg, Baishakhi Ray, Varun Kumar, Anoop Deoras
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Programming assistants powered by large language models have transformed software development, yet most benchmarks focus narrowly on code generation tasks. Recent efforts like InfiBench and StackEval attempt to address this gap using Stack Overflow data but remain limited to single-turn interactions in isolated contexts, require significant manual curation, and fail to represent complete project environments. We introduce CodeAssistBench (CAB), the first benchmark framework for evaluating multi-turn programming assistance in realistic settings that address real-world questions about actual codebases. Unlike existing programming Q&A benchmarks, CAB automatically generates scalable datasets from question-related GitHub issues using configurable parameters (e.g., repository creation date, star count, programming languages), and includes automatic containerization of codebases for evaluation. It then evaluates models through simulated users in these containerized environments with full codebase access. Using this framework, we constructed a test set of 3,286 real-world programming questions across 231 repositories, spanning seven programming languages and diverse problem domains. Our evaluation of leading LLMs reveals a substantial capability gap: while models perform well on Stack Overflow questions with success rates of 70-83%, they resolve only up to 16.49% of CAB's recent issues. This discrepancy highlights the challenges of providing assistance in complex, project-specific contexts versus answering standalone questions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T01:38:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10646v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10646v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 GUI Test Migration via Abstraction and Concretization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yakun Zhang, Chen Liu, Xiaofei Xie, Yun Lin, Jin Song Dong, Dan Hao, Lu Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> GUI test migration aims to produce test cases with events and assertions to test specific functionalities of a target app. Existing migration approaches typically focus on the widget-mapping paradigm that maps widgets from source apps to target apps. However, since different apps may implement the same functionality in different ways, direct mapping may result in incomplete or buggy test cases, thus significantly impacting the effectiveness of testing target functionality and the practical applicability of migration approaches.   In this paper, we propose a new migration paradigm (i.e., the abstraction-concretization paradigm) that first abstracts the test logic for the target functionality and then utilizes this logic to generate the concrete GUI test case. Furthermore, we introduce MACdroid, the first approach that migrates GUI test cases based on this paradigm. Specifically, we propose an abstraction technique that utilizes source test cases from source apps targeting the same functionality to extract a general test logic for that functionality. Then, we propose a concretization technique that utilizes the general test logic to guide an LLM in generating the corresponding GUI test case (including events and assertions) for the target app. We evaluate MACdroid on two widely-used datasets (including 31 apps, 34 functionalities, and 123 test cases). On the FrUITeR dataset, the test cases generated by MACdroid successfully test 64% of the target functionalities, improving the baselines by 191%. On the Lin dataset, MACdroid successfully tests 75% of the target functionalities, outperforming the baselines by 42%. These results underscore the effectiveness of MACdroid in GUI test migration.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T01:36:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3726525' target='_blank'>doi</a><a href='http://arxiv.org/abs/2409.05028v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.05028v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Scaling Trends for Data Poisoning in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs produce harmful and undesirable behavior when trained on datasets containing even a small fraction of poisoned data. We demonstrate that GPT models remain vulnerable to fine-tuning on poisoned data, even when safeguarded by moderation systems. Given the persistence of data poisoning vulnerabilities in today's most capable models, this paper investigates whether these risks increase with model scaling. We evaluate three threat models -- malicious fine-tuning, imperfect data curation, and intentional data contamination -- across 24 frontier LLMs ranging from 1.5 to 72 billion parameters. Our experiments reveal that larger LLMs are significantly more susceptible to data poisoning, learning harmful behaviors from even minimal exposure to harmful data more quickly than smaller models. These findings underscore the need for leading AI companies to thoroughly red team fine-tuning APIs before public release and to develop more robust safeguards against data poisoning, particularly as models continue to scale in size and capability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T01:19:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.02946v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.02946v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 LLM-RecG: A Semantic Bias-Aware Framework for Zero-Shot Sequential
  Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunzhe Li, Junting Wang, Hari Sundaram, Zhining Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions in unseen domains without additional training or fine-tuning, addressing the limitations of traditional models in sparse data environments. Recent advancements in large language models (LLMs) have significantly enhanced ZCDSR by facilitating cross-domain knowledge transfer through rich, pretrained representations. Despite this progress, domain semantic bias -- arising from differences in vocabulary and content focus between domains -- remains a persistent challenge, leading to misaligned item embeddings and reduced generalization across domains. To address this, we propose a novel semantic bias-aware framework that enhances LLM-based ZCDSR by improving cross-domain alignment at both the item and sequential levels. At the item level, we introduce a generalization loss that aligns the embeddings of items across domains (inter-domain compactness), while preserving the unique characteristics of each item within its own domain (intra-domain diversity). This ensures that item embeddings can be transferred effectively between domains without collapsing into overly generic or uniform representations. At the sequential level, we develop a method to transfer user behavioral patterns by clustering source domain user sequences and applying attention-based aggregation during target domain inference. We dynamically adapt user embeddings to unseen domains, enabling effective zero-shot recommendations without requiring target-domain interactions...
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T01:08:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3705328.3748077' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.19232v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.19232v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 SHREC: A Framework for Advancing Next-Generation Computational
  Phenotyping with Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sarah Pungitore, Shashank Yadav, Molly Douglas, Jarrod Mosier, Vignesh Subbian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Objective: Computational phenotyping is a central informatics activity with resulting cohorts supporting a wide variety of applications. However, it is time-intensive because of manual data review, limited automation, and difficulties in adapting algorithms across sources. Since LLMs have demonstrated promising capabilities for text classification, comprehension, and generation, we posit they will perform well at repetitive manual review tasks traditionally performed by human experts. To support next-generation computational phenotyping methods, we developed SHREC, a framework for comprehensive integration of LLMs into end-to-end phenotyping pipelines. Methods: We applied and tested the ability of three lightweight LLMs (Gemma2 27 billion, Mistral Small 24 billion, and Phi-4 14 billion) to classify concepts and phenotype patients using previously developed phenotypes for ARF respiratory support therapies. Results: All models performed well on concept classification, with the best model (Mistral) achieving an AUROC of 0.896 across all relevant concepts. For phenotyping, models demonstrated near-perfect specificity for all phenotypes, and the top-performing model (Mistral) reached an average AUROC of 0.853 for single-therapy phenotypes, despite lower performance on multi-therapy phenotypes. Conclusion: Current lightweight LLMs can feasibly assist researchers with resource-intensive phenotyping tasks such as manual data review. There are several advantages of LLMs that support their application to computational phenotyping, such as their ability to adapt to new tasks with prompt engineering alone and their ability to incorporate raw EHR data. Future steps to advance next-generation phenotyping methods include determining optimal strategies for integrating biomedical data, exploring how LLMs reason, and advancing generative model methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-17T00:41:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.QM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.16359v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.16359v3' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    