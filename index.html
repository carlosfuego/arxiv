
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 The Bicameral Cache: a split cache for vector architectures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Susana Rebolledo, Borja Perez, Jose Luis Bosque, Peter Hsu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Bicameral Cache is a cache organization proposal for a vector architecture that segregates data according to their access type, distinguishing scalar from vector references. Its aim is to avoid both types of references from interfering in each other's data locality, with a special focus on prioritizing the performance on vector references. The proposed system incorporates an additional, non-polluting prefetching mechanism to help populate the long vector cache lines in advance to increase the hit rate by further exploiting the spatial locality on vector data. Its evaluation was conducted on the Cavatools simulator, comparing the performance to a standard conventional cache, over different typical vector benchmarks for several vector lengths. The results proved the proposed cache speeds up performance on stride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition, the prefetching feature consistently provided an additional value.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:18:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15440v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15440v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 At Least Factor-of-Two Optimization for RWLE-Based Homomorphic
  Encryption</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Ly
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Many modern applications that deal with sensitive data, such as healthcare and government services, outsource computation to cloud platforms. In such untrusted environments, privacy is of vital importance. One solution to this problem is homomorphic encryption (HE), a family of cryptographic schemes that support certain algebraic operations on encrypted data without the need for decryption. However, despite major advancements, encryption in modern HE schemes still comes with a non-trivial computational overhead that can hamper data-intensive workloads. To resolve this, recent research has shown that leveraging caching techniques, such as Rache, can significantly enhance the performance of HE schemes while maintaining security. Rache unfortunately displays a key limitation in the time complexity of its caching procedure, which scales with the size of the plaintext space. Smuche is another caching scheme that simultaneously improves the scalability of the caching procedure and turns the encryption process into a constant-time operation, utilizing only a single scalar multiplication. Even still, more can be done. In this paper, we present an encryption method we call ``Zinc" which entirely forgoes the multiple caching process, replacing it with a single scalar addition, and then injecting randomness that takes constant time with respect to the plaintext space. This injection of randomness is similar to Smuche, and a great improvement from Rache, allowing Zinc to achieve efficiency without compromising security. We implement the scheme using Microsoft SEAL and compare its performance to vanilla CKKS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T05:42:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07304v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07304v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 ConfusedPilot: Confused Deputy Risks in RAG-based LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.   In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T05:24:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.04870v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.04870v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Cache-Aided MIMO Communications: DoF Analysis and Transmitter
  Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad NaseriTehrani, MohammadJavad Salehi, Antti Tölli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cache-aided MIMO communications aims to jointly exploit both coded caching~(CC) and spatial multiplexing gains to enhance communication efficiency. In this paper, we first analyze the achievable degrees of freedom~(DoF) in a MIMO-CC system with CC gain \(t\), where a server with \(L\) transmit antennas communicates with \(K\) users, each equipped with \(G\) receive antennas. We demonstrate that the enhanced achievable DoF is \(\max_{\beta, \Omega} \Omega \beta\), where the number of users \(\Omega\) served in each transmission is fine-tuned to maximize DoF, and \(\beta \le \min\big(G, \nicefrac{L \binom{\Omega-1}{t}}{1 + (\Omega - t - 1)\binom{\Omega-1}{t}}\big)\) represents the number of parallel streams decoded by each user. Second, we introduce an effective transmit covariance matrix design aimed at maximizing the symmetric rate, solved iteratively via successive convex approximation. Third, we propose a new class of MIMO-CC schemes using a novel scheduling mechanism leveraging maximal multicasting opportunities to maximize delivery rates at given SNR levels while adhering to linear processing constraints. Lastly, we devise linear multicast beamforming strategies tailored for the flexible scheduling schemes in MIMO-CC systems and present an iterative solution for the efficient design of beamformers. Extensive numerical simulations are used to verify the results of the paper.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T13:56:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>eess.SP</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15743v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15743v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Ownership in low-level intermediate representation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Siddharth Priya, Arie Gurfinkel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The concept of ownership in high level languages can aid both the programmer and the compiler to reason about the validity of memory operations. Previously, ownership semantics has been used successfully in high level automatic program verification to model a reference to data by a first order logic (FOL) representation of data instead of maintaining an address map. However, ownership semantics is not used in low level program verification. We have identified two challenges. First, ownership information is lost when a program is compiled to a low level intermediate representation (e.g., in LLVM IR). Second, pointers in low level programs point to bytes using an address map (e.g., in unsafe Rust) and thus the verification condition (VC) cannot always replace a pointer by its FOL abstraction. To remedy the situation, we develop ownership semantics for an LLVM like low level intermediate representation. Using these semantics, the VC can opportunistically model some memory accesses by a direct access of a pointer cache that stores byte representation of data. This scheme reduces instances where an address map must be maintained, especially for mostly safe programs that follow ownership semantics. For unsafe functionality, memory accesses are modelled by operations on an address map and we provide mechanisms to keep the address map and pointer cache in sync. We implement these semantics in SEABMC, a bit precise bounded model checker for LLVM. For evaluation, the source programs are assumed to be written in C. Since C does not have ownership built in, suitable macros are added that introduce and preserve ownership during translation to LLVM like IR for verification. This approach is evaluated on mature open source C code. For both handcrafted benchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT solving.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T13:31:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span><span>cs.SE</span><span>D.2.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.04043v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.04043v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Decision-Focused Learning to Predict Action Costs for Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jayanta Mandi, Marco Foschini, Daniel Holler, Sylvie Thiebaux, Jorg Hoffmann, Tias Guns
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward. Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality. This approach yields better results than treating prediction and optimization as separate tasks. In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs. There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning. We propose novel methods for gradient computation to avoid this issue. (2) DFL requires repeated planner calls during training, which can limit the scalability of the method. We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process. As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T13:14:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06876v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06876v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache
  Consumption</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T09:55:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.18003v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.18003v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Finch: Prompt-guided Key-Value Cache Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giulio Corallo, Paolo Papotti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent large language model applications, such as Retrieval-Augmented Generation and chatbots, have led to an increased need to process longer input contexts. However, this requirement is hampered by inherent limitations. Architecturally, models are constrained by a context window defined during training. Additionally, processing extensive texts requires substantial GPU memory. We propose a novel approach, Finch, to compress the input context by leveraging the pre-trained model weights of the self-attention. Given a prompt and a long text, Finch iteratively identifies the most relevant Key (K) and Value (V) pairs over chunks of the text conditioned on the prompt. Only such pairs are stored in the KV cache, which, within the space constrained by the context window, ultimately contains a compressed version of the long text. Our proposal enables models to consume large inputs even with high compression (up to 93x) while preserving semantic integrity without the need for fine-tuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T09:08:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.00167v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.00167v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Value-based Proactive Caching for Sensing Data in Internet of Vehicles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yantong Wang, Ke Liu, Hui Ji, Jiande Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sensing data (SD) plays an important role in safe-related applications for Internet of Vehicles. Proactively caching required sensing data (SD) is a pivotal strategy for alleviating network congestion and improving data accessibility. Despite merits, existing studies predominantly address SD caching within a single time slot, which may not be scalable to scenarios involving multi-slots. Furthermore, the oversight of service capacity at caching nodes could lead to significant queuing delays in SD reception. To tackle these limitations, we jointly consider the problem of anchoring caching placement and requests allocation for SD. A value model incorporating both temporal and spacial characteristics is first proposed to estimate the significance of different caching decisions. Subsequently, a stochastic integer nonlinear programming model is provided to optimize the long-term system performance, which is converted into a series of online optimization problem by leveraging the Lyapunov method and linearized via introducing auxiliary variables. To expedite the solution, we provide a binary quantum particle swarm optimization based algorithm with quadratic time complexity. Numerical investigations demonstrate the superiority of proposed algorithms compared with other schemes in terms of energy consumption, response latency, and cache-hit ratio.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-12T08:46:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05996v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05996v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open
  Source RISC-V application processor</h2>
                <div class="authors">
                    <strong>Authors:</strong> Riccardo Tedeschi, Luca Valente, Gianmarco Ottavi, Enrico Zelioli, Nils Wistoff, Massimiliano Giacometti, Abdul Basit Sajjad, Luca Benini, Davide Rossi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Symmetric Multi-Processing (SMP) based on cache coherency is crucial for high-end embedded systems like automotive applications. RISC-V is gaining traction, and open-source hardware (OSH) platforms offer solutions to issues such as IP costs and vendor dependency. Existing multi-core cache-coherent RISC-V platforms are complex and not efficient for small embedded core clusters. We propose an open-source SystemVerilog implementation of a lightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our design uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with Splash-3 benchmarks, our solution shows up to 32.87% faster performance in a dual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized using GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of the system area.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-12T07:47:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19895v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19895v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Correct Wrong Path</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bhargav Reddy Godala, Sankara Prasad Ramesh, Krishnam Tibrewala, Chrysanthos Pepi, Gino Chacon, Svilen Kanev, Gilles A. Pokam, Daniel A. Jiménez, Paul V. Gratz, David I. August
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern OOO CPUs have very deep pipelines with large branch misprediction recovery penalties. Speculatively executed instructions on the wrong path can significantly change cache state, depending on speculation levels. Architects often employ trace-driven simulation models in the design exploration stage, which sacrifice precision for speed. Trace-driven simulators are orders of magnitude faster than execution-driven models, reducing the often hundreds of thousands of simulation hours needed to explore new micro-architectural ideas. Despite this strong benefit of trace-driven simulation, these often fail to adequately model the consequences of wrong path because obtaining them is nontrivial. Prior works consider either a positive or negative impact of wrong path but not both. Here, we examine wrong path execution in simulation results and design a set of infrastructure for enabling wrong-path execution in a trace driven simulator. Our analysis shows the wrong path affects structures on both the instruction and data sides extensively, resulting in performance variations ranging from $-3.05$\% to $20.9$\% when ignoring wrong path. To benefit the research community and enhance the accuracy of simulators, we opened our traces and tracing utility in the hopes that industry can provide wrong-path traces generated by their internal simulators, enabling academic simulation without exposing industry IP.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-12T03:53:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05912v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05912v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Post-Training Sparse Attention with Double Sparsity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuo Yang, Ying Sheng, Joseph E. Gonzalez, Ion Stoica, Lianmin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces "Double Sparsity," a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve \(\frac{1}{16}\) token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\times$ acceleration in attention operations and a 1.9$\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at \url{https://github.com/andy-yang-1/DoubleSparse}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-11T18:40:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07092v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07092v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Hierarchical Coded Caching with Low Subpacketization and Coding Delay</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rashid Ummer N. T., B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory. Motivated by practical scenarios, Karamchandani \textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories. Low subpacketization level coded caching schemes are desirable for practical implementations. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \textit{et al.} in [4]. Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks. Kong \textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs. Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20]. Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network. This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs. Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay. Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-11T16:35:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.12747v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.12747v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Genie: Smart ROS-based Caching for Connected Autonomous Robots</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zexin Li, Soroush Bateni, Cong Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite the promising future of autonomous robots, several key issues currently remain that can lead to compromised performance and safety. One such issue is latency, where we find that even the latest embedded platforms from NVIDIA fail to execute intelligence tasks (e.g., object detection) of autonomous vehicles in a real-time fashion. One remedy to this problem is the promising paradigm of edge computing. Through collaboration with our industry partner, we identify key prohibitive limitations of the current edge mindset: (1) servers are not distributed enough and thus, are not close enough to vehicles, (2) current proposed edge solutions do not provide substantially better performance and extra information specific to autonomous vehicles to warrant their cost to the user, and (3) the state-of-the-art solutions are not compatible with popular frameworks used in autonomous systems, particularly the Robot Operating System (ROS).   To remedy these issues, we provide Genie, an encapsulation technique that can enable transparent caching in ROS in a non-intrusive way (i.e., without modifying the source code), can build the cache in a distributed manner (in contrast to traditional central caching methods), and can construct a collective three-dimensional object map to provide substantially better latency (even on low-power edge servers) and higher quality data to all vehicles in a certain locality. We fully implement our design on state-of-the-art industry-adopted embedded and edge platforms, using the prominent autonomous driving software Autoware, and find that Genie can enhance the latency of Autoware Vision Detector by 82% on average, enable object reusability 31% of the time on average and as much as 67% for the incoming requests, and boost the confidence in its object map considerably over time.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-11T08:07:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.19410v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.19410v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Eigen Attention: Attention in Low-Rank Space for KV Cache Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-10T22:47:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05646v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05646v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using
  Gaussian Mixture Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanqiu Chen, Yitu Wang, Luis Vitorio Cargnini, Mohammadreza Soltaniyeh, Dongyang Li, Gongjin Sun, Pradeep Subedi, Andrew Chang, Yiran Chen, Cong Hao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Compute Express Link (CXL) emerges as a solution for wide gap between computational speed and data communication rates among host and multiple devices. It fosters a unified and coherent memory space between host and CXL storage devices such as such as Solid-state drive (SSD) for memory expansion, with a corresponding DRAM implemented as the device cache. However, this introduces challenges such as substantial cache miss penalties, sub-optimal caching due to data access granularity mismatch between the DRAM "cache" and SSD "memory", and inefficient hardware cache management. To address these issues, we propose a novel solution, named ICGMM, which optimizes caching and eviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based approach. We prototype our solution on an FPGA board, which demonstrates a noteworthy improvement compared to the classic Least Recently Used (LRU) cache strategy. We observe a decrease in the cache miss rate ranging from 0.32% to 6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD access latency. Furthermore, when compared to the state-of-the-art Long Short-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA showcases an impressive latency reduction of over 10,000 times. Remarkably, this is achieved while demanding much fewer hardware resources.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-10T19:17:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.ET</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05614v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05614v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Time-resolved measurement of neutron energy isotropy in a
  sheared-flow-stabilized Z pinch</h2>
                <div class="authors">
                    <strong>Authors:</strong> R. A. Ryan, P. E. Tsai, A. R. Johansen, A. Youmans, D. P. Higginson, J. M. Mitrani, C. S. Adams, D. A. Sutherland, B. Levitt, U. Shumlak
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Previous measurements of neutron energy using fast plastic scintillators while operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of any yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been operated at increasingly higher input power, resulting in increased plasma current and larger fusion neutron yields. A detailed experimental study of the neutron energy isotropy in these regimes applies more stringent limits to possible contributions from beam-target fusion. The FuZE device operated at $-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and D-D fusion neutron yields of $4\times10^7$ neutrons per discharge. Measurements of the neutron energy isotropy under these operating conditions demonstrates the energy of deuteron beams is less than $7.4 \pm 5.6^\mathrm{(stat)} \pm 3.7^\mathrm{(syst)}~keV$. Characterization of the detector response has reduced the number of free parameters in the fit of the neutron energy distribution, improving the confidence in the forward-fit method. Gamma backgrounds have been measured and the impact of these contributions on the isotropy results have been studied. Additionally, a time dependent measurement of the isotropy has been resolved for the first time, indicating increases to possible deuteron beam energies at late times. This suggests the possible growth of $m$=0 instabilities at the end of the main radiation event but confirms that the majority of the neutron production exhibits isotropy consistent with thermonuclear origin.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-09T16:48:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.plasm-ph</span><span>nucl-ex</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05171v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05171v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 NACL: A General and Effective KV Cache Eviction Framework for LLMs at
  Inference Time</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, Hua Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have ignited an innovative surge of AI applications, marking a new era of exciting possibilities equipped with extended context windows. However, hosting these models is cost-prohibitive mainly due to the extensive memory consumption of KV Cache involving long-context modeling. Despite several works proposing to evict unnecessary tokens from the KV Cache, most of them rely on the biased local statistics of accumulated attention scores and report performance using unconvincing metric like perplexity on inadequate short-text evaluation. In this paper, we propose NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase. Due to NACL's efficiency, we combine more accurate attention score statistics in PROXY TOKENS EVICTION with the diversified random eviction strategy of RANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance the robustness in maintaining pivotal tokens for long-context modeling tasks. Notably, our method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50% with over 95% performance maintenance. The code is available at https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-08T01:20:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.03675v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.03675v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals
  and Future Trends</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yao Zhao, Youyang Qu, Yong Xiang, Md Palash Uddin, Dezhong Peng, Longxiang Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in edge computing~(EC) have pushed cloud-based data caching services to edge, however, such emerging edge storage comes with numerous challenging and unique security issues. One of them is the problem of edge data integrity verification (EDIV) which coordinates multiple participants (e.g., data owners and edge nodes) to inspect whether data cached on edge is authentic. To date, various solutions have been proposed to address the EDIV problem, while there is no systematic review. Thus, we offer a comprehensive survey for the first time, aiming to show current research status, open problems, and potentially promising insights for readers to further investigate this under-explored field. Specifically, we begin by stating the significance of the EDIV problem, the integrity verification difference between data cached on cloud and edge, and three typical system models with corresponding inspection processes. To thoroughly assess prior research efforts, we synthesize a universal criteria framework that an effective verification approach should satisfy. On top of it, a schematic development timeline is developed to reveal the research advance on EDIV in a sequential manner, followed by a detailed review of the existing EDIV solutions. Finally, we highlight intriguing research challenges and possible directions for future work, along with a discussion on how forthcoming technology, e.g., machine learning and context-aware security, can augment security in EC. Given our findings, some major observations are: there is a noticeable trend to equip EDIV solutions with various functions and diversify study scenarios; completing EDIV within two types of participants (i.e., data owner and edge nodes) is garnering escalating interest among researchers; although the majority of existing methods rely on cryptography, emerging technology is being explored to handle the EDIV problem.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-07T23:48:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2210.10978v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2210.10978v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Zero-Delay QKV Compression for Mitigating KV Cache and Network
  Bottlenecks in LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeyu Zhang, Haiying Shen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In large-language models, memory constraints in the key-value cache (KVC) pose a challenge during inference, especially with long prompts. In this work, we observed that compressing KV values is more effective than compressing the model regarding accuracy and job completion time (JCT). However, quantizing KV values and dropping less-important tokens incur significant runtime computational time overhead, delaying JCT. These methods also cannot reduce computation time or high network communication time overhead in sequence-parallelism (SP) frameworks for long prompts. To tackle these issues, based on our insightful observations from experimental analysis, we propose ZeroC, a Zero-delay QKV Compression system that eliminates time overhead and even reduces computation and communication time of the model operations. ZeroC innovatively embeds compression and decompression operations within model operations and adaptively determines compression ratios at a hybrid layer-token level. Further, it enables a communication-efficient SP inference framework. Trace-driven experiments demonstrate that ZeroC achieves up to 80% lower average JCT, 35% lower average perplexity, and 2.8x higher throughput with the same latency compared to state-of-the-art compression methods. ZeroC also reduces the average JCT of current LLM serving systems by up to 91% with the constraint of 0.1 perplexity increase. We open-sourced the code.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-07T22:10:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.04107v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.04107v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Temporal Feature Matters: A Framework for Diffusion Model Quantization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yushi Huang, Ruihao Gong, Xianglong Liu, Jing Liu, Yuhang Li, Jiwen Lu, Dacheng Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Diffusion models, widely used for image generation, face significant challenges related to their broad applicability due to prolonged inference times and high memory demands. Efficient Post-Training Quantization (PTQ) is crucial to address these issues. However, unlike traditional models, diffusion models critically rely on the time-step for the multi-round denoising. Typically, each time-step is encoded into a hypersensitive temporal feature by several modules. Despite this, existing PTQ methods do not optimize these modules individually. Instead, they employ unsuitable reconstruction objectives and complex calibration methods, leading to significant disturbances in the temporal feature and denoising trajectory, as well as reduced compression efficiency. To address these challenges, we introduce a novel quantization framework that includes three strategies: 1) TIB-based Maintenance: Based on our innovative Temporal Information Block (TIB) definition, Temporal Information-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are developed to efficiently align original temporal features. 2) Cache-based Maintenance: Instead of indirect and complex optimization for the related modules, pre-computing and caching quantized counterparts of temporal features are developed to minimize errors. 3) Disturbance-aware Selection: Employ temporal feature errors to guide a fine-grained selection between the two maintenance strategies for further disturbance reduction. This framework preserves most of the temporal information and ensures high-quality end-to-end generation. Extensive testing on various datasets, diffusion models and hardware confirms our superior performance and acceleration..
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-07T20:43:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19547v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19547v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest
  Neighbor Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ahmed Abdou, Tasneem Mohsen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) that aims to identify and classify entities in text into predefined categories. However, when applied to Arabic data, NER encounters unique challenges stemming from the language's rich morphological inflections, absence of capitalization cues, and spelling variants, where a single word can comprise multiple morphemes. In this paper, we introduce Arabic KNN-NER, our submission to the Wojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the shared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained flat-entity recognition for Arabic text, where we identify a single main entity and possibly zero or multiple sub-entities for each word. Arabic KNN-NER augments the probability distribution of a fine-tuned model with another label probability distribution derived from performing a KNN search over the cached training data. Our submission achieved 91% on the test set on the WojoodFine dataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-07T09:34:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.03652v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.03652v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Potential and Limitation of High-Frequency Cores and Caches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kunal Pai, Anusheel Nand, Jason Lowe-Power
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper explores the potential of cryogenic computing and superconducting electronics as promising alternatives to traditional semiconductor devices. As semiconductor devices face challenges such as increased leakage currents and reduced performance at higher temperatures, these novel technologies offer high performance and low power computation. Cryogenic computing operates at ultra-low temperatures near 77 K, leading to lower leakage currents and improved electron mobility. On the other hand, superconducting electronics, operating near 0 K, allow electrons to flow without resistance, offering the potential for ultra-low-power, high-speed computation. This study presents a comprehensive performance modeling and analysis of these technologies and provides insights into their potential benefits and limitations. We implement models of in-order and out-of-order cores operating at high clock frequencies associated with superconducting electronics and cryogenic computing in gem5. We evaluate the performance of these components using workloads representative of real-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the potential speedups achievable by these components and the limitations posed by cache bandwidth. This work provides valuable insights into the performance implications and design trade-offs associated with cryogenic and superconducting technologies, laying the foundation for future research in this field using gem5.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-06T17:16:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.03308v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.03308v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lekai Chen, Ashutosh Trivedi, Alvaro Velasquez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning. This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning. Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata. We propose the $\mathtt{Discrimination}$ prompt as well as the $\mathtt{Verification}$ prompt and explore their advantages over common prompts. Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms. To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifies and corrects conflicting queries by combining the active and passive learning algorithms. The empirical results demonstrate the robustness and efficiency of our approach, providing a theoretical foundation for automata learning with LLMs in the loop.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-06T07:12:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.FL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.02999v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.02999v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 NVPC: A Transparent NVM Page Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guoyu Wang, Xilong Che, Haoyang Wei, Shuo Chen, Puyi He, Juncheng Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Towards a compatible utilization of NVM, NVM-specialized kernel file systems and NVM-based disk file system accelerators have been proposed. However, these studies only focus on one or several characteristics of NVM, while failing to exploit its best practice by putting NVM in the proper position of the whole storage stack. In this paper, we present NVPC, a transparent acceleration to existing kernel file systems with an NVM-enhanced page cache. The acceleration lies in two aspects, respectively matching the desperate needs of existing disk file systems: sync writes and cache-missed operations. Besides, the fast DRAM page cache is preserved for cache-hit operations. For sync writes, a high-performance log-based sync absorbing area is provided to redirect data destination from the slow disk to the fast NVM. Meanwhile, the byte-addressable feature of NVM is used to prevent write amplification. For cache-missed operations, NVPC makes use of the idle space on NVM to extend the DRAM page cache, so that more and larger workloads can fit into the cache. NVPC is entirely implemented as a page cache, thus can provide efficient speed-up to disk file systems with full transparency to users and full compatibility to lower file systems.   In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x faster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger than DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and SPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in 62.5% of the tested cases in our read/write/sync mixed evaluation, demonstrating that NVPC is more balanced and adaptive to complex real-world workloads. Experimental results also show that NVPC is the only method that accelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to any other use cases.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-06T02:51:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.02911v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.02911v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Electron-beam-induced modification of gold microparticles in an SEM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kristina Weinel, Marc Benjamin Hahn, Axel Lubk, Wen Feng, Ignacio Gonzalez Martinez, Bernd Büchner, Leonardo Agudo Jácome
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Electron-beam-induced conversion of materials in a transmission electron microscope uses the high power density of a localized electron beam of acceleration voltages above 100 kV as an energy source to transform matter at the sub-micron scale. Here, the e-beam-induced transformation of precursor microparticles employing a low-energy e-beam with an acceleration voltage of 30 kV in a scanning electron microscope is developed to increase the versatility and efficiency of the technique. Under these conditions, the technique can be classified between e-beam lithography, where the e-beam is used to mill holes in or grow some different material onto a substrate, and e-beam welding, where matter can be welded together when overcoming the melting phase. Modifying gold microparticles on an amorphous SiOx substrate reveals the dominant role of inelastic electron-matter interaction and subsequent localized heating for the observed melting and vaporization of the precursor microparticles under the electron beam. Monte-Carlo scattering simulations and thermodynamic modeling further support the findings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-05T12:09:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.02409v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.02409v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference
  Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andreas Kosmas Kakolyris, Dimosthenis Masouros, Petros Vavaroutsos, Sotirios Xydis, Dimitrios Soudris
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \textit{throttLL'eM} achieves up to 43.8\% lower energy consumption and an energy efficiency improvement of at least $1.71\times$ under SLOs, when compared to NVIDIA's Triton server.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-05T09:07:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.AR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05235v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05235v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 TriForce: Lossless Acceleration of Long Sequence Generation with
  Hierarchical Speculative Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, Beidi Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TriForce, a hierarchical speculative decoding system that is scalable for long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31$\times$ on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token$\unicode{x2014}$only half as slow as the auto-regressive baseline on an A100, which attains 7.78$\times$ on our optimized offloading system. Additionally, TriForce performs 4.86$\times$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-04T00:58:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.11912v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.11912v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Cross-layer Attention Sharing for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongyu Mu, Yuzhang Wu, Yuchun Fan, Chenglong Wang, Hengyu Li, Qiaozhi He, Murun Yang, Tong Xiao, Jingbo Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) evolve, the increase in model depth and parameter number leads to substantial redundancy. To enhance the efficiency of the attention mechanism, previous works primarily compress the KV cache or group attention heads, while largely overlooking redundancy between layers. Our comprehensive analyses across various LLMs show that highly similar attention patterns persist within most layers. It's intuitive to save the computation by sharing attention weights across layers. However, further analysis reveals two challenges: (1) Directly sharing the weight matrix without carefully rearranging the attention heads proves to be ineffective; (2) Shallow layers are vulnerable to small deviations in attention weights. Driven by these insights, we introduce LiSA, a lightweight substitute for self-attention in well-trained LLMs. LiSA employs tiny feed-forward networks to align attention heads between adjacent layers and low-rank matrices to approximate differences in layer-wise attention weights. Evaluations encompassing 13 typical benchmarks demonstrate that LiSA maintains high response quality in terms of accuracy and perplexity while reducing redundant attention calculations within 53-84% of the total layers. Our implementations of LiSA achieve a 6X compression of Q and K, with maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for LLaMA2-7B.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-04T00:38:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.01890v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.01890v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiao Jiang, Grace J. Gang, J. Webster Stayman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Many spectral CT applications require accurate material decomposition. Existing material decomposition algorithms are often susceptible to significant noise magnification or, in the case of one-step model-based approaches, hampered by slow convergence rates and large computational requirements. In this work, we proposed a novel framework - spectral diffusion posterior sampling (spectral DPS) - for one-step reconstruction and multi-material decomposition, which combines sophisticated prior information captured by one-time unsupervised learning and an arbitrary analytic physical system model. Spectral DPS is built upon a general DPS framework for nonlinear inverse problems. Several strategies developed in previous work, including jumpstart sampling, Jacobian approximation, and multi-step likelihood updates are applied facilitate stable and accurate decompositions. The effectiveness of spectral DPS was evaluated on a simulated dual-layer and a kV-switching spectral system as well as on a physical cone-beam CT (CBCT) test bench. In simulation studies, spectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53% to 57.30% over MBMD, depending on the the region of interest. In physical phantom study, spectral DPS achieved a <1% error in estimating the mean density in a homogeneous region. Compared with baseline DPS, spectral DPS effectively avoided generating false structures in the homogeneous phantom and reduced the variability around edges. Both simulation and physical phantom studies demonstrated the superior performance of spectral DPS for stable and accurate material decomposition.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-02T18:25:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.med-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.01519v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.01519v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching
  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yun-Chih Chen, Yuan-Hao Chang, Tei-Wei Kuo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To index the increasing volume of data, modern data indexes are typically stored on SSDs and cached in DRAM. However, searching such an index has resulted in significant I/O traffic due to limited access locality and inefficient cache utilization. At the heart of index searching is the operation of filtering through vast data spans to isolate a small, relevant subset, which involves basic equality tests rather than the complex arithmetic provided by modern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which demonstrates the feasibility of performing data filtering directly within a NAND flash memory chip, transmitting only relevant search results rather than complete pages. Instead of adding complex circuits, we propose repurposing existing circuitry for efficient and accurate bitwise parallel matching. We demonstrate how different data structures can use our flexible SIMD command interface to offload index searches. This strategy not only frees up the CPU for more computationally demanding tasks, but it also optimizes DRAM usage for write buffering, significantly lowering energy consumption associated with I/O transmission between the CPU and DRAM. Extensive testing across a wide range of workloads reveals up to a 9X speedup in write-heavy workloads and up to 45% energy savings due to reduced read and write I/O. Furthermore, we achieve significant reductions in median and tail read latencies of up to 89% and 85% respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-02T07:37:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.00327v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.00327v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Caching Aided Multi-Tenant Serverless Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chu Qiao, Cong Wang, Zhenkai Zhang, Yuede Ji, Xing Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> One key to enabling high-performance serverless computing is to mitigate cold-starts. Current solutions utilize a warm pool to keep function alive: a warm-start can be analogous to a CPU cache-hit. However, modern cache has multiple hierarchies and the last-level cache is shared among cores, whereas the warm pool is limited to a single tenant for security concerns. Also, the warm pool keep-alive policy can be further optimized using cache replacement algorithms. In this paper, we borrow practical optimizations from caching, and design FaasCamp, a caching-aided multi-tenant serverless computing framework. FaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim pool introduced enabling secure function instance sharing among tenants. Also, FaasCamp leverages machine learning to approximate the optimal cache replacement policy to improve the warm rate. We have implemented a prototype and conducted extensive experiments under multiple scenarios. The results show that FaasCamp can outperform existing platforms with minimal overhead.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-01T23:52:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.00957v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.00957v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Do language models plan ahead for future tokens?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wilson Wu, John X. Morris, Lionel Levine
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at time step $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present during training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a constructed synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis, though pre-caching increases with model scale.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-01T21:21:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.00859v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.00859v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Intermittent Semi-working Mask: A New Masking Paradigm for LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingcong Lu, Jiangcai Zhu, Wang Hao, Zheng Li, Shusheng Zhang, Kailai Shao, Chao Chen, Nan Li, Feng Wang, Xin Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-turn dialogues are a key interaction method between humans and Large Language Models (LLMs), as conversations extend over multiple rounds, keeping LLMs' high generation quality and low latency is a challenge. Mainstream LLMs can be grouped into two categories based on masking strategy: causal LLM and prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform causal ones in scenarios that heavily depend on historical context such as multi-turn dialogues or in-context learning, thanks to their bidirectional attention on prefix sequences. However, prefix LLMs have an inherent inefficient training problem in multi-turn dialogue datasets. In addition, the attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV Cache) across dialogue rounds to reduce generation latency. In this paper, we propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to address these problems. Specifically, we apply alternate bidirectional and unidirectional attention on queries and answers in the dialogue history. In this way, ISM is able to maintain the high quality of prefix LLM and low generation latency of causal LLM, simultaneously. Extensive experiments illustrate that our ISM achieves significant performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-01T13:22:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.00539v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.00539v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 MoE-Infinity: Offloading-Efficient MoE Model Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh Marina
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents MoE-Infinity, an offloading-efficient serving system for sparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity achieves novel request-level tracing for expert activation, capturing MoE's sparse execution patterns such as selective activation, group activation, and skewed reuse. Leveraging the request-level trace, MoE-Infinity performs effective expert prefetching and expert caching, achieving high efficiency in transferring model parameters from host memory to GPU memory. Experimental results demonstrate that MoE-Infinity achieves low latency comparable to expensive full-GPU deployments, which require up to 4X more GPU resources than MoE-Infinity. Compared to offloading-supporting LLM serving systems such as DeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm, MoE-Infinity exhibits superior latency performance, providing 2-20X improvements when serving various MoE models for a large collection of LLM tasks. MoE-Infinity's source code is publicly available a https://github.com/TorchMoE/MoE-Infinity
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-01T13:21:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.14361v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.14361v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and
  Two-Phase Partition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lu Ye, Ze Tao, Yong Huang, Yang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\times$ compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-01T07:51:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.15220v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.15220v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph
  Neural Network Training with Communication Reduction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuai Zhang, Zite Jiang, Haihang You
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph neural network training is mainly categorized into mini-batch and full-batch training methods. The mini-batch training method samples subgraphs from the original graph in each iteration. This sampling operation introduces extra computation overhead and reduces the training accuracy. Meanwhile, the full-batch training method calculates the features and corresponding gradients of all vertices in each iteration, and therefore has higher convergence accuracy. However, in the distributed cluster, frequent remote accesses of vertex features and gradients lead to huge communication overhead, thus restricting the overall training efficiency.   In this paper, we introduce the cached-based distributed full-batch graph neural network training framework (CDFGNN). We propose the adaptive cache mechanism to reduce the remote vertex access by caching the historical features and gradients of neighbor vertices. Besides, we further optimize the communication overhead by quantifying the messages and designing the graph partition algorithm for the hierarchical communication architecture. Experiments show that the adaptive cache mechanism reduces remote vertex accesses by 63.14% on average. Combined with communication quantization and hierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed full-batch training frameworks by 30.39% in our experiments. Our results indicate that CDFGNN has great potential in accelerating distributed full-batch GNN training tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-01T01:57:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.00232v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.00232v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Towards Variable-Length In-Network Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gyuyeong Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present StarCache, a new in-network caching architecture that can cache variable-length items to balance a wide range of key-value workloads. Unlike existing works, StarCache does not cache hot items in the switch memory. Instead, we make hot items revisit the switch data plane continuously by exploiting packet recirculation. Our approach keeps cached key-value pairs in the switch data plane while freeing them from item size limitations caused by hardware constraints. We implement a StarCache prototype on an Intel Tofino switch. Our experimental results show that StarCache can balance highly skewed workloads with various key and value sizes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-01T00:41:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.21324v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.21324v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 A2SF: Accumulative Attention Scoring with Forgetting Factor for Token
  Pruning in Transformer Decoder</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hyun-rae Jo, Dongkun Shin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, large language models (LLM) based on transformers are facing memory bottleneck issues due to KV cache, especially in long sequence handling. Previous researches proposed KV cache compression techniques that identify insignificant tokens based on Accumulative Attention Scores and removes their items from KV cache, noting that only few tokens play an important role in attention operations. However, we have observed that the existing Accumulative Attention Score is not suitable for the transformer decoder structure. In the decoder model, the number of times the Attention Score accumulates varies depending on the order of token appearance due to the effect of masking, causing an uneven comparison between tokens. To solve this, we propose Accumulative Attention Score with Forgetting Factor (A2SF) technique, which introduces a Forgetting Factor in the Attention Score accumulation process. A2SF applies a penalty to the past Attention Score generated from old tokens by repeatedly multiplying the Forgetting Factor to the Attention Score over time. Therefore, older tokens receive a larger penalty, providing fairness among different ages of tokens. Through the fair comparison among tokens, we can more effectively select important tokens. We have verified the accuracy improvement through A2SF in the OPT and LLaMA models and A2SF improves the accuracy of LLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-31T02:02:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.20485v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.20485v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Electric field control of magnetocaloric effect in cylindrical MnAs/PZT
  magnetoelectric composite</h2>
                <div class="authors">
                    <strong>Authors:</strong> Abdulkarim A. Amirov, Maksim A. Koliushenkov, Abdula A. Mukhuchev, Dibir M. Yusupov, Valeriya V. Govorina, Dmitriy S. Neznakhin, Gennady A. Govor, Akhmed M. Aliev
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The possibility of electric field control of magnetocaloric effect through quasi-isostatic compression as a result of the converse piezoelectric effect was demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was shown that an electric voltage of 100 V corresponding to an electric field of E ~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the MnAs/PZT composite contributes to an increase in the maximum adiabatic temperature change by 0.2 K in the temperature range of the magnetostructural phase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations using the finite element method have shown that an electric field voltage of 100 V is capable of creating a quasi-isostatic mechanical stress in the region inside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak pressures up to 10 MPa, the contribution to the MCE from piezo compression linearly depends on the electrical voltage that can be used for control the MCE
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-30T21:27:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.21201v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.21201v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Palu: Compressing KV-Cache with Low-Rank Projection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Kai-Chiang Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> KV-Cache compression methods generally sample a KV-Cache of effectual tokens or quantize it into lower bits. However, these methods cannot exploit the redundancy of the hidden dimension of KV tensors. This paper investigates a unique hidden dimension approach called Palu, a novel KV-Cache compression framework that utilizes low-rank projection. Palu decomposes the linear layers into low-rank matrices, caches the smaller intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a low-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU kernels. Our extensive experiments with popular LLMs show that Palu can compress KV-Cache by more than 91.25% while maintaining a significantly better accuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache quantization methods at a similar or even higher memory usage. When compressing KV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the attention module. Our code is publicly available at https://github.com/shadowpa0327/Palu.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-30T18:19:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.21118v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.21118v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 ThinK: Thinner Key Cache by Query-Driven Pruning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, Doyen Sahoo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-30T17:59:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.21018v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.21018v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 SpChar: Characterizing the Sparse Puzzle via Decision Trees</h2>
                <div class="authors">
                    <strong>Authors:</strong> Francesco Sgherzi, Marco Siracusa, Ivan Fernandez, Adrià Armejach, Miquel Moretó
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse matrix computation is crucial in various modern applications, including large-scale graph analytics, deep learning, and recommender systems. The performance of sparse kernels varies greatly depending on the structure of the input matrix, making it difficult to gain a comprehensive understanding of sparse computation and its relationship to inputs, algorithms, and target machine architecture. Despite extensive research on certain sparse kernels, such as Sparse Matrix-Vector Multiplication (SpMV), the overall family of sparse algorithms has yet to be investigated as a whole. This paper introduces SpChar, a workload characterization methodology for general sparse computation. SpChar employs tree-based models to identify the most relevant hardware and input characteristics, starting from hardware and input-related metrics gathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis enables the creation of a characterization loop that facilitates the optimization of sparse computation by mapping the impact of architectural features to inputs and algorithmic choices. We apply SpChar to more than 600 matrices from the SuiteSparse Matrix collection and three state-of-the-art Arm CPUs to determine the critical hardware and software characteristics that affect sparse computation. In our analysis, we determine that the biggest limiting factors for high-performance sparse computation are (1) the latency of the memory system, (2) the pipeline flush overhead resulting from branch misprediction, and (3) the poor reuse of cached elements. Additionally, we propose software and hardware optimizations that designers can implement to create a platform suitable for sparse computation. We then investigate these optimizations using the gem5 simulator to achieve a significant speedup of up to 2.63x compared to a CPU where the optimizations are not applied.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-30T13:06:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>B.8.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2304.06944v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2304.06944v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 UpDown: Programmable fine-grained Events for Scalable Performance on
  Irregular Applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andronicus Rajasukumar, Jiya Su, Yuqing, Wang, Tianshuo Su, Marziyeh Nourian, Jose M Monsalve Diaz, Tianchi Zhang, Jianru Ding, Wenyi Wang, Ziyi Zhang, Moubarak Jeje, Henry Hoffmann, Yanjing Li, Andrew A. Chien
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Applications with irregular data structures, data-dependent control flows and fine-grained data transfers (e.g., real-world graph computations) perform poorly on cache-based systems. We propose the UpDown accelerator that supports fine-grained execution with novel architecture mechanisms - lightweight threading, event-driven scheduling, efficient ultra-short threads, and split-transaction DRAM access with software-controlled synchronization. These hardware primitives support software programmable events, enabling high performance on diverse data structures and algorithms. UpDown also supports scalable performance; hardware replication enables programs to scale up performance. Evaluation results show UpDown's flexibility and scalability enable it to outperform CPUs on graph mining and analytics computations by up to 116-195x geomean speedup and more than 4x speedup over prior accelerators. We show that UpDown generates high memory parallelism (~4.6x over CPU) required for memory intensive graph computations. We present measurements that attribute the performance of UpDown (23x architectural advantage) to its individual architectural mechanisms. Finally, we also analyze the area and power cost of UpDown's mechanisms for software programmability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-30T12:16:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.20773v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.20773v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eman Ali, Muhammad Haris Khan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in large-scale vision-language models have achieved impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability and generalizability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows the learning of effective target models with few unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is knowledge-guided cache refinement, which refines pair values (i.e., pseudo-labels) and cache weights by leveraging knowledge distillation from large-scale vision language models. Extensive experiments show that NtUA achieves superior performance consistently across multiple widely adopted benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-30T08:39:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2309.14928v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2309.14928v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Robust Federated Learning for Wireless Networks: A Demonstration with
  Channel Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zexin Fang, Bin Han, Hans D. Schotten
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Federated learning (FL) offers a privacy-preserving collaborative approach for training models in wireless networks, with channel estimation emerging as a promising application. Despite extensive studies on FL-empowered channel estimation, the security concerns associated with FL require meticulous attention. In a scenario where small base stations (SBSs) serve as local models trained on cached data, and a macro base station (MBS) functions as the global model setting, an attacker can exploit the vulnerability of FL, launching attacks with various adversarial attacks or deployment tactics. In this paper, we analyze such vulnerabilities, corresponding solutions were brought forth, and validated through simulation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-30T08:19:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.NI</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.03088v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.03088v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyue Qiu, Juncheng Yang, Mor Harchol-Balter
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Software caches are an intrinsic component of almost every computer system. Consequently, caching algorithms, particularly eviction policies, are the topic of many papers. Almost all these prior papers evaluate the caching algorithm based on its hit ratio, namely the fraction of requests that are found in the cache, as opposed to disk. The hit ratio is viewed as a proxy for traditional performance metrics like system throughput or response time. Intuitively it makes sense that higher hit ratio should lead to higher throughput (and lower response time), since more requests are found in the cache (low access time) as opposed to the disk (high access time).   This paper challenges this intuition. We show that increasing the hit ratio can actually hurt the throughput (and response time) for many caching algorithms. Our investigation follows a three-pronged approach involving (i) queueing modeling and analysis, (ii) implementation and measurement, and (iii) simulation to validate the accuracy of the queueing model. We also show that the phenomenon of throughput decreasing at higher hit ratios is likely to be more pronounced in future systems, where the trend is towards faster disks and higher numbers of cores per CPU.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-30T04:01:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.16219v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.16219v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 STT-RAM-based Hierarchical In-Memory Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhruv Gajaria, Kevin Antony Gomez, Tosiron Adegbija
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-memory computing promises to overcome the von Neumann bottleneck in computer systems by performing computations directly within the memory. Previous research has suggested using Spin-Transfer Torque RAM (STT-RAM) for in-memory computing due to its non-volatility, low leakage power, high density, endurance, and commercial viability. This paper explores hierarchical in-memory computing, where different levels of the memory hierarchy are augmented with processing elements to optimize workload execution. The paper investigates processing in memory (PiM) using non-volatile STT-RAM and processing in cache (PiC) using volatile STT-RAM with relaxed retention, which helps mitigate STT-RAM's write latency and energy overheads. We analyze tradeoffs and overheads associated with data movement for PiC versus write overheads for PiM using STT-RAMs for various workloads. We examine workload characteristics, such as computational intensity and CPU-dependent workloads with limited instruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using these workloads, we evaluate computing in STT-RAM versus SRAM at different cache hierarchy levels and explore the potential of heterogeneous STT-RAM cache architectures with various retention times for PiC and CPU-based computing. Our experiments reveal significant advantages of STT-RAM-based PiC over PiM for specific workloads. Finally, we describe open research problems in hierarchical in-memory computing architectures to further enhance this paradigm.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-29T01:43:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/TPDS.2024.3430853' target='_blank'>doi</a><a href='http://arxiv.org/abs/2407.19637v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19637v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory
  Processing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhruv Gajaria, Tosiron Adegbija, Kevin Gomez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures, especially those utilizing bit-line computing, offer promising solutions to mitigate data movement bottlenecks within the memory hierarchy. While previous studies have explored the integration of compute units within individual memory levels, the complexity and potential overheads associated with these designs have often limited their capabilities. This paper introduces a novel PiC/PiM architecture, Concurrent Hierarchical In-Memory Processing (CHIME), which strategically incorporates heterogeneous compute units across multiple levels of the memory hierarchy. This design targets the efficient execution of diverse, domain-specific workloads by placing computations closest to the data where it optimizes performance, energy consumption, data movement costs, and area. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing, such as high density, low leakage, and better resiliency to data corruption from activating multiple word lines. We demonstrate that CHIME enhances concurrency and improves compute unit utilization at each level of the memory hierarchy. We present strategies for exploring the design space, grouping, and placing the compute units across the memory hierarchy. Experiments reveal that, compared to the state-of-the-art bit-line computing approaches, CHIME achieves significant speedup and energy savings of 57.95% and 78.23% for various domain-specific workloads, while reducing the overheads associated with single-level compute designs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-29T01:17:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19627v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19627v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient
  Multicore Processors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhruv Gajaria, Tosiron Adegbija
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been widely studied as a way to reduce STT-RAM's write energy and latency overheads. Given a relaxed retention time STT-RAM level one (L1) cache, we analyze the impacts of dynamic voltage and frequency scaling (DVFS) -- a common optimization in modern processors -- on STT-RAM L1 cache design. Our analysis reveals that, apart from the fact that different applications may require different retention times, the clock frequency, which is typically ignored in most STT-RAM studies, may also significantly impact applications' retention time needs. Based on our findings, we propose an asymmetric-retention core (ARC) design for multicore architectures. ARC features retention time heterogeneity to specialize STT-RAM retention times to applications' needs. We also propose a runtime prediction model to determine the best core on which to run an application, based on the applications' characteristics, their retention time requirements, and available DVFS settings. Results reveal that the proposed approach can reduce the average cache energy by 20.19% and overall processor energy by 7.66%, compared to a homogeneous STT-RAM cache design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-28T23:43:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3357526.3357553' target='_blank'>doi</a><a href='http://arxiv.org/abs/2407.19612v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19612v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhruv Gajaria, Kyle Kuan, Tosiron Adegbija
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prior studies have shown that the retention time of the non-volatile spin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's write energy and latency. However, since different applications may require different retention times, STT-RAM retention times must be critically explored to satisfy various applications' needs. This process can be challenging due to exploration overhead, and exacerbated by the fact that STT-RAM caches are emerging and are not readily available for design time exploration. This paper explores using known and easily obtainable statistics (e.g., SRAM statistics) to predict the appropriate STT-RAM retention times, in order to minimize exploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model, which utilizes machine learning to enable design time or runtime prediction of right-provisioned STT-RAM retention times for latency or energy optimization. Experimental results show that, on average, SCART can reduce the latency and energy by 20.34% and 29.12%, respectively, compared to a homogeneous retention time while reducing the exploration overheads by 52.58% compared to prior work.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-28T22:34:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/IGSC48788.2019.8957182' target='_blank'>doi</a><a href='http://arxiv.org/abs/2407.19604v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19604v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Application State Management (ASM) in the Modern Web and Mobile
  Applications: A Comprehensive Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anujkumarsinh Donvir, Apeksha Jain, Pradeep Kumar Saraswathi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid evolution of web and mobile applications has necessitated robust mechanisms for managing application state to ensure consistency, performance, and user-friendliness. This comprehensive review examines the most effective Application State Management (ASM) techniques, categorized into Local State Management, State Management Libraries, and Server-Side State Management. By analyzing popular front end frameworks the study delves into local state management mechanisms. It also evaluates the state of front end management libraries, highlighting their implementations, benefits, and limitations. Server-side state management techniques, particularly caching, are discussed for their roles in enhancing data retrieval efficiency. This paper offers actionable insights for developers to build scalable, responsive applications, aiming to bridge the gap between theoretical knowledge and practical application. This study's critical analysis and recommendations aim to guide future research and development in ASM, contributing to the advancement of modern application architecture.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-27T18:26:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19318v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19318v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Symmetric Locality: Definition and Initial Results</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giordan Escalona, Dylan McKellips, Chen Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this short paper, we characterize symmetric locality. In designing algorithms, compilers, and systems, data movement is a common bottleneck in high-performance computation, in which we improve cache and memory performance. We study a special type of data reuse in the form of repeated traversals, or re-traversals, which are based on the symmetric group. The cyclic and sawtooth traces are previously known results in symmetric locality, and in this work, we would like to generalize this result for any re-traversal. Then, we also provide an abstract framework for applications in compiler design and machine learning models to improve the memory performance of certain programs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-27T16:20:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19291v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19291v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for
  Multi-Tenant DNN Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongkang Zhang, Haoxuan Yu, Chenxia Han, Cheng Wang, Baotong Lu, Yang Li, Xiaowen Chu, Huaicheng Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Colocating high-priority, latency-sensitive (LS) and low-priority, best-effort (BE) DNN inference services reduces the total cost of ownership (TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts and PCIe bus contentions, existing GPU sharing solutions are unable to avoid resource conflicts among concurrently executing tasks, failing to achieve both low latency for LS tasks and high throughput for BE tasks. To bridge this gap, this paper presents Missile, a general GPU sharing solution for multi-tenant DNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware resource isolation between multiple LS and BE DNN tasks at software level. Through comprehensive reverse engineering, Missile first reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel conflicts using software-level cache coloring. It also isolates the PCIe bus and fairly allocates PCIe bandwidth using completely fair scheduler. We evaluate 12 mainstream DNNs with synthetic and real-world workloads on four GPUs. The results show that compared to the state-of-the-art GPU sharing solutions, Missile reduces tail latency for LS services by up to ~50%, achieves up to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants on-demand for optimal performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-27T08:52:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AR</span><span>cs.PF</span><span>D.4.9; I.2.5</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.13996v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.13996v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's
  Impact on Spatio-Temporal Cross-Attentions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Zinuo Li, Hamid Laga, Farid Boussaid
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper investigates the role of CLIP image embeddings within the Stable Video Diffusion (SVD) framework, focusing on their impact on video generation quality and computational efficiency. Our findings indicate that CLIP embeddings, while crucial for aesthetic quality, do not significantly contribute towards the subject and background consistency of video outputs. Moreover, the computationally expensive cross-attention mechanism can be effectively replaced by a simpler linear layer. This layer is computed only once at the first diffusion inference step, and its output is then cached and reused throughout the inference process, thereby enhancing efficiency while maintaining high-quality outputs. Building on these insights, we introduce the VCUT, a training-free approach optimized for efficiency within the SVD architecture. VCUT eliminates temporal cross-attention and replaces spatial cross-attention with a one-time computed linear layer, significantly reducing computational load. The implementation of VCUT leads to a reduction of up to 322T Multiple-Accumulate Operations (MACs) per video and a decrease in model parameters by up to 50M, achieving a 20% reduction in latency compared to the baseline. Our approach demonstrates that conditioning during the Semantic Binding stage is sufficient, eliminating the need for continuous computation across all inference steps and setting a new standard for efficient video generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-27T08:21:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19205v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19205v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 MetaHive: A Cache-Optimized Metadata Management for Heterogeneous
  Key-Value Stores</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alireza Heidari, Amirhossein Ahmadi, Zefeng Zhi, Wei Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cloud key-value (KV) stores provide businesses with a cost-effective and adaptive alternative to traditional on-premise data management solutions. KV stores frequently consist of heterogeneous clusters, characterized by varying hardware specifications of the deployment nodes, with each node potentially running a distinct version of the KV store software. This heterogeneity is accompanied by the diverse metadata that they need to manage. In this study, we introduce MetaHive, a cache-optimized approach to managing metadata in heterogeneous KV store clusters. MetaHive disaggregates the original data from its associated metadata to promote independence between them, while maintaining their interconnection during usage. This makes the metadata opaque from the downstream processes and the other KV stores in the cluster. MetaHive also ensures that the KV and metadata entries are stored in the vicinity of each other in memory and storage. This allows MetaHive to optimally utilize the caching mechanism without extra storage read overhead for metadata retrieval. We deploy MetaHive to ensure data integrity in RocksDB and demonstrate its rapid data validation with minimal effect on performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-26T21:11:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19090v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19090v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Efficient Inference of Vision Instruction-Following Models with Elastic
  Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zuyan Liu, Benlin Liu, Jiahui Wang, Yuhao Dong, Guangyi Chen, Yongming Rao, Ranjay Krishna, Jiwen Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the field of instruction-following large vision-language models (LVLMs), the efficient deployment of these models faces challenges, notably due to the high memory demands of their key-value (KV) caches. Conventional cache management strategies for LLMs focus on cache eviction, which often fails to address the specific needs of multimodal instruction-following models. Recognizing this gap, in this paper, we introduce Elastic Cache, a novel approach that benefits from applying distinct acceleration methods for instruction encoding and output generation stages. We investigate the metrics of importance in different stages and propose an importance-driven cache merging strategy to prune redundancy caches. Instead of discarding less important caches, our strategy identifies important key/value vectors as anchor points. Surrounding less important caches are then merged with these anchors, enhancing the preservation of contextual information in the KV caches while yielding an arbitrary acceleration ratio. For instruction encoding, we utilize the frequency to evaluate the importance of caches. Regarding output generation, we prioritize tokens based on their distance with an offset, by which both the initial and most recent tokens are retained. Results on a range of LVLMs demonstrate that Elastic Cache not only boosts efficiency but also notably outperforms existing pruning methods in language generation across various tasks. Code is available at https://github.com/liuzuyan/ElasticCache
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-25T15:29:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.18121v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.18121v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, Xia Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using $\mathbf{2.6\times}$ less peak memory (including model weight). This reduction in memory usage enables up to $\mathbf{4\times}$ larger batch size, bringing $\mathbf{2.35\times \sim 3.47\times}$ throughput on real LLM inference workload. The source code is available at https://github.com/jy-yuan/KIVI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-25T09:16:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.13140/RG.2.2.28167.37282' target='_blank'>doi</a><a href='http://arxiv.org/abs/2402.02750v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.02750v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 An Efficient Inference Framework for Early-exit Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruijie Miao, Yihan Yan, Xinshuo Yao, Tong Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Building efficient inference framework has gained increasing interests for research community. Early-exit models, a variant of LLMs, improves the inference efficiency of LLMs by skipping rest layers and directly generate output tokens when they are confident enough. However, there is no work of LLM inference framework that takes early-exit models into consideration. This is non-trivial as prior art on LLM inference cannot be directly applied to early-exit models. In this work, we solves two key challenges in building efficient inference framework for early-exit models: (1) batch inference at iteration-level granularity; and (2) KV cache management. For the former, we propose to process the batch until all sequences surpass the early-exit confidence threshold. For the latter, we propose to fill the KV cache of rest layers before the iteration terminates. Our evaluation shows that, compared with the original vLLM operating at full layers, our solution achieves up to 1.25x speed up.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-25T07:50:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.20272v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.20272v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Efficient LLM Training and Serving with Heterogeneous Context Sharding
  among Attention Heads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Xia Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing LLM training and inference frameworks struggle in boosting efficiency with sparsity while maintaining the integrity of context and model architecture. Inspired by the sharding concept in database and the fact that attention parallelizes over heads on accelerators, we propose Sparsely-Sharded (S2) Attention, an attention algorithm that allocates heterogeneous context partitions for different attention heads to divide and conquer. S2-Attention enforces each attention head to only attend to a partition of contexts following a strided sparsity pattern, while the full context is preserved as the union of all the shards. As attention heads are processed in separate thread blocks, the context reduction for each head can thus produce end-to-end speed-up and memory reduction. At inference, LLMs trained with S2-Attention can then take the KV cache reduction as free meals with guaranteed model quality preserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X wall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction in end-to-end training time and 10X inference latency, (2) on-par model training quality compared to default attention, (3)perfect needle retrieval accuracy over 32K context window. On top of the algorithm, we build DKernel, an LLM training and inference kernel library that allows users to customize sparsity patterns for their own models. We open-sourced DKerneland make it compatible with Megatron, Pytorch, and vLLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-25T00:27:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.17678v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.17678v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval
  from Distributed System with Blind and Adversarial Servers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qifa Yan, Xiaohu Tang, Zhengchun Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, a distributed server system composed of multiple servers that holds some coded files and multiple users that are interested in retrieving the linear functions of the files is investigated, where the servers are robust, blind and adversarial in the sense that any $J$ servers can together recover all files, while any $I$ colluding servers cannot obtain any information about the files, and at most $A$ servers maliciously provides erroneous information. In addition, the file library must be secure from a wiretapper who obtains all the signals, and the demands of any subset of users must kept private from the other users and servers, even if they collude. A coding scheme is proposed by incorporating the ideas of Shamir's secret sharing and key superposition into the framework of Placement Delivery Array (PDA), originally proposed to characterize the single-server coded caching system without any security or privacy constraints. It is shown that PDAs associated to Maddah-Ali and Niesen's coded caching scheme results in an achievable memory-storage-communication region, such that the storage size and communication load were optimal to within a multiplicative gap, except for the small memory regime when the number of files was smaller than the number of users.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-24T13:36:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2301.08711v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2301.08711v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic
  Violations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Craig Innes, Subramanian Ramamoorthy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous Vehicles (AVs) are often tested in simulation to estimate the probability they will violate safety specifications. Two common issues arise when using existing techniques to produce this estimation: If violations occur rarely, simple Monte-Carlo sampling techniques can fail to produce efficient estimates; if simulation horizons are too long, importance sampling techniques (which learn proposal distributions from past simulations) can fail to converge. This paper addresses both issues by interleaving rare-event sampling techniques with online specification monitoring algorithms. We use adaptive multi-level splitting to decompose simulations into partial trajectories, then calculate the distance of those partial trajectories to failure by leveraging robustness metrics from Signal Temporal Logic (STL). By caching those partial robustness metric values, we can efficiently re-use computations across multiple sampling stages. Our experiments on an interstate lane-change scenario show our method is viable for testing simulated AV-pipelines, efficiently estimating failure probabilities for STL specifications based on real traffic rules. We produce better estimates than Monte-Carlo and importance sampling in fewer simulations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-24T12:56:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.15771v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.15771v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Efficient Tuning and Inference for Large Language Models on Textual
  Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yun Zhu, Yaoke Wang, Haizhou Shi, Siliang Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Rich textual and topological information of textual graphs need to be modeled in real-world applications such as webpages, e-commerce, and academic articles. Practitioners have been long following the path of adopting a shallow text encoder and a subsequent graph neural network (GNN) to solve this problem. In light of recent advancements in large language models (LLMs), it is apparent that integrating LLMs for enhanced textual encoding can substantially improve the performance of textual graphs. Nevertheless, the efficiency of these methods poses a significant challenge. In this paper, we propose ENGINE, a parameter- and memory-efficient fine-tuning method for textual graphs with an LLM encoder. The key insight is to combine the LLMs and GNNs through a tunable side structure, which significantly reduces the training complexity without impairing the joint model's capacity. Extensive experiments on textual graphs demonstrate our method's effectiveness by achieving the best model performance, meanwhile having the lowest training cost compared to previous methods. Moreover, we introduce two variants with caching and dynamic early exit to further enhance training and inference speed. Specifically, caching accelerates ENGINE's training by 12x, and dynamic early exit achieves up to 5x faster inference with a negligible performance drop (at maximum 1.17% relevant drop across 7 datasets). Our codes are available at: https://github.com/ZhuYun97/ENGINE
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-24T08:56:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.15569v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.15569v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-23T17:55:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.09636v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.09636v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 6G at $\frac{1}{6}g$: The Future of Cislunar Communications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sahan Liyanaarachchi, Stavros Mitrolaris, Purbesh Mitra, Sennur Ulukus
                </div>
                <div class="summary">
                    <strong>Summary:</strong> What will the future of cislunar communications be? The ever-expanding horizons of the space exploration missions, and the need for establishing sustainable space communication and navigation infrastructure necessitate to think this question thoroughly. In this article, we examine how some of the concepts of 6G technologies developed for terrestrial networks can be relevant in the context of cislunar networks. We discuss how 6G concepts, such as reconfigurable intelligent surfaces, quantum-resistant physical layer security, private information read/write/cache networks, semantic and goal-oriented communications, information freshness based quality of communication metrics, multi-relay and cooperative networks, hold the potential to shape the future of cislunar communications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-23T17:42:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>cs.ET</span><span>cs.NI</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.16672v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.16672v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Hidden Web Caches Discovery</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matteo Golinelli, Bruno Crispo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Web caches play a crucial role in web performance and scalability. However, detecting cached responses is challenging when web servers do not reliably communicate the cache status through standardized headers. This paper presents a novel methodology for cache detection using timing analysis. Our approach eliminates the dependency on cache status headers, making it applicable to any web server. The methodology relies on sending paired requests using HTTP multiplexing functionality and makes heavy use of cache-busting to control the origin of the responses. By measuring the time it takes to receive responses from paired requests, we can determine if a response is cached or not. In each pair, one request is cache-busted to force retrieval from the origin server, while the other request is not and might be served from the cache, if present. A faster response time for the non-cache-busted request compared to the cache-busted one suggests the first one is coming from the cache. We implemented this approach in a tool and achieved an estimated accuracy of 89.6% compared to state-of-the-art methods based on cache status headers. Leveraging our cache detection approach, we conducted a large-scale experiment on the Tranco Top 50k websites. We identified a significant presence of hidden caches (5.8%) that do not advertise themselves through headers. Additionally, we employed our methodology to detect Web Cache Deception (WCD) vulnerabilities in these hidden caches. We discovered that 1.020 of them are susceptible to WCD vulnerabilities, potentially leaking sensitive data. Our findings demonstrate the effectiveness of our timing analysis methodology for cache discovery and highlight the importance of a tool that does not rely on cache-communicated cache status headers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-23T08:58:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3678890.3678931' target='_blank'>doi</a><a href='http://arxiv.org/abs/2407.16303v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.16303v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 A Programming Model for Disaggregated Memory over CXL</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gal Assa, Michal Friedman, Ori Lahav
                </div>
                <div class="summary">
                    <strong>Summary:</strong> CXL (Compute Express Link) is an emerging open industry-standard interconnect between processing and memory devices that is expected to revolutionize the way systems are designed in the near future. It enables cache-coherent shared memory pools in a disaggregated fashion at unprecedented scales, allowing algorithms to interact with a variety of storage devices using simple loads and stores in a cacheline granularity. Alongside with unleashing unique opportunities for a wide range of applications, CXL introduces new challenges of data management and crash consistency. Alas, CXL lacks an adequate programming model, which makes reasoning about the correctness and expected behaviors of algorithms and systems on top of it nearly impossible.   In this work, we present CXL0, the first programming model for concurrent programs running on top of CXL. We propose a high-level abstraction for CXL memory accesses and formally define operational semantics on top of that abstraction. We provide a set of general transformations that adapt concurrent algorithms to the new disruptive technology. Using these transformations, every linearizable algorithm can be easily transformed into its provably correct version in the face of a full-system or sub-system crash. We believe that this work will serve as the stepping stone for systems design and modelling on top of CXL, and support the development of future models as software and hardware evolve.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-23T08:55:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.16300v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.16300v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 A deeper look at depth pruning of LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich, Thomas Breuel, Jan Kautz, David Krueger, Pavlo Molchanov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are not only resource-intensive to train but even more costly to deploy in production. Therefore, recent work has attempted to prune blocks of LLMs based on cheap proxies for estimating block importance, effectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b models without any significant degradation of downstream metrics. In this paper, we explore different block importance metrics by considering adaptive metrics such as Shapley value in addition to static ones explored in prior work. We show that adaptive metrics exhibit a trade-off in performance between tasks i.e., improvement on one task may degrade performance on the other due to differences in the computed block influences. Furthermore, we extend this analysis from a complete block to individual self-attention and feed-forward layers, highlighting the propensity of the self-attention layers to be more amendable to pruning, even allowing removal of upto 33% of the self-attention layers without incurring any performance degradation on MMLU for Mistral 7b (significant reduction in costly maintenance of KV-cache). Finally, we look at simple performance recovery techniques to emulate the pruned layers by training lightweight additive bias or low-rank linear adapters. Performance recovery using emulated updates avoids performance degradation for the initial blocks (up to 5% absolute improvement on MMLU), which is either competitive or superior to the learning-based technique.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-23T08:40:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.16286v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.16286v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiale Xu, Rui Zhang, Cong Guo, Weiming Hu, Zihan Liu, Feiyang Wu, Yu Feng, Shixuan Sun, Changxu Shao, Yuhong Guo, Junping Zhao, Ke Zhang, Minyi Guo, Jingwen Leng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are widely used across various domains, processing millions of daily requests. This surge in demand poses significant challenges in optimizing throughput and latency while keeping costs manageable. The Key-Value (KV) cache, a standard method for retaining previous computations, makes LLM inference highly bounded by memory. While batching strategies can enhance performance, they frequently lead to significant memory fragmentation. Even though cutting-edge systems like vLLM mitigate KV cache fragmentation using paged Attention mechanisms, they still suffer from inefficient memory and computational operations due to the tightly coupled page management and computation kernels.   This study introduces the vTensor, an innovative tensor structure for LLM inference based on GPU virtual memory management (VMM). vTensor addresses existing limitations by decoupling computation from memory defragmentation and offering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous approach, ensuring efficient, fragmentation-free memory management while accommodating various computation kernels across different LLM architectures. Experimental results indicate that vTensor achieves an average speedup of 1.86x across different models, with up to 2.42x in multi-turn chat scenarios. Additionally, vTensor provides average speedups of 2.12x and 3.15x in kernel evaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton prefix-prefilling kernels and vLLM paged Attention kernel, respectively. Furthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100 GPU compared to vLLM, enabling more memory-intensive workloads.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-22T14:37:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15309v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15309v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 vLSM: Low tail latency and I/O amplification in LSM-based KV stores</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giorgos Xanthakis, Antonios Katsarakis, Giorgos Saloustros, Angelos Bilas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LSM-based key-value (KV) stores are an important component in modern data infrastructures. However, they suffer from high tail latency, in the order of several seconds, making them less attractive for user-facing applications. In this paper, we introduce the notion of compaction chains and we analyse how they affect tail latency. Then, we show that modern designs reduce tail latency, by trading I/O amplification or require large amounts of memory. Based on our analysis, we present vLSM, a new KV store design that improves tail latency significantly without compromising on memory or I/O amplification. vLSM reduces (a) compaction chain width by using small SSTs and eliminating the tiering compaction required in L0 by modern systems and (b) compaction chain length by using a larger than typical growth factor between L1 and L2 and introducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate it using db_bench and YCSB. Our evaluation highlights the underlying trade-off among memory requirements, I/O amplification, and tail latency, as well as the advantage of vLSM over current approaches. vLSM improves P99 tail latency by up to 4.8x for writes and by up to 12.5x for reads, reduces cumulative write stalls by up to 60% while also slightly improves I/O amplification at the same memory budget.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-22T12:17:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15581v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15581v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Hausmann, Lutz Schröder
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The coalgebraic $\mu$-calculus provides a generic semantic framework for fixpoint logics over systems whose branching type goes beyond the standard relational setup, e.g. probabilistic, weighted, or game-based. Previous work on the coalgebraic $\mu$-calculus includes an exponential-time upper bound on satisfiability checking, which however relies on the availability of tableau rules for the next-step modalities that are sufficiently well-behaved in a formally defined sense; in particular, rule matches need to be representable by polynomial-sized codes, and the sequent duals of the rules need to absorb cut. While such rule sets have been identified for some important cases, they are not known to exist in all cases of interest, in particular ones involving either integer weights as in the graded $\mu$-calculus, or real-valued weights in combination with non-linear arithmetic. In the present work, we prove the same upper complexity bound under more general assumptions, specifically regarding the complexity of the (much simpler) satisfiability problem for the underlying one-step logic, roughly described as the nesting-free next-step fragment of the logic. The bound is realized by a generic global caching algorithm that supports on-the-fly satisfiability checking. Notably, our approach directly accommodates unguarded formulae, and thus avoids use of the guardedness transformation. Example applications include new exponential-time upper bounds for satisfiability checking in an extension of the graded $\mu$-calculus with polynomial inequalities (including positive Presburger arithmetic), as well as an extension of the (two-valued) probabilistic $\mu$-calculus with polynomial inequalities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-22T10:02:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LO</span><span>03B70, 03B44</span><span>F.4.1</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.46298/lmcs-20(3:9)2024' target='_blank'>doi</a><a href='http://arxiv.org/abs/2212.11055v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2212.11055v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Dissecting Multiplication in Transformers: Insights into LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luyu Qiu, Jianing Li, Chi Su, Chen Jason Zhang, Lei Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based large language models have achieved remarkable performance across various natural language processing tasks. However, they often struggle with seemingly easy tasks like arithmetic despite their vast capabilities. This stark disparity raise human's concerns about their safe and ethical use, hinder their widespread adoption.In this paper, we focus on a typical arithmetic task, integer multiplication, to explore and explain the imperfection of transformers in this domain. We provide comprehensive analysis of a vanilla transformer trained to perform n-digit integer multiplication. Our observations indicate that the model decomposes multiplication task into multiple parallel subtasks, sequentially optimizing each subtask for each digit to complete the final multiplication. Based on observation and analysis, we infer the reasons of transformers deficiencies in multiplication tasks lies in their difficulty in calculating successive carryovers and caching intermediate results, and confirmed this inference through experiments. Guided by these findings, we propose improvements to enhance transformers performance on multiplication tasks. These enhancements are validated through rigorous testing and mathematical modeling, not only enhance transformer's interpretability, but also improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit integer multiplication with a tiny transformer, outperform LLMs GPT-4. Our method contributes to the broader fields of model understanding and interpretability, paving the way for analyzing more complex tasks and Transformer models. This work underscores the importance of explainable AI, helping to build trust in large language models and promoting their adoption in critical applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-22T04:07:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15360v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15360v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 RazorAttention: Efficient KV Cache Compression Through Retrieval Heads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Shikuan Hong, Yiwu Yao, Gongyi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The memory and computational demands of Key-Value (KV) cache present significant challenges for deploying long-context language models. Previous approaches attempt to mitigate this issue by selectively dropping tokens, which irreversibly erases critical information that might be needed for future queries. In this paper, we propose a novel compression technique for KV cache that preserves all token information. Our investigation reveals that: i) Most attention heads primarily focus on the local context; ii) Only a few heads, denoted as retrieval heads, can essentially pay attention to all input tokens. These key observations motivate us to use separate caching strategy for attention heads. Therefore, we propose RazorAttention, a training-free KV cache compression algorithm, which maintains a full cache for these crucial retrieval heads and discards the remote tokens in non-retrieval heads. Furthermore, we introduce a novel mechanism involving a "compensation token" to further recover the information in the dropped tokens. Extensive evaluations across a diverse set of large language models (LLMs) demonstrate that RazorAttention achieves a reduction in KV cache size by over 70% without noticeable impacts on performance. Additionally, RazorAttention is compatible with FlashAttention, rendering it an efficient and plug-and-play solution that enhances LLM inference efficiency without overhead or retraining of the original model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-22T01:12:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15891v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15891v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing
  Data Transfer Scheme</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeongmin Brian Park, Kun Wu, Vikram Sharma Mailthody, Zaid Quresh, Scott Mahlke, Wen-mei Hwu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph Neural Networks (GNNs) are widely used today in recommendation systems, fraud detection, and node/link classification tasks. Real world GNNs continue to scale in size and require a large memory footprint for storing graphs and embeddings that often exceed the memory capacities of the target GPUs used for training. To address limited memory capacities, traditional GNN training approaches use graph partitioning and sharding techniques to scale up across multiple GPUs within a node and/or scale out across multiple nodes. However, this approach suffers from the high computational costs of graph partitioning algorithms and inefficient communication across GPUs.   To address these overheads, we propose Large-scale Storage-based Multi-GPU GNN framework (LSM-GNN), a storagebased approach to train GNN models that utilizes a novel communication layer enabling GPU software caches to function as a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid eviction policy that intelligently manages cache space by using both static and dynamic node information to significantly enhance cache performance. Furthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a mechanism for prefetching node feature data from a Victim Buffer located in CPU pinned-memory to further reduce the pressure on the storage devices. Experimental results show that despite the lower compute capabilities and memory capacities, LSM-GNN in a single node with two GPUs offers superior performance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x speed up on end-to-end epoch time while running large-scale GNN training
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-21T20:41:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15264v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15264v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Farewell to Length Extrapolation, a Training-Free Infinite Context with
  Finite Attention Scope</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoran Liu, Qipeng Guo, Yuerong Song, Zhigeng Liu, Kai Lv, Hang Yan, Linlin Li, Qun Liu, Xipeng Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The maximum supported context length is a critical bottleneck limiting the practical application of the Large Language Model (LLM). Although existing length extrapolation methods can extend the context of LLMs to millions of tokens, these methods all have an explicit upper bound. In this work, we propose LongCache, a training-free approach that enables LLM to support an infinite context with finite context scope, through full-context cache selection and training-free integration. This effectively frees LLMs from the length extrapolation issue. We validate LongCache on the LongBench and L-Eval and demonstrate its performance is on par with traditional full-attention mechanisms. Furthermore, we have applied LongCache on mainstream LLMs, including LLaMA3 and Mistral-v0.3, enabling them to support context lengths of at least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of LongCache by GPU-aware optimization soon.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-21T14:23:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15176v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15176v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for
  Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models have excelled in various fields but encounter efficiency limitations due to the substantial Key-Value (KV) cache required for long-sequence inference. Recent efforts try to evict non-critical cache elements during runtime, thereby reducing cache size within given memory budgets while preserving generation quality. Our reexamination of foundational principles reveals that prevailing methods aim to minimize an upper bound of eviction loss, quantified as the L1 distance between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across different attention heads during cache eviction hinder their budget utilization, negatively impacting generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the loss upper bound in theory but also reduces the eviction loss in practice by aligning with the intrinsic patterns of self-attention mechanisms. Integrating this algorithm into two advanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm that they both significantly boost performance across various tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-21T14:08:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.11550v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.11550v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Split Learning without Local Weight Sharing to Enhance Client-side Data
  Privacy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ngoc Duy Pham, Tran Khoa Phan, Alsharif Abuadbba, Yansong Gao, Doan Nguyen, Naveen Chilamkurti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Split learning (SL) aims to protect user data privacy by distributing deep models between client-server and keeping private data locally. In SL training with multiple clients, the local model weights are shared among the clients for local model update. This paper first reveals data privacy leakage exacerbated from local weight sharing among the clients in SL through model inversion attacks. Then, to reduce the data privacy leakage issue, we propose and analyze privacy-enhanced SL (P-SL) (or SL without local weight sharing). We further propose parallelized P-SL to expedite the training process by duplicating multiple server-side model instances without compromising accuracy. Finally, we explore P-SL with late participating clients and devise a server-side cache-based training method to address the forgetting phenomenon in SL when late clients join. Experimental results demonstrate that P-SL helps reduce up to 50% of client-side data leakage, which essentially achieves a better privacy-accuracy trade-off than the current trend by using differential privacy mechanisms. Moreover, P-SL and its cache-based version achieve comparable accuracy to baseline SL under various data distributions, while cost less computation and communication. Additionally, caching-based training in P-SL mitigates the negative effect of forgetting, stabilizes the learning, and enables practical and low-complexity training in a dynamic environment with late-arriving clients.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-21T11:47:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2212.00250v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2212.00250v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on
  Long-Context Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheng Wang, Boxiao Jin, Zhongzhi Yu, Minjia Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> How to efficiently serve Large Language Models (LLMs) has become a pressing issue because of their huge computational cost in their autoregressive generation process. To mitigate computational costs, LLMs often employ the KV Cache technique to improve the generation speed. While improving the computational efficiency, the storage requirements of the KV cache are substantial, particularly in long-context scenarios, leading to significant memory consumption. Existing KV cache eviction methods often degrade the performance of LLMs in long-context scenarios due to the information loss introduced by eviction. In this paper, we propose a novel KV cache merging approach, called KVMerger, to achieve adaptive KV cache compression for long-context tasks without significant performance degradation under constrained memory budgets. Our approach is inspired by the intriguing observation that key states exhibit high similarity at the token level within a single sequence. To facilitate merging, we develop an effective yet straightforward merging set identification algorithm to identify suitable KV states for merging. Our merging set identification algorithm stimulates the second observation that KV cache sparsity, from similarity perspective, is independent of the dataset and remains persistent at the model level. Subsequently, we propose a Gaussian kernel weighted merging algorithm to selectively merge all states within each merging set. We conduct extensive experiments to demonstrate the effectiveness of KVMerger for long-context tasks under constrained memory budgets, applying it to models including Llama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll benchmarks, we compare our method with other KV cache compression techniques, including H2O and CaM, showing that our method achieves superior performance across tasks with both 50% and 35% KV cache budgets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-21T02:37:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.08454v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.08454v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Distributed Neural Representation for Reactive in situ Visualization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qi Wu, Joseph A. Insley, Victor A. Mateevitsi, Silvio Rizzi, Michael E. Papka, Kwan-Liu Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Implicit neural representations (INRs) have emerged as a powerful tool for compressing large-scale volume data. This opens up new possibilities for in situ visualization. However, the efficient application of INRs to distributed data remains an underexplored area. In this work, we develop a distributed volumetric neural representation and optimize it for in situ visualization. Our technique eliminates data exchanges between processes, achieving state-of-the-art compression speed, quality and ratios. Our technique also enables the implementation of an efficient strategy for caching large-scale simulation data in high temporal frequencies, further facilitating the use of reactive in situ visualization in a wider range of scientific problems. We integrate this system with the Ascent infrastructure and evaluate its performance and usability using real-world simulations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-20T22:14:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2304.10516v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2304.10516v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 SquareSort: a cache-oblivious sorting algorithm</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michal Koucký, Josef Matějka
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper we consider sorting in the cache-oblivious model of Frigo, Leiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting algorithm in that model which has asymptotically optimal IO complexity $O(\frac{n}{B} \log_{M/B} n)$, where $n$ is the instance size, $M$ size of the cache and $B$ size of a memory block. This is the same as the complexity of the best known cache-oblivious sorting algorithm FunnelSort.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-20T08:21:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.14801v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.14801v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 CacheGen: KV Cache Compression and Streaming for Fast Large Language
  Model Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge. Yet using long contexts is challenging, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause high extra network delays.   CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, leveraging KV cache's distributional properties to encode a KV cache into more compact bitstream representations with negligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts the compression level of different parts of a KV cache to cope with changes in available bandwidth, in order to maintain low context-loading delay and high generation quality. % When available bandwidth drops, CacheGen may raise the compression level for a part of the context or recompute its KV cache on the fly. We test CacheGen on popular LLMs and datasets. Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x and the total delay in fetching and processing contexts by 3.2-3.7x with negligible impact on the LLM response quality. Our code is at: https://github.com/UChi-JCL/CacheGen.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-19T21:04:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.07240v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.07240v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Improving Retrieval in Sponsored Search by Leveraging Query Context
  Signals</h2>
                <div class="authors">
                    <strong>Authors:</strong> Akash Kumar Mohankumar, Gururaj K, Gagan Madan, Amit Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurately retrieving relevant bid keywords for user queries is critical in Sponsored Search but remains challenging, particularly for short, ambiguous queries. Existing dense and generative retrieval models often fail to capture nuanced user intent in these cases. To address this, we propose an approach to enhance query understanding by augmenting queries with rich contextual signals derived from web search results and large language models, stored in an online cache. Specifically, we use web search titles and snippets to ground queries in real-world information and utilize GPT-4 to generate query rewrites and explanations that clarify user intent. These signals are efficiently integrated through a Fusion-in-Decoder based Unity architecture, enabling both dense and generative retrieval with serving costs on par with traditional context-free models. To address scenarios where context is unavailable in the cache, we introduce context glancing, a curriculum learning strategy that improves model robustness and performance even without contextual signals during inference. Extensive offline experiments demonstrate that our context-aware approach substantially outperforms context-free models. Furthermore, online A/B testing on a prominent search engine across 160+ countries shows significant improvements in user engagement and revenue.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-19T14:28:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.14346v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.14346v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 SparQ Attention: Bandwidth-Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-19T09:37:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.04985v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.04985v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari, Mahyar Najibi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-19T06:34:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.14057v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.14057v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Data-driven Forecasting of Deep Learning Performance on GPUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seonho Lee, Amar Phanishayee, Divya Mahajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deep learning kernels exhibit predictable memory accesses and compute patterns, making GPUs' parallel architecture well-suited for their execution. Software and runtime systems for GPUs are optimized to better utilize the stream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As deep learning models and GPUs evolve, access to newer GPUs is often limited, raising questions about the performance of new model architectures on existing GPUs, existing models on new GPUs, and new model architectures on new GPUs. To address these questions, we introduce NeuSight, a framework to predict the performance of various deep learning models, for both training and inference, on unseen GPUs without requiring actual execution. The framework leverages both GPU hardware behavior and software library optimizations to estimate end-to-end performance. Previous work uses regression models that capture linear trends or multilayer perceptrons to predict the overall latency of deep learning kernels on GPUs. These approaches suffer from higher error percentages when forecasting performance on unseen models and new GPUs. Instead, NeuSight decomposes the prediction problem into smaller problems, bounding the prediction through fundamental performance laws. NeuSight decomposes a single deep learning kernel prediction into smaller working sets called tiles, which are executed independently on the GPU. Tile-granularity predictions are determined using a machine learning approach and aggregated to estimate end-to-end latency. NeuSight outperforms prior work across various deep learning workloads and the latest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in predicting the latency of GPT3 model for training and inference on H100, compared to state-of-the-art prior works, where both GPT3 and H100 were not used to train the framework.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-18T18:47:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.13853v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.13853v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero
  Overhead</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amir Zandieh, Majid Daliri, Insu Han
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An effective approach to compress KV cache is quantization. However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2 bits per quantized number. We introduce QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants. We propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion. We have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation. When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime. Codes are available at \url{https://github.com/amirzandieh/QJL}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-18T16:31:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.03482v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.03482v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 MCU-Wide Timing Side Channels and Their Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Johannes Müller, Anna Lena Duque Antón, Lucas Deutschmann, Dino Mehmedagić, Cristiano Rodrigues, Daniel Oliveira, Keerthikumara Devarajegowda, Mohammad Rahmani Fadiheh, Sandro Pinto, Dominik Stoffel, Wolfgang Kunz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Microarchitectural timing side channels have been thoroughly investigated as a security threat in hardware designs featuring shared buffers (e.g., caches) or parallelism between attacker and victim task execution. However, contradicting common intuitions, recent activities demonstrate that this threat is real even in microcontroller SoCs without such features. In this paper, we describe SoC-wide timing side channels previously neglected by security analysis and present a new formal method to close this gap. In a case study on the RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously unknown attack variant that allows an attacker to obtain information about a victim's memory access behavior. After implementing a conservative fix, we were able to verify that the SoC is now secure w.r.t. the considered class of timing side channels.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-18T09:06:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3649329.3656541' target='_blank'>doi</a><a href='http://arxiv.org/abs/2309.12925v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2309.12925v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for
  Dynamic Graph Processing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongfu Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> An efficient data structure is fundamental to meeting the growing demands in dynamic graph processing. However, the dual requirements for graph computation efficiency (with contiguous structures) and graph update efficiency (with linked list-like structures) present a conflict in the design principles of graph structures. After experimental studies of existing state-of-the-art dynamic graph structures, we observe that the overhead of cache misses accounts for a major portion of the graph computation time. This paper presents GastCoCo, a system with graph storage and coroutine-based prefetch co-design. By employing software prefetching via stackless coroutines and introducing a prefetch-friendly data structure CBList, GastCoCo significantly alleviates the performance degradation caused by cache misses. Our results show that GastCoCo outperforms state-of-the-art graph storage systems by 1.3x - 180x in graph updates and 1.4x - 41.1x in graph computation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-18T06:18:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.14396v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.14396v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Faster Diffusion via Temporal Attention Decomposition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, Juan-Manuel Perez-Rua, Jürgen Schmidhuber
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We explore the role of attention mechanism during inference in text-conditional diffusion models. Empirical observations suggest that cross-attention outputs converge to a fixed point after several inference steps. The convergence time naturally divides the entire inference process into two phases: an initial phase for planning text-oriented visual semantics, which are then translated into images in a subsequent fidelity-improving phase. Cross-attention is essential in the initial phase but almost irrelevant thereafter. However, self-attention initially plays a minor role but becomes crucial in the second phase. These findings yield a simple and training-free method known as temporally gating the attention (TGATE), which efficiently generates images by caching and reusing attention outputs at scheduled time steps. Experimental results show when widely applied to various existing text-conditional diffusion models, TGATE accelerates these models by 10%-50%. The code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-17T23:09:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.02747v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.02747v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and
  Feature Balance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeke Xia, Ming Hu, Dengke Yan, Xiaofei Xie, Tianlin Li, Anran Li, Junlong Zhou, Mingsong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Federated Learning (FL) as a promising distributed machine learning paradigm has been widely adopted in Artificial Intelligence of Things (AIoT) applications. However, the efficiency and inference capability of FL is seriously limited due to the presence of stragglers and data imbalance across massive AIoT devices, respectively. To address the above challenges, we present a novel asynchronous FL approach named CaBaFL, which includes a hierarchical Cache-based aggregation mechanism and a feature Balance-guided device selection strategy. CaBaFL maintains multiple intermediate models simultaneously for local training. The hierarchical cache-based aggregation mechanism enables each intermediate model to be trained on multiple devices to align the training time and mitigate the straggler issue. In specific, each intermediate model is stored in a low-level cache for local training and when it is trained by sufficient local devices, it will be stored in a high-level cache for aggregation. To address the problem of imbalanced data, the feature balance-guided device selection strategy in CaBaFL adopts the activation distribution as a metric, which enables each intermediate model to be trained across devices with totally balanced data distributions before aggregation. Experimental results show that compared with the state-of-the-art FL methods, CaBaFL achieves up to 9.26X training acceleration and 19.71\% accuracy improvements.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-17T16:56:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.12850v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.12850v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 CXL Shared Memory Programming: Barely Distributed and Almost Persistent</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Xu, Suyash Mahar, Ziheng Liu, Mingyao Shen, Steven Swanson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Compute Express Link (CXL) enables support for cache-coherent shared memory among multiple nodes, it also introduces new types of failures--processes can fail before data does, or data might fail before a process does. The lack of a failure model for CXL-based shared memory makes it challenging to understand and mitigate these failures.   To solve these challenges, in this paper, we describe a model categorizing and handling the CXL-based shared memory's failures: data and process failures. Data failures in CXL-based shared memory render data inaccessible or inconsistent for a currently running application. We argue that such failures are unlike data failures in distributed storage systems and require CXL-specific handling. To address this, we look into traditional data failure mitigation techniques like erasure coding and replication and propose new solutions to better handle data failures in CXL-based shared memory systems. Next, we look into process failures and compare the failures and potential solutions with PMEM's failure model and programming solutions. We argue that although PMEM shares some of CXL's characteristics, it does not fully address CXL's volatile nature and low access latencies. Finally, taking inspiration from PMEM programming solutions, we propose techniques to handle these new failures.   Thus, this paper is the first work to define the CXL-based shared memory failure model and propose tailored solutions that address challenges specific to CXL-based systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-17T03:02:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.19626v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.19626v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill
  and Extreme KV-Cache Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Goldstein, Fares Obeid, Eric Alcaide, Guangyu Song, Eugene Cheah
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model that uses a new technique to efficiently generate a highly compressed and reusable KV-Cache in linear time and space with respect to sequence length. GoldFinch stacks our new GOLD transformer on top of an enhanced version of the Finch (RWKV-6) architecture. We train up to 1.5B parameter class models of the Finch, Llama, and GoldFinch architectures, and find dramatically improved modeling performance relative to both Finch and Llama. Our cache size savings increase linearly with model layer count, ranging from 756-2550 times smaller than the traditional transformer cache for common sizes, enabling inference of extremely large context lengths even on limited hardware. Although autoregressive generation has O(n) time complexity per token because of attention, pre-fill computation of the entire initial cache state for a submitted context costs only O(1) time per token due to the use of a recurrent neural network (RNN) to generate this cache. We release our trained weights and training code under the Apache 2.0 license for community use.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-16T18:00:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.12077v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.12077v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Coherently amplified ultrafast imaging using a free-electron
  interferometer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tomer Bucher, Harel Nahari, Hanan Herzig Sheinfux, Ron Ruimy, Arthur Niedermayr, Raphael Dahan, Qinghui Yan, Yuval Adiv, Michael Yannai, Jialin Chen, Yaniv Kurman, Sang Tae Park, Daniel J. Masiel, Eli Janzen, James H. Edgar, Fabrizio Carbone, Guy Bartal, Shai Tsesses, Frank H. L. Koppens, Giovanni Maria Vanacore, Ido Kaminer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accessing the low-energy non-equilibrium dynamics of materials and their polaritons with simultaneous high spatial and temporal resolution has been a bold frontier of electron microscopy in recent years. One of the main challenges lies in the ability to retrieve extremely weak signals while simultaneously disentangling amplitude and phase information. Here, we present Free-Electron Ramsey Imaging (FERI), a microscopy approach based on light-induced electron modulation that enables coherent amplification of optical near-fields in electron imaging. We provide simultaneous time-, space-, and phase-resolved measurements of a micro-drum made from a hexagonal boron nitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets therein. The phase-resolved measurements reveals vortex-anti-vortex singularities on the polariton wavefronts, together with an intriguing phenomenon of a traveling wave mimicking the amplitude profile of a standing wave. Our experiments show a 20-fold coherent amplification of the near-field signal compared to conventional electron near-field imaging, resolving peak field intensities in the order of ~W/cm2, corresponding to field amplitudes of a few kV/m. As a result, our work paves the way for spatio-temporal electron microscopy of biological specimens and quantum materials, exciting yet delicate samples that are currently difficult to investigate.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-16T09:05:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.optics</span><span>eess.SP</span><span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1038/s41566-024-01451-w' target='_blank'>doi</a><a href='http://arxiv.org/abs/2305.04877v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2305.04877v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Performance Analysis of Internet of Vehicles Mesh Networks Based on
  Actual Switch Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jialin Hu, Zhiyuan Ren, Wenchi Cheng, Zhiliang Shuai, Zhao Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid growth of the automotive industry has exacerbated the conflict between the complex traffic environment, increasing communication demands, and limited resources. Given the imperative to mitigate traffic and network congestion, analyzing the performance of Internet of Vehicles (IoV) mesh networks is of great practical significance. Most studies focus solely on individual performance metrics and influencing factors, and the adopted simulation tools, such as OPNET, cannot achieve the dynamic link generation of IoV mesh networks. To address these problems, a network performance analysis model based on actual switches is proposed. First, a typical IoV mesh network architecture is constructed and abstracted into a mathematical model that describes how the link and topology changes over time. Then, the task generation model and the task forwarding model based on actual switches are proposed to obtain the real traffic distribution of the network. Finally, a scientific network performance indicator system is constructed. Simulation results demonstrate that, with rising task traffic and decreasing node caching capacity, the packet loss rate increases, and the task arrival rate decreases in the network. The proposed model can effectively evaluate the network performance across various traffic states and provide valuable insights for network construction and enhancement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-16T08:18:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.11483v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.11483v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Heterogeneous integration of high endurance ferroelectric and
  piezoelectric epitaxial BaTiO$_3$ devices on Si</h2>
                <div class="authors">
                    <strong>Authors:</strong> Asraful Haque, Harshal Jason D'Souza, Shubham Kumar Parate, Rama Satya Sandilya, Srinivasan Raghavan, Pavan Nukala
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its ferroelectric, piezoelectric, and nonlinear optical properties in microelectronics. Recently, heterogeneous integration approaches that involve growth of BTO on ideal substrates followed by transfer to a desired substrate show promise of achieving excellent device-quality films. However, beyond simple demonstrations of the existence of ferroelectricity, robust devices with high endurance were not yet demonstrated on Si using the latter approach. Here, using a novel two-step approach to synthesize epitaxial BTO using pulsed laser deposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO) substrates), we demonstrate successful integration of high-quality BTO capacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and electromechanical endurance of greater than $10^6$ cycles. We further address the challenge of cracking and disintegration of thicker films by first transferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm thick) on the desired substrate, followed by the growth of high-quality BTO on this substrate, as revealed by HRXRD and HRSTEM measurements. These templated Si substrates offer a versatile platform for integrating any epitaxial complex oxides with diverse functionalities onto any inorganic substrate.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-16T03:08:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.app-ph</span><span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.11338v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.11338v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 MeanCache: User-Centric Semantic Cache for Large Language Model Based
  Web Services</h2>
                <div class="authors">
                    <strong>Authors:</strong> Waris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ammar Ahmed, Ali Anwar, Muhammad Ali Gulzar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) like ChatGPT and Llama have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters, where inference demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries, which constitute about 31% of the total queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries nor do they operate on contextual queries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a user-centric semantic cache for LLM-based services that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache leverages Federated Learning (FL) to collaboratively train a query similarity model without violating user privacy. By placing a local cache in each user's device and using FL, MeanCache reduces the latency and costs and enhances model performance, resulting in lower false hit rates. MeanCache also encodes context chains for every cached query, offering a simple yet highly effective mechanism to discern contextual query responses from standalone. Our experiments benchmarked against the state-of-the-art caching method, reveal that MeanCache attains an approximately 17% higher F-score and a 20% increase in precision during semantic cache hit-and-miss decisions while performing even better on contextual queries. It also reduces the storage requirement by 83% and accelerates semantic cache hit-and-miss decisions by 11%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-15T22:33:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>cs.CR</span><span>cs.DC</span><span>I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.02694v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.02694v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor
  Network</h2>
                <div class="authors">
                    <strong>Authors:</strong> Raghavendra Kanakagiri, Edgar Solomonik
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse tensor decomposition and completion are common in numerous applications, ranging from machine learning to computational quantum chemistry. Typically, the main bottleneck in optimization of these models are contractions of a single large sparse tensor with a network of several dense matrices or tensors (SpTTN). Prior works on high-performance tensor decomposition and completion have focused on performance and scalability optimizations for specific SpTTN kernels. We present algorithms and a runtime system for identifying and executing the most efficient loop nest for any SpTTN kernel. We consider both enumeration of such loop nests for autotuning and efficient algorithms for finding the lowest cost loop-nest for simpler metrics, such as buffer size or cache miss models. Our runtime system identifies the best choice of loop nest without user guidance, and also provides a distributed-memory parallelization of SpTTN kernels. We evaluate our framework using both real-world and synthetic tensors. Our results demonstrate that our approach outperforms available generalized state-of-the-art libraries and matches the performance of specialized codes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-15T18:38:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.MS</span><span>cs.PF</span><span>cs.PL</span><span>G.1.3; D.1.3</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3626183.3659985' target='_blank'>doi</a><a href='http://arxiv.org/abs/2307.05740v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2307.05740v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 In-Loop Filtering via Trained Look-Up Tables</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuoyuan Li, Jiacheng Li, Yao Li, Li Li, Dong Liu, Feng Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-loop filtering (ILF) is a key technology for removing the artifacts in image/video coding standards. Recently, neural network-based in-loop filtering methods achieve remarkable coding gains beyond the capability of advanced video coding standards, which becomes a powerful coding tool candidate for future video coding standards. However, the utilization of deep neural networks brings heavy time and computational complexity, and high demands of high-performance hardware, which is challenging to apply to the general uses of coding scene. To address this limitation, inspired by explorations in image restoration, we propose an efficient and practical in-loop filtering scheme by adopting the Look-up Table (LUT). We train the DNN of in-loop filtering within a fixed filtering reference range, and cache the output values of the DNN into a LUT via traversing all possible inputs. At testing time in the coding process, the filtered pixel is generated by locating input pixels (to-be-filtered pixel with reference pixels) and interpolating cached filtered pixel values. To further enable the large filtering reference range with the limited storage cost of LUT, we introduce the enhanced indexing mechanism in the filtering process, and clipping/finetuning mechanism in the training. The proposed method is implemented into the Versatile Video Coding (VVC) reference software, VTM-11.0. Experimental results show that the ultrafast, very fast, and fast mode of the proposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39% BD-rate reduction, under the all intra (AI) and random access (RA) configurations. Especially, our method has friendly time and computational complexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel, and only 164-1148 KB storage cost for a single model. Our solution may shed light on the journey of practical neural network-based coding tool evolution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-15T17:25:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.IV</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.10926v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.10926v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory
  Encryption</h2>
                <div class="authors">
                    <strong>Authors:</strong> Martin Unterguggenberger, Lukas Lamster, David Schrammel, Martin Schwarzl, Stefan Mangard
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient cloud computing relies on in-process isolation to optimize performance by running workloads within a single process. Without heavy-weight process isolation, memory safety errors pose a significant security threat by allowing an adversary to extract or corrupt the private data of other co-located tenants. Existing in-process isolation mechanisms are not suitable for modern cloud requirements, e.g., MPK's 16 protection domains are insufficient to isolate thousands of cloud workers per process. Consequently, cloud service providers have a strong need for lightweight in-process isolation on commodity x86 machines.   This paper presents TME-Box, a novel isolation technique that enables fine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing Intel TME-MK, which is intended for the encryption of virtual machines, TME-Box offers lightweight and efficient in-process isolation. TME-Box enforces that sandboxes use their designated encryption keys for memory interactions through compiler instrumentation. This cryptographic isolation enables fine-grained access control, from single cache lines to full pages, and supports flexible data relocation. In addition, the design of TME-Box allows the efficient isolation of up to 32K concurrent sandboxes. We present a performance-optimized TME-Box prototype, utilizing x86 segment-based addressing, that showcases geomean performance overheads of 5.2 % for data isolation and 9.7 % for code and data isolation, evaluated with the SPEC CPU2017 benchmark suite.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-15T14:09:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.10740v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.10740v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Conceptual Codebook Learning for Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Zhang, Ke Yu, Siqi Wu, Zhihai He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel fine-tuning method for vision-language models (VLMs) to address the challenge of improving the generalization capability of VLMs while fine-tuning them on downstream tasks in a few-shot setting. We recognize that visual concepts, such as textures, shapes, and colors are naturally transferable across domains and play a crucial role in generalization tasks. Motivated by this interesting finding, we learn a conceptual codebook consisting of visual concepts as keys and conceptual prompts as values, which serves as a link between the image encoder's outputs and the text encoder's inputs. Specifically, for a given image, we leverage the codebook to identify the most relevant conceptual prompts associated with the class embeddings to perform the classification. Additionally, we incorporate a handcrafted concept cache as a regularization to alleviate the overfitting issues in low-shot scenarios. We observe that this conceptual codebook learning method is able to achieve enhanced alignment between visual and linguistic modalities. Extensive experimental results demonstrate that our CoCoLe method remarkably outperforms the existing state-of-the-art methods across various evaluation settings, including base-to-new generalization, cross-dataset evaluation, and domain generalization tasks. Detailed ablation studies further confirm the efficacy of each component in CoCoLe.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-07-15T14:00:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.02350v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.02350v3' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, Amine Mhedhbi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Schema linking is a crucial step in Text-to-SQL pipelines, which translate natural language queries into SQL. The goal of schema linking is to retrieve relevant tables and columns (signal) while disregarding irrelevant ones (noise). However, imperfect schema linking can often exclude essential columns needed for accurate query generation. In this work, we revisit the need for schema linking when using the latest generation of large language models (LLMs). We find empirically that newer models are adept at identifying relevant schema elements during generation, without the need for explicit schema linking. This allows Text-to-SQL pipelines to bypass schema linking entirely and instead pass the full database schema to the LLM, eliminating the risk of excluding necessary information. Furthermore, as alternatives to schema linking, we propose techniques that improve Text-to-SQL accuracy without compromising on essential schema information. Our approach achieves 71.83\% execution accuracy on the BIRD benchmark, ranking first at the time of submission.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T17:59:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07702v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07702v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Profile Likelihoods in Cosmology: When, Why and How illustrated with
  $Λ$CDM, Massive Neutrinos and Dark Energy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Laura Herold, Elisa G. M. Ferreira, Lukas Heinrich
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Frequentist parameter inference using profile likelihoods has received increased attention in the cosmology literature recently since it can give important complementary information to Bayesian credible intervals. Here, we give a pedagogical review to frequentist parameter inference in cosmology with particular focus on when the graphical profile likelihood construction gives meaningful constraints, i.e.\ confidence intervals with correct coverage. This construction rests on the assumption of the asymptotic limit of a large data set such as in \textit{Wilks' theorem}. We assess the validity of this assumption in the context of three cosmological models with \textit{Planck} 2018 \texttt{Plik\_lite} data: While our tests for the $\Lambda$CDM model indicate that the profile likelihood method gives correct coverage, $\Lambda$CDM with the sum of neutrino masses as a free parameter appears consistent with a Gaussian near a boundary motivating the use of the boundary-corrected or Feldman-Cousins graphical method; for $w_0$CDM with the equation of state of dark energy, $w_0$, as a free parameter, we find indication of a violation of the assumptions. Finally, we compare frequentist and Bayesian constraints of these models. Our results motivate care when using the graphical profile likelihood method in cosmology.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T17:57:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07700v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07700v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Kilonova Light Curve Parameter Estimation Using Likelihood-Free
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Malina Desai, Deep Chatterjee, Sahil Jhawar, Philip Harris, Erik Katsavounidis, Michael Coughlin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a parameter estimation algorithm on kilonova light curves using likelihood-free inference. Our inference is optimized through a pre-trained embedding network that marginalizes the time of arrival and the luminosity distance of the signal. We find that parameter inference utilizing a pre-trained embedding outperforms the use of likelihood-free inference alone, reducing training time and offering the capability to marginalize over certain nuisance parameters. The model is capable of retrieving the intrinsic parameters of the kilonova light curves with a comparable accuracy and precision to nested sampling methods while taking significantly less computational time. This framework has been integrated into the publicly available Nuclear Multi-Messenger Astronomy codebase so users can leverage the model for their inference purposes. This algorithm is broadly applicable to parameterized or simulated light curves of other transient objects.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T17:53:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.IM</span><span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06947v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06947v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Kilonova Emissions from Neutron Star Merger Remnants: Implications for
  Nuclear Equation of State</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kelsey A. Lund, Rahul Somasundaram, Gail C. McLaughlin, Jonah M. Miller, Matthew R. Mumpower, Ingo Tews
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-messenger observation of binary neutron-star mergers can provide valuable information on the nuclear equation of state (EoS). Here, we investigate to which extent electromagnetic observations of the associated kilonovae allow us to place constraints on the EoS. For this, we use state-of-the-art three-dimensional general-relativistic magneto-hydrodynamics simulations and detailed nucleosynthesis modeling to connect properties of observed light curves to properties of the accretion disk, and hence, the EoS. Using our general approach, we use multi-messenger observations of GW170817/AT2017gfo to study the impact of various sources of uncertainty on inferences of the EoS. We constrain the radius of a $\rm{1.4 M_\odot}$ neutron star to lie within $\rm{10.19\leq R_{1.4}\leq 13.0}$~km and the maximum mass to be $\rm{M_{TOV}\leq 3.06 M_\odot}$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T17:39:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07686v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07686v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Learning Optimal Signal Temporal Logic Decision Trees for
  Classification: A Max-Flow MILP Formulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaier Liang, Gustavo A. Cardona, Disha Kamale, Cristian-Ioan Vasile
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a novel framework for inferring timed temporal logic properties from data. The dataset comprises pairs of finite-time system traces and corresponding labels, denoting whether the traces demonstrate specific desired behaviors, e.g. whether the ship follows a safe route or not. Our proposed approach leverages decision-tree-based methods to infer Signal Temporal Logic classifiers using primitive formulae. We formulate the inference process as a mixed integer linear programming optimization problem, recursively generating constraints to determine both data classification and tree structure. Applying a max-flow algorithm on the resultant tree transforms the problem into a global optimization challenge, leading to improved classification rates compared to prior methodologies. Moreover, we introduce a technique to reduce the number of constraints by exploiting the symmetry inherent in STL primitives, which enhances the algorithm's time performance and interpretability. To assess our algorithm's effectiveness and classification performance, we conduct three case studies involving two-class, multi-class, and complex formula classification scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T17:35:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.21090v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.21090v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,
  Applications and Opportunities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, Dacheng Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at \url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T01:49:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07666v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07666v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi-Cheng Lin, Wei-Chih Chen, Hung-yi Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Warning: This paper may contain texts with uncomfortable content.   Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T16:55:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07665v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07665v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Data-driven computation of adjoint sensitivities without adjoint
  solvers: An application to thermoacoustics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Defne E. Ozan, Luca Magri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Adjoint methods have been the pillar of gradient-based optimization for decades. They enable the accurate computation of a gradient (sensitivity) of a quantity of interest with respect to all system's parameters in one calculation. When the gradient is embedded in an optimization routine, the quantity of interest can be optimized for the system to have the desired behaviour. Adjoint methods require the system's governing equations and their Jacobian. We propose a computational strategy to infer the adjoint sensitivities from data when the governing equations might be unknown (or partly unknown), and noise might be present. The key component of this strategy is an echo state network, which learns the dynamics of nonlinear regimes with varying parameters, and evolves dynamically via a hidden state. Although the framework is general, we focus on thermoacoustics governed by nonlinear and time-delayed systems. First, we show that a parameter-aware Echo State Network (ESN) infers the parameterized dynamics. Second, we derive the adjoint of the ESN to compute the sensitivity to parameters and initial conditions. Third, we propose the Thermoacoustic Echo State Network (T-ESN), which embeds the physical knowledge in the network architecture. Fourth, we apply the framework to a variety of nonlinear thermoacoustic regimes of a prototypical system. We show that the T-ESN accurately infers the correct adjoint sensitivities of the time-averaged acoustic energy with respect to the flame parameters. The results are robust to noisy data, from periodic, through quasiperiodic, to chaotic regimes. A single network predicts the nonlinear bifurcations on unseen scenarios, and so the inferred adjoint sensitivities are employed to suppress an instability via steepest descent. This work opens new possibilities for gradient-based data-driven design optimization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T16:45:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.flu-dyn</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.11738v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.11738v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Properties of sunspot light bridges on a geometric height scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> S. Esteban Pozuelo, A. Asensio Ramos, C. J. Díaz Baso, B. Ruiz Cobo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Investigating light bridges (LBs) helps us comprehend key aspects of sunspots. However, few studies have analyzed the properties of LBs in terms of the geometric height, which is a more realistic perspective given the corrugation of the solar atmosphere. We aim to shed light on LBs by studying the variation in their physical properties with geometric height. We used the SICON code to infer the physical quantities in terms of the optical depth and the Wilson depression values of three LBs hosted by a sunspot observed with Hinode/SP in the Fe I 630 nm pair lines. We also used SIR inversions to cross-check the height variation of the field inclination in the LBs. In both output sets, we performed linear interpolation to convert the physical parameters from optical depth into a geometric height scale in each pixel. We classified each LB as filamentary, grainy, or umbral. They appear as ridges that reach different maximum heights, with the umbral LB being the deepest. While the filamentary LB hosts a plasma inflow from the penumbra, the results for the grainy LB are compatible with an injection of hot plasma through convective cells of reduced field strength. Only a few positions reveal hints suggesting a cusp-like magnetic canopy. Moreover, strong gradients in the magnetic field strength and inclination usually exhibit enhanced electric currents, with the filamentary LB having remarkably strong currents that appear to be related to chromospheric events. The height stratification in filamentary and grainy LBs differ, indicating diverse mechanisms at work. Our results are in general incompatible with a magnetic canopy scenario, and further analysis is needed to confirm whether it exists along the entire LB or only at specific locations. Furthermore, this work assesses the usefulness of SICON when determining the height stratification of solar structures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T16:17:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.SR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07645v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07645v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Likelihood-based Inference for Random Networks with Changepoints</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Cirkovic, Tiandong Wang, Xianyang Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative, temporal network models play an important role in analyzing the dependence structure and evolution patterns of complex networks. Due to the complicated nature of real network data, it is often naive to assume that the underlying data-generative mechanism itself is invariant with time. Such observation leads to the study of changepoints or sudden shifts in the distributional structure of the evolving network. In this paper, we propose a likelihood-based methodology to detect changepoints in undirected, affine preferential attachment networks, and establish a hypothesis testing framework to detect a single changepoint, together with a consistent estimator for the changepoint. Such results require establishing consistency and asymptotic normality of the MLE under the changepoint regime, which suffers from long range dependence. The methodology is then extended to the multiple changepoint setting via both a sliding window method and a more computationally efficient score statistic. We also compare the proposed methodology with previously developed non-parametric estimators of the changepoint via simulation, and the methods developed herein are applied to modeling the popularity of a topic in a Twitter network over time.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T16:14:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>05C82, 60F05, 60G44</span><span>G.3; I.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2206.01076v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2206.01076v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Likelihood Based Inference for ARMA Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jesse Wheeler, Edward L. Ionides
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoregressive moving average (ARMA) models are frequently used to analyze time series data. Despite the popularity of these models, likelihood-based inference for ARMA models has subtleties that have been previously identified but continue to cause difficulties in widely used data analysis strategies. We discuss common pitfalls that may lead to sub-optimal maximum likelihood parameter estimates. We propose a random initialization algorithm for parameter estimation that frequently yields higher likelihoods than traditional maximum likelihood estimation procedures. We then investigate the parameter uncertainty of maximum likelihood estimates, and propose the use of profile confidence intervals as a superior alternative to intervals derived from the Fisher information matrix. Through a data analysis example and a series of simulation studies, we demonstrate the efficacy of our proposed algorithm and the improved nominal coverage of profile confidence intervals compared to the normal approximation based on Fisher information.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T16:10:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.01198v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.01198v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Massive Activations in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers. Code is available at https://github.com/locuslab/massive-activations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T16:00:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.17762v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.17762v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 An Event Structure-aware Generative Model for Biomedical Event
  Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haohan Yuan, Siu Cheung Hui, Haopeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Biomedical Event Extraction (BEE) is a challenging task that involves modeling complex relationships between fine-grained entities in biomedical text. BEE has traditionally been formulated as a classification problem. With the recent technological advancements in large language models (LLMs), generation-based models that cast event extraction as a sequence generation problem have attracted much attention from the NLP research communities. However, current generative models often overlook the importance of cross-instance information from complex event structures such as nested events and overlapping events, which contribute quite significantly in the benchmark datasets. In this paper, we propose an event structure-aware generative model called GenBEE, which can capture complex event structures in biomedical text for biomedical event extraction. In particular, GenBEE constructs event prompts that distill knowledge from LLMs for incorporating both label semantics and argument dependency relationships into the proposed model. In addition, GenBEE also generates prefixes with event structural prompts to incorporate structural features for improving the model's overall performance. We have evaluated the proposed GenBEE model on three widely used biomedical event extraction benchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that GenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets, and achieved competitive results when compared to the state-of-the-art classification-based models on the PHEE dataset.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T15:24:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06583v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06583v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 GTree: GPU-Friendly Privacy-preserving Decision Tree Training and
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qifan Wang, Shujie Cui, Lei Zhou, Ye Dong, Jianli Bai, Yun Sing Koh, Giovanni Russello
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Decision tree (DT) is a widely used machine learning model due to its versatility, speed, and interpretability. However, for privacy-sensitive applications, outsourcing DT training and inference to cloud platforms raise concerns about data privacy. Researchers have developed privacy-preserving approaches for DT training and inference using cryptographic primitives, such as Secure Multi-Party Computation (MPC). While these approaches have shown progress, they still suffer from heavy computation and communication overheads. Few recent works employ Graphical Processing Units (GPU) to improve the performance of MPC-protected deep learning. This raises a natural question: \textit{can MPC-protected DT training and inference be accelerated by GPU?}   We present GTree, the first scheme that uses GPU to accelerate MPC-protected secure DT training and inference. GTree is built across 3 parties who securely and jointly perform each step of DT training and inference with GPU. Each MPC protocol in GTree is designed in a GPU-friendly version. The performance evaluation shows that GTree achieves ${\thicksim}11{\times}$ and ${\thicksim}21{\times}$ improvements in training SPECT and Adult datasets, compared to the prior most efficient CPU-based work. For inference, GTree shows its superior efficiency when the DT has less than 10 levels, which is $126\times$ faster than the prior most efficient work when inferring $10^4$ instances with a tree of 7 levels. GTree also achieves a stronger security guarantee than prior solutions, which only leaks the tree depth and size of data samples while prior solutions also leak the tree structure. With \textit{oblivious array access}, the access pattern on GPU is also protected.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T15:35:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2305.00645v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2305.00645v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Exploring LLM Multi-Agents for ICD Coding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rumeng Li, Xun Wang, Hong Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To address the limitations of Large Language Models (LLMs) in the International Classification of Diseases (ICD) coding task, where they often produce inaccurate and incomplete prediction results due to the high-dimensional and skewed distribution of the ICD codes, and often lack interpretability and reliability as well. We introduce an innovative multi-agent approach for ICD coding which mimics the ICD coding assignment procedure in real-world settings, comprising five distinct agents: the patient, physician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based model tailored to their specific role within the coding process. We also integrate the system with Electronic Health Record (HER)'s SOAP (subjective, objective, assessment and plan) structure to boost the performances. We compare our method with a system of agents designed solely by LLMs and other strong baselines and evaluate it using the Medical Information Mart for Intensive Care III (MIMIC-III) dataset. Our multi-agent coding framework significantly outperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency with CoT (CoT-SC) in coding common and rare ICD codes. An ablation study validates the effectiveness of the designated agent roles. it also outperforms the LLM-designed agent system. Moreover, our method achieves comparable results to state-of-the-art ICD coding methods that require extensive pre-training or fine-tuning, and outperforms them in rare code accuracy, and explainability. Additionally, we demonstrate the method's practical applicability by presenting its performance in scenarios not limited by the common or rare ICD code constraints.The proposed multi-agent method for ICD coding effectively mimics the real-world coding process and improves performance on both common and rare codes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T15:32:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.15363v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.15363v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Compact Model Training by Low-Rank Projection with Energy Transfer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kailing Guo, Zhenquan Lin, Canyang Chen, Xiaofen Xing, Fang Liu, Xiangmin Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low-rankness plays an important role in traditional machine learning, but is not so popular in deep learning. Most previous low-rank network compression methods compress networks by approximating pre-trained models and re-training. However, the optimal solution in the Euclidean space may be quite different from the one with low-rank constraint. A well-pre-trained model is not a good initialization for the model with low-rank constraints. Thus, the performance of a low-rank compressed network degrades significantly. Compared with other network compression methods such as pruning, low-rank methods attract less attention in recent years. In this paper, we devise a new training method, low-rank projection with energy transfer (LRPET), that trains low-rank compressed networks from scratch and achieves competitive performance. We propose to alternately perform stochastic gradient descent training and projection of each weight matrix onto the corresponding low-rank manifold. Compared to re-training on the compact model, this enables full utilization of model capacity since solution space is relaxed back to Euclidean space after projection. The matrix energy (the sum of squares of singular values) reduction caused by projection is compensated by energy transfer. We uniformly transfer the energy of the pruned singular values to the remaining ones. We theoretically show that energy transfer eases the trend of gradient vanishing caused by projection. In modern networks, a batch normalization (BN) layer can be merged into the previous convolution layer for inference, thereby influencing the optimal low-rank approximation of the previous layer. We propose BN rectification to cut off its effect on the optimal low-rank approximation, which further improves the performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T15:31:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2204.05566v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2204.05566v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation
  Integrating Web Search and Knowledge Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However, LLMs are prone to produce factually incorrect information and often produce "phantom" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a "Retrieval-Augmented Generation (RAG)" system. First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T15:19:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07611v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07611v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Crossover Designs in Software Engineering Experiments: Review of the
  State of Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Julian Frattini, Davide Fucci, Sira Vegas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Experimentation is an essential method for causal inference in any empirical discipline. Crossover-design experiments are common in Software Engineering (SE) research. In these, subjects apply more than one treatment in different orders. This design increases the amount of obtained data and deals with subject variability but introduces threats to internal validity like the learning and carryover effect. Vegas et al. reviewed the state of practice for crossover designs in SE research and provided guidelines on how to address its threats during data analysis while still harnessing its benefits. In this paper, we reflect on the impact of these guidelines and review the state of analysis of crossover design experiments in SE publications between 2015 and March 2024. To this end, by conducting a forward snowballing of the guidelines, we survey 136 publications reporting 67 crossover-design experiments and evaluate their data analysis against the provided guidelines. The results show that the validity of data analyses has improved compared to the original state of analysis. Still, despite the explicit guidelines, only 29.5% of all threats to validity were addressed properly. While the maturation and the optimal sequence threats are properly addressed in 35.8% and 38.8% of all studies in our sample respectively, the carryover threat is only modeled in about 3% of the observed cases. The lack of adherence to the analysis guidelines threatens the validity of the conclusions drawn from crossover design experiments
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:49:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07594v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07594v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Multi-periodicity dependency Transformer based on spectrum offset for
  radio frequency fingerprint identification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jing Xiao, Wenrui Ding, Zeqi Shao, Duona Zhang, Yanan Ma, Yufeng Wang, Jian Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Radio Frequency Fingerprint Identification (RFFI) has emerged as a pivotal task for reliable device authentication. Despite advancements in RFFI methods, background noise and intentional modulation features result in weak energy and subtle differences in the RFF features. These challenges diminish the capability of RFFI methods in feature representation, complicating the effective identification of device identities. This paper proposes a novel Multi-Periodicity Dependency Transformer (MPDFormer) to address these challenges. The MPDFormer employs a spectrum offset-based periodic embedding representation to augment the discrepency of intrinsic features. We delve into the intricacies of the periodicity-dependency attention mechanism, integrating both inter-period and intra-period attention mechanisms. This mechanism facilitates the extraction of both long and short-range periodicity-dependency features , accentuating the feature distinction whilst concurrently attenuating the perturbations caused by background noise and weak-periodicity features. Empirical results demonstrate MPDFormer's superiority over established baseline methods, achieving a 0.07s inference time on NVIDIA Jetson Orin NX.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:48:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07592v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07592v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Natural Language to Verilog: Design of a Recurrent Spiking Neural
  Network using Large Language Models and ChatGPT</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paola Vitolo, George Psaltakis, Michael Tomlinson, Gian Domenico Licciardo, Andreas G. Andreou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper investigates the use of Large Language Models (LLMs) for automating the generation of hardware description code, aiming to explore their potential in supporting and enhancing the development of efficient neuromorphic computing architectures. Building on our prior work, we employ OpenAI's ChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a programmable recurrent spiking neural network, while also generating test benches to assess the system's correctness. The resultant design was validated in three case studies, the exclusive OR,the IRIS flower classification and the MNIST hand-written digit classification, achieving accuracies of up to 96.6%. To verify its synthesizability and implementability, the design was prototyped on a field-programmable gate array and implemented on SkyWater 130 nm technology by using an open-source electronic design automation flow. Additionally, we have submitted it to Tiny Tapeout 6 chip fabrication program to further evaluate the system on-chip performance in the future.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:42:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.01419v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.01419v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Transformers and Large Language Models for Efficient Intrusion Detection
  Systems: A Comprehensive Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hamza Kheddar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:28:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span><span>cs.CV</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07583v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07583v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Disentangled Representation Learning with Transmitted Information
  Bottleneck</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuohang Dang, Minnan Luo, Chengyou Jia, Guang Dai, Jihong Wang, Xiaojun Chang, Jingdong Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Encoding only the task-related information from the raw data, \ie, disentangled representation learning, can greatly contribute to the robustness and generalizability of models. Although significant advances have been made by regularizing the information in representations with information theory, two major challenges remain: 1) the representation compression inevitably leads to performance drop; 2) the disentanglement constraints on representations are in complicated optimization. To these issues, we introduce Bayesian networks with transmitted information to formulate the interaction among input and representations during disentanglement. Building upon this framework, we propose \textbf{DisTIB} (\textbf{T}ransmitted \textbf{I}nformation \textbf{B}ottleneck for \textbf{Dis}entangled representation learning), a novel objective that navigates the balance between information compression and preservation. We employ variational inference to derive a tractable estimation for DisTIB. This estimation can be simply optimized via standard gradient descent with a reparameterization trick. Moreover, we theoretically prove that DisTIB can achieve optimal disentanglement, underscoring its superior efficacy. To solidify our claims, we conduct extensive experiments on various downstream tasks to demonstrate the appealing efficacy of DisTIB and validate our theoretical analyses.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:11:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2311.01686v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2311.01686v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Multi-task Heterogeneous Graph Learning on Electronic Health Records</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tsai Hor Chan, Guosheng Yin, Kyongtae Bae, Lequan Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning electronic health records (EHRs) has received emerging attention because of its capability to facilitate accurate medical diagnosis. Since the EHRs contain enriched information specifying complex interactions between entities, modeling EHRs with graphs is shown to be effective in practice. The EHRs, however, present a great degree of heterogeneity, sparsity, and complexity, which hamper the performance of most of the models applied to them. Moreover, existing approaches modeling EHRs often focus on learning the representations for a single task, overlooking the multi-task nature of EHR analysis problems and resulting in limited generalizability across different tasks. In view of these limitations, we propose a novel framework for EHR modeling, namely MulT-EHR (Multi-Task EHR), which leverages a heterogeneous graph to mine the complex relations and model the heterogeneity in the EHRs. To mitigate the large degree of noise, we introduce a denoising module based on the causal inference framework to adjust for severe confounding effects and reduce noise in the EHR data. Additionally, since our model adopts a single graph neural network for simultaneous multi-task prediction, we design a multi-task learning module to leverage the inter-task knowledge to regularize the training process. Extensive empirical studies on MIMIC-III and MIMIC-IV datasets validate that the proposed method consistently outperforms the state-of-the-art designs in four popular EHR analysis tasks -- drug recommendation, and predictions of the length of stay, mortality, and readmission. Thorough ablation studies demonstrate the robustness of our method upon variations to key components and hyperparameters.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:06:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07569v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07569v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Testing the assumptions of the Effective Field Theory of Large-Scale
  Structure</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mandar Karandikar, Cristiano Porciani, Oliver Hahn
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Effective Field Theory of Large-Scale Structure (EFTofLSS) attempts to amend some of the shortcomings of the traditional perturbative methods used in cosmology. It models the evolution of long-wavelength perturbations above a cutoff scale without the need for a detailed description of the short-wavelength ones. Short-scale physics is encoded in the coefficients of a series of operators composed of the long-wavelength fields, and ordered in a systematic expansion. As applied in the literature, the EFTofLSS corrects a summary statistic (such as the power spectrum) calculated from standard perturbation theory by matching it to $N$-body simulations or observations. This `bottom-up' construction is remarkably successful in extending the range of validity of perturbation theory. In this work, we compare this framework to a `top-down' approach, which estimates the EFT coefficients from the stress tensor of an $N$-body simulation, and propagates the corrections to the summary statistic. We consider simple initial conditions, viz. two sinusoidal, plane-parallel density perturbations with substantially different frequencies and amplitudes. We find that the leading EFT correction to the power spectrum in the top-down model is in excellent agreement with that inferred from the bottom-up approach which, by construction, provides an exact match to the numerical data. This result is robust to changes in the wavelength separation between the two linear perturbations. However, in our setup, the leading EFT coefficient does not always grow linearly with the cosmic expansion factor as assumed in the literature based on perturbative considerations. Instead, it decreases after orbit crossing takes place.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:03:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span><span>hep-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1088/1475-7516/2024/01/051' target='_blank'>doi</a><a href='http://arxiv.org/abs/2307.11090v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2307.11090v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Variant Specific Treatment Effects with Applications in Vaccine Studies</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gellert Perenyi, Mats J. Stensrud
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Pathogens usually exist in heterogeneous variants, like subtypes and strains. Quantifying treatment effects on the different variants is important for guiding prevention policies and treatment development. Here we ground analyses of variant-specific effects on a formal framework for causal inference. This allows us to clarify the interpretation of existing methods and define new estimands. Unlike most of the existing literature, we explicitly consider the (realistic) setting with interference in the target population: even if individuals can be sensibly perceived as iid in randomized trial data, there will often be interference in the target population where treatments, like vaccines, are rolled out. Thus, one of our contributions is to derive explicit conditions guaranteeing that commonly reported vaccine efficacy parameters quantify well-defined causal effects, also in the presence of interference. Furthermore, our results give alternative justifications for reporting estimands on the relative, rather than absolute, scale. We illustrate the findings with an analysis of a large HIV1 vaccine trial, where there is interest in distinguishing vaccine effects on viruses with different genome sequences.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:52:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07560v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07560v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 GS-Pose: Generalizable Segmentation-based 6D Object Pose Estimation with
  3D Gaussian Splatting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dingding Cai, Janne Heikkilä, Esa Rahtu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces GS-Pose, a unified framework for localizing and estimating the 6D pose of novel objects. GS-Pose begins with a set of posed RGB images of a previously unseen object and builds three distinct representations stored in a database. At inference, GS-Pose operates sequentially by locating the object in the input image, estimating its initial 6D pose using a retrieval approach, and refining the pose with a render-and-compare method. The key insight is the application of the appropriate object representation at each stage of the process. In particular, for the refinement step, we leverage 3D Gaussian splatting, a novel differentiable rendering technique that offers high rendering speed and relatively low optimization time. Off-the-shelf toolchains and commodity hardware, such as mobile phones, can be used to capture new objects to be added to the database. Extensive evaluations on the LINEMOD and OnePose-LowTexture datasets demonstrate excellent performance, establishing the new state-of-the-art. Project page: https://dingdingcai.github.io/gs-pose.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:43:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.10683v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.10683v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Inverse Rendering of Fusion Plasmas: Inferring Plasma Composition from
  Imaging Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ekin Öztürk, Rob Akers, Stanislas Pamela, The MAST Team, Pieter Peers, Abhijeet Ghosh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we develop a differentiable rendering pipeline for visualising plasma emission within tokamaks, and estimating the gradients of the emission and estimating other physical quantities. Unlike prior work, we are able to leverage arbitrary representations of plasma quantities and easily incorporate them into a non-linear optimisation framework. The efficiency of our method enables not only estimation of a physically plausible image of plasma, but also recovery of the neutral Deuterium distribution from imaging and midplane measurements alone. We demonstrate our method with three different levels of complexity showing first that a poloidal neutrals density distribution can be recovered from imaging alone, second that the distributions of neutral Deuterium, electron density and electron temperature can be recovered jointly, and finally, that this can be done in the presence of realistic imaging systems that incorporate sensor cropping and quantisation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:41:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.plasm-ph</span><span>cs.GR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07555v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07555v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 R2Human: Real-Time 3D Human Appearance Rendering from a Single Image</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuanwang Yang, Qiao Feng, Yu-Kun Lai, Kun Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Rendering 3D human appearance from a single image in real-time is crucial for achieving holographic communication and immersive VR/AR. Existing methods either rely on multi-camera setups or are constrained to offline operations. In this paper, we propose R2Human, the first approach for real-time inference and rendering of photorealistic 3D human appearance from a single image. The core of our approach is to combine the strengths of implicit texture fields and explicit neural rendering with our novel representation, namely Z-map. Based on this, we present an end-to-end network that performs high-fidelity color reconstruction of visible areas and provides reliable color inference for occluded regions. To further enhance the 3D perception ability of our network, we leverage the Fourier occupancy field as a prior for generating the texture field and providing a sampling surface in the rendering stage. We also propose a consistency loss and a spatial fusion strategy to ensure the multi-view coherence. Experimental results show that our method outperforms the state-of-the-art methods on both synthetic data and challenging real-world images, in real-time. The project page can be found at http://cic.tju.edu.cn/faculty/likun/projects/R2Human.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:41:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.05826v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.05826v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 LLM Voting: Human Choices and AI Collective Decision Making</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joshua C. Yang, Damian Dailisan, Marcin Korecki, Carina I. Hausladen, Dirk Helbing
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:41:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>cs.LG</span><span>econ.GN</span><span>q-fin.EC</span><span>68T05, 91B14, 91C20</span><span>I.2.7; J.4; K.4.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.01766v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.01766v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform
  Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sang-Hoon Lee, Ha-Yeong Choi, Seong-Whan Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, universal waveform generation tasks have been investigated conditioned on various out-of-distribution scenarios. Although GAN-based methods have shown their strength in fast waveform generation, they are vulnerable to train-inference mismatch scenarios such as two-stage text-to-speech. Meanwhile, diffusion-based models have shown their powerful generative performance in other domains; however, they stay out of the limelight due to slow inference speed in waveform generation tasks. Above all, there is no generator architecture that can explicitly disentangle the natural periodic features of high-resolution waveform signals. In this paper, we propose PeriodWave, a novel universal waveform generation model. First, we introduce a period-aware flow matching estimator that can capture the periodic features of the waveform signal when estimating the vector fields. Additionally, we utilize a multi-period estimator that avoids overlaps to capture different periodic features of waveform signals. Although increasing the number of periods can improve the performance significantly, this requires more computational costs. To reduce this issue, we also propose a single period-conditional universal estimator that can feed-forward parallel by period-wise batch inference. Additionally, we utilize discrete wavelet transform to losslessly disentangle the frequency information of waveform signals for high-frequency modeling, and introduce FreeU to reduce the high-frequency noise for waveform generation. The experimental results demonstrated that our model outperforms the previous models both in Mel-spectrogram reconstruction and text-to-speech tasks. All source code will be available at \url{https://github.com/sh-lee-prml/PeriodWave}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:36:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span><span>cs.LG</span><span>eess.AS</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07547v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07547v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 $χ$SPN: Characteristic Interventional Sum-Product Networks for Causal
  Inference in Hybrid Domains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Harsh Poonia, Moritz Willig, Zhongjie Yu, Matej Zečević, Kristian Kersting, Devendra Singh Dhami
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Causal inference in hybrid domains, characterized by a mixture of discrete and continuous variables, presents a formidable challenge. We take a step towards this direction and propose Characteristic Interventional Sum-Product Network ($\chi$SPN) that is capable of estimating interventional distributions in presence of random variables drawn from mixed distributions. $\chi$SPN uses characteristic functions in the leaves of an interventional SPN (iSPN) thereby providing a unified view for discrete and continuous random variables through the Fourier-Stieltjes transform of the probability measures. A neural network is used to estimate the parameters of the learned iSPN using the intervened data. Our experiments on 3 synthetic heterogeneous datasets suggest that $\chi$SPN can effectively capture the interventional distributions for both discrete and continuous variables while being expressive and causally adequate. We also show that $\chi$SPN generalize to multiple interventions while being trained only on a single intervention data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:31:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07545v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07545v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson
  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Simon Kloker, Herbertson Bukoli, Twaha Kateete
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Introduction: Poor educational quality in Secondary Schools is still regarded as one of the major struggles in 21st century Uganda - especially in rural areas. Research identifies several problems, including low quality or absent teacher lesson planning. As the government pushes towards the implementation of a new curriculum, exiting lesson plans become obsolete and the problem is worsened. Using a Retrieval Augmented Generation approach, we developed a prototype that generates customized lesson plans based on the government-accredited textbooks. This helps teachers create lesson plans more efficiently and with better quality, ensuring they are fully aligned the new curriculum and the competence-based learning approach.   Methods: The prototype was created using Cohere LLM and Sentence Embeddings, and LangChain Framework - and thereafter made available on a public website. Vector stores were trained for three new curriculum textbooks (ICT, Mathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were generated following a pseudo-random generation protocol, based on the suggested periods in the textbooks. The lesson plans were analyzed regarding their technical quality by three independent raters following the Lesson Plan Analysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically designed for East Africa and competence-based curriculums.   Results: Evaluation of 24 lesson plans using the LPAP resulted in an average quality of between 75 and 80%, corresponding to "very good lesson plan". None of the lesson plans scored below 65%, although one lesson plan could be argued to have been missing the topic. In conclusion, the quality of the generated lesson plans is at least comparable, if not better, than those created by humans, as demonstrated in a study in Rwanda, whereby no lesson plan even reached the benchmark of 50%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:22:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span><span>cs.IR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07542v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07542v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Lost in Overlap: Exploring Watermark Collision in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiyang Luo, Ke Lin, Chao Gu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread usage of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks, such as paraphrasing or translation. In this paper, we introduce watermark collision as a novel and general philosophy for watermark attacks, aimed at enhancing attack performance on top of any other attacking methods. We also provide a comprehensive demonstration that watermark collision poses a threat to all logit-based watermark algorithms, impacting not only specific attack scenarios but also downstream applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:15:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.10020v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.10020v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Usefulness of data flow diagrams and large language models for security
  threat validation: a registered report</h2>
                <div class="authors">
                    <strong>Authors:</strong> Winnie Bahati Mbaka, Katja Tuma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well. Threat analysis and risk assessment are used to identify security threats for new or refactored systems. Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis. Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats. We propose a controlled experiment with practitioners to investigate whether some analysis material (like LLM-generated advice) is better than none and whether more material (the system's data flow diagram and LLM-generated advice) is better than some material. In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design. Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T09:30:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07537v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07537v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving
  Neural Networks (Inference)</h2>
                <div class="authors">
                    <strong>Authors:</strong> John Chiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we present a novel matrix-encoding method that is particularly convenient for neural networks to make predictions in a privacy-preserving manner using homomorphic encryption. Based on this encoding method, we implement a convolutional neural network for handwritten image classification over encryption. For two matrices $A$ and $B$ to perform homomorphic multiplication, the main idea behind it, in a simple version, is to encrypt matrix $A$ and the transpose of matrix $B$ into two ciphertexts respectively. With additional operations, the homomorphic matrix multiplication can be calculated over encrypted matrices efficiently. For the convolution operation, we in advance span each convolution kernel to a matrix space of the same size as the input image so as to generate several ciphertexts, each of which is later used together with the ciphertext encrypting input images for calculating some of the final convolution results. We accumulate all these intermediate results and thus complete the convolution operation.   In a public cloud with 40 vCPUs, our convolutional neural network implementation on the MNIST testing dataset takes $\sim$ 287 seconds to compute ten likelihoods of 32 encrypted images of size $28 \times 28$ simultaneously. The data owner only needs to upload one ciphertext ($\sim 19.8$ MB) encrypting these 32 images to the public cloud.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:07:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2201.12577v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2201.12577v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Development of a Multi-Agent Clinical Decision Support System for Korean
  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in
  Emergency Departments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seungjun Han, Wongyung Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.   We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.   The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.   Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:03:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07531v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07531v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Towards Real-time Video Compressive Sensing on Mobile Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Miao Cao, Lishun Wang, Huan Wang, Guoqing Wang, Xin Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video Snapshot Compressive Imaging (SCI) uses a low-speed 2D camera to capture high-speed scenes as snapshot compressed measurements, followed by a reconstruction algorithm to retrieve the high-speed video frames. The fast evolving mobile devices and existing high-performance video SCI reconstruction algorithms motivate us to develop mobile reconstruction methods for real-world applications. Yet, it is still challenging to deploy previous reconstruction algorithms on mobile devices due to the complex inference process, let alone real-time mobile reconstruction. To the best of our knowledge, there is no video SCI reconstruction model designed to run on the mobile devices. Towards this end, in this paper, we present an effective approach for video SCI reconstruction, dubbed MobileSCI, which can run at real-time speed on the mobile devices for the first time. Specifically, we first build a U-shaped 2D convolution-based architecture, which is much more efficient and mobile-friendly than previous state-of-the-art reconstruction methods. Besides, an efficient feature mixing block, based on the channel splitting and shuffling mechanisms, is introduced as a novel bottleneck block of our proposed MobileSCI to alleviate the computational burden. Finally, a customized knowledge distillation strategy is utilized to further improve the reconstruction quality. Extensive results on both simulated and real data show that our proposed MobileSCI can achieve superior reconstruction quality with high efficiency on the mobile devices. Particularly, we can reconstruct a 256 X 256 X 8 snapshot compressed measurement with real-time performance (about 35 FPS) on an iPhone 15. Code is available at https://github.com/mcao92/MobileSCI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:03:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07530v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07530v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Causal modelling without introducing counterfactuals or abstract
  distributions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benedikt Höltgen, Robert C. Williamson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The most common approach to causal modelling is the potential outcomes framework due to Neyman and Rubin. In this framework, outcomes of counterfactual treatments are assumed to be well-defined. This metaphysical assumption is often thought to be problematic yet indispensable. The conventional approach relies not only on counterfactuals but also on abstract notions of distributions and assumptions of independence that are not directly testable. In this paper, we construe causal inference as treatment-wise predictions for finite populations where all assumptions are testable; this means that one can not only test predictions themselves (without any fundamental problem) but also investigate sources of error when they fail. The new framework highlights the model-dependence of causal claims as well as the difference between statistical and scientific inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:01:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>cs.LG</span><span>econ.EM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.17385v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.17385v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Learning-based Models for Vulnerability Detection: An Extensive Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chao Ni, Liyu Shen, Xiaodan Xu, Xin Yin, Shaohua Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Though many deep learning-based models have made great progress in vulnerability detection, we have no good understanding of these models, which limits the further advancement of model capability, understanding of the mechanism of model detection, and efficiency and safety of practical application of models. In this paper, we extensively and comprehensively investigate two types of state-of-the-art learning-based approaches (sequence-based and graph-based) by conducting experiments on a recently built large-scale dataset. We investigate seven research questions from five dimensions, namely model capabilities, model interpretation, model stability, ease of use of model, and model economy. We experimentally demonstrate the priority of sequence-based models and the limited abilities of both LLM (ChatGPT) and graph-based models. We explore the types of vulnerability that learning-based models skilled in and reveal the instability of the models though the input is subtlely semantical-equivalently changed. We empirically explain what the models have learned. We summarize the pre-processing as well as requirements for easily using the models. Finally, we initially induce the vital information for economically and safely practical usage of these models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:01:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07526v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07526v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Fast Inference for Probabilistic Answer Set Programs via the Residual
  Program</h2>
                <div class="authors">
                    <strong>Authors:</strong> Damiano Azzolini, Fabrizio Riguzzi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> When we want to compute the probability of a query from a Probabilistic Answer Set Program, some parts of a program may not influence the probability of a query, but they impact on the size of the grounding. Identifying and removing them is crucial to speed up the computation. Algorithms for SLG resolution offer the possibility of returning the residual program which can be used for computing answer sets for normal programs that do have a total well-founded model. The residual program does not contain the parts of the program that do not influence the probability. In this paper, we propose to exploit the residual program for performing inference. Empirical results on graph datasets show that the approach leads to significantly faster inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T12:58:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07524v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07524v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Learning Coefficients in Semi-Regular Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuki Kurumadani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances have clarified theoretical learning accuracy in Bayesian inference, revealing that the asymptotic behavior of metrics such as generalization loss and free energy, assessing predictive accuracy, is dictated by a rational number unique to each statistical model, termed the learning coefficient (real log canonical threshold) . For models meeting regularity conditions, their learning coefficients are known. However, for singular models not meeting these conditions, exact values of learning coefficients are provided for specific models like reduced-rank regression, but a broadly applicable calculation method for these learning coefficients in singular models remains elusive. The problem of determining learning coefficients relates to finding normal crossings of Kullback-Leibler divergence in algebraic geometry. In this context, it is crucial to perform appropriate coordinate transformations and blow-ups. This paper introduces an approach that utilizes properties of the log-likelihood ratio function for constructing specific variable transformations and blow-ups to uniformly calculate the real log canonical threshold. It was found that linear independence in the differential structure of the log-likelihood ratio function significantly influences the real log canonical threshold. This approach has not been considered in previous research. In this approach, the paper presents cases and methods for calculating the exact values of learning coefficients in statistical models that satisfy a simple condition next to the regularity conditions (semi-regular models), offering examples of learning coefficients for two-parameter semi-regular models and mixture distribution models with a constant mixing ratio.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T12:54:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.02646v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.02646v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 WavLLM: Towards Robust and Adaptive Speech Large Language Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, Furu Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \url{aka.ms/wavllm}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T12:42:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.00656v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.00656v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Large Language Models Know What Makes Exemplary Contexts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Quanyu Long, Jianda Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-context learning (ICL) has proven to be a significant capability with the advancement of Large Language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without needing to update millions of parameters. This paper presents a unified framework for LLMs that allows them to self-select influential in-context examples to compose their contexts; self-rank candidates with different demonstration compositions; self-optimize the demonstration selection and ordering through reinforcement learning. Specifically, our method designs a parameter-efficient retrieval head that generates the optimized demonstration after training with rewards from LLM's own preference. Experimental results validate the proposed method's effectiveness in enhancing ICL performance. Additionally, our approach effectively identifies and selects the most representative examples for the current task, and includes more diversity in retrieval.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T12:32:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07505v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07505v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Bayesian Learning in a Nonlinear Multiscale State-Space Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nayely Vélez-Cruz, Manfred D. Laubichler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The ubiquity of multiscale interactions in complex systems is well-recognized, with development and heredity serving as a prime example of how processes at different temporal scales influence one another. This work introduces a novel multiscale state-space model to explore the dynamic interplay between systems interacting across different time scales, with feedback between each scale. We propose a Bayesian learning framework to estimate unknown states by learning the unknown process noise covariances within this multiscale model. We develop a Particle Gibbs with Ancestor Sampling (PGAS) algorithm for inference and demonstrate through simulations the efficacy of our approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T12:22:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span><span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06425v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06425v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 QirK: Question Answering via Intermediate Representation on Knowledge
  Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jan Luca Scheerer, Anton Lykov, Moe Kayali, Ilias Fountalis, Dan Olteanu, Nikolaos Vasiloglou, Dan Suciu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We demonstrate QirK, a system for answering natural language questions on Knowledge Graphs (KG). QirK can answer structurally complex questions that are still beyond the reach of emerging Large Language Models (LLMs). It does so using a unique combination of database technology, LLMs, and semantic search over vector embeddings. The glue for these components is an intermediate representation (IR). The input question is mapped to IR using LLMs, which is then repaired into a valid relational database query with the aid of a semantic search on vector embeddings. This allows a practical synthesis of LLM capabilities and KG reliability.   A short video demonstrating QirK is available at https://youtu.be/6c81BLmOZ0U.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T12:19:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07494v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07494v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Training Overhead Ratio: A Practical Reliability Metric for Large
  Language Model Training Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ning Lu, Qian Xie, Hao Zhang, Wenyi Fang, Yang Zheng, Jiantao Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are revolutionizing the AI industry with their superior capabilities. Training these models requires large-scale GPU clusters and significant computing time, leading to frequent failures that significantly increase training costs. Despite its significance, this field lacks a metric for evaluating reliability. In this work, we introduce a novel reliability metric called \emph{Training Overhead Ratio} (TOR) to evaluate the reliability of fault-tolerant LLM training systems. TOR is defined as the ratio of optimal training time to the observed training time of a system, serving as a practical tool for users to estimate the actual time required to train an LLM on a given system. Furthermore, our investigation identifies the key factor for enhancing reliability and present TOR equations for various types of failures encountered in practice.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:55:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07482v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07482v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 FIPO: Free-form Instruction-oriented Prompt Optimization with Preference
  Dataset and Modular Fine-tuning Schema</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junru Lu, Siyu An, Min Zhang, Yulan He, Di Yin, Xing Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> When the quality of naive prompts is carefully optimized by human experts, the task performance of large language models (LLMs) can be significantly improved. However, expert-based prompt optimizations are expensive. Herein, some works have proposed Automatic Prompt Optimization (APO), to optimize naive prompts according to task outputs of given in-box testing models, with the help of advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing schemes suffer from poor generalization ability and privacy risk. To this end, we collect the first large-scale Prompt Optimization Preference dataset (POP), fine-tune offline local LLM-based optimizers, then fairly test with various downstream models. Our method allows accurate optimization of the core task instruction part within the naive prompt in a model-agnostic manner, and thus is named Free-from Instruction-oriented Prompt Optimization (FIPO). In specific, FIPO uses a modular APO template that dynamically integrate the naive task instruction, optional instruction responses, and optional ground truth to produce finely optimized prompts. The POP dataset is meticulously constructed using advanced LLMs, undergoing rigorous cross-validation by human experts and analytical models. Leveraging insights from the data with Tulu2 models and diverse fine-tuning strategies, we validate the efficacy of FIPO framework across five public benchmarks and six testing models. Check codes and data here: https://github.com/LuJunru/FIPO_Project.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:47:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.11811v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.11811v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenjun Huang, Jianguo Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) have attracted much attention for their multifunctionality. However, traditional Transformer architectures incur significant overhead due to their secondary computational complexity. To address this issue, we introduce ML-Mamba, a multimodal language model, which utilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known for its linear scalability and fast processing of long sequences. We replace the Transformer-based backbone with a pre-trained Mamba-2 model and explore methods for integrating 2D visual selective scanning mechanisms into multimodal learning while also trying various visual encoders and Mamba-2 model variants. Our extensive experiments in various multimodal benchmark tests demonstrate the competitive performance of ML-Mamba and highlight the potential of state space models in multimodal tasks. The experimental results show that: (1) we empirically explore how to effectively apply the 2D vision selective scan mechanism for multimodal learning. We propose a novel multimodal connector called the Mamba-2 Scan Connector (MSC), which enhances representational capabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art methods such as TinyLaVA and MobileVLM v2 through its linear sequential modeling while faster inference speed; (3) Compared to multimodal models utilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference performance and effectiveness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:42:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19832v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19832v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Debiasing astro-Photometric Observations with Corrections Using
  Statistics (DePhOCUS)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tobias Hoffmann, Marco Micheli, Juan Luis Cano, Maxime Devogèle, Davide Farnocchia, Petr Pravec, Peter Vereš, Björn Poppe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Photometric measurements allow the determination of an asteroid's absolute magnitude, which often represents the sole means to infer its size. Photometric observations can be obtained in a variety of filters that can be unique to a specific observatory. Those observations are then calibrated into specific bands with respect to reference star catalogs. In order to combine all the different measurements for evaluation, photometric observations need to be converted to a common band, typically V-band. Current band-correction schemes in use by IAU's Minor Planet Center, JPL's Center for Near Earth Object Studies and ESA's NEO Coordination Centre use average correction values for the apparent magnitude derived from photometry of asteroids as the corrections are dependent on the typically unknown spectrum of the object to be corrected. By statistically analyzing the photometric residuals of asteroids, we develop a new photometric correction scheme that does not only consider the band, but also accounts for reference catalog and observatory. We describe a new statistical photometry correction scheme for asteroid observations with debiased corrections. Testing this scheme on a reference group of asteroids, we see a 36% reduction in the photometric residuals. Moreover, the new scheme leads to a more accurate and debiased determination of the H-G magnitude system and, in turn, to more reliable inferred sizes. We discuss the significant shift in the corrections with this "DePhOCUS" debiasing system, its limitations, and the impact for photometric and physical properties of all asteroids, especially Near-Earth Objects.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:34:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.EP</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07474v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07474v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Bridging and Modeling Correlations in Pairwise Data for Direct
  Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxin Jiang, Bo Huang, Yufei Wang, Xingshan Zeng, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align large language models (LLMs) with human-desired behaviors using pairwise preference data. However, the winning response and the losing response within pairwise data are generated isolatedly, leading to weak correlations between them as well as suboptimal alignment performance. To address this issue, we propose an effective framework named BMC, for bridging and modeling correlations in pairwise data. Firstly, we increase the consistency and informativeness of the pairwise preference signals by targeted modifications, synthesizing a pseudo winning response through improving the losing response based on the winning response. Secondly, we identify that DPO alone is insufficient to model these correlations and capture nuanced variations. Therefore, we propose learning token-level correlations by dynamically leveraging the policy model's confidence during training. Comprehensive experiments on QA, math, and instruction-following tasks demonstrate the effectiveness of our approach, significantly surpassing competitive baselines, including DPO. Additionally, our in-depth quantitative analysis reveals the reasons behind our method's superior performance over DPO and showcases its versatility to other DPO variants.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:29:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07471v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07471v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Large Language Models Prompting With Episodic Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dai Do, Quan Tran, Svetha Venkatesh, Hung Le
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prompt optimization is essential for enhancing the performance of Large Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks, particularly in scenarios of few-shot learning where training examples are incorporated directly into the prompt. Despite the growing interest in optimizing prompts with few-shot examples, existing methods for prompt optimization are often resource-intensive or perform inadequately. In this work, we propose PrOmpting with Episodic Memory (POEM), a novel prompt optimization technique that is simple, efficient, and demonstrates strong generalization capabilities. We approach prompt optimization as a Reinforcement Learning (RL) challenge, using episodic memory to archive combinations of input data, permutations of few-shot examples, and the rewards observed during training. In the testing phase, we optimize the sequence of examples for each test query by selecting the sequence that yields the highest total rewards from the top-k most similar training examples in the episodic memory. Our results show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over 5.3% in various text classification tasks. Furthermore, our approach adapts well to broader language understanding tasks, consistently outperforming conventional heuristic methods for ordering examples.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:19:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07465v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07465v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Limit Theorems for Weakly Dependent Non-stationary Random Field Arrays
  and Asymptotic Inference of Dynamic Spatio-temporal Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yue Pan, Jiazhu Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We obtain the law of large numbers (LLN) and the central limit theorem (CLT) for weakly dependent non-stationary arrays of random fields with asymptotically unbounded moments. The weak dependence condition for arrays of random fields is proved to be inherited through transformation and infinite shift. This paves a way to prove the consistency and asymptotic normality of maximum likelihood estimation for dynamic spatio-temporal models (i.e. so-called ultra high-dimensional time series models) when the sample size and/or dimension go to infinity. Especially the asymptotic properties of estimation for network autoregression are obtained under reasonable regularity conditions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T10:06:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07429v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07429v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Beyond Inter-Item Relations: Dynamic Adaptive Mixture-of-Experts for
  LLM-Based Sequential Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> CanYi Liu, Wei Li, Youchen, Zhang, Hui Li, Rongrong Ji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sequential recommender system (SRS) predicts the next items that users may prefer based on user historical interaction sequences. Inspired by the rise of large language models (LLMs) in various AI applications, there is a surge of work on LLM-based SRS. Despite their attractive performance, existing LLM-based SRS still exhibit some limitations, including neglecting intra-item relations, ignoring long-term collaborative knowledge and using inflexible architecture designs for adaption. To alleviate these issues, we propose an LLM-based SRS named MixRec. Built on top of coarse-grained adaption for capturing inter-item relations, MixRec is further enhanced with (1) context masking that models intra-item relations to help LLM better understand token and item semantics in the context of SRS, (2) collaborative knowledge injection that helps LLM incorporate long-term collaborative knowledge, and (3) a dynamic adaptive mixture-of-experts design that can flexibly choose expert architectures based on Bayesian optimization to better incorporate different sequential information. Extensive experiments demonstrate that MixRec can effectively handle sequential recommendation in a dynamic and adaptive manner.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T10:03:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07427v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07427v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Exploring Retrieval Augmented Generation in Arabic</h2>
                <div class="authors">
                    <strong>Authors:</strong> Samhaa R. El-Beltagy, Mohamed A. Abdallah
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful technique in natural language processing, combining the strengths of retrieval-based and generation-based models to enhance text generation tasks. However, the application of RAG in Arabic, a language with unique characteristics and resource constraints, remains underexplored. This paper presents a comprehensive case study on the implementation and evaluation of RAG for Arabic text. The work focuses on exploring various semantic embedding models in the retrieval stage and several LLMs in the generation stage, in order to investigate what works and what doesn't in the context of Arabic. The work also touches upon the issue of variations between document dialect and query dialect in the retrieval stage. Results show that existing semantic embedding models and LLMs can be effectively employed to build Arabic RAG pipelines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T10:03:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07425v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07425v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fan Yang, Sicheng Zhao, Yanhao Zhang, Haoxiang Chen, Hui Chen, Wenbo Tang, Haonan Lu, Pengfei Xu, Zhenyu Yang, Jungong Han, Guiguang Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in autonomous driving, augmented reality, robotics, and embodied intelligence have necessitated 3D perception algorithms. However, current 3D perception methods, particularly small models, struggle with processing logical reasoning, question-answering, and handling open scenario categories. On the other hand, generative multimodal large language models (MLLMs) excel in general capacity but underperform in 3D tasks, due to weak spatial and local object perception, poor text-based geometric numerical output, and inability to handle camera focal variations. To address these challenges, we propose the following solutions: Spatial-Enhanced Local Feature Mining for better spatial feature extraction, 3D Query Token-Derived Info Decoding for precise geometric regression, and Geometry Projection-Based 3D Reasoning for handling camera focal length variations. We employ parameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a powerful 3D perception MLLM. Additionally, we have constructed the IG3D dataset, which provides fine-grained descriptions and question-answer annotations. Extensive experiments demonstrate that our LLMI3D achieves state-of-the-art performance, significantly outperforming existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T10:00:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07422v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07422v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 BioRAG: A RAG-LLM Framework for Biological Question Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chengrui Wang, Qingqing Long, Meng Xiao, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, Yuanchun Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The question-answering system for Life science research, which is characterized by the rapid pace of discovery, evolving insights, and complex interactions among knowledge entities, presents unique challenges in maintaining a comprehensive knowledge warehouse and accurate information retrieval. To address these issues, we introduce BioRAG, a novel Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs) framework. Our approach starts with parsing, indexing, and segmenting an extensive collection of 22 million scientific papers as the basic knowledge, followed by training a specialized embedding model tailored to this domain. Additionally, we enhance the vector retrieval process by incorporating a domain-specific knowledge hierarchy, which aids in modeling the intricate interrelationships among each query and context. For queries requiring the most current information, BioRAG deconstructs the question and employs an iterative retrieval process incorporated with the search engine for step-by-step reasoning. Rigorous experiments have demonstrated that our model outperforms fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks across multiple life science question-answering tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:54:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.01107v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.01107v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge
  Editing for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Knowledge editing aims to update outdated or incorrect knowledge in large language models (LLMs). However, current knowledge editing methods have limited scalability for lifelong editing. This study explores the fundamental reason why knowledge editing fails in lifelong editing. We begin with the closed-form solution derived from linear associative memory, which underpins state-of-the-art knowledge editing methods. We extend the solution from single editing to lifelong editing, and through rigorous mathematical derivation, identify an interference term in the final solution, suggesting that editing knowledge may impact irrelevant knowledge. Further analysis of the interference term reveals a close relationship with superposition between knowledge representations. When knowledge superposition does not exist in language models, the interference term vanishes, allowing for lossless knowledge editing. Experiments across numerous language models reveal that knowledge superposition is universal, exhibiting high kurtosis, zero mean, and heavy-tailed distributions with clear scaling laws. Ultimately, by combining theory and experiments, we demonstrate that knowledge superposition is the fundamental reason for the failure of lifelong editing. Moreover, this is the first study to investigate knowledge editing from the perspective of superposition and provides a comprehensive observation of superposition across numerous real-world language models. Code available at https://github.com/ChenhuiHu/knowledge_in_superposition.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:43:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07413v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07413v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Modeling of Measurement Error in Financial Returns Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ajay Jasra, Mohamed Maama, Aleksandar Mijatović
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper we consider the modeling of measurement error for fund returns data. In particular, given access to a time-series of discretely observed log-returns and the associated maximum over the observation period, we develop a stochastic model which models the true log-returns and maximum via a L\'evy process and the data as a measurement error there-of. The main technical difficulty of trying to infer this model, for instance Bayesian parameter estimation, is that the joint transition density of the return and maximum is seldom known, nor can it be simulated exactly. Based upon the novel stick breaking representation of [12] we provide an approximation of the model. We develop a Markov chain Monte Carlo (MCMC) algorithm to sample from the Bayesian posterior of the approximated posterior and then extend this to a multilevel MCMC method which can reduce the computational cost to approximate posterior expectations, relative to ordinary MCMC. We implement our methodology on several applications including for real data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:24:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.CO</span><span>q-fin.CP</span><span>q-fin.ST</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07405v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07405v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Sum-Product-Set Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Milan Papež, Martin Rektoris, Tomáš Pevný, Václav Šmídl
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Daily internet communication relies heavily on tree-structured graphs, embodied by popular data formats such as XML and JSON. However, many recent generative (probabilistic) models utilize neural networks to learn a probability distribution over undirected cyclic graphs. This assumption of a generic graph structure brings various computational challenges, and, more importantly, the presence of non-linearities in neural networks does not permit tractable probabilistic inference. We address these problems by proposing sum-product-set networks, an extension of probabilistic circuits from unstructured tensor data to tree-structured graph data. To this end, we use random finite sets to reflect a variable number of nodes and edges in the graph and to allow for exact and efficient inference. We demonstrate that our tractable model performs comparably to various intractable models based on neural networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:13:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07394v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07394v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jan Hendrik Metzen, Piyapat Saranrittichai, Chaithanya Kumar Mummadi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. Up until now, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, i.e., classify to the class that maximizes cosine similarity between its averaged encoded class descriptors and the image encoding. However, weighing all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP tunes per-image weights to each prompt template at inference time, based on statistics of class descriptor-image similarities. AutoCLIP is fully unsupervised, has only a minor additional computation overhead, and can be easily implemented in few lines of code. We show that AutoCLIP outperforms baselines across a broad range of vision-language models, datasets, and prompt templates consistently and by up to 3 percent point accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:06:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2309.16414v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2309.16414v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 ProCom: A Few-shot Targeted Community Detection Algorithm</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xixi Wu, Kaiyu Xiong, Yun Xiong, Xiaoxin He, Yao Zhang, Yizhu Jiao, Jiawei Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Targeted community detection aims to distinguish a particular type of community in the network. This is an important task with a lot of real-world applications, e.g., identifying fraud groups in transaction networks. Traditional community detection methods fail to capture the specific features of the targeted community and detect all types of communities indiscriminately. Semi-supervised community detection algorithms, emerged as a feasible alternative, are inherently constrained by their limited adaptability and substantial reliance on a large amount of labeled data, which demands extensive domain knowledge and manual effort.   In this paper, we address the aforementioned weaknesses in targeted community detection by focusing on few-shot scenarios. We propose ProCom, a novel framework that extends the ``pre-train, prompt'' paradigm, offering a low-resource, high-efficiency, and transferable solution. Within the framework, we devise a dual-level context-aware pre-training method that fosters a deep understanding of latent communities in the network, establishing a rich knowledge foundation for downstream task. In the prompt learning stage, we reformulate the targeted community detection task into pre-training objectives, allowing the extraction of specific knowledge relevant to the targeted community to facilitate effective and efficient inference. By leveraging both the general community knowledge acquired during pre-training and the specific insights gained from the prompt communities, ProCom exhibits remarkable adaptability across different datasets. We conduct extensive experiments on five benchmarks to evaluate the ProCom framework, demonstrating its SOTA performance under few-shot scenarios, strong efficiency, and transferability across diverse datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T08:34:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07369v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07369v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Fast Bayesian inference in a class of sparse linear mixed effects models</h2>
                <div class="authors">
                    <strong>Authors:</strong> M-Z. Spyropoulou, J. Hopker, J. E. Griffin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Linear mixed effects models are widely used in statistical modelling. We consider a mixed effects model with Bayesian variable selection in the random effects using spike-and-slab priors and developed a variational Bayes inference scheme that can be applied to large data sets. An EM algorithm is proposed for the model with normal errors where the posterior distribution of the variable inclusion parameters is approximated using an Occam's window approach. Placing this approach within a variational Bayes scheme also the algorithm to be extended to the model with skew-t errors. The performance of the algorithm is evaluated in a simulation study and applied to a longitudinal model for elite athlete performance in the 100 metre sprint and weightlifting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T08:23:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07365v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07365v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Model Attribution in LLM-Generated Disinformation: A Domain
  Generalization Approach with Supervised Contrastive Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu, Huan Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Model attribution for LLM-generated disinformation poses a significant challenge in understanding its origins and mitigating its spread. This task is especially challenging because modern large language models (LLMs) produce disinformation with human-like quality. Additionally, the diversity in prompting methods used to generate disinformation complicates accurate source attribution. These methods introduce domain-specific features that can mask the fundamental characteristics of the models. In this paper, we introduce the concept of model attribution as a domain generalization problem, where each prompting method represents a unique domain. We argue that an effective attribution model must be invariant to these domain-specific features. It should also be proficient in identifying the originating models across all scenarios, reflecting real-world detection challenges. To address this, we introduce a novel approach based on Supervised Contrastive Learning. This method is designed to enhance the model's robustness to variations in prompts and focuses on distinguishing between different source LLMs. We evaluate our model through rigorous experiments involving three common prompting methods: ``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs: ``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the effectiveness of our approach in model attribution tasks, achieving state-of-the-art performance across diverse and unseen datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T08:10:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.21264v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.21264v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 The Elephant in the Room: Rethinking the Usage of Pre-trained Language
  Model in Sequential Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zekai Qu, Ruobing Xie, Chaojun Xiao, Xingwu Sun, Zhanhui Kang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sequential recommendation (SR) has seen significant advancements with the help of Pre-trained Language Models (PLMs). Some PLM-based SR models directly use PLM to encode user historical behavior's text sequences to learn user representations, while there is seldom an in-depth exploration of the capability and suitability of PLM in behavior sequence modeling. In this work, we first conduct extensive model analyses between PLMs and PLM-based SR models, discovering great underutilization and parameter redundancy of PLMs in behavior sequence modeling. Inspired by this, we explore different lightweight usages of PLMs in SR, aiming to maximally stimulate the ability of PLMs for SR while satisfying the efficiency and usability demands of practical systems. We discover that adopting behavior-tuned PLMs for item initializations of conventional ID-based SR models is the most economical framework of PLM-based SR, which would not bring in any additional inference cost but could achieve a dramatic performance boost compared with the original version. Extensive experiments on five datasets show that our simple and universal framework leads to significant improvement compared to classical SR and SOTA PLM-based SR models without additional inference costs. Our code can be found in https://github.com/777pomingzi/Rethinking-PLM-in-RS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T08:06:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.08796v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.08796v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Only One Relation Possible? Modeling the Ambiguity in Event Temporal
  Relation Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yutong Hu, Quzhe Huang, Yansong Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Event Temporal Relation Extraction (ETRE) aims to identify the temporal relationship between two events, which plays an important role in natural language understanding. Most previous works follow a single-label classification style, classifying an event pair into either a specific temporal relation (e.g., \textit{Before}, \textit{After}), or a special label \textit{Vague} when there may be multiple possible temporal relations between the pair. In our work, instead of directly making predictions on \textit{Vague}, we propose a multi-label classification solution for ETRE (METRE) to infer the possibility of each temporal relation independently, where we treat \textit{Vague} as the cases when there is more than one possible relation between two events. We design a speculation mechanism to explore the possible relations hidden behind \textit{Vague}, which enables the latent information to be used efficiently. Experiments on TB-Dense, MATRES and UDS-T show that our method can effectively utilize the \textit{Vague} instances to improve the recognition for specific temporal relations and outperforms most state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:57:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07353v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07353v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Semi-Supervised Quantile Estimation: Robust and Efficient Inference in
  High Dimensional Settings</h2>
                <div class="authors">
                    <strong>Authors:</strong> Abhishek Chakrabortty, Guorong Dai, Raymond J. Carroll
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider quantile estimation in a semi-supervised setting, characterized by two available data sets: (i) a small or moderate sized labeled data set containing observations for a response and a set of possibly high dimensional covariates, and (ii) a much larger unlabeled data set where only the covariates are observed. We propose a family of semi-supervised estimators for the response quantile(s) based on the two data sets, to improve the estimation accuracy compared to the supervised estimator, i.e., the sample quantile from the labeled data. These estimators use a flexible imputation strategy applied to the estimating equation along with a debiasing step that allows for full robustness against misspecification of the imputation model. Further, a one-step update strategy is adopted to enable easy implementation of our method and handle the complexity from the non-linear nature of the quantile estimating equation. Under mild assumptions, our estimators are fully robust to the choice of the nuisance imputation model, in the sense of always maintaining root-n consistency and asymptotic normality, while having improved efficiency relative to the supervised estimator. They also attain semi-parametric optimality if the relation between the response and the covariates is correctly specified via the imputation model. As an illustration of estimating the nuisance imputation function, we consider kernel smoothing type estimators on lower dimensional and possibly estimated transformations of the high dimensional covariates, and we establish novel results on their uniform convergence rates in high dimensions, involving responses indexed by a function class and usage of dimension reduction techniques. These results may be of independent interest. Numerical results on both simulated and real data confirm our semi-supervised approach's improved performance, in terms of both estimation and inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:45:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>math.ST</span><span>stat.ML</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2201.10208v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2201.10208v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Chain-of-Factors Paper-Reviewer Matching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu Zhang, Yanzhen Shen, SeongKu Kang, Xiusi Chen, Bowen Jin, Jiawei Han
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapid increase in paper submissions to academic conferences, the need for automated and accurate paper-reviewer matching is more critical than ever. Previous efforts in this area have considered various factors to assess the relevance of a reviewer's expertise to a paper, such as the semantic similarity, shared topics, and citation connections between the paper and the reviewer's previous works. However, most of these studies focus on only one factor, resulting in an incomplete evaluation of the paper-reviewer relevance. To address this issue, we propose a unified model for paper-reviewer matching that jointly considers semantic, topic, and citation factors. To be specific, during training, we instruction-tune a contextualized language model shared across all factors to capture their commonalities and characteristics; during inference, we chain the three factors to enable step-by-step, coarse-to-fine search for qualified reviewers given a submission. Experiments on four datasets (one of which is newly contributed by us) spanning various fields such as machine learning, computer vision, information retrieval, and data mining consistently demonstrate the effectiveness of our proposed Chain-of-Factors model in comparison with state-of-the-art paper-reviewer matching methods and scientific pre-trained language models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:42:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.CL</span><span>cs.DL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.14483v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.14483v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Gradient Alignment Improves Test-Time Adaptation for Medical Image
  Segmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyang Chen, Yiwen Ye, Yongsheng Pan, Yong Xia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although recent years have witnessed significant advancements in medical image segmentation, the pervasive issue of domain shift among medical images from diverse centres hinders the effective deployment of pre-trained models. Many Test-time Adaptation (TTA) methods have been proposed to address this issue by fine-tuning pre-trained models with test data during inference. These methods, however, often suffer from less-satisfactory optimization due to suboptimal optimization direction (dictated by the gradient) and fixed step-size (predicated on the learning rate). In this paper, we propose the Gradient alignment-based Test-time adaptation (GraTa) method to improve both the gradient direction and learning rate in the optimization procedure. Unlike conventional TTA methods, which primarily optimize the pseudo gradient derived from a self-supervised objective, our method incorporates an auxiliary gradient with the pseudo one to facilitate gradient alignment. Such gradient alignment enables the model to excavate the similarities between different gradients and correct the gradient direction to approximate the empirical gradient related to the current segmentation task. Additionally, we design a dynamic learning rate based on the cosine similarity between the pseudo and auxiliary gradients, thereby empowering the adaptive fine-tuning of pre-trained models on diverse test data. Extensive experiments establish the effectiveness of the proposed gradient alignment and dynamic learning rate and substantiate the superiority of our GraTa method over other state-of-the-art TTA methods on a benchmark medical image segmentation task. The code and weights of pre-trained source models will be available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:37:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07343v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07343v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 SentenceVAE: Enable Next-sentence Prediction for Large Language Models
  with Faster Speed, Higher Accuracy and Longer Context</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongjun An, Yifan Chen, Zhe Sun, Xuelong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current large language models (LLMs) primarily utilize next-token prediction method for inference, which significantly impedes their processing speed. In this paper, we introduce a novel inference methodology termed next-sentence prediction, aiming at enhancing the inference efficiency of LLMs. We present Sentence Variational Autoencoder (SentenceVAE), which includes a Sentence Encoder to compress multiple tokens in a sentence into a single token, and a Sentence Decoder to reconstruct it. By integrating SentenceVAE into the input and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference method. In addition, the SentenceVAE module of SLLMs can maintain the integrity of the original semantic content by segmenting the context into sentences, thereby improving accuracy while boosting inference speed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over equivalent context length, significantly reducing memory demands for self-attention computation and facilitating the handling of longer context. Extensive experiments on Wanjuan dataset have revealed that the proposed method can accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75% of its original metric, and decrease memory overhead by 86~91% for the equivalent context length, compared to previous token-by-token methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:34:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.00655v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.00655v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 AI Coders Are Among Us: Rethinking Programming Language Grammar Towards
  Efficient Code Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhensu Sun, Xiaoning Du, Zhou Yang, Li Li, David Lo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artificial Intelligence (AI) models have emerged as another important audience for programming languages alongside humans and machines, as we enter the era of large language models (LLMs). LLMs can now perform well in coding competitions and even write programs like developers to solve various tasks, including mathematical problems. However, the grammar and layout of current programs are designed to cater the needs of human developers -- with many grammar tokens and formatting tokens being used to make the code easier for humans to read. While this is helpful, such a design adds unnecessary computational work for LLMs, as each token they either use or produce consumes computational resources. To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar. This aims to represent code in a way that better suits the working mechanism of AI models. Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively. To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named SimPy. SimPy is crafted by revising the original Python grammar through a series of heuristic rules. Programs written in SimPy maintain identical AST structures to those in standard Python. This allows for not only execution via a modified AST parser, but also seamless transformation between programs written in Python and SimPy, enabling human developers and LLMs to use Python and SimPy, respectively, when they need to collaborate. In the experiments, compared with Python, SimPy enables a reduction in token usage by 13.5% and 10.4% for CodeLlama and GPT-4, respectively, when completing the same set of code-related tasks. Additionally, these models can maintain or even improve their performance when using SimPy instead of Python for these tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:34:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.16333v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.16333v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 PsyDI: Towards a Personalized and Progressively In-depth Chatbot for
  Psychological Measurements</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xueyan Li, Xinyan Chen, Yazhe Niu, Shuai Hu, Yu Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the field of psychology, traditional assessment methods, such as standardized scales, are frequently critiqued for their static nature, lack of personalization, and reduced participant engagement, while comprehensive counseling evaluations are often inaccessible. The complexity of quantifying psychological traits further limits these methods. Despite advances with large language models (LLMs), many still depend on single-round Question-and-Answer interactions. To bridge this gap, we introduce PsyDI, a personalized and progressively in-depth chatbot designed for psychological measurements, exemplified by its application in the Myers-Briggs Type Indicator (MBTI) framework. PsyDI leverages user-related multi-modal information and engages in customized, multi-turn interactions to provide personalized, easily accessible measurements, while ensuring precise MBTI type determination. To address the challenge of unquantifiable psychological traits, we introduce a novel training paradigm that involves learning the ranking of proxy variables associated with these traits, culminating in a robust score model for MBTI measurements. The score model enables PsyDI to conduct comprehensive and precise measurements through multi-turn interactions within a unified estimation context. Through various experiments, we validate the efficacy of both the score model and the PsyDI pipeline, demonstrating its potential to serve as a general framework for psychological measurements. Furthermore, the online deployment of PsyDI has garnered substantial user engagement, with over 3,000 visits, resulting in the collection of numerous multi-turn dialogues annotated with MBTI types, which facilitates further research. The source code for the training and web service components is publicly available as a part of OpenDILab at: https://github.com/opendilab/PsyDI
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T06:58:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.CY</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.03337v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.03337v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Read and Think: An Efficient Step-wise Multimodal Language Model for
  Document Understanding and Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jinxu Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the contents of multimodal documents is essential to accurately extract relevant evidence and use it for reasoning. Existing document understanding models tend to generate answers with a single word or phrase directly, ignoring the source document's evidence and lacking interpretability. In this work, we address the lack of step-wise capabilities through data augmentation and extension. Specifically, We use Multi-modal Large Language Models (MLLMs), which have strong visual understanding and reasoning abilities, as data generators to generate step-wise question-and-answer pairs for document images and use a high-performance LLM as the error detector to filter out noisy data. This step-wise data generation pipeline is implemented using both template-based and few-shot methods. We then use the generated high-quality data to train a humanized document understanding and reasoning model, specifically designed to solve complex questions that require reasoning or multi-hop question answering, dubbed DocAssistant. Experimental results demonstrate the effectiveness and application value of step-wise generation, showing a 5 improvement on InfoVQA with complex layouts and a 7 improvement on ChartQA with complex reasoning, compared to directly generated answers. We hope our work highlights the potential of synthetic data and encourages further exploration of multi-modal document reasoning capabilities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:26:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.00816v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.00816v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 A General Framework for Treatment Effect Estimation in Semi-Supervised
  and High Dimensional Settings</h2>
                <div class="authors">
                    <strong>Authors:</strong> Abhishek Chakrabortty, Guorong Dai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this article, we aim to provide a general and complete understanding of semi-supervised (SS) causal inference for treatment effects. Specifically, we consider two such estimands: (a) the average treatment effect and (b) the quantile treatment effect, as prototype cases, in an SS setting, characterized by two available data sets: (i) a labeled data set of size $n$, providing observations for a response and a set of high dimensional covariates, as well as a binary treatment indicator; and (ii) an unlabeled data set of size $N$, much larger than $n$, but without the response observed. Using these two data sets, we develop a family of SS estimators which are ensured to be: (1) more robust and (2) more efficient than their supervised counterparts based on the labeled data set only. Beyond the 'standard' double robustness results (in terms of consistency) that can be achieved by supervised methods as well, we further establish root-n consistency and asymptotic normality of our SS estimators whenever the propensity score in the model is correctly specified, without requiring specific forms of the nuisance functions involved. Such an improvement of robustness arises from the use of the massive unlabeled data, so it is generally not attainable in a purely supervised setting. In addition, our estimators are shown to be semi-parametrically efficient as long as all the nuisance functions are correctly specified. Moreover, as an illustration of the nuisance estimators, we consider inverse-probability-weighting type kernel smoothing estimators involving unknown covariate transformation mechanisms, and establish in high dimensional scenarios novel results on their uniform convergence rates, which should be of independent interest. Numerical results on both simulated and real data validate the advantage of our methods over their supervised counterparts with respect to both robustness and efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:24:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>math.ST</span><span>stat.ML</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2201.00468v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2201.00468v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 LPU: A Latency-Optimized and Highly Scalable Processor for Large
  Language Model Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seungjae Moon, Jung-Hoon Kim, Junsoo Kim, Seongmin Hong, Junseo Cha, Minsu Kim, Sukbin Lim, Gyubin Choi, Dongjin Seo, Jongho Kim, Hunjong Lee, Hyunjun Park, Ryeowook Ko, Soongyu Choi, Jongse Park, Jinwon Lee, Joo-Young Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The explosive arrival of OpenAI's ChatGPT has fueled the globalization of large language model (LLM), which consists of billions of pretrained parameters that embodies the aspects of syntax and semantics. HyperAccel introduces latency processing unit (LPU), a latency-optimized and highly scalable processor architecture for the acceleration of LLM inference. LPU perfectly balances the memory bandwidth and compute logic with streamlined dataflow to maximize performance and efficiency. LPU is equipped with expandable synchronization link (ESL) that hides data synchronization latency between multiple LPUs. HyperDex complements LPU as an intuitive software framework to run LLM applications. LPU achieves 1.25 ms/token and 20.9 ms/token for 1.3B and 66B model, respectively, which is 2.09x and 1.37x faster than the GPU. LPU, synthesized using Samsung 4nm process, has total area of 0.824 mm2 and power consumption of 284.31 mW. LPU-based servers achieve 1.33x and 1.32x energy efficiency over NVIDIA H100 and L4 servers, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:56:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07326v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07326v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 LLM-Enhanced Static Analysis for Precise Identification of Vulnerable
  OSS Versions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiran Cheng, Lwin Khin Shar, Ting Zhang, Shouguo Yang, Chaopeng Dong, David Lo, Shichao Lv, Zhiqiang Shi, Limin Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open-source software (OSS) has experienced a surge in popularity, attributed to its collaborative development model and cost-effective nature. However, the adoption of specific software versions in development projects may introduce security risks when these versions bring along vulnerabilities. Current methods of identifying vulnerable versions typically analyze and trace the code involved in vulnerability patches using static analysis with pre-defined rules. They then use syntactic-level code clone detection to identify the vulnerable versions. These methods are hindered by imprecisions due to (1) the inclusion of vulnerability-irrelevant code in the analysis and (2) the inadequacy of syntactic-level code clone detection. This paper presents Vercation, an approach designed to identify vulnerable versions of OSS written in C/C++. Vercation combines program slicing with a Large Language Model (LLM) to identify vulnerability-relevant code from vulnerability patches. It then backtraces historical commits to gather previous modifications of identified vulnerability-relevant code. We propose semantic-level code clone detection to compare the differences between pre-modification and post-modification code, thereby locating the vulnerability-introducing commit (vic) and enabling to identify the vulnerable versions between the patch commit and the vic. We curate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate Vercation. On this dataset, our approach achieves the F1 score of 92.4%, outperforming current state-of-the-art methods. More importantly, Vercation detected 134 incorrect vulnerable OSS versions in NVD reports.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:43:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07321v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07321v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xianming Li, Julius Lipp, Aamir Shakir, Rui Huang, Jing Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> BM25, a widely-used lexical search algorithm, remains crucial in information retrieval despite the rise of pre-trained and large language models (PLMs/LLMs). However, it neglects query-document similarity and lacks semantic understanding, limiting its performance. We revisit BM25 and introduce BMX, a novel extension of BM25 incorporating entropy-weighted similarity and semantic enhancement techniques. Extensive experiments demonstrate that BMX consistently outperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in long-context and real-world retrieval benchmarks. This study bridges the gap between classical lexical search and modern semantic approaches, offering a promising direction for future information retrieval research. The reference implementation of BMX can be found in Baguetter, which was created in the context of this work. The code can be found here: https://github.com/mixedbread-ai/baguetter.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:18:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06643v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06643v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal
  Data for Mental Health</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongquan Hu, Shuning Zhang, Ting Dang, Hong Jia, Flora D. Salim, Wen Hu, Aaron J. Quigley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. Recent advancements with Large Language Models (LLMs) position them as prospective ``health agents'' for mental health assessment. However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data. Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting. Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text). The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment. Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential. Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:14:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3675094.3678494' target='_blank'>doi</a><a href='http://arxiv.org/abs/2408.07313v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07313v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 MultiSurf-GPT: Facilitating Context-Aware Reasoning with Large-Scale
  Language Models for Multimodal Surface Sensing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongquan Hu, Black Sun, Pengcheng An, Zhuying Li, Wen Hu, Aaron J. Quigley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Surface sensing is widely employed in health diagnostics, manufacturing and safety monitoring. Advances in mobile sensing affords this potential for context awareness in mobile computing, typically with a single sensing modality. Emerging multimodal large-scale language models offer new opportunities. We propose MultiSurf-GPT, which utilizes the advanced capabilities of GPT-4o to process and interpret diverse modalities (radar, microscope and multispectral data) uniformly based on prompting strategies (zero-shot and few-shot prompting). We preliminarily validated our framework by using MultiSurf-GPT to identify low-level information, and to infer high-level context-aware analytics, demonstrating the capability of augmenting context-aware insights. This framework shows promise as a tool to expedite the development of more complex context-aware applications in the future, providing a faster, more cost-effective, and integrated solution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:07:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3640471.3680450' target='_blank'>doi</a><a href='http://arxiv.org/abs/2408.07311v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07311v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Enhancing Visual Question Answering through Ranking-Based Hybrid
  Training and Multimodal Fusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Peiyuan Chen, Zecheng Zhang, Yiping Dong, Li Zhou, Han Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual Question Answering (VQA) is a challenging task that requires systems to provide accurate answers to questions based on image content. Current VQA models struggle with complex questions due to limitations in capturing and integrating multimodal information effectively. To address these challenges, we propose the Rank VQA model, which leverages a ranking-inspired hybrid training strategy to enhance VQA performance. The Rank VQA model integrates high-quality visual features extracted using the Faster R-CNN model and rich semantic text features obtained from a pre-trained BERT model. These features are fused through a sophisticated multimodal fusion technique employing multi-head self-attention mechanisms. Additionally, a ranking learning module is incorporated to optimize the relative ranking of answers, thus improving answer accuracy. The hybrid training strategy combines classification and ranking losses, enhancing the model's generalization ability and robustness across diverse datasets. Experimental results demonstrate the effectiveness of the Rank VQA model. Our model significantly outperforms existing state-of-the-art models on standard VQA datasets, including VQA v2.0 and COCO-QA, in terms of both accuracy and Mean Reciprocal Rank (MRR). The superior performance of Rank VQA is evident in its ability to handle complex questions that require understanding nuanced details and making sophisticated inferences from the image and text. This work highlights the effectiveness of a ranking-based hybrid training strategy in improving VQA performance and lays the groundwork for further research in multimodal learning methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T05:18:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07303v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07303v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Contextual Distillation Model for Diversified Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fan Li, Xu Si, Shisong Tang, Dingmin Wang, Kunyan Han, Bing Han, Guorui Zhou, Yang Song, Hechang Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The diversity of recommendation is equally crucial as accuracy in improving user experience. Existing studies, e.g., Determinantal Point Process (DPP) and Maximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively select items that optimize both accuracy and diversity. However, prior methods typically exhibit quadratic complexity, limiting their applications to the re-ranking stage and are not applicable to other recommendation stages with a larger pool of candidate items, such as the pre-ranking and ranking stages. In this paper, we propose Contextual Distillation Model (CDM), an efficient recommendation model that addresses diversification, suitable for the deployment in all stages of industrial recommendation pipelines. Specifically, CDM utilizes the candidate items in the same user request as context to enhance the diversification of the results. We propose a contrastive context encoder that employs attention mechanisms to model both positive and negative contexts. For the training of CDM, we compare each target item with its context embedding and utilize the knowledge distillation framework to learn the win probability of each target item under the MMR algorithm, where the teacher is derived from MMR outputs. During inference, ranking is performed through a linear combination of the recommendation and student model scores, ensuring both diversity and efficiency. We perform offline evaluations on two industrial datasets and conduct online A/B test of CDM on the short-video platform KuaiShou. The considerable enhancements observed in both recommendation quality and diversity, as shown by metrics, provide strong superiority for the effectiveness of CDM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T04:52:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3637528.3671514' target='_blank'>doi</a><a href='http://arxiv.org/abs/2406.09021v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.09021v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Evaluating Large Language Model based Personal Information Extraction
  and Countermeasures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yupei Liu, Yuqi Jia, Jinyuan Jia, Neil Zhenqiang Gong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automatically extracting personal information--such as name, phone number, and email address--from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods--such as regular expression, keyword search, and entity detection--achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction attacks; collect three datasets including a synthetic dataset generated by GPT-4 and two real-world datasets with manually labeled 8 categories of personal information; introduce a novel mitigation strategy based on \emph{prompt injection}; and systematically benchmark LLM-based attacks and countermeasures using 10 LLMs and our 3 datasets. Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms conventional methods at such extraction; and prompt injection can mitigate such risk to a large extent and outperforms conventional countermeasures. Our code and data are available at: \url{https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T04:49:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07291v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07291v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 DRAMA: An Efficient End-to-end Motion Planner for Autonomous Driving
  with Mamba</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chengran Yuan, Zhanqi Zhang, Jiawei Sun, Shuo Sun, Zefan Huang, Christina Dao Wen Lee, Dongen Li, Yuhang Han, Anthony Wong, Keng Peng Tee, Marcelo H. Ang Jr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Motion planning is a challenging task to generate safe and feasible trajectories in highly dynamic and complex environments, forming a core capability for autonomous vehicles. In this paper, we propose DRAMA, the first Mamba-based end-to-end motion planner for autonomous vehicles. DRAMA fuses camera, LiDAR Bird's Eye View images in the feature space, as well as ego status information, to generate a series of future ego trajectories. Unlike traditional transformer-based methods with quadratic attention complexity for sequence length, DRAMA is able to achieve a less computationally intensive attention complexity, demonstrating potential to deal with increasingly complex scenarios. Leveraging our Mamba fusion module, DRAMA efficiently and effectively fuses the features of the camera and LiDAR modalities. In addition, we introduce a Mamba-Transformer decoder that enhances the overall planning performance. This module is universally adaptable to any Transformer-based model, especially for tasks with long sequence inputs. We further introduce a novel feature state dropout which improves the planner's robustness without increasing training and inference times. Extensive experimental results show that DRAMA achieves higher accuracy on the NAVSIM dataset compared to the baseline Transfuser, with fewer parameters and lower computational costs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T04:31:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.03601v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.03601v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Graph Agent Network: Empowering Nodes with Inference Capabilities for
  Adversarial Resilience</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ao Liu, Wenshan Li, Tao Li, Beibei Li, Guangquan Xu, Pan Zhou, Wengang Ma, Hanyuan Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> End-to-end training with global optimization have popularized graph neural networks (GNNs) for node classification, yet inadvertently introduced vulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit the inherent opened interfaces of GNNs' input and output, perturbing critical edges and thus manipulating the classification results. Current defenses, due to their persistent utilization of global-optimization-based end-to-end training schemes, inherently encapsulate the vulnerabilities of GNNs. This is specifically evidenced in their inability to defend against targeted secondary attacks. In this paper, we propose the Graph Agent Network (GAgN) to address the aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent network in which each node is designed as an 1-hop-view agent. Through the decentralized interactions between agents, they can learn to infer global perceptions to perform tasks including inferring embeddings, degrees and neighbor relationships for given nodes. This empowers nodes to filtering adversarial edges while carrying out classification tasks. Furthermore, agents' limited view prevents malicious messages from propagating globally in GAgN, thereby resisting global-optimization-based secondary attacks. We prove that single-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient to achieve these functionalities. Experimental results show that GAgN effectively implements all its intended capabilities and, compared to state-of-the-art defenses, achieves optimal classification accuracy on the perturbed datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T04:26:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CR</span><span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2306.06909v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2306.06909v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 A Semantic Space is Worth 256 Language Descriptions: Make Stronger
  Segmentation Models with Descriptive Properties</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junfei Xiao, Ziqi Zhou, Wenxuan Li, Shiyi Lan, Jieru Mei, Zhiding Yu, Alan Yuille, Yuyin Zhou, Cihang Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces ProLab, a novel approach using property-level label space for creating strong interpretable segmentation models. Instead of relying solely on category-specific annotations, ProLab uses descriptive properties grounded in common sense knowledge for supervising segmentation models. It is based on two core designs. First, we employ Large Language Models (LLMs) and carefully crafted prompts to generate descriptions of all involved categories that carry meaningful common sense knowledge and follow a structured format. Second, we introduce a description embedding model preserving semantic correlation across descriptions and then cluster them into a set of descriptive properties (e.g., 256) using K-Means. These properties are based on interpretable common sense knowledge consistent with theories of human recognition. We empirically show that our approach makes segmentation models perform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal Context, Cityscapes, and BDD). Our method also shows better scalability with extended training steps than category-level supervision. Our interpretable segmentation framework also emerges with the generalization ability to segment out-of-domain or unknown categories using only in-domain descriptive properties. Code is available at https://github.com/lambert-x/ProLab.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T10:03:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.13764v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.13764v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 HAIFIT: Human-to-AI Fashion Image Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianan Jiang, Xinglin Li, Weiren Yu, Di Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the realm of fashion design, sketches serve as the canvas for expressing an artist's distinctive drawing style and creative vision, capturing intricate details like stroke variations and texture nuances. The advent of sketch-to-image cross-modal translation technology has notably aided designers. However, existing methods often compromise these sketch details during image generation, resulting in images that deviate from the designer's intended concept. This limitation hampers the ability to offer designers a precise preview of the final output. To overcome this challenge, we introduce HAIFIT, a novel approach that transforms sketches into high-fidelity, lifelike clothing images by integrating multi-scale features and capturing extensive feature map dependencies from diverse perspectives. Through extensive qualitative and quantitative evaluations conducted on our self-collected dataset, our method demonstrates superior performance compared to existing methods in generating photorealistic clothing images. Our method excels in preserving the distinctive style and intricate details essential for fashion design applications. In addition, our method also has obvious advantages in model training and inference speed, contributing to reducing designers' time costs and improving design efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T03:56:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.08651v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.08651v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ho Fung Tsoi, Vladimir Loncar, Sridhara Dasu, Philip Harris
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Contrary to genetic programming, the neural network approach to symbolic regression can efficiently handle high-dimensional inputs and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity often involve multistage pruning with fine-tuning, which can result in significant performance loss. In this work, we propose $\tt{SymbolNet}$, a neural network approach to symbolic regression in a novel framework that allows dynamic pruning of model weights, input features, and mathematical operators in a single training process, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term for each pruning type, which can adaptively adjust its strength, leading to convergence at a target sparsity ratio. Unlike most existing symbolic regression methods that struggle with datasets containing more than $\mathcal{O}(10)$ inputs, we demonstrate the effectiveness of our model on the LHC jet tagging task (16 inputs), MNIST (784 inputs), and SVHN (3072 inputs). Our approach enables symbolic regression to achieve fast inference with nanosecond-scale latency on FPGAs for high-dimensional datasets in environments with stringent computational resource constraints, such as the high-energy physics experiments at the LHC.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T03:45:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>hep-ex</span><span>physics.ins-det</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.09949v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.09949v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 NL2OR: Solve Complex Operations Research Problems Using Natural Language
  Inputs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junxuan Li, Ryan Wickman, Sahil Bhatnagar, Raj Kumar Maity, Arko Mukherjee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Operations research (OR) uses mathematical models to enhance decision-making, but developing these models requires expert knowledge and can be time-consuming. Automated mathematical programming (AMP) has emerged to simplify this process, but existing systems have limitations. This paper introduces a novel methodology that uses recent advances in Large Language Model (LLM) to create and edit OR solutions from non-expert user queries expressed using Natural Language. This reduces the need for domain expertise and the time to formulate a problem. The paper presents an end-to-end pipeline, named NL2OR, that generates solutions to OR problems from natural language input, and shares experimental results on several important OR problems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T03:42:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07272v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07272v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Eavesdropping Mobile Apps and Actions through Wireless Traffic in the
  Open World</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoguang Yang, Yong Huang, Junli Guo, Dalong Zhang, Qingxian Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While smartphones and WiFi networks are bringing many positive changes to people's lives, they are susceptible to traffic analysis attacks, which infer user's private information from encrypted traffic. Existing traffic analysis attacks mainly target TCP/IP layers or are limited to the closed-world assumption, where all possible apps and actions have been involved in the model training. To overcome these limitations, we propose MACPrint, a novel system that infers mobile apps and in-app actions based on WiFi MAC layer traffic in the open-world setting. MACPrint first extracts rich statistical and contextual features of encrypted wireless traffic. Then, we develop Label Recorder, an automatic traffic labeling app, to improve labeling accuracy in the training phase. Finally, TCN models with OpenMax functions are used to recognize mobile apps and actions in the open world accurately. To evaluate our system, we collect MAC layer traffic data over 125 hours from more than 40 apps. The experimental results show that MAC-Print can achieve an accuracy of over 96% for recognizing apps and actions in the closed-world setting, and obtains an accuracy of over 86% in the open-world setting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T03:03:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07263v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07263v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 V-STaR: Training Verifiers for Self-Taught Reasoners</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Common self-improvement approaches for large language models (LLMs), such as STaR, iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T02:41:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.06457v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.06457v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 OpenEP: Open-Ended Future Event Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yong Guan, Hao Peng, Xiaozhi Wang, Lei Hou, Juanzi Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Future event prediction (FEP) is a long-standing and crucial task in the world, as understanding the evolution of events enables early risk identification, informed decision-making, and strategic planning. Existing work typically treats event prediction as classification tasks and confines the outcomes of future events to a fixed scope, such as yes/no questions, candidate set, and taxonomy, which is difficult to include all possible outcomes of future events. In this paper, we introduce OpenEP (an Open-Ended Future Event Prediction task), which generates flexible and diverse predictions aligned with real-world scenarios. This is mainly reflected in two aspects: firstly, the predictive questions are diverse, covering different stages of event development and perspectives; secondly, the outcomes are flexible, without constraints on scope or format. To facilitate the study of this task, we construct OpenEPBench, an open-ended future event prediction dataset. For question construction, we pose questions from seven perspectives, including location, time, event development, event outcome, event impact, event response, and other, to facilitate an in-depth analysis and understanding of the comprehensive evolution of events. For outcome construction, we collect free-form text containing the outcomes as ground truth to provide semantically complete and detail-enriched outcomes. Furthermore, we propose StkFEP, a stakeholder-enhanced future event prediction framework, that incorporates event characteristics for open-ended settings. Our method extracts stakeholders involved in events to extend questions to gather diverse information. We also collect historically events that are relevant and similar to the question to reveal potential evolutionary patterns. Experiment results indicate that accurately predicting future events in open-ended settings is challenging for existing LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T01:37:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06578v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06578v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 GQE: Generalized Query Expansion for Enhanced Text-Video Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zechen Bai, Tianjun Xiao, Tong He, Pichao Wang, Zheng Zhang, Thomas Brox, Mike Zheng Shou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the rapidly expanding domain of web video content, the task of text-video retrieval has become increasingly critical, bridging the semantic gap between textual queries and video data. This paper introduces a novel data-centric approach, Generalized Query Expansion (GQE), to address the inherent information imbalance between text and video, enhancing the effectiveness of text-video retrieval systems. Unlike traditional model-centric methods that focus on designing intricate cross-modal interaction mechanisms, GQE aims to expand the text queries associated with videos both during training and testing phases. By adaptively segmenting videos into short clips and employing zero-shot captioning, GQE enriches the training dataset with comprehensive scene descriptions, effectively bridging the data imbalance gap. Furthermore, during retrieval, GQE utilizes Large Language Models (LLM) to generate a diverse set of queries and a query selection module to filter these queries based on relevance and diversity, thus optimizing retrieval performance while reducing computational overhead. Our contributions include a detailed examination of the information imbalance challenge, a novel approach to query expansion in video-text datasets, and the introduction of a query selection strategy that enhances retrieval accuracy without increasing computational costs. GQE achieves state-of-the-art performance on several benchmarks, including MSR-VTT, MSVD, LSMDC, and VATEX, demonstrating the effectiveness of addressing text-video retrieval from a data-centric perspective.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T01:24:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07249v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07249v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Seeing and Understanding: Bridging Vision with Chemical Knowledge Via
  ChemVLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Weiyun Wang, Zhe Chen, Wenhai Wang, Wei Li, Shufei Zhang, Mao Su, Wanli Ouyang, Yuqiang Li, Dongzhan Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this technical report, we propose ChemVLM, the first open-source multimodal large language model dedicated to the fields of chemistry, designed to address the incompatibility between chemical image understanding and text analysis. Built upon the VIT-MLP-LLM architecture, we leverage ChemLLM-20B as the foundational large model, endowing our model with robust capabilities in understanding and utilizing chemical text knowledge. Additionally, we employ InternVIT-6B as a powerful image encoder. We have curated high-quality data from the chemical domain, including molecules, reaction formulas, and chemistry examination data, and compiled these into a bilingual multimodal question-answering dataset. We test the performance of our model on multiple open-source benchmarks and three custom evaluation sets. Experimental results demonstrate that our model achieves excellent performance, securing state-of-the-art results in five out of six involved tasks. Our model can be found at https://huggingface.co/AI4Chem/ChemVLM-26B.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T01:16:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07246v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07246v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Sign language recognition based on deep learning and low-cost
  handcrafted descriptors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alvaro Leandro Cavalcante Carneiro, Denis Henrique Pinheiro Salvadeo, Lucas de Brito Silva
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In recent years, deep learning techniques have been used to develop sign language recognition systems, potentially serving as a communication tool for millions of hearing-impaired individuals worldwide. However, there are inherent challenges in creating such systems. Firstly, it is important to consider as many linguistic parameters as possible in gesture execution to avoid ambiguity between words. Moreover, to facilitate the real-world adoption of the created solution, it is essential to ensure that the chosen technology is realistic, avoiding expensive, intrusive, or low-mobility sensors, as well as very complex deep learning architectures that impose high computational requirements. Based on this, our work aims to propose an efficient sign language recognition system that utilizes low-cost sensors and techniques. To this end, an object detection model was trained specifically for detecting the interpreter's face and hands, ensuring focus on the most relevant regions of the image and generating inputs with higher semantic value for the classifier. Additionally, we introduced a novel approach to obtain features representing hand location and movement by leveraging spatial information derived from centroid positions of bounding boxes, thereby enhancing sign discrimination. The results demonstrate the efficiency of our handcrafted features, increasing accuracy by 7.96% on the AUTSL dataset, while adding fewer than 700 thousand parameters and incurring less than 10 milliseconds of additional inference time. These findings highlight the potential of our technique to strike a favorable balance between computational cost and accuracy, making it a promising approach for practical sign language recognition applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T00:56:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>I.2.10; I.4.8; I.4.7; I.5.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07244v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07244v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 GPTVoiceTasker: Advancing Multi-step Mobile Task Efficiency Through
  Dynamic Interface Exploration and Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minh Duc Vu, Han Wang, Zhuang Li, Jieshan Chen, Shengdong Zhao, Zhenchang Xing, Chunyang Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Virtual assistants have the potential to play an important role in helping users achieves different tasks. However, these systems face challenges in their real-world usability, characterized by inefficiency and struggles in grasping user intentions. Leveraging recent advances in Large Language Models (LLMs), we introduce GptVoiceTasker, a virtual assistant poised to enhance user experiences and task efficiency on mobile devices. GptVoiceTasker excels at intelligently deciphering user commands and executing relevant device interactions to streamline task completion. The system continually learns from historical user commands to automate subsequent usages, further enhancing execution efficiency. Our experiments affirm GptVoiceTasker's exceptional command interpretation abilities and the precision of its task automation module. In our user study, GptVoiceTasker boosted task efficiency in real-world scenarios by 34.85%, accompanied by positive participant feedback. We made GptVoiceTasker open-source, inviting further research into LLMs utilization for diverse tasks through prompt engineering and leveraging user usage data to improve efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T00:48:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3654777.3676356' target='_blank'>doi</a><a href='http://arxiv.org/abs/2401.14268v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.14268v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge
  Distillation Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tong Wang, K. Sudhir, Dat Hong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior performance in complex human-like interactions. But they are costly, or too large for edge devices such as smartphones and harder to self-host, leading to security and privacy concerns. This paper introduces a novel interpretable knowledge distillation approach to enhance the performance of smaller, more economical LLMs that firms can self-host. We study this problem in the context of building a customer service agent aimed at achieving high customer satisfaction through goal-oriented dialogues. Unlike traditional knowledge distillation, where the "student" model learns directly from the "teacher" model's responses via fine-tuning, our interpretable "strategy" teaching approach involves the teacher providing strategies to improve the student's performance in various scenarios. This method alternates between a "scenario generation" step and a "strategies for improvement" step, creating a customized library of scenarios and optimized strategies for automated prompting. The method requires only black-box access to both student and teacher models; hence it can be used without manipulating model parameters. In our customer service application, the method improves performance, and the learned strategies are transferable to other LLMs and scenarios beyond the training set. The method's interpretabilty helps safeguard against potential harms through human audit.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T23:59:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07238v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07238v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Neural embedding of beliefs reveals the role of relative dissonance in
  human decision-making</h2>
                <div class="authors">
                    <strong>Authors:</strong> Byunghwee Lee, Rachith Aiyappa, Yong-Yeol Ahn, Haewoon Kwak, Jisun An
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Beliefs serve as the foundation for human cognition and decision-making. They guide individuals in deriving meaning from their lives, shaping their behaviors, and forming social connections. Therefore, a model that encapsulates beliefs and their interrelationships is crucial for quantitatively studying the influence of beliefs on our actions. Despite its importance, research on the interplay between human beliefs has often been limited to a small set of beliefs pertaining to specific issues, with a heavy reliance on surveys or experiments. Here, we propose a method for extracting nuanced relations between thousands of beliefs by leveraging large-scale user participation data from an online debate platform and mapping these beliefs to an embedding space using a fine-tuned large language model (LLM). This belief embedding space effectively encapsulates the interconnectedness of diverse beliefs as well as polarization across various social issues. We discover that the positions within this belief space predict new beliefs of individuals. Furthermore, we find that the relative distance between one's existing beliefs and new beliefs can serve as a quantitative estimate of cognitive dissonance, allowing us to predict new beliefs. Our study highlights how modern LLMs, when combined with collective online records of human beliefs, can offer insights into the fundamental principles that govern human belief formation and decision-making processes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T23:58:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CY</span><span>physics.soc-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07237v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07237v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset</h2>
                <div class="authors">
                    <strong>Authors:</strong> Peitong Duan, Chin-yi Chen, Gang Li, Bjoern Hartmann, Yang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automated UI evaluation can be beneficial for the design process; for example, to compare different UI designs, or conduct automated heuristic evaluation. LLM-based UI evaluation, in particular, holds the promise of generalizability to a wide variety of UI types and evaluation tasks. However, current LLM-based techniques do not yet match the performance of human evaluators. We hypothesize that automatic evaluation can be improved by collecting a targeted UI feedback dataset and then using this dataset to enhance the performance of general-purpose LLMs. We present a targeted dataset of 3,059 design critiques and quality ratings for 983 mobile UIs, collected from seven experienced designers. We carried out an in-depth analysis to characterize the dataset's features. We then applied this dataset to achieve a 55% performance gain in LLM-generated UI feedback via various few-shot and visual prompting techniques. We also discuss future applications of this dataset, including training a reward model for generative UI techniques, and fine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T23:41:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.08850v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.08850v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 ConfusedPilot: Confused Deputy Risks in RAG-based LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.   In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T05:24:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.04870v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.04870v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Training LLMs over Neurally Compressed Text</h2>
                <div class="authors">
                    <strong>Authors:</strong> Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T22:01:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.03626v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.03626v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Interpolating many-body wave functions for accelerated molecular
  dynamics on the near-exact electronic surface</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yannic Rath, George H. Booth
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While there have been many developments in computational probes of both strongly-correlated molecular systems and machine-learning accelerated molecular dynamics, there remains a significant gap in capabilities in simulating accurate non-local electronic structure over timescales on which atoms move. We develop an approach to bridge these fields with a practical interpolation scheme for the correlated many-electron state through the space of atomic configurations, whilst avoiding the exponential complexity of these underlying electronic states. With a small number of accurate correlated wave functions as a training set, we demonstrate provable convergence to near-exact potential energy surfaces for subsequent dynamics with propagation of a valid many-body wave function and inference of its variational energy whilst retaining a mean-field computational scaling. This represents a profoundly different paradigm to the direct interpolation of potential energy surfaces in established machine-learning approaches. We combine this with modern electronic structure approaches to systematically resolve molecular dynamics trajectories and converge thermodynamic quantities with a high-throughput of several million interpolated wave functions with explicit validation of their accuracy from only a few numerically exact quantum chemical calculations. We also highlight the comparison to traditional machine-learned potentials or dynamics on mean-field surfaces.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T21:57:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.chem-ph</span><span>cond-mat.str-el</span><span>physics.comp-ph</span><span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.11097v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.11097v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, Amine Mhedhbi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Schema linking is a crucial step in Text-to-SQL pipelines, which translate natural language queries into SQL. The goal of schema linking is to retrieve relevant tables and columns (signal) while disregarding irrelevant ones (noise). However, imperfect schema linking can often exclude essential columns needed for accurate query generation. In this work, we revisit the need for schema linking when using the latest generation of large language models (LLMs). We find empirically that newer models are adept at identifying relevant schema elements during generation, without the need for explicit schema linking. This allows Text-to-SQL pipelines to bypass schema linking entirely and instead pass the full database schema to the LLM, eliminating the risk of excluding necessary information. Furthermore, as alternatives to schema linking, we propose techniques that improve Text-to-SQL accuracy without compromising on essential schema information. Our approach achieves 71.83\% execution accuracy on the BIRD benchmark, ranking first at the time of submission.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T17:59:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07702v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07702v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 On the Parameter Selection of Phase-transmittance Radial Basis Function
  Neural Networks for Communication Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan A. Soares, Kayol S. Mayer, Dalton S. Arantes
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the ever-evolving field of digital communication systems, complex-valued neural networks (CVNNs) have become a cornerstone, delivering exceptional performance in tasks like equalization, channel estimation, beamforming, and decoding. Among the myriad of CVNN architectures, the phase-transmittance radial basis function neural network (PT-RBF) stands out, especially when operating in noisy environments such as 5G MIMO systems. Despite its capabilities, achieving convergence in multi-layered, multi-input, and multi-output PT-RBFs remains a daunting challenge. Addressing this gap, this paper presents a novel Deep PT-RBF parameter initialization technique. Through rigorous simulations conforming to 3GPP TS 38 standards, our method not only outperforms conventional initialization strategies like random, $K$-means, and constellation-based methods but is also the only approach to achieve successful convergence in deep PT-RBF architectures. These findings pave the way to more robust and efficient neural network deployments in complex digital communication systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T17:50:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span><span>I.2.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07692v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07692v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,
  Applications and Opportunities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, Dacheng Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at \url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T01:49:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07666v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07666v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi-Cheng Lin, Wei-Chih Chen, Hung-yi Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Warning: This paper may contain texts with uncomfortable content.   Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T16:55:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07665v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07665v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Massive Activations in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers. Code is available at https://github.com/locuslab/massive-activations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T16:00:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.17762v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.17762v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 An Event Structure-aware Generative Model for Biomedical Event
  Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haohan Yuan, Siu Cheung Hui, Haopeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Biomedical Event Extraction (BEE) is a challenging task that involves modeling complex relationships between fine-grained entities in biomedical text. BEE has traditionally been formulated as a classification problem. With the recent technological advancements in large language models (LLMs), generation-based models that cast event extraction as a sequence generation problem have attracted much attention from the NLP research communities. However, current generative models often overlook the importance of cross-instance information from complex event structures such as nested events and overlapping events, which contribute quite significantly in the benchmark datasets. In this paper, we propose an event structure-aware generative model called GenBEE, which can capture complex event structures in biomedical text for biomedical event extraction. In particular, GenBEE constructs event prompts that distill knowledge from LLMs for incorporating both label semantics and argument dependency relationships into the proposed model. In addition, GenBEE also generates prefixes with event structural prompts to incorporate structural features for improving the model's overall performance. We have evaluated the proposed GenBEE model on three widely used biomedical event extraction benchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that GenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets, and achieved competitive results when compared to the state-of-the-art classification-based models on the PHEE dataset.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T15:24:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06583v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06583v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Exploring LLM Multi-Agents for ICD Coding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rumeng Li, Xun Wang, Hong Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To address the limitations of Large Language Models (LLMs) in the International Classification of Diseases (ICD) coding task, where they often produce inaccurate and incomplete prediction results due to the high-dimensional and skewed distribution of the ICD codes, and often lack interpretability and reliability as well. We introduce an innovative multi-agent approach for ICD coding which mimics the ICD coding assignment procedure in real-world settings, comprising five distinct agents: the patient, physician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based model tailored to their specific role within the coding process. We also integrate the system with Electronic Health Record (HER)'s SOAP (subjective, objective, assessment and plan) structure to boost the performances. We compare our method with a system of agents designed solely by LLMs and other strong baselines and evaluate it using the Medical Information Mart for Intensive Care III (MIMIC-III) dataset. Our multi-agent coding framework significantly outperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency with CoT (CoT-SC) in coding common and rare ICD codes. An ablation study validates the effectiveness of the designated agent roles. it also outperforms the LLM-designed agent system. Moreover, our method achieves comparable results to state-of-the-art ICD coding methods that require extensive pre-training or fine-tuning, and outperforms them in rare code accuracy, and explainability. Additionally, we demonstrate the method's practical applicability by presenting its performance in scenarios not limited by the common or rare ICD code constraints.The proposed multi-agent method for ICD coding effectively mimics the real-world coding process and improves performance on both common and rare codes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T15:32:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.15363v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.15363v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation
  Integrating Web Search and Knowledge Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However, LLMs are prone to produce factually incorrect information and often produce "phantom" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a "Retrieval-Augmented Generation (RAG)" system. First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T15:19:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07611v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07611v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Natural Language to Verilog: Design of a Recurrent Spiking Neural
  Network using Large Language Models and ChatGPT</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paola Vitolo, George Psaltakis, Michael Tomlinson, Gian Domenico Licciardo, Andreas G. Andreou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper investigates the use of Large Language Models (LLMs) for automating the generation of hardware description code, aiming to explore their potential in supporting and enhancing the development of efficient neuromorphic computing architectures. Building on our prior work, we employ OpenAI's ChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a programmable recurrent spiking neural network, while also generating test benches to assess the system's correctness. The resultant design was validated in three case studies, the exclusive OR,the IRIS flower classification and the MNIST hand-written digit classification, achieving accuracies of up to 96.6%. To verify its synthesizability and implementability, the design was prototyped on a field-programmable gate array and implemented on SkyWater 130 nm technology by using an open-source electronic design automation flow. Additionally, we have submitted it to Tiny Tapeout 6 chip fabrication program to further evaluate the system on-chip performance in the future.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:42:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.01419v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.01419v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Optimizing UAV Trajectory for Emergency Response Operations under Real
  3D Environments: Integrating Priority Levels and LoS Constraints</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad Taghi Dabiri, Mazen Hasna, Saud Althunibat, Khalid Qaraqe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unmanned Aerial Vehicles (UAVs) have emerged as a critical component in next-generation wireless networks, particularly for disaster recovery scenarios, due to their flexibility, mobility, and rapid deployment capabilities. This paper focuses on optimizing UAV trajectories to ensure effective communication in disaster-stricken areas using terahertz (THz) links. We address specific challenges such as energy consumption, user priority levels, and navigating complex urban environments to maintain Line of Sight (LoS) connections amidst 3D obstacles. Our contributions include the development of a detailed modeling approach using online 3D map data, the formulation of an optimal trajectory optimization problem, and the proposal of a Genetic Algorithm (GA)-based method alongside an enhanced heuristic algorithm for faster convergence. Through 3D simulations, we demonstrate the trade-off between minimizing total service time and prioritizing higher-weight nodes, showing the impact of different priority weight factors on the trajectory time. The proposed algorithms are evaluated using real-world data from the West Bay area of Doha, Qatar, demonstrating their effectiveness in optimizing UAV trajectories for emergency response.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:41:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07589v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07589v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Transformers and Large Language Models for Efficient Intrusion Detection
  Systems: A Comprehensive Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hamza Kheddar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T14:28:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span><span>cs.CV</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07583v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07583v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 LLM Voting: Human Choices and AI Collective Decision Making</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joshua C. Yang, Damian Dailisan, Marcin Korecki, Carina I. Hausladen, Dirk Helbing
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:41:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>cs.LG</span><span>econ.GN</span><span>q-fin.EC</span><span>68T05, 91B14, 91C20</span><span>I.2.7; J.4; K.4.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.01766v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.01766v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson
  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Simon Kloker, Herbertson Bukoli, Twaha Kateete
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Introduction: Poor educational quality in Secondary Schools is still regarded as one of the major struggles in 21st century Uganda - especially in rural areas. Research identifies several problems, including low quality or absent teacher lesson planning. As the government pushes towards the implementation of a new curriculum, exiting lesson plans become obsolete and the problem is worsened. Using a Retrieval Augmented Generation approach, we developed a prototype that generates customized lesson plans based on the government-accredited textbooks. This helps teachers create lesson plans more efficiently and with better quality, ensuring they are fully aligned the new curriculum and the competence-based learning approach.   Methods: The prototype was created using Cohere LLM and Sentence Embeddings, and LangChain Framework - and thereafter made available on a public website. Vector stores were trained for three new curriculum textbooks (ICT, Mathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were generated following a pseudo-random generation protocol, based on the suggested periods in the textbooks. The lesson plans were analyzed regarding their technical quality by three independent raters following the Lesson Plan Analysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically designed for East Africa and competence-based curriculums.   Results: Evaluation of 24 lesson plans using the LPAP resulted in an average quality of between 75 and 80%, corresponding to "very good lesson plan". None of the lesson plans scored below 65%, although one lesson plan could be argued to have been missing the topic. In conclusion, the quality of the generated lesson plans is at least comparable, if not better, than those created by humans, as demonstrated in a study in Rwanda, whereby no lesson plan even reached the benchmark of 50%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:22:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span><span>cs.IR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07542v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07542v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Lost in Overlap: Exploring Watermark Collision in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiyang Luo, Ke Lin, Chao Gu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread usage of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks, such as paraphrasing or translation. In this paper, we introduce watermark collision as a novel and general philosophy for watermark attacks, aimed at enhancing attack performance on top of any other attacking methods. We also provide a comprehensive demonstration that watermark collision poses a threat to all logit-based watermark algorithms, impacting not only specific attack scenarios but also downstream applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:15:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.10020v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.10020v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Usefulness of data flow diagrams and large language models for security
  threat validation: a registered report</h2>
                <div class="authors">
                    <strong>Authors:</strong> Winnie Bahati Mbaka, Katja Tuma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well. Threat analysis and risk assessment are used to identify security threats for new or refactored systems. Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis. Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats. We propose a controlled experiment with practitioners to investigate whether some analysis material (like LLM-generated advice) is better than none and whether more material (the system's data flow diagram and LLM-generated advice) is better than some material. In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design. Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T09:30:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07537v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07537v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Development of a Multi-Agent Clinical Decision Support System for Korean
  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in
  Emergency Departments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seungjun Han, Wongyung Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.   We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.   The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.   Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:03:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07531v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07531v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Learning-based Models for Vulnerability Detection: An Extensive Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chao Ni, Liyu Shen, Xiaodan Xu, Xin Yin, Shaohua Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Though many deep learning-based models have made great progress in vulnerability detection, we have no good understanding of these models, which limits the further advancement of model capability, understanding of the mechanism of model detection, and efficiency and safety of practical application of models. In this paper, we extensively and comprehensively investigate two types of state-of-the-art learning-based approaches (sequence-based and graph-based) by conducting experiments on a recently built large-scale dataset. We investigate seven research questions from five dimensions, namely model capabilities, model interpretation, model stability, ease of use of model, and model economy. We experimentally demonstrate the priority of sequence-based models and the limited abilities of both LLM (ChatGPT) and graph-based models. We explore the types of vulnerability that learning-based models skilled in and reveal the instability of the models though the input is subtlely semantical-equivalently changed. We empirically explain what the models have learned. We summarize the pre-processing as well as requirements for easily using the models. Finally, we initially induce the vital information for economically and safely practical usage of these models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T13:01:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07526v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07526v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 WavLLM: Towards Robust and Adaptive Speech Large Language Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, Furu Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \url{aka.ms/wavllm}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T12:42:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.00656v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.00656v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Large Language Models Know What Makes Exemplary Contexts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Quanyu Long, Jianda Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-context learning (ICL) has proven to be a significant capability with the advancement of Large Language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without needing to update millions of parameters. This paper presents a unified framework for LLMs that allows them to self-select influential in-context examples to compose their contexts; self-rank candidates with different demonstration compositions; self-optimize the demonstration selection and ordering through reinforcement learning. Specifically, our method designs a parameter-efficient retrieval head that generates the optimized demonstration after training with rewards from LLM's own preference. Experimental results validate the proposed method's effectiveness in enhancing ICL performance. Additionally, our approach effectively identifies and selects the most representative examples for the current task, and includes more diversity in retrieval.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T12:32:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07505v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07505v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 QirK: Question Answering via Intermediate Representation on Knowledge
  Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jan Luca Scheerer, Anton Lykov, Moe Kayali, Ilias Fountalis, Dan Olteanu, Nikolaos Vasiloglou, Dan Suciu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We demonstrate QirK, a system for answering natural language questions on Knowledge Graphs (KG). QirK can answer structurally complex questions that are still beyond the reach of emerging Large Language Models (LLMs). It does so using a unique combination of database technology, LLMs, and semantic search over vector embeddings. The glue for these components is an intermediate representation (IR). The input question is mapped to IR using LLMs, which is then repaired into a valid relational database query with the aid of a semantic search on vector embeddings. This allows a practical synthesis of LLM capabilities and KG reliability.   A short video demonstrating QirK is available at https://youtu.be/6c81BLmOZ0U.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T12:19:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07494v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07494v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Training Overhead Ratio: A Practical Reliability Metric for Large
  Language Model Training Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ning Lu, Qian Xie, Hao Zhang, Wenyi Fang, Yang Zheng, Jiantao Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are revolutionizing the AI industry with their superior capabilities. Training these models requires large-scale GPU clusters and significant computing time, leading to frequent failures that significantly increase training costs. Despite its significance, this field lacks a metric for evaluating reliability. In this work, we introduce a novel reliability metric called \emph{Training Overhead Ratio} (TOR) to evaluate the reliability of fault-tolerant LLM training systems. TOR is defined as the ratio of optimal training time to the observed training time of a system, serving as a practical tool for users to estimate the actual time required to train an LLM on a given system. Furthermore, our investigation identifies the key factor for enhancing reliability and present TOR equations for various types of failures encountered in practice.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:55:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07482v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07482v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 FIPO: Free-form Instruction-oriented Prompt Optimization with Preference
  Dataset and Modular Fine-tuning Schema</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junru Lu, Siyu An, Min Zhang, Yulan He, Di Yin, Xing Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> When the quality of naive prompts is carefully optimized by human experts, the task performance of large language models (LLMs) can be significantly improved. However, expert-based prompt optimizations are expensive. Herein, some works have proposed Automatic Prompt Optimization (APO), to optimize naive prompts according to task outputs of given in-box testing models, with the help of advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing schemes suffer from poor generalization ability and privacy risk. To this end, we collect the first large-scale Prompt Optimization Preference dataset (POP), fine-tune offline local LLM-based optimizers, then fairly test with various downstream models. Our method allows accurate optimization of the core task instruction part within the naive prompt in a model-agnostic manner, and thus is named Free-from Instruction-oriented Prompt Optimization (FIPO). In specific, FIPO uses a modular APO template that dynamically integrate the naive task instruction, optional instruction responses, and optional ground truth to produce finely optimized prompts. The POP dataset is meticulously constructed using advanced LLMs, undergoing rigorous cross-validation by human experts and analytical models. Leveraging insights from the data with Tulu2 models and diverse fine-tuning strategies, we validate the efficacy of FIPO framework across five public benchmarks and six testing models. Check codes and data here: https://github.com/LuJunru/FIPO_Project.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:47:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.11811v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.11811v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Bridging and Modeling Correlations in Pairwise Data for Direct
  Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxin Jiang, Bo Huang, Yufei Wang, Xingshan Zeng, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align large language models (LLMs) with human-desired behaviors using pairwise preference data. However, the winning response and the losing response within pairwise data are generated isolatedly, leading to weak correlations between them as well as suboptimal alignment performance. To address this issue, we propose an effective framework named BMC, for bridging and modeling correlations in pairwise data. Firstly, we increase the consistency and informativeness of the pairwise preference signals by targeted modifications, synthesizing a pseudo winning response through improving the losing response based on the winning response. Secondly, we identify that DPO alone is insufficient to model these correlations and capture nuanced variations. Therefore, we propose learning token-level correlations by dynamically leveraging the policy model's confidence during training. Comprehensive experiments on QA, math, and instruction-following tasks demonstrate the effectiveness of our approach, significantly surpassing competitive baselines, including DPO. Additionally, our in-depth quantitative analysis reveals the reasons behind our method's superior performance over DPO and showcases its versatility to other DPO variants.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:29:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07471v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07471v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Large Language Models Prompting With Episodic Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dai Do, Quan Tran, Svetha Venkatesh, Hung Le
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prompt optimization is essential for enhancing the performance of Large Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks, particularly in scenarios of few-shot learning where training examples are incorporated directly into the prompt. Despite the growing interest in optimizing prompts with few-shot examples, existing methods for prompt optimization are often resource-intensive or perform inadequately. In this work, we propose PrOmpting with Episodic Memory (POEM), a novel prompt optimization technique that is simple, efficient, and demonstrates strong generalization capabilities. We approach prompt optimization as a Reinforcement Learning (RL) challenge, using episodic memory to archive combinations of input data, permutations of few-shot examples, and the rewards observed during training. In the testing phase, we optimize the sequence of examples for each test query by selecting the sequence that yields the highest total rewards from the top-k most similar training examples in the episodic memory. Our results show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over 5.3% in various text classification tasks. Furthermore, our approach adapts well to broader language understanding tasks, consistently outperforming conventional heuristic methods for ordering examples.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T11:19:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07465v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07465v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt
  Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Asif Hanif, Fahad Shamshad, Muhammad Awais, Muzammal Naseer, Fahad Shahbaz Khan, Karthik Nandakumar, Salman Khan, Rao Muhammad Anwer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Medical foundation models are gaining prominence in the medical community for their ability to derive general representations from extensive collections of medical image-text pairs. Recent research indicates that these models are susceptible to backdoor attacks, which allow them to classify clean images accurately but fail when specific triggers are introduced. However, traditional backdoor attacks necessitate a considerable amount of additional data to maliciously pre-train a model. This requirement is often impractical in medical imaging applications due to the usual scarcity of data. Inspired by the latest developments in learnable prompts, this work introduces a method to embed a backdoor into the medical foundation model during the prompt learning phase. By incorporating learnable prompts within the text encoder and introducing imperceptible learnable noise trigger to the input images, we exploit the full capabilities of the medical foundation models (Med-FM). Our method, BAPLe, requires only a minimal subset of data to adjust the noise trigger and the text prompts for downstream tasks, enabling the creation of an effective backdoor attack. Through extensive experiments with four medical foundation models, each pre-trained on different modalities and evaluated across six downstream datasets, we demonstrate the efficacy of our approach. BAPLe achieves a high backdoor success rate across all models and datasets, outperforming the baseline backdoor attack methods. Our work highlights the vulnerability of Med-FMs towards backdoor attacks and strives to promote the safe adoption of Med-FMs before their deployment in real-world applications. Code is available at https://asif-hanif.github.io/baple/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T10:45:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07440v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07440v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Beyond Inter-Item Relations: Dynamic Adaptive Mixture-of-Experts for
  LLM-Based Sequential Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> CanYi Liu, Wei Li, Youchen, Zhang, Hui Li, Rongrong Ji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sequential recommender system (SRS) predicts the next items that users may prefer based on user historical interaction sequences. Inspired by the rise of large language models (LLMs) in various AI applications, there is a surge of work on LLM-based SRS. Despite their attractive performance, existing LLM-based SRS still exhibit some limitations, including neglecting intra-item relations, ignoring long-term collaborative knowledge and using inflexible architecture designs for adaption. To alleviate these issues, we propose an LLM-based SRS named MixRec. Built on top of coarse-grained adaption for capturing inter-item relations, MixRec is further enhanced with (1) context masking that models intra-item relations to help LLM better understand token and item semantics in the context of SRS, (2) collaborative knowledge injection that helps LLM incorporate long-term collaborative knowledge, and (3) a dynamic adaptive mixture-of-experts design that can flexibly choose expert architectures based on Bayesian optimization to better incorporate different sequential information. Extensive experiments demonstrate that MixRec can effectively handle sequential recommendation in a dynamic and adaptive manner.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T10:03:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07427v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07427v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Exploring Retrieval Augmented Generation in Arabic</h2>
                <div class="authors">
                    <strong>Authors:</strong> Samhaa R. El-Beltagy, Mohamed A. Abdallah
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful technique in natural language processing, combining the strengths of retrieval-based and generation-based models to enhance text generation tasks. However, the application of RAG in Arabic, a language with unique characteristics and resource constraints, remains underexplored. This paper presents a comprehensive case study on the implementation and evaluation of RAG for Arabic text. The work focuses on exploring various semantic embedding models in the retrieval stage and several LLMs in the generation stage, in order to investigate what works and what doesn't in the context of Arabic. The work also touches upon the issue of variations between document dialect and query dialect in the retrieval stage. Results show that existing semantic embedding models and LLMs can be effectively employed to build Arabic RAG pipelines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T10:03:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07425v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07425v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fan Yang, Sicheng Zhao, Yanhao Zhang, Haoxiang Chen, Hui Chen, Wenbo Tang, Haonan Lu, Pengfei Xu, Zhenyu Yang, Jungong Han, Guiguang Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in autonomous driving, augmented reality, robotics, and embodied intelligence have necessitated 3D perception algorithms. However, current 3D perception methods, particularly small models, struggle with processing logical reasoning, question-answering, and handling open scenario categories. On the other hand, generative multimodal large language models (MLLMs) excel in general capacity but underperform in 3D tasks, due to weak spatial and local object perception, poor text-based geometric numerical output, and inability to handle camera focal variations. To address these challenges, we propose the following solutions: Spatial-Enhanced Local Feature Mining for better spatial feature extraction, 3D Query Token-Derived Info Decoding for precise geometric regression, and Geometry Projection-Based 3D Reasoning for handling camera focal length variations. We employ parameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a powerful 3D perception MLLM. Additionally, we have constructed the IG3D dataset, which provides fine-grained descriptions and question-answer annotations. Extensive experiments demonstrate that our LLMI3D achieves state-of-the-art performance, significantly outperforming existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T10:00:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07422v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07422v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 BioRAG: A RAG-LLM Framework for Biological Question Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chengrui Wang, Qingqing Long, Meng Xiao, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, Yuanchun Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The question-answering system for Life science research, which is characterized by the rapid pace of discovery, evolving insights, and complex interactions among knowledge entities, presents unique challenges in maintaining a comprehensive knowledge warehouse and accurate information retrieval. To address these issues, we introduce BioRAG, a novel Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs) framework. Our approach starts with parsing, indexing, and segmenting an extensive collection of 22 million scientific papers as the basic knowledge, followed by training a specialized embedding model tailored to this domain. Additionally, we enhance the vector retrieval process by incorporating a domain-specific knowledge hierarchy, which aids in modeling the intricate interrelationships among each query and context. For queries requiring the most current information, BioRAG deconstructs the question and employs an iterative retrieval process incorporated with the search engine for step-by-step reasoning. Rigorous experiments have demonstrated that our model outperforms fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks across multiple life science question-answering tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:54:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.01107v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.01107v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge
  Editing for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Knowledge editing aims to update outdated or incorrect knowledge in large language models (LLMs). However, current knowledge editing methods have limited scalability for lifelong editing. This study explores the fundamental reason why knowledge editing fails in lifelong editing. We begin with the closed-form solution derived from linear associative memory, which underpins state-of-the-art knowledge editing methods. We extend the solution from single editing to lifelong editing, and through rigorous mathematical derivation, identify an interference term in the final solution, suggesting that editing knowledge may impact irrelevant knowledge. Further analysis of the interference term reveals a close relationship with superposition between knowledge representations. When knowledge superposition does not exist in language models, the interference term vanishes, allowing for lossless knowledge editing. Experiments across numerous language models reveal that knowledge superposition is universal, exhibiting high kurtosis, zero mean, and heavy-tailed distributions with clear scaling laws. Ultimately, by combining theory and experiments, we demonstrate that knowledge superposition is the fundamental reason for the failure of lifelong editing. Moreover, this is the first study to investigate knowledge editing from the perspective of superposition and provides a comprehensive observation of superposition across numerous real-world language models. Code available at https://github.com/ChenhuiHu/knowledge_in_superposition.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:43:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07413v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07413v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Efficient Edge AI: Deploying Convolutional Neural Networks on FPGA with
  the Gemmini Accelerator</h2>
                <div class="authors">
                    <strong>Authors:</strong> Federico Nicolas Peccia, Svetlana Pavlitska, Tobias Fleck, Oliver Bringmann
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing concerns regarding energy consumption and privacy have prompted the development of AI solutions deployable on the edge, circumventing the substantial CO2 emissions associated with cloud servers and mitigating risks related to sharing sensitive data. But deploying Convolutional Neural Networks (CNNs) on non-off-the-shelf edge devices remains a complex and labor-intensive task. In this paper, we present and end-to-end workflow for deployment of CNNs on Field Programmable Gate Arrays (FPGAs) using the Gemmini accelerator, which we modified for efficient implementation on FPGAs. We describe how we leverage the use of open source software on each optimization step of the deployment process, the customizations we added to them and its impact on the final system's performance. We were able to achieve real-time performance by deploying a YOLOv7 model on a Xilinx ZCU102 FPGA with an energy efficiency of 36.5 GOP/s/W. Our FPGA-based solution demonstrates superior power efficiency compared with other embedded hardware devices, and even outperforms other FPGA reference implementations. Finally, we present how this kind of solution can be integrated into a wider system, by testing our proposed platform in a traffic monitoring scenario.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:24:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07404v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07404v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Model Attribution in LLM-Generated Disinformation: A Domain
  Generalization Approach with Supervised Contrastive Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu, Huan Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Model attribution for LLM-generated disinformation poses a significant challenge in understanding its origins and mitigating its spread. This task is especially challenging because modern large language models (LLMs) produce disinformation with human-like quality. Additionally, the diversity in prompting methods used to generate disinformation complicates accurate source attribution. These methods introduce domain-specific features that can mask the fundamental characteristics of the models. In this paper, we introduce the concept of model attribution as a domain generalization problem, where each prompting method represents a unique domain. We argue that an effective attribution model must be invariant to these domain-specific features. It should also be proficient in identifying the originating models across all scenarios, reflecting real-world detection challenges. To address this, we introduce a novel approach based on Supervised Contrastive Learning. This method is designed to enhance the model's robustness to variations in prompts and focuses on distinguishing between different source LLMs. We evaluate our model through rigorous experiments involving three common prompting methods: ``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs: ``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the effectiveness of our approach in model attribution tasks, achieving state-of-the-art performance across diverse and unseen datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T08:10:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.21264v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.21264v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 From photon momentum transfer to acceleration sensing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianyu Yang, Nan Li, Yuyao Pan, Jing Yang, Zhiming Chen, Han Cai, Yuliang Wang, Chuankun Han, Xingfan Chen, Cheng Liu, Huizhu Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As a typical application of photon momentum transfer, optical levitation systems are known for their ideal isolation from mechanical dissipation and thermal noise. These characters offer extraordinary potential for acceleration precision sensing and have attracted extensive attention in both fundamental and applied physics. Although considerable improvements of optical levitation accelerometers has been reported, the dynamic testing of the sensing performance remains a crucial challenge before the utilization in practical application scenarios. In this work, we present a dual-beam optical levitation accelerometer and demonstrate the test with dynamic inputs for the first time. An acceleration sensing sensitivity of $0.1\mu g$ and a measurement range of $ 1g$ are achieved. These advancements solidify the potential of optical levitation accelerometer for deployment in practical domains, including navigation, intelligent driving, and industrial automation, building a bridge between the laboratory systems and real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:43:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.optics</span><span>physics.app-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07348v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07348v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive
  Language-Image Pre-traning Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Chen, Xiaozhen Qiao, Zhe Sun, Xuelong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Contrastive Language-Image Pre-training (CLIP) model excels in integrating semantic information between images and text through contrastive learning techniques. It has achieved remarkable performance in various multimodal tasks. However, the deployment of large CLIP models is hindered in resource-limited environments, while smaller models frequently fail to meet the performance benchmarks required for practical applications. In this paper, we propose a novel approach, ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive Language-Image Pre-traning Model, which aims to comprehensively distill the knowledge from a large teacher CLIP model into a smaller student model, ensuring comparable performance with significantly reduced parameters. ComKD-CLIP is composed of two key mechanisms: Image Feature Alignment (IFAlign) and Educational Attention (EduAttention). IFAlign makes the image features extracted by the student model closely match those extracted by the teacher model, enabling the student to learn teacher's knowledge of extracting image features. EduAttention explores the cross-relationships between text features extracted by the teacher model and image features extracted by the student model, enabling the student model to learn how the teacher model integrates text-image features. In addition, ComKD-CLIP can refine the knowledge distilled from IFAlign and EduAttention by leveraging the text-image feature fusion results of the teacher model, ensuring the student model accurately absorbs the teacher's knowledge. Extensive experiments conducted on 11 datasets have demonstrated the superiority of the proposed method.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:43:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.04145v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.04145v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Gradient Alignment Improves Test-Time Adaptation for Medical Image
  Segmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyang Chen, Yiwen Ye, Yongsheng Pan, Yong Xia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although recent years have witnessed significant advancements in medical image segmentation, the pervasive issue of domain shift among medical images from diverse centres hinders the effective deployment of pre-trained models. Many Test-time Adaptation (TTA) methods have been proposed to address this issue by fine-tuning pre-trained models with test data during inference. These methods, however, often suffer from less-satisfactory optimization due to suboptimal optimization direction (dictated by the gradient) and fixed step-size (predicated on the learning rate). In this paper, we propose the Gradient alignment-based Test-time adaptation (GraTa) method to improve both the gradient direction and learning rate in the optimization procedure. Unlike conventional TTA methods, which primarily optimize the pseudo gradient derived from a self-supervised objective, our method incorporates an auxiliary gradient with the pseudo one to facilitate gradient alignment. Such gradient alignment enables the model to excavate the similarities between different gradients and correct the gradient direction to approximate the empirical gradient related to the current segmentation task. Additionally, we design a dynamic learning rate based on the cosine similarity between the pseudo and auxiliary gradients, thereby empowering the adaptive fine-tuning of pre-trained models on diverse test data. Extensive experiments establish the effectiveness of the proposed gradient alignment and dynamic learning rate and substantiate the superiority of our GraTa method over other state-of-the-art TTA methods on a benchmark medical image segmentation task. The code and weights of pre-trained source models will be available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:37:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07343v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07343v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 SentenceVAE: Enable Next-sentence Prediction for Large Language Models
  with Faster Speed, Higher Accuracy and Longer Context</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongjun An, Yifan Chen, Zhe Sun, Xuelong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current large language models (LLMs) primarily utilize next-token prediction method for inference, which significantly impedes their processing speed. In this paper, we introduce a novel inference methodology termed next-sentence prediction, aiming at enhancing the inference efficiency of LLMs. We present Sentence Variational Autoencoder (SentenceVAE), which includes a Sentence Encoder to compress multiple tokens in a sentence into a single token, and a Sentence Decoder to reconstruct it. By integrating SentenceVAE into the input and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference method. In addition, the SentenceVAE module of SLLMs can maintain the integrity of the original semantic content by segmenting the context into sentences, thereby improving accuracy while boosting inference speed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over equivalent context length, significantly reducing memory demands for self-attention computation and facilitating the handling of longer context. Extensive experiments on Wanjuan dataset have revealed that the proposed method can accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75% of its original metric, and decrease memory overhead by 86~91% for the equivalent context length, compared to previous token-by-token methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:34:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.00655v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.00655v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 AI Coders Are Among Us: Rethinking Programming Language Grammar Towards
  Efficient Code Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhensu Sun, Xiaoning Du, Zhou Yang, Li Li, David Lo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artificial Intelligence (AI) models have emerged as another important audience for programming languages alongside humans and machines, as we enter the era of large language models (LLMs). LLMs can now perform well in coding competitions and even write programs like developers to solve various tasks, including mathematical problems. However, the grammar and layout of current programs are designed to cater the needs of human developers -- with many grammar tokens and formatting tokens being used to make the code easier for humans to read. While this is helpful, such a design adds unnecessary computational work for LLMs, as each token they either use or produce consumes computational resources. To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar. This aims to represent code in a way that better suits the working mechanism of AI models. Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively. To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named SimPy. SimPy is crafted by revising the original Python grammar through a series of heuristic rules. Programs written in SimPy maintain identical AST structures to those in standard Python. This allows for not only execution via a modified AST parser, but also seamless transformation between programs written in Python and SimPy, enabling human developers and LLMs to use Python and SimPy, respectively, when they need to collaborate. In the experiments, compared with Python, SimPy enables a reduction in token usage by 13.5% and 10.4% for CodeLlama and GPT-4, respectively, when completing the same set of code-related tasks. Additionally, these models can maintain or even improve their performance when using SimPy instead of Python for these tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:34:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.16333v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.16333v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 PsyDI: Towards a Personalized and Progressively In-depth Chatbot for
  Psychological Measurements</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xueyan Li, Xinyan Chen, Yazhe Niu, Shuai Hu, Yu Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the field of psychology, traditional assessment methods, such as standardized scales, are frequently critiqued for their static nature, lack of personalization, and reduced participant engagement, while comprehensive counseling evaluations are often inaccessible. The complexity of quantifying psychological traits further limits these methods. Despite advances with large language models (LLMs), many still depend on single-round Question-and-Answer interactions. To bridge this gap, we introduce PsyDI, a personalized and progressively in-depth chatbot designed for psychological measurements, exemplified by its application in the Myers-Briggs Type Indicator (MBTI) framework. PsyDI leverages user-related multi-modal information and engages in customized, multi-turn interactions to provide personalized, easily accessible measurements, while ensuring precise MBTI type determination. To address the challenge of unquantifiable psychological traits, we introduce a novel training paradigm that involves learning the ranking of proxy variables associated with these traits, culminating in a robust score model for MBTI measurements. The score model enables PsyDI to conduct comprehensive and precise measurements through multi-turn interactions within a unified estimation context. Through various experiments, we validate the efficacy of both the score model and the PsyDI pipeline, demonstrating its potential to serve as a general framework for psychological measurements. Furthermore, the online deployment of PsyDI has garnered substantial user engagement, with over 3,000 visits, resulting in the collection of numerous multi-turn dialogues annotated with MBTI types, which facilitates further research. The source code for the training and web service components is publicly available as a part of OpenDILab at: https://github.com/opendilab/PsyDI
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T06:58:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.CY</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.03337v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.03337v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Read and Think: An Efficient Step-wise Multimodal Language Model for
  Document Understanding and Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jinxu Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the contents of multimodal documents is essential to accurately extract relevant evidence and use it for reasoning. Existing document understanding models tend to generate answers with a single word or phrase directly, ignoring the source document's evidence and lacking interpretability. In this work, we address the lack of step-wise capabilities through data augmentation and extension. Specifically, We use Multi-modal Large Language Models (MLLMs), which have strong visual understanding and reasoning abilities, as data generators to generate step-wise question-and-answer pairs for document images and use a high-performance LLM as the error detector to filter out noisy data. This step-wise data generation pipeline is implemented using both template-based and few-shot methods. We then use the generated high-quality data to train a humanized document understanding and reasoning model, specifically designed to solve complex questions that require reasoning or multi-hop question answering, dubbed DocAssistant. Experimental results demonstrate the effectiveness and application value of step-wise generation, showing a 5 improvement on InfoVQA with complex layouts and a 7 improvement on ChartQA with complex reasoning, compared to directly generated answers. We hope our work highlights the potential of synthetic data and encourages further exploration of multi-modal document reasoning capabilities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T07:26:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.00816v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.00816v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 An Offline Meta Black-box Optimization Framework for Adaptive Design of
  Urban Traffic Light Management Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Taeyoung Yun, Kanghoon Lee, Sujin Yun, Ilmyung Kim, Won-Woo Jung, Min-Cheol Kwon, Kyujin Choi, Yoohyeon Lee, Jinkyoo Park
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Complex urban road networks with high vehicle occupancy frequently face severe traffic congestion. Designing an effective strategy for managing multiple traffic lights plays a crucial role in managing congestion. However, most current traffic light management systems rely on human-crafted decisions, which may not adapt well to diverse traffic patterns. In this paper, we delve into two pivotal design components of the traffic light management system that can be dynamically adjusted to various traffic conditions: phase combination and phase time allocation. While numerous studies have sought an efficient strategy for managing traffic lights, most of these approaches consider a fixed traffic pattern and are limited to relatively small road networks. To overcome these limitations, we introduce a novel and practical framework to formulate the optimization of such design components using an offline meta black-box optimization. We then present a simple yet effective method to efficiently find a solution for the aforementioned problem. In our framework, we first collect an offline meta dataset consisting of pairs of design choices and corresponding congestion measures from various traffic patterns. After collecting the dataset, we employ the Attentive Neural Process (ANP) to predict the impact of the proposed design on congestion across various traffic patterns with well-calibrated uncertainty. Finally, Bayesian optimization, with ANP as a surrogate model, is utilized to find an optimal design for unseen traffic patterns through limited online simulations. Our experiment results show that our method outperforms state-of-the-art baselines on complex road networks in terms of the number of waiting vehicles. Surprisingly, the deployment of our method into a real-world traffic system was able to improve traffic throughput by 4.80\% compared to the original strategy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:57:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07327v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07327v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 LPU: A Latency-Optimized and Highly Scalable Processor for Large
  Language Model Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seungjae Moon, Jung-Hoon Kim, Junsoo Kim, Seongmin Hong, Junseo Cha, Minsu Kim, Sukbin Lim, Gyubin Choi, Dongjin Seo, Jongho Kim, Hunjong Lee, Hyunjun Park, Ryeowook Ko, Soongyu Choi, Jongse Park, Jinwon Lee, Joo-Young Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The explosive arrival of OpenAI's ChatGPT has fueled the globalization of large language model (LLM), which consists of billions of pretrained parameters that embodies the aspects of syntax and semantics. HyperAccel introduces latency processing unit (LPU), a latency-optimized and highly scalable processor architecture for the acceleration of LLM inference. LPU perfectly balances the memory bandwidth and compute logic with streamlined dataflow to maximize performance and efficiency. LPU is equipped with expandable synchronization link (ESL) that hides data synchronization latency between multiple LPUs. HyperDex complements LPU as an intuitive software framework to run LLM applications. LPU achieves 1.25 ms/token and 20.9 ms/token for 1.3B and 66B model, respectively, which is 2.09x and 1.37x faster than the GPU. LPU, synthesized using Samsung 4nm process, has total area of 0.824 mm2 and power consumption of 284.31 mW. LPU-based servers achieve 1.33x and 1.32x energy efficiency over NVIDIA H100 and L4 servers, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:56:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07326v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07326v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 LLM-Enhanced Static Analysis for Precise Identification of Vulnerable
  OSS Versions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiran Cheng, Lwin Khin Shar, Ting Zhang, Shouguo Yang, Chaopeng Dong, David Lo, Shichao Lv, Zhiqiang Shi, Limin Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open-source software (OSS) has experienced a surge in popularity, attributed to its collaborative development model and cost-effective nature. However, the adoption of specific software versions in development projects may introduce security risks when these versions bring along vulnerabilities. Current methods of identifying vulnerable versions typically analyze and trace the code involved in vulnerability patches using static analysis with pre-defined rules. They then use syntactic-level code clone detection to identify the vulnerable versions. These methods are hindered by imprecisions due to (1) the inclusion of vulnerability-irrelevant code in the analysis and (2) the inadequacy of syntactic-level code clone detection. This paper presents Vercation, an approach designed to identify vulnerable versions of OSS written in C/C++. Vercation combines program slicing with a Large Language Model (LLM) to identify vulnerability-relevant code from vulnerability patches. It then backtraces historical commits to gather previous modifications of identified vulnerability-relevant code. We propose semantic-level code clone detection to compare the differences between pre-modification and post-modification code, thereby locating the vulnerability-introducing commit (vic) and enabling to identify the vulnerable versions between the patch commit and the vic. We curate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate Vercation. On this dataset, our approach achieves the F1 score of 92.4%, outperforming current state-of-the-art methods. More importantly, Vercation detected 134 incorrect vulnerable OSS versions in NVD reports.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:43:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07321v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07321v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xianming Li, Julius Lipp, Aamir Shakir, Rui Huang, Jing Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> BM25, a widely-used lexical search algorithm, remains crucial in information retrieval despite the rise of pre-trained and large language models (PLMs/LLMs). However, it neglects query-document similarity and lacks semantic understanding, limiting its performance. We revisit BM25 and introduce BMX, a novel extension of BM25 incorporating entropy-weighted similarity and semantic enhancement techniques. Extensive experiments demonstrate that BMX consistently outperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in long-context and real-world retrieval benchmarks. This study bridges the gap between classical lexical search and modern semantic approaches, offering a promising direction for future information retrieval research. The reference implementation of BMX can be found in Baguetter, which was created in the context of this work. The code can be found here: https://github.com/mixedbread-ai/baguetter.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:18:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06643v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06643v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal
  Data for Mental Health</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongquan Hu, Shuning Zhang, Ting Dang, Hong Jia, Flora D. Salim, Wen Hu, Aaron J. Quigley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. Recent advancements with Large Language Models (LLMs) position them as prospective ``health agents'' for mental health assessment. However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data. Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting. Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text). The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment. Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential. Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T06:14:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3675094.3678494' target='_blank'>doi</a><a href='http://arxiv.org/abs/2408.07313v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07313v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 On a Scale-Invariant Approach to Bundle Recommendations in Candy Crush
  Saga</h2>
                <div class="authors">
                    <strong>Authors:</strong> Styliani Katsarou, Francesca Carminati, Martin Dlask, Marta Braojos, Lavena Patra, Richard Perkins, Carlos Garcia Ling, Maria Paskevich
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A good understanding of player preferences is crucial for increasing content relevancy, especially in mobile games. This paper illustrates the use of attentive models for producing item recommendations in a mobile game scenario. The methodology comprises a combination of supervised and unsupervised approaches to create user-level recommendations while introducing a novel scale-invariant approach to the prediction. The methodology is subsequently applied to a bundle recommendation in Candy Crush Saga. The strategy of deployment, maintenance, and monitoring of ML models that are scaled up to serve millions of users is presented, along with the best practices and design patterns adopted to minimize technical debt typical of ML systems. The recommendation approach is evaluated both offline and online, with a focus on understanding the increase in engagement, click- and take rates, novelty effects, recommendation diversity, and the impact of degenerate feedback loops. We have demonstrated that the recommendation enhances user engagement by 30% concerning click rate and by more than 40% concerning take rate. In addition, we empirically quantify the diminishing effects of recommendation accuracy on user engagement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T05:44:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06799v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06799v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Contextual Distillation Model for Diversified Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fan Li, Xu Si, Shisong Tang, Dingmin Wang, Kunyan Han, Bing Han, Guorui Zhou, Yang Song, Hechang Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The diversity of recommendation is equally crucial as accuracy in improving user experience. Existing studies, e.g., Determinantal Point Process (DPP) and Maximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively select items that optimize both accuracy and diversity. However, prior methods typically exhibit quadratic complexity, limiting their applications to the re-ranking stage and are not applicable to other recommendation stages with a larger pool of candidate items, such as the pre-ranking and ranking stages. In this paper, we propose Contextual Distillation Model (CDM), an efficient recommendation model that addresses diversification, suitable for the deployment in all stages of industrial recommendation pipelines. Specifically, CDM utilizes the candidate items in the same user request as context to enhance the diversification of the results. We propose a contrastive context encoder that employs attention mechanisms to model both positive and negative contexts. For the training of CDM, we compare each target item with its context embedding and utilize the knowledge distillation framework to learn the win probability of each target item under the MMR algorithm, where the teacher is derived from MMR outputs. During inference, ranking is performed through a linear combination of the recommendation and student model scores, ensuring both diversity and efficiency. We perform offline evaluations on two industrial datasets and conduct online A/B test of CDM on the short-video platform KuaiShou. The considerable enhancements observed in both recommendation quality and diversity, as shown by metrics, provide strong superiority for the effectiveness of CDM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T04:52:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3637528.3671514' target='_blank'>doi</a><a href='http://arxiv.org/abs/2406.09021v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.09021v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Evaluating Large Language Model based Personal Information Extraction
  and Countermeasures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yupei Liu, Yuqi Jia, Jinyuan Jia, Neil Zhenqiang Gong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automatically extracting personal information--such as name, phone number, and email address--from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods--such as regular expression, keyword search, and entity detection--achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction attacks; collect three datasets including a synthetic dataset generated by GPT-4 and two real-world datasets with manually labeled 8 categories of personal information; introduce a novel mitigation strategy based on \emph{prompt injection}; and systematically benchmark LLM-based attacks and countermeasures using 10 LLMs and our 3 datasets. Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms conventional methods at such extraction; and prompt injection can mitigate such risk to a large extent and outperforms conventional countermeasures. Our code and data are available at: \url{https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T04:49:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07291v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07291v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 A Modeling Framework for Equitable Deployment of Energy Storage in
  Disadvantaged Communities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Miguel Heleno, Paul Lesur, Alexandre Moreira
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper provides an analytical framework to incorporate the deployment of behind-the-meter energy storage coupled with rooftop solar, and their associated revenue streams, in the context of equitable energy policy interventions. We propose an extension to the Justice40 optimization model by adding storage and incorporating more realistic solar compensation mechanisms, such as net-billing, which allows for temporal revenue differentiation and the economic viability of behind-the-meter energy storage devices. The extended model includes household-level PV plus storage co-deployment alongside existing interventions, such as weatherization, rooftop PV only, community solar, and community wind. From a modeling perspective, we propose a novel approximation method to represent storage operations and revenue streams without expanding the temporal dimension of model, thus maintaining its computational efficiency. The proposed model is validated using a case study in Wayne County, Michigan, involving 3,651 energy insecure households.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T04:36:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.OC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07288v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07288v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 A Semantic Space is Worth 256 Language Descriptions: Make Stronger
  Segmentation Models with Descriptive Properties</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junfei Xiao, Ziqi Zhou, Wenxuan Li, Shiyi Lan, Jieru Mei, Zhiding Yu, Alan Yuille, Yuyin Zhou, Cihang Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces ProLab, a novel approach using property-level label space for creating strong interpretable segmentation models. Instead of relying solely on category-specific annotations, ProLab uses descriptive properties grounded in common sense knowledge for supervising segmentation models. It is based on two core designs. First, we employ Large Language Models (LLMs) and carefully crafted prompts to generate descriptions of all involved categories that carry meaningful common sense knowledge and follow a structured format. Second, we introduce a description embedding model preserving semantic correlation across descriptions and then cluster them into a set of descriptive properties (e.g., 256) using K-Means. These properties are based on interpretable common sense knowledge consistent with theories of human recognition. We empirically show that our approach makes segmentation models perform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal Context, Cityscapes, and BDD). Our method also shows better scalability with extended training steps than category-level supervision. Our interpretable segmentation framework also emerges with the generalization ability to segment out-of-domain or unknown categories using only in-domain descriptive properties. Code is available at https://github.com/lambert-x/ProLab.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T10:03:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.13764v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.13764v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 NL2OR: Solve Complex Operations Research Problems Using Natural Language
  Inputs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junxuan Li, Ryan Wickman, Sahil Bhatnagar, Raj Kumar Maity, Arko Mukherjee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Operations research (OR) uses mathematical models to enhance decision-making, but developing these models requires expert knowledge and can be time-consuming. Automated mathematical programming (AMP) has emerged to simplify this process, but existing systems have limitations. This paper introduces a novel methodology that uses recent advances in Large Language Model (LLM) to create and edit OR solutions from non-expert user queries expressed using Natural Language. This reduces the need for domain expertise and the time to formulate a problem. The paper presents an end-to-end pipeline, named NL2OR, that generates solutions to OR problems from natural language input, and shares experimental results on several important OR problems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T03:42:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07272v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07272v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 V-STaR: Training Verifiers for Self-Taught Reasoners</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Common self-improvement approaches for large language models (LLMs), such as STaR, iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T02:41:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.06457v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.06457v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Camera Perspective Transformation to Bird's Eye View via Spatial
  Transformer Model for Road Intersection Monitoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rukesh Prajapati, Amr S. El-Wakeel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Road intersection monitoring and control research often utilize bird's eye view (BEV) simulators. In real traffic settings, achieving a BEV akin to that in a simulator necessitates the deployment of drones or specific sensor mounting, which is neither feasible nor practical. Consequently, traffic intersection management remains confined to simulation environments given these constraints. In this paper, we address the gap between simulated environments and real-world implementation by introducing a novel deep-learning model that converts a single camera's perspective of a road intersection into a BEV. We created a simulation environment that closely resembles a real-world traffic junction. The proposed model transforms the vehicles into BEV images, facilitating road intersection monitoring and control model processing. Inspired by image transformation techniques, we propose a Spatial-Transformer Double Decoder-UNet (SDD-UNet) model that aims to eliminate the transformed image distortions. In addition, the model accurately estimates the vehicle's positions and enables the direct application of simulation-trained models in real-world contexts. SDD-UNet model achieves an average dice similarity coefficient (DSC) above 95% which is 40% better than the original UNet model. The mean absolute error (MAE) is 0.102 and the centroid of the predicted mask is 0.14 meters displaced, on average, indicating high accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T02:20:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05577v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05577v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Can We Improve Channel Reciprocity via Loop-back Compensation for
  RIS-assisted Physical Layer Key Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ningya Xu, Guoshun Nan, Xiaofeng Tao, Na Li, Pengxuan Mao, Tianyuan Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reconfigurable intelligent surface (RIS) facilitates the extraction of unpredictable channel features for physical layer key generation (PKG), securing communications among legitimate users with symmetric keys. Previous works have demonstrated that channel reciprocity plays a crucial role in generating symmetric keys in PKG systems, whereas, in reality, reciprocity is greatly affected by hardware interference and RIS-based jamming attacks. This motivates us to propose LoCKey, a novel approach that aims to improve channel reciprocity by mitigating interferences and attacks with a loop-back compensation scheme, thus maximizing the secrecy performance of the PKG system. Specifically, our proposed LoCKey is capable of effectively compensating for the CSI non-reciprocity by the combination of transmit-back signal value and error minimization module. Firstly, we introduce the entire flowchart of our method and provide an in-depth discussion of each step. Following that, we delve into a theoretical analysis of the performance optimizations when our LoCKey is applied for CSI reciprocity enhancement. Finally, we conduct experiments to verify the effectiveness of the proposed LoCKey in improving channel reciprocity under various interferences for RIS-assisted wireless communications. The results demonstrate a significant improvement in both the rate of key generation assisted by the RIS and the consistency of the generated keys, showing great potential for the practical deployment of our LoCKey in future wireless systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T01:55:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.17575v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.17575v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 OpenEP: Open-Ended Future Event Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yong Guan, Hao Peng, Xiaozhi Wang, Lei Hou, Juanzi Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Future event prediction (FEP) is a long-standing and crucial task in the world, as understanding the evolution of events enables early risk identification, informed decision-making, and strategic planning. Existing work typically treats event prediction as classification tasks and confines the outcomes of future events to a fixed scope, such as yes/no questions, candidate set, and taxonomy, which is difficult to include all possible outcomes of future events. In this paper, we introduce OpenEP (an Open-Ended Future Event Prediction task), which generates flexible and diverse predictions aligned with real-world scenarios. This is mainly reflected in two aspects: firstly, the predictive questions are diverse, covering different stages of event development and perspectives; secondly, the outcomes are flexible, without constraints on scope or format. To facilitate the study of this task, we construct OpenEPBench, an open-ended future event prediction dataset. For question construction, we pose questions from seven perspectives, including location, time, event development, event outcome, event impact, event response, and other, to facilitate an in-depth analysis and understanding of the comprehensive evolution of events. For outcome construction, we collect free-form text containing the outcomes as ground truth to provide semantically complete and detail-enriched outcomes. Furthermore, we propose StkFEP, a stakeholder-enhanced future event prediction framework, that incorporates event characteristics for open-ended settings. Our method extracts stakeholders involved in events to extend questions to gather diverse information. We also collect historically events that are relevant and similar to the question to reveal potential evolutionary patterns. Experiment results indicate that accurately predicting future events in open-ended settings is challenging for existing LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T01:37:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06578v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06578v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 GQE: Generalized Query Expansion for Enhanced Text-Video Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zechen Bai, Tianjun Xiao, Tong He, Pichao Wang, Zheng Zhang, Thomas Brox, Mike Zheng Shou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the rapidly expanding domain of web video content, the task of text-video retrieval has become increasingly critical, bridging the semantic gap between textual queries and video data. This paper introduces a novel data-centric approach, Generalized Query Expansion (GQE), to address the inherent information imbalance between text and video, enhancing the effectiveness of text-video retrieval systems. Unlike traditional model-centric methods that focus on designing intricate cross-modal interaction mechanisms, GQE aims to expand the text queries associated with videos both during training and testing phases. By adaptively segmenting videos into short clips and employing zero-shot captioning, GQE enriches the training dataset with comprehensive scene descriptions, effectively bridging the data imbalance gap. Furthermore, during retrieval, GQE utilizes Large Language Models (LLM) to generate a diverse set of queries and a query selection module to filter these queries based on relevance and diversity, thus optimizing retrieval performance while reducing computational overhead. Our contributions include a detailed examination of the information imbalance challenge, a novel approach to query expansion in video-text datasets, and the introduction of a query selection strategy that enhances retrieval accuracy without increasing computational costs. GQE achieves state-of-the-art performance on several benchmarks, including MSR-VTT, MSVD, LSMDC, and VATEX, demonstrating the effectiveness of addressing text-video retrieval from a data-centric perspective.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T01:24:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07249v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07249v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Seeing and Understanding: Bridging Vision with Chemical Knowledge Via
  ChemVLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Weiyun Wang, Zhe Chen, Wenhai Wang, Wei Li, Shufei Zhang, Mao Su, Wanli Ouyang, Yuqiang Li, Dongzhan Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this technical report, we propose ChemVLM, the first open-source multimodal large language model dedicated to the fields of chemistry, designed to address the incompatibility between chemical image understanding and text analysis. Built upon the VIT-MLP-LLM architecture, we leverage ChemLLM-20B as the foundational large model, endowing our model with robust capabilities in understanding and utilizing chemical text knowledge. Additionally, we employ InternVIT-6B as a powerful image encoder. We have curated high-quality data from the chemical domain, including molecules, reaction formulas, and chemistry examination data, and compiled these into a bilingual multimodal question-answering dataset. We test the performance of our model on multiple open-source benchmarks and three custom evaluation sets. Experimental results demonstrate that our model achieves excellent performance, securing state-of-the-art results in five out of six involved tasks. Our model can be found at https://huggingface.co/AI4Chem/ChemVLM-26B.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T01:16:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07246v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07246v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 GPTVoiceTasker: Advancing Multi-step Mobile Task Efficiency Through
  Dynamic Interface Exploration and Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minh Duc Vu, Han Wang, Zhuang Li, Jieshan Chen, Shengdong Zhao, Zhenchang Xing, Chunyang Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Virtual assistants have the potential to play an important role in helping users achieves different tasks. However, these systems face challenges in their real-world usability, characterized by inefficiency and struggles in grasping user intentions. Leveraging recent advances in Large Language Models (LLMs), we introduce GptVoiceTasker, a virtual assistant poised to enhance user experiences and task efficiency on mobile devices. GptVoiceTasker excels at intelligently deciphering user commands and executing relevant device interactions to streamline task completion. The system continually learns from historical user commands to automate subsequent usages, further enhancing execution efficiency. Our experiments affirm GptVoiceTasker's exceptional command interpretation abilities and the precision of its task automation module. In our user study, GptVoiceTasker boosted task efficiency in real-world scenarios by 34.85%, accompanied by positive participant feedback. We made GptVoiceTasker open-source, inviting further research into LLMs utilization for diverse tasks through prompt engineering and leveraging user usage data to improve efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T00:48:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3654777.3676356' target='_blank'>doi</a><a href='http://arxiv.org/abs/2401.14268v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.14268v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge
  Distillation Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tong Wang, K. Sudhir, Dat Hong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior performance in complex human-like interactions. But they are costly, or too large for edge devices such as smartphones and harder to self-host, leading to security and privacy concerns. This paper introduces a novel interpretable knowledge distillation approach to enhance the performance of smaller, more economical LLMs that firms can self-host. We study this problem in the context of building a customer service agent aimed at achieving high customer satisfaction through goal-oriented dialogues. Unlike traditional knowledge distillation, where the "student" model learns directly from the "teacher" model's responses via fine-tuning, our interpretable "strategy" teaching approach involves the teacher providing strategies to improve the student's performance in various scenarios. This method alternates between a "scenario generation" step and a "strategies for improvement" step, creating a customized library of scenarios and optimized strategies for automated prompting. The method requires only black-box access to both student and teacher models; hence it can be used without manipulating model parameters. In our customer service application, the method improves performance, and the learned strategies are transferable to other LLMs and scenarios beyond the training set. The method's interpretabilty helps safeguard against potential harms through human audit.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T23:59:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07238v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07238v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Neural embedding of beliefs reveals the role of relative dissonance in
  human decision-making</h2>
                <div class="authors">
                    <strong>Authors:</strong> Byunghwee Lee, Rachith Aiyappa, Yong-Yeol Ahn, Haewoon Kwak, Jisun An
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Beliefs serve as the foundation for human cognition and decision-making. They guide individuals in deriving meaning from their lives, shaping their behaviors, and forming social connections. Therefore, a model that encapsulates beliefs and their interrelationships is crucial for quantitatively studying the influence of beliefs on our actions. Despite its importance, research on the interplay between human beliefs has often been limited to a small set of beliefs pertaining to specific issues, with a heavy reliance on surveys or experiments. Here, we propose a method for extracting nuanced relations between thousands of beliefs by leveraging large-scale user participation data from an online debate platform and mapping these beliefs to an embedding space using a fine-tuned large language model (LLM). This belief embedding space effectively encapsulates the interconnectedness of diverse beliefs as well as polarization across various social issues. We discover that the positions within this belief space predict new beliefs of individuals. Furthermore, we find that the relative distance between one's existing beliefs and new beliefs can serve as a quantitative estimate of cognitive dissonance, allowing us to predict new beliefs. Our study highlights how modern LLMs, when combined with collective online records of human beliefs, can offer insights into the fundamental principles that govern human belief formation and decision-making processes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T23:58:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CY</span><span>physics.soc-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07237v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07237v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset</h2>
                <div class="authors">
                    <strong>Authors:</strong> Peitong Duan, Chin-yi Chen, Gang Li, Bjoern Hartmann, Yang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automated UI evaluation can be beneficial for the design process; for example, to compare different UI designs, or conduct automated heuristic evaluation. LLM-based UI evaluation, in particular, holds the promise of generalizability to a wide variety of UI types and evaluation tasks. However, current LLM-based techniques do not yet match the performance of human evaluators. We hypothesize that automatic evaluation can be improved by collecting a targeted UI feedback dataset and then using this dataset to enhance the performance of general-purpose LLMs. We present a targeted dataset of 3,059 design critiques and quality ratings for 983 mobile UIs, collected from seven experienced designers. We carried out an in-depth analysis to characterize the dataset's features. We then applied this dataset to achieve a 55% performance gain in LLM-generated UI feedback via various few-shot and visual prompting techniques. We also discuss future applications of this dataset, including training a reward model for generative UI techniques, and fine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T23:41:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.08850v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.08850v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 ConfusedPilot: Confused Deputy Risks in RAG-based LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.   In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T05:24:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.04870v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.04870v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Training LLMs over Neurally Compressed Text</h2>
                <div class="authors">
                    <strong>Authors:</strong> Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T22:01:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.03626v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.03626v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Can Large Language Models Reason? A Characterization via 3-SAT</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rishi Hazra, Gabriele Venturato, Pedro Zuidberg Dos Martires, Luc De Raedt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are said to possess advanced reasoning abilities. However, some skepticism exists as recent works show how LLMs often bypass true reasoning using shortcuts. Current methods for assessing the reasoning abilities of LLMs typically rely on open-source benchmarks that may be overrepresented in LLM training data, potentially skewing performance. We instead provide a computational theory perspective of reasoning, using 3-SAT -- the prototypical NP-complete problem that lies at the core of logical reasoning and constraint satisfaction tasks. By examining the phase transitions in 3-SAT, we empirically characterize the reasoning abilities of LLMs and show how they vary with the inherent hardness of the problems. Our experimental evidence shows that LLMs cannot perform true reasoning, as is required for solving 3-SAT problems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T21:54:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07215v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07215v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Time series forecasting with high stakes: A field study of the air cargo
  industry</h2>
                <div class="authors">
                    <strong>Authors:</strong> Abhinav Garg, Naman Shukla, Maarten Wormer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Time series forecasting in the air cargo industry presents unique challenges due to volatile market dynamics and the significant impact of accurate forecasts on generated revenue. This paper explores a comprehensive approach to demand forecasting at the origin-destination (O\&D) level, focusing on the development and implementation of machine learning models in decision-making for the air cargo industry. We leverage a mixture of experts framework, combining statistical and advanced deep learning models to provide reliable forecasts for cargo demand over a six-month horizon. The results demonstrate that our approach outperforms industry benchmarks, offering actionable insights for cargo capacity allocation and strategic decision-making in the air cargo industry. While this work is applied in the airline industry, the methodology is broadly applicable to any field where forecast-based decision-making in a volatile environment is crucial.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T21:40:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.20192v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.20192v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, Rafael Rafailov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this ga-through supervised fine-tuning on curated expert demonstrations-often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. We validate our approach in the WebShop environment-a simulated e-commerce platform where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, our methodology boosts Llama-3 70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search. We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T20:52:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07199v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07199v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 DataNarrative: Automated Data-Driven Storytelling with Visualizations
  and Texts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Data-driven storytelling is a powerful method for conveying insights by combining narrative techniques with visualizations and text. These stories integrate visual aids, such as highlighted bars and lines in charts, along with textual annotations explaining insights. However, creating such stories requires a deep understanding of the data and meticulous narrative planning, often necessitating human intervention, which can be time-consuming and mentally taxing. While Large Language Models (LLMs) excel in various NLP tasks, their ability to generate coherent and comprehensive data stories remains underexplored. In this work, we introduce a novel task for data story generation and a benchmark containing 1,449 stories from diverse sources. To address the challenges of crafting coherent data stories, we propose a multiagent framework employing two LLM agents designed to replicate the human storytelling process: one for understanding and describing the data (Reflection), generating the outline, and narration, and another for verification at each intermediary step. While our agentic framework generally outperforms non-agentic counterparts in both model-based and human evaluations, the results also reveal unique challenges in data story generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T20:46:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05346v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05346v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Upol Ehsan, Mark O. Riedl
                </div>
                <div class="summary">
                    <strong>Summary:</strong> When the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial) "black-box" of AI so that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open the black-box is increasingly limited especially when it comes to non-AI expert end-users. In this paper, we challenge the assumption of "opening" the black-box in the LLM era and argue for a shift in our XAI expectations. Highlighting the epistemic blind spots of an algorithm-centered XAI view, we argue that a human-centered perspective can be a path forward. We operationalize the argument by synthesizing XAI research along three dimensions: explainability outside the black-box, explainability around the edges of the black box, and explainability that leverages infrastructural seams. We conclude with takeaways that reflexively inform XAI as a domain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T19:39:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3686169.3686185' target='_blank'>doi</a><a href='http://arxiv.org/abs/2408.05345v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05345v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop
  Question Answering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, Ninghao Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions, since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that traditional similarity-based searches might miss. In addition, our framework includes a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hallucination problem. Our framework is supported by theoretical justification for its fact retrieval efficacy. Finally, comprehensive evaluation across various LLMs validates RAE's ability in providing accurate answers with updated knowledge. Our code is available at: https://github.com/sycny/RAE.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T19:34:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.19631v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.19631v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Recent Advances in Generative AI and Large Language Models: Current
  Status, Challenges, and Perspectives</h2>
                <div class="authors">
                    <strong>Authors:</strong> Desta Haileselassie Hagos, Rick Battle, Danda B. Rawat
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The emergence of Generative Artificial Intelligence (AI) and Large Language Models (LLMs) has marked a new era of Natural Language Processing (NLP), introducing unprecedented capabilities that are revolutionizing various domains. This paper explores the current state of these cutting-edge technologies, demonstrating their remarkable advancements and wide-ranging applications. Our paper contributes to providing a holistic perspective on the technical foundations, practical applications, and emerging challenges within the evolving landscape of Generative AI and LLMs. We believe that understanding the generative capabilities of AI systems and the specific context of LLMs is crucial for researchers, practitioners, and policymakers to collaboratively shape the responsible and ethical integration of these technologies into various domains. Furthermore, we identify and address main research gaps, providing valuable insights to guide future research endeavors within the AI research community.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T19:17:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.14962v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.14962v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Large Model Strategic Thinking, Small Model Efficiency: Transferring
  Theory of Mind in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nunzio Lore, Alireza Sepehr Ilami, Babak Heydari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As the performance of larger, newer Large Language Models continues to improve for strategic Theory of Mind (ToM) tasks, the demand for these state of the art models increases commensurately. However, their deployment is costly both in terms of processing power and time. In this paper, we investigate the feasibility of creating smaller, simulation-ready agents by way of fine-tuning. To do this, we present a large pre-trained model with 20 unique scenarios that combine a social context with a social dilemma, recording its answers, and using them for Q\&A fine-tuning on a smaller model of the same family. Our focus is on in-context game-theoretic decision-making, the same domain within which human interaction occurs and that requires both a theory of mind (or a semblance thereof) and an understanding of social dynamics. We find that the fine-tuned smaller language model exhibited significant performance closer to that of its larger relative, and that their improvements extended in areas and contexts beyond the ones provided in the training examples. On average for all games, through fine-tuning, the smaller model showed a \%46 improvement in aligning with the behavior of the larger model, with \%100 representing complete alignment. This suggests that our pipeline represents an efficient method to transmit some form of theory of mind to smaller models, creating improved and cheaply deployable algorithms in the process. Despite their simplicity and their associated shortcomings and limitations, our findings represent a stepping stone in the pursuit and training of specialized models for strategic and social decision making.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T19:09:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>cs.ET</span><span>cs.GT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05241v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05241v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Never Lost in the Middle: Mastering Long-Context Question Answering with
  Position-Agnostic Decompositional Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Qianguo Sun, Yuxin Liang, Hao Wang, Enming Zhang, Jiaxing Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The "lost in the middle" problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Attention Strengthening Multi-doc QA (ASM QA). Following these tasks, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7% absolute gain in shuffled settings, by 21.5% in passage retrieval task. We release our model, Ziya-Reader to promote related research in the community.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T19:04:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2311.09198v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2311.09198v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Control-Flow Attestation: Concepts, Solutions, and Open Challenges</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhanyu Sha, Carlton Shepherd, Amir Rafi, Konstantinos Markantonakis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Control-flow attestation unifies the worlds of control-flow integrity and platform attestation by measuring and reporting a target's run-time behaviour to a verifier. Trust assurances in the target are provided by testing whether its execution follows an authorised control-flow path. The problem has been explored in various settings, such as assessing the trustworthiness of cyber-physical systems, Internet of Things devices, cloud platforms, and many others. Despite a significant number of proposals being made in recent years, the area remains fragmented, addressing different adversarial behaviours, verification paradigms, and deployment challenges. In this paper, we present the first survey of control-flow attestation, examining the core ideas and solutions in state-of-the-art schemes. In total, we survey over 30 papers published between 2016-2024, consolidate and compare their key features, and pose several challenges and recommendations for future research in the area.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T18:56:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AR</span><span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06304v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06304v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal
  Advice</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yutong Hu, Kangcheng Luo, Yansong Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite remarkable performance in legal consultation exhibited by legal Large Language Models(LLMs) combined with legal article retrieval components, there are still cases when the advice given is incorrect or baseless. To alleviate these problems, we propose {\bf ELLA}, a tool for {\bf E}mpowering {\bf L}LMs for interpretable, accurate, and informative {\bf L}egal {\bf A}dvice. ELLA visually presents the correlation between legal articles and LLM's response by calculating their similarities, providing users with an intuitive legal basis for the responses. Besides, based on the users' queries, ELLA retrieves relevant legal articles and displays them to users. Users can interactively select legal articles for LLM to generate more accurate responses. ELLA also retrieves relevant legal cases for user reference. Our user study shows that presenting the legal basis for the response helps users understand better. The accuracy of LLM's responses also improves when users intervene in selecting legal articles for LLM. Providing relevant legal cases also aids individuals in obtaining comprehensive information.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T18:12:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07137v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07137v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Better Alignment with Instruction Back-and-Forth Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thao Nguyen, Jeffrey Li, Sewoong Oh, Ludwig Schmidt, Jason Weston, Luke Zettlemoyer, Xian Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose a new method, instruction back-and-forth translation, to construct high-quality synthetic data grounded in world knowledge for aligning large language models (LLMs). Given documents from a web corpus, we generate and curate synthetic instructions using the backtranslation approach proposed by Li et al.(2023a), and rewrite the responses to improve their quality further based on the initial documents. Fine-tuning with the resulting (backtranslated instruction, rewritten response) pairs yields higher win rates on AlpacaEval than using other common instruction datasets such as Humpback, ShareGPT, Open Orca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the responses with an LLM outperforms direct distillation, and the two generated text distributions exhibit significant distinction in embedding space. Further analysis shows that our backtranslated instructions are of higher quality than other sources of synthetic instructions, while our responses are more diverse and complex than those obtained from distillation. Overall we find that instruction back-and-forth translation combines the best of both worlds -- making use of the information diversity and quantity found on the web, while ensuring the quality of the responses which is necessary for effective alignment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T18:00:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.04614v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.04614v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Diversity Empowers Intelligence: Integrating Expertise of Software
  Engineering Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kexun Zhang, Weiran Yao, Zuxin Liu, Yihao Feng, Zhiwei Liu, Rithesh Murthy, Tian Lan, Lei Li, Renze Lou, Jiacheng Xu, Bo Pang, Yingbo Zhou, Shelby Heinecke, Silvio Savarese, Huan Wang, Caiming Xiong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite. However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others. To fully harness the diversity of these agents, we propose DEI (Diversity Empowered Intelligence), a framework that leverages their unique expertise. DEI functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving. Experimental results show that a DEI-guided committee of agents is able to surpass the best individual agent's performance by a large margin. For instance, a group of open-source SWE agents, with a maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3% resolve rate with DEI, making a 25% improvement and beating most closed-source solutions. Our best-performing group excels with a 55% resolve rate, securing the highest ranking on SWE-Bench Lite. Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T17:50:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07060v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07060v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words. Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has seen during supervised fine-tuning (SFT). In other words, their output limitation is due to the scarcity of long-output examples in existing SFT datasets. To address this, we introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we construct LongWriter-6k, a dataset containing 6,000 SFT data with output lengths ranging from 2k to 32k words. By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10,000 words while maintaining output quality. We also develop LongBench-Write, a comprehensive benchmark for evaluating ultra-long generation capabilities. Our 9B parameter model, further improved through DPO, achieves state-of-the-art performance on this benchmark, surpassing even much larger proprietary models. In general, our work demonstrates that existing long context LLM already possesses the potential for a larger output window--all you need is data with extended output during model alignment to unlock this capability. Our code & models are at: https://github.com/THUDM/LongWriter.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T17:46:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07055v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07055v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Gromov-Wasserstein unsupervised alignment reveals structural
  correspondences between the color similarity structures of humans and large
  language models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Genji Kawakita, Ariel Zeleznikow-Johnston, Naotsugu Tsuchiya, Masafumi Oizumi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs), such as the General Pre-trained Transformer (GPT), have shown remarkable performance in various cognitive tasks. However, it remains unclear whether these models have the ability to accurately infer human perceptual representations. Previous research has addressed this question by quantifying correlations between similarity response patterns of humans and LLMs. Correlation provides a measure of similarity, but it relies pre-defined item labels and does not distinguish category- and item- level similarity, falling short of characterizing detailed structural correspondence between humans and LLMs. To assess their structural equivalence in more detail, we propose the use of an unsupervised alignment method based on Gromov-Wasserstein optimal transport (GWOT). GWOT allows for the comparison of similarity structures without relying on pre-defined label correspondences and can reveal fine-grained structural similarities and differences that may not be detected by simple correlation analysis. Using a large dataset of similarity judgments of 93 colors, we compared the color similarity structures of humans (color-neurotypical and color-atypical participants) and two GPT models (GPT-3.5 and GPT-4). Our results show that the similarity structure of color-neurotypical participants can be remarkably well aligned with that of GPT-4 and, to a lesser extent, to that of GPT-3.5. These results contribute to the methodological advancements of comparing LLMs with human perception, and highlight the potential of unsupervised alignment methods to reveal detailed structural correspondences. This work has been published in Scientific Reports, DOI: https://doi.org/10.1038/s41598-024-65604-1.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T17:11:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.NC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1038/s41598-024-65604-1' target='_blank'>doi</a><a href='http://dx.doi.org/10.1038/s41598-024-65604-1' target='_blank'>doi</a><a href='http://dx.doi.org/' target='_blank'>doi</a><a href='http://dx.doi.org/' target='_blank'>doi</a><a href='http://dx.doi.org/10.1038/s41598-024-65604-1' target='_blank'>doi</a><a href='http://arxiv.org/abs/2308.04381v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2308.04381v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Casper: Prompt Sanitization for Protecting User Privacy in Web-Based
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chun Jie Chong, Chenxi Hou, Zhihao Yao, Seyed Mohammadjavad Seyed Talebi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience. Third-party plugins enhance the functionalities of LLM by enabling access to real-world data and services. However, the privacy consequences associated with these services and their third-party plugins are not well understood. Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins. In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and removing sensitive information from user inputs before sending them to LLM services. Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services. At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier. We evaluate Casper on a dataset of 4000 synthesized prompts and show that it can effectively filter out Personal Identifiable Information (PII) and privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T16:08:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07004v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07004v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Generative AI for automatic topic labelling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Diego Kozlowski, Carolina Pradier, Pierre Benz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Topic Modeling has become a prominent tool for the study of scientific fields, as they allow for a large scale interpretation of research trends. Nevertheless, the output of these models is structured as a list of keywords which requires a manual interpretation for the labelling. This paper proposes to assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini for topic labelling. Drawing on previous research leveraging BERTopic, we generate topics from a dataset of all the scientific articles (n=34,797) authored by all biology professors in Switzerland (n=465) between 2008 and 2020, as recorded in the Web of Science database. We assess the output of the three models both quantitatively and qualitatively and find that, first, both GPT models are capable of accurately and precisely label topics from the models' output keywords. Second, 3-word labels are preferable to grasp the complexity of research topics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T16:07:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07003v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07003v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 LLMs can Schedule</h2>
                <div class="authors">
                    <strong>Authors:</strong> Henrik Abgaryan, Ararat Harutyunyan, Tristan Cazenave
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The job shop scheduling problem (JSSP) remains a significant hurdle in optimizing production processes. This challenge involves efficiently allocating jobs to a limited number of machines while minimizing factors like total processing time or job delays. While recent advancements in artificial intelligence have yielded promising solutions, such as reinforcement learning and graph neural networks, this paper explores the potential of Large Language Models (LLMs) for JSSP. We introduce the very first supervised 120k dataset specifically designed to train LLMs for JSSP. Surprisingly, our findings demonstrate that LLM-based scheduling can achieve performance comparable to other neural approaches. Furthermore, we propose a sampling method that enhances the effectiveness of LLMs in tackling JSSP.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T15:53:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06993v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06993v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 NEO-BENCH: Evaluating Robustness of Large Language Models with
  Neologisms</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Zheng, Alan Ritter, Wei Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T15:20:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.12261v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.12261v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 PayOff: A Regulated Central Bank Digital Currency with Private Offline
  Payments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Carolin Beer, Sheila Zingg, Kari Kostiainen, Karl Wüst, Vedran Capkun, Srdjan Capkun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The European Central Bank is preparing for the potential issuance of a central bank digital currency (CBDC), called the digital euro. A recent regulatory proposal by the European Commission defines several requirements for the digital euro, such as support for both online and offline payments. Offline payments are expected to enable cash-like privacy, local payment settlement, and the enforcement of holding limits. While other central banks have expressed similar desired functionality, achieving such offline payments poses a novel technical challenge. We observe that none of the existing research solutions, including offline E-cash schemes, are fully compliant. Proposed solutions based on secure elements offer no guarantees in case of compromise and can therefore lead to significant payment fraud.   The main contribution of this paper is PayOff, a novel CBDC design motivated by the digital euro regulation, which focuses on offline payments. We analyze the security implications of local payment settlement and identify new security objectives. PayOff protects user privacy, supports complex regulations such as holding limits, and implements safeguards to increase robustness against secure element failure. Our analysis shows that PayOff provides strong privacy and identifies residual leakages that may arise in real-world deployments. Our evaluation shows that offline payments can be fast and that the central bank can handle high payment loads with moderate computing resources. However, the main limitation of PayOff is that offline payment messages and storage requirements grow in the number of payments that the sender makes or receives without going online in between.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T15:15:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06956v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06956v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for
  Spatial Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hartwig H. Hochmair, Levente Juhasz, Takoda Kemp
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative AI including large language models (LLMs) has recently gained significant interest in the geo-science community through its versatile task-solving capabilities including programming, arithmetic reasoning, generation of sample data, time-series forecasting, toponym recognition, or image classification. Most existing performance assessments of LLMs for spatial tasks have primarily focused on ChatGPT, whereas other chatbots received less attention. To narrow this research gap, this study conducts a zero-shot correctness evaluation for a set of 76 spatial tasks across seven task categories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini, Claude-3, and Copilot. The chatbots generally performed well on tasks related to spatial literacy, GIS theory, and interpretation of programming code and functions, but revealed weaknesses in mapping, code writing, and spatial reasoning. Furthermore, there was a significant difference in correctness of results between the four chatbots. Responses from repeated tasks assigned to each chatbot showed a high level of consistency in responses with matching rates of over 80% for most task categories in the four chatbots.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T15:02:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1111/tgis.13233' target='_blank'>doi</a><a href='http://arxiv.org/abs/2401.02404v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.02404v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 OpenResearcher: Unleashing AI for Accelerated Scientific Research</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, Yang Xu, Qingkai Min, Zizhao Zhang, Yiwen Wang, Wenjie Li, Pengfei Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid growth of scientific literature imposes significant challenges for researchers endeavoring to stay updated with the latest advancements in their fields and delve into new areas. We introduce OpenResearcher, an innovative platform that leverages Artificial Intelligence (AI) techniques to accelerate the research process by answering diverse questions from researchers. OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to integrate Large Language Models (LLMs) with up-to-date, domain-specific knowledge. Moreover, we develop various tools for OpenResearcher to understand researchers' queries, search from the scientific literature, filter retrieved information, provide accurate and comprehensive answers, and self-refine these answers. OpenResearcher can flexibly use these tools to balance efficiency and effectiveness. As a result, OpenResearcher enables researchers to save time and increase their potential to discover new insights and drive scientific breakthroughs. Demo, video, and code are available at: https://github.com/GAIR-NLP/OpenResearcher.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T14:59:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06941v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06941v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 FuxiTranyu: A Multilingual Large Language Model Trained with Balanced
  Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Du, Yikun Lei, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, Deyi Xiong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated prowess in a wide range of tasks. However, many LLMs exhibit significant performance discrepancies between high- and low-resource languages. To mitigate this challenge, we present FuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the need of the research community for balanced and high-performing multilingual capabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is trained from scratch on a meticulously balanced multilingual data repository that contains 600 billion tokens covering 43 natural languages and 16 programming languages. In addition to the base model, we also develop two instruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse multilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined with DPO on a preference dataset for enhanced alignment ability. Extensive experiments on a wide range of multilingual benchmarks demonstrate the competitive performance of FuxiTranyu against existing multilingual LLMs, e.g., BLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability analyses at both the neuron and representation level suggest that FuxiTranyu is able to learn consistent multilingual representations across different languages. To promote further research into multilingual LLMs and their working mechanisms, we release both the base and instruction-tuned FuxiTranyu models together with 58 pretraining checkpoints at HuggingFace and Github.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T14:57:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06273v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06273v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Evaluating Cultural Adaptability of a Large Language Model via
  Simulation of Synthetic Personas</h2>
                <div class="authors">
                    <strong>Authors:</strong> Louis Kwok, Michal Bravansky, Lewis D. Griffin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users' diverse cultural backgrounds. We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment. Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits. Our analysis shows that specifying a person's country of residence improves GPT-3.5's alignment with their responses. In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance. These findings suggest that while direct nationality information enhances the model's cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model's effectiveness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T14:32:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06929v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06929v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 SceneGPT: A Language Model for 3D Scene Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shivam Chandhok
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Building models that can understand and reason about 3D scenes is difficult owing to the lack of data sources for 3D supervised training and large-scale training regimes. In this work we ask - How can the knowledge in a pre-trained language model be leveraged for 3D scene understanding without any 3D pre-training. The aim of this work is to establish whether pre-trained LLMs possess priors/knowledge required for reasoning in 3D space and how can we prompt them such that they can be used for general purpose spatial reasoning and object understanding in 3D. To this end, we present SceneGPT, an LLM based scene understanding system which can perform 3D spatial reasoning without training or explicit 3D supervision. The key components of our framework are - 1) a 3D scene graph, that serves as scene representation, encoding the objects in the scene and their spatial relationships 2) a pre-trained LLM that can be adapted with in context learning for 3D spatial reasoning. We evaluate our framework qualitatively on object and scene understanding tasks including object semantics, physical properties and affordances (object-level) and spatial understanding (scene-level).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T14:26:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06926v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06926v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge
  Perspectives</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhihu Wang, Shiwan Zhao, Yu Wang, Heyuan Huang, Jiaxin Shi, Sitao Xie, Zhixing Wang, Yubo Zhang, Hongyan Li, Junchi Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) continue to scale, their enhanced performance often proves insufficient for solving domain-specific tasks. Systematically analyzing their failures and effectively enhancing their performance remain significant challenges. This paper introduces the Re-TASK framework, a novel theoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge perspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space Theory. The Re-TASK framework provides a systematic methodology to deepen our understanding, evaluation, and enhancement of LLMs for domain-specific tasks. It explores the interplay among an LLM's capabilities, the knowledge it processes, and the skills it applies, elucidating how these elements are interconnected and impact task performance. Our application of the Re-TASK framework reveals that many failures in domain-specific tasks can be attributed to insufficient knowledge or inadequate skill adaptation. With this insight, we propose structured strategies for enhancing LLMs through targeted knowledge injection and skill adaptation. Specifically, we identify key capability items associated with tasks and employ a deliberately designed prompting strategy to enhance task performance, thereby reducing the need for extensive fine-tuning. Alternatively, we fine-tune the LLM using capability-specific instructions, further validating the efficacy of our framework. Experimental results confirm the framework's effectiveness, demonstrating substantial improvements in both the performance and applicability of LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T13:58:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06904v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06904v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shi Lin, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid development of Large Language Models (LLMs) has brought remarkable generative capabilities across diverse tasks. However, despite the impressive achievements, these LLMs still have numerous inherent vulnerabilities, particularly when faced with jailbreak attacks. By investigating jailbreak attacks, we can uncover hidden weaknesses in LLMs and inform the development of more robust defense mechanisms to fortify their security. In this paper, we further explore the boundary of jailbreak attacks on LLMs and propose Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes advantage of LLMs' growing analyzing and reasoning capability and reveals their underlying vulnerabilities when facing analyzing-based tasks. We conduct a detailed evaluation of ABJ across various open-source and closed-source LLMs, which achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE) on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and efficiency. Our research highlights the importance of prioritizing and enhancing the safety of LLMs to mitigate the risks of misuse. The code is publicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This paper contains examples of LLMs that might be offensive or harmful.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T13:46:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.16205v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.16205v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Leveraging Language Models for Emotion and Behavior Analysis in
  Education</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaito Tanaka, Benjamin Tan, Brian Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The analysis of students' emotions and behaviors is crucial for enhancing learning outcomes and personalizing educational experiences. Traditional methods often rely on intrusive visual and physiological data collection, posing privacy concerns and scalability issues. This paper proposes a novel method leveraging large language models (LLMs) and prompt engineering to analyze textual data from students. Our approach utilizes tailored prompts to guide LLMs in detecting emotional and engagement states, providing a non-intrusive and scalable solution. We conducted experiments using Qwen, ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and chain-of-thought (CoT) prompting. Results demonstrate that our method significantly outperforms the baselines in both accuracy and contextual understanding. This study highlights the potential of LLMs combined with prompt engineering to offer practical and effective tools for educational emotion and behavior analysis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T13:11:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06874v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06874v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling
  for LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Quandong Wang, Yuxuan Yuan, Xiaoyu Yang, Ruike Zhang, Kang Zhao, Wei Liu, Jian Luan, Daniel Povey, Bin Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Large Language Models (LLMs) have achieved remarkable success in various fields, the efficiency of training and inference remains a major challenge. To address this issue, we propose SUBLLM, short for Subsampling-Upsampling-Bypass Large Language Model, an innovative architecture that extends the core decoder-only framework by incorporating subsampling, upsampling, and bypass modules. The subsampling modules are responsible for shortening the sequence, while the upsampling modules restore the sequence length, and the bypass modules enhance convergence. In comparison to LLaMA, the proposed SUBLLM exhibits significant enhancements in both training and inference speeds as well as memory usage, while maintaining competitive few-shot performance. During training, SUBLLM increases speeds by 26% and cuts memory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces memory by 1GB per GPU. The training and inference speeds can be enhanced by 34% and 52% respectively when the context window is expanded to 8192. Our code is available at https://github.com/XiaoMi/subllm.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T12:49:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.06571v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.06571v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Yuan, Dongya Jia, Xiaobin Zhuang, Yuanzhe Chen, Zhengxi Liu, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xubo Liu, Xiyuan Kang, Mark D. Plumbley, Wenwu Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative models have shown significant achievements in audio generation tasks. However, existing models struggle with complex and detailed prompts, leading to potential performance degradation. We hypothesize that this problem stems from the simplicity and scarcity of the training data. This work aims to create a large-scale audio dataset with rich captions for improving audio generation models. We first develop an automated pipeline to generate detailed captions by transforming predicted visual captions, audio captions, and tagging labels into comprehensive descriptions using a Large Language Model (LLM). The resulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption pairs with enriched details including audio event orders, occurred places and environment information. We then demonstrate that training the text-to-audio generation models with Sound-VECaps significantly improves the performance on complex prompts. Furthermore, we conduct ablation studies of the models on several downstream audio-language tasks, showing the potential of Sound-VECaps in advancing audio-text representation learning. Our dataset and models are available online.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T21:30:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.MM</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.04416v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.04416v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jia-Chen Zhang, Yu-Jie Xiong, He-Xi Qiu, Dong-Hai Zhu, Chun-Ming Xia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-tuning large language models (LLMs) with high parameter efficiency for downstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA) significantly reduces the number of trainable parameters for fine-tuning. Although it has demonstrated commendable performance, updating parameters within a single scale may not be the optimal choice for complex downstream tasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$. We first combine orthogonal projection theory to train a set of LoRAs in two mutually orthogonal planes. Then, we improve the importance score algorithm, which reduce parameter sensitivity score calculations by approximately 98.5\%. By pruning singular values with lower importance scores, thereby enhancing adaptability to various downstream tasks. Extensive experiments are conducted on two widely used pre-trained models to validate the effectiveness of LoRA$^2$. Results show that it significantly reduces the number of trainable parameters to just 0.72\% compared to full fine-tuning, while still delivering highly impressive performance. Even when the parameters are further reduced to 0.17M, it still achieves comparable results to the baseline with 8 times more parameters. Our code is available here: https://anonymous.4open.science/r/LoRA-2-5B4C
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T12:31:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06854v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06854v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 DSLR: Document Refinement with Sentence-Level Re-ranking and
  Reconstruction to Enhance Retrieval-Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Taeho Hwang, Soyeong Jeong, Sukmin Cho, SeungYoon Han, Jong C. Park
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in Large Language Models (LLMs) have significantly improved their performance across various Natural Language Processing (NLP) tasks. However, LLMs still struggle with generating non-factual responses due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) systems address this issue by incorporating external knowledge with a retrieval module. Despite their successes, however, current RAG systems face challenges with retrieval failures and the limited ability of LLMs to filter out irrelevant information. Therefore, in this work, we propose DSLR (Document Refinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised framework that decomposes retrieved documents into sentences, filters out irrelevant sentences, and reconstructs them again into coherent passages. We experimentally validate DSLR on multiple open-domain QA datasets and the results demonstrate that DSLR significantly enhances the RAG performance over conventional fixed-size passage. Furthermore, our DSLR enhances performance in specific, yet realistic scenarios without the need for additional training, providing an effective and efficient solution for refining retrieved documents in RAG systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T12:27:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.03627v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.03627v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Causal Agent based on Large Language Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kairong Han, Kun Kuang, Ziyu Zhao, Junjian Ye, Fei Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have achieved significant success across various domains. However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLMs to comprehend and use them effectively. Causal methods are not easily conveyed through natural language, which hinders LLMs' ability to apply them accurately. Additionally, causal datasets are typically tabular, while LLMs excel in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data. This lack of causal reasoning capability limits the development of LLMs. To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems. The causal agent comprises tools, memory, and reasoning modules. In the tools module, the causal agent applies causal methods to align tabular data with natural language. In the reasoning module, the causal agent employs the ReAct framework to perform reasoning through multiple iterations with the tools. In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs. To verify the causal ability of the causal agent, we established a benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for these four levels of issues and tested the causal agent on the datasets. Our methodology demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80%. For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/Kairong-Han/Causal_Agent.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T12:22:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06849v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06849v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 DracoGPT: Extracting Visualization Design Preferences from Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huichen Will Wang, Mitchell Gordon, Leilani Battle, Jeffrey Heer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices. However, if they fail to do so, they might provide unreliable visualization recommendations. What visualization design preferences, then, have LLMs learned? We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs. To assess varied tasks, we develop two pipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to either rank or recommend visual encoding specifications. We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research. We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints. Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments. Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T12:11:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06845v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06845v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study
  on Bar Charts with Varying Layouts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huichen Will Wang, Jane Hoffswell, Sao Myat Thazin Thane, Victor S. Bursztyn, Cindy Xiong Bearfield
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways? Graphical perception literature has shown that human chart takeaways are sensitive to visualization design choices, such as spatial layouts. In this work, we examine the extent to which LLMs exhibit such sensitivity when generating takeaways, using bar charts with varying spatial layouts as a case study. We conducted three experiments and tested four common bar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. In Experiment 1, we identified the optimal configurations to generate meaningful chart takeaways by testing four LLMs, two temperature settings, nine chart specifications, and two prompting strategies. We found that even state-of-the-art LLMs struggled to generate semantically diverse and factually accurate takeaways. In Experiment 2, we used the optimal configurations to generate 30 chart takeaways each for eight visualizations across four layouts and two datasets in both zero-shot and one-shot settings. Compared to human takeaways, we found that the takeaways LLMs generated often did not match the types of comparisons made by humans. In Experiment 3, we examined the effect of chart context and data on LLM takeaways. We found that LLMs, unlike humans, exhibited variation in takeaway comparison types for different bar charts using the same bar layout. Overall, our case study evaluates the ability of LLMs to emulate human interpretations of data and points to challenges and opportunities in using LLMs to predict human chart takeaways.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T11:54:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06837v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06837v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 MICL: Improving In-Context Learning through Multiple-Label Words in
  Demonstration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhu Zixiao, Feng Zijian, Zhou Hanzhang, Qian Junlang, Mao Kezhi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-context learning (ICL) enables large language models (LLMs) to perform new tasks by using sample-label pairs as demonstrations. However, variations in demonstrations can lead to significantly different performances. Current research mainly focuses on selecting demonstration samples, preassuming the class name to be the label word when creating sample-label pairs. However, the choice of label words is crucial for ICL performance. Besides, we observe that using a single class name in demonstration may not yield optimal results while using multiple label words in one sample-label pair can enhance ICL performance. In this paper, we propose a comprehensive approach that organizes both samples and labels in demonstrations based on LLMs' output space distribution. This approach uses multiple label words in one sample-label pair to enhance label instruction. Evaluation results from seven classification datasets show that this demonstration organization method, which incorporates multiple label words to provide diverse label information, improves ICL performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T11:46:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.10908v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.10908v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data
  Uncertainty</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongjin Yang, Haneul Yoo, Hwaran Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although large language models (LLMs) are capable of performing various tasks, they still suffer from producing plausible but incorrect responses. To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not. However, most uncertainty quantification methods have been evaluated on questions requiring a single clear answer, ignoring the existence of data uncertainty that arises from irreducible randomness. Instead, these methods only consider model uncertainty, which arises from a lack of knowledge. In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty. Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs. Our findings show that entropy and consistency-based methods estimate the model uncertainty well even under data uncertainty, while other methods for white- and black-box LLMs struggle depending on the tasks. Additionally, methods designed for white-box LLMs suffer from overconfidence in reasoning tasks compared to simple knowledge queries. We believe our observations will pave the way for future work on uncertainty quantification in realistic setting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T11:17:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06816v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06816v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Review-driven Personalized Preference Reasoning with Large Language
  Models for Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jieyong Kim, Hyunseo Kim, Hyunjin Cho, SeongKu Kang, Buru Chang, Jinyoung Yeo, Dongha Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks, generating significant interest in their application to recommendation systems. However, existing methods have not fully capitalized on the potential of LLMs, often constrained by limited input information or failing to fully utilize their advanced reasoning capabilities. To address these limitations, we introduce EXP3RT, a novel LLM-based recommender designed to leverage rich preference information contained in user and item reviews. EXP3RT is basically fine-tuned through distillation from a teacher LLM to perform three key tasks in order: EXP3RT first extracts and encapsulates essential subjective preferences from raw reviews, aggregates and summarizes them according to specific criteria to create user and item profiles. It then generates detailed step-by-step reasoning followed by predicted rating, i.e., reasoning-enhanced rating prediction, by considering both subjective and objective information from user/item profiles and item descriptions. This personalized preference reasoning from EXP3RT enhances rating prediction accuracy and also provides faithful and reasonable explanations for recommendation. Extensive experiments show that EXP3RT outperforms existing methods on both rating prediction and candidate item reranking for top-k recommendation, while significantly enhancing the explainability of recommendation systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T11:05:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06276v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06276v2' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    