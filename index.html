
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 Revolutionizing Reinforcement Learning Framework for Diffusion Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:58:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06949v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06949v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Amplifying Effective CXL Memory Bandwidth for LLM Inference via
  Transparent Near-Data Processing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) inference is bottlenecked by the limited bandwidth of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a transparent near-data processing architecture that amplifies effective CXL bandwidth without requiring changes to the CXL.mem interface or AI models. CXL-NDP integrates a precision-scalable bit-plane layout for dynamic quantization with transparent lossless compression of weights and KV caches directly within the CXL device. In end-to-end serving, CXL-NDP improves throughput by 43%, extends the maximum context length by 87%, and reduces the KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms its practicality with a modest silicon footprint, lowering the barrier for adopting efficient, scalable CXL-based memory in generative AI infrastructure.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:22:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.03377v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.03377v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and
  Extreme KV Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guihong Li, Mehdi Rezagholizadeh, Mingyu Yang, Vikram Appia, Emad Barsoum
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question: can we use MLA's benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. The experimental results show that our proposed method can effectively compress the KV cache while preserving the performance on the benchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression achieves the same average score by using only 3.6B training tokens and 70 GPU hours on AMD MI300, whereas a 10.6x compression have less than 0.1% average score drop with 7B training tokens and 140 GPU hours. The code for this work is available at https://github.com/AMD-AGI/AMD-Hybrid-Models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:34:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.11132v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.11132v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View
  Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Kong, Daniel Watson, Yannick Str√ºmpler, Michael Niemeyer, Federico Tombari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-view diffusion models have shown promise in 3D novel view synthesis, but most existing methods adopt a non-autoregressive formulation. This limits their applicability in world modeling, as they only support a fixed number of views and suffer from slow inference due to denoising all frames simultaneously. To address these limitations, we propose CausNVS, a multi-view diffusion model in an autoregressive setting, which supports arbitrary input-output view configurations and generates views sequentially. We train CausNVS with causal masking and per-frame noise, using pairwise-relative camera pose encodings (CaPE) for precise camera control. At inference time, we combine a spatially-aware sliding-window with key-value caching and noise conditioning augmentation to mitigate drift. Our experiments demonstrate that CausNVS supports a broad range of camera trajectories, enables flexible autoregressive novel view synthesis, and achieves consistently strong visual quality across diverse settings. Project page: https://kxhit.github.io/CausNVS.html.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:49:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06579v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06579v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM
  Step-Provers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, Xia Xiao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T09:54:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06493v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06493v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Physical Autoregressive Model for Robotic Manipulation without Action
  Pretraining</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zijian Song, Sihan Qin, Tianshui Chen, Liang Lin, Guangrun Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining. The project page is here: https://hcplab-sysu.github.io/PhysicalAutoregressiveModel/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T09:09:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09822v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09822v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 HyFedRAG: A Federated Retrieval-Augmented Generation Framework for
  Heterogeneous and Privacy-Sensitive Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cheng Qian, Hainan Zhang, Yongxin Tong, Hong-Wei Zheng, Zhiming Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive data, especially in distributed healthcare settings where patient data spans SQL, knowledge graphs, and clinical notes. Clinicians face difficulties retrieving rare disease cases due to privacy constraints and the limitations of traditional cloud-based RAG systems in handling diverse formats and edge devices. To address this, we introduce HyFedRAG, a unified and efficient Federated RAG framework tailored for Hybrid data modalities. By leveraging an edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across diverse data sources while preserving data privacy. Our key contributions are: (1) We design an edge-cloud collaborative RAG framework built on Flower, which supports querying structured SQL data, semi-structured knowledge graphs, and unstructured documents. The edge-side LLMs convert diverse data into standardized privacy-preserving representations, and the server-side LLMs integrates them for global reasoning and generation. (2) We integrate lightweight local retrievers with privacy-aware LLMs and provide three anonymization tools that enable each client to produce semantically rich, de-identified summaries for global inference across devices. (3) To optimize response latency and reduce redundant computation, we design a three-tier caching strategy consisting of local cache, intermediate representation cache, and cloud inference cache. Experimental results on PMC-Patients demonstrate that HyFedRAG outperforms existing baselines in terms of retrieval quality, generation consistency, and system efficiency. Our framework offers a scalable and privacy-compliant solution for RAG over structural-heterogeneous data, unlocking the potential of LLMs in sensitive and diverse data environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T08:44:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06444v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06444v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Tree of Agents: Improving Long-Context Capabilities of Large Language
  Models through Multi-Perspective Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Song Yu, Xiaofei Xu, Ke Deng, Li Li, Lin Tian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T08:34:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06436v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06436v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 FineServe: Precision-Aware KV Slab and Two-Level Scheduling for
  Heterogeneous Precision LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kyungmin Bin, Seungbeom Choi, Jimyoung Son, Jieun Choi, Daseul Bae, Daehyeon Baek, Kihyo Moon, Minsung Jang, Hyojung Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in Post-Training Quantization (PTQ) techniques have significantly increased demand for serving quantized large language models (LLMs), enabling higher throughput and substantially reduced memory usage with minimal accuracy loss. Quantized models address memory constraints in LLMs and enhance GPU resource utilization through efficient GPU sharing. However, quantized models have smaller KV block sizes than non-quantized models, causing limited memory efficiency due to memory fragmentation. Also, distinct resource usage patterns between quantized and non-quantized models require efficient scheduling to maximize throughput. To address these challenges, we propose FineServe, an inference serving framework for mixed-precision LLMs. FineServe's key contributions include: (1) KV Slab, a precision-aware adaptive memory management technique dynamically allocating KV cache based on model quantization characteristics, significantly reducing GPU memory fragmentation, and (2) a two-level scheduling framework comprising a global scheduler that places models to GPUs based on request rates, latency SLOs, and memory constraints and efficiency, and a local scheduler that adaptively adjusts batch sizes according to real-time request fluctuations. Experimental results demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x higher token generation throughput compared to the state-of-the-art GPU sharing systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T00:57:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06261v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06261v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 A facile vector substrate platform via BaTiO3 membrane transfer enables
  high quality solution processed epitaxial PZT on silicon</h2>
                <div class="authors">
                    <strong>Authors:</strong> Asraful Haque, Antony Jeyaseelan, Shubham Kumar Parate, Srinivasan Raghavan, Pavan Nukala
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The direct integration of high-performance ferroelectric oxides with silicon remains challenging due to lattice mismatch, thermal incompatibility, and the need for high-temperature epitaxial growth. Here, a hybrid integration approach is demonstrated in which crystalline BaTiO3 (BTO) membranes are first transferred onto Pt coated Si substrates and subsequently used as vector substrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin films via chemical solution deposition (CSD). A KI and HCl based etchant enables rapid and complete dissolution of the SrVO3 sacrificial layer in about 30 minutes, reducing the release time from days to minutes compared with conventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr, Ba). The BTO VS imposes dominant (00l) out of plane orientation and in plane cube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization 10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable switching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we extract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on conventional Pt Si substrates. This approach demonstrates a scalable and cost effective route for integrating functional ferroelectric materials onto silicon and offers a promising platform for future CMOS compatible oxide electronics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-07T13:15:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06047v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06047v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Tight Cache Contention Analysis for WCET Estimation on Multicore Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuai Zhao, Jieyu Jiang, Shenlin Cai, Yaowei Liang, Chen Jie, Yinjie Fang, Wei Zhang, Guoquan Zhang, Yaoyao Gu, Xiang Xiao, Wei Qin, Xiangzhen Ouyang, Wanli Chang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> WCET (Worst-Case Execution Time) estimation on multicore architecture is particularly challenging mainly due to the complex accesses over cache shared by multiple cores. Existing analysis identifies possible contentions between parallel tasks by leveraging the partial order of the tasks or their program regions. Unfortunately, they overestimate the number of cache misses caused by a remote block access without considering the actual cache state and the number of accesses. This paper reports a new analysis for inter-core cache contention. Based on the order of program regions in a task, we first identify memory references that could be affected if a remote access occurs in a region. Afterwards, a fine-grained contention analysis is constructed that computes the number of cache misses based on the access quantity of local and remote blocks. We demonstrate that the overall inter-core cache interference of a task can be obtained via dynamic programming. Experiments show that compared to existing methods, the proposed analysis reduces inter-core cache interference and WCET estimations by 52.31% and 8.94% on average, without significantly increasing computation overhead.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-06T05:58:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13863v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13863v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 RapidGNN: Energy and Communication-Efficient Distributed Training on
  Large-Scale Graph Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arefin Niam, Tevfik Kosar, M S Q Zulkar Nine
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph Neural Networks (GNNs) have become popular across a diverse set of tasks in exploring structural relationships between entities. However, due to the highly connected structure of the datasets, distributed training of GNNs on large-scale graphs poses significant challenges. Traditional sampling-based approaches mitigate the computational loads, yet the communication overhead remains a challenge. This paper presents RapidGNN, a distributed GNN training framework with deterministic sampling-based scheduling to enable efficient cache construction and prefetching of remote features. Evaluation on benchmark graph datasets demonstrates RapidGNN's effectiveness across different scales and topologies. RapidGNN improves end-to-end training throughput by 2.46x to 3.00x on average over baseline methods across the benchmark datasets, while cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further demonstrates near-linear scalability with an increasing number of computing units efficiently. Furthermore, it achieves increased energy efficiency over the baseline methods for both CPU and GPU by 44% and 32%, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-05T16:10:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.05207v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.05207v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 KVCompose: Efficient Structured KV Cache Compression with Composite
  Tokens</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dmitry Akulov, Mohamed Sana, Antonio De Domenico, Tareq Si Salem, Nicola Piovesan, Fadhel Ayed
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) rely on key-value (KV) caches for efficient autoregressive decoding; however, cache size grows linearly with context length and model depth, becoming a major bottleneck in long-context inference. Prior KV cache compression methods either enforce rigid heuristics, disrupt tensor layouts with per-attention-head variability, or require specialized compute kernels.   We propose a simple, yet effective, KV cache compression framework based on attention-guided, layer-adaptive composite tokens. Our method aggregates attention scores to estimate token importance, selects head-specific tokens independently, and aligns them into composite tokens that respect the uniform cache structure required by existing inference engines. A global allocation mechanism further adapts retention budgets across layers, assigning more capacity to layers with informative tokens. This approach achieves significant memory reduction while preserving accuracy, consistently outperforming prior structured and semi-structured methods. Crucially, our approach remains fully compatible with standard inference pipelines, offering a practical and scalable solution for efficient long-context LLM deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-05T14:58:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.05165v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.05165v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Mainframe-Style Channel Controllers for Modern Disaggregated Memory
  Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zikai Liu, Jasmin Schult, Pengcheng Xu, Timothy Roscoe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite the promise of alleviating the main memory bottleneck, and the existence of commercial hardware implementations, techniques for Near-Data Processing have seen relatively little real-world deployment. The idea has received renewed interest with the appearance of disaggregated or "far" memory, for example in the use of CXL memory pools.   However, we argue that the lack of a clear OS-centric abstraction of Near-Data Processing is a major barrier to adoption of the technology. Inspired by the channel controllers which interface the CPU to disk drives in mainframe systems, we propose memory channel controllers as a convenient, portable, and virtualizable abstraction of Near-Data Processing for modern disaggregated memory systems.   In addition to providing a clean abstraction that enables OS integration while requiring no changes to CPU architecture, memory channel controllers incorporate another key innovation: they exploit the cache coherence provided by emerging interconnects to provide a much richer programming model, with more fine-grained interaction, than has been possible with existing designs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-05T10:39:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.AR</span><span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3725783.3764403' target='_blank'>doi</a><a href='http://arxiv.org/abs/2506.09758v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.09758v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 PagedEviction: Structured Block-wise KV Cache Pruning for Efficient
  Large Language Model Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Krishna Teja Chitty-Venkata, Jie Ye, Xian-He Sun, Anthony Kougkas, Murali Emani, Venkatram Vishwanath, Bogdan Nicolae
                </div>
                <div class="summary">
                    <strong>Summary:</strong> KV caching significantly improves the efficiency of Large Language Model (LLM) inference by storing attention states from previously processed tokens, enabling faster generation of subsequent tokens. However, as sequence length increases, the KV cache quickly becomes a major memory bottleneck. To address this, we propose PagedEviction, a novel fine-grained, structured KV cache pruning strategy that enhances the memory efficiency of vLLM's PagedAttention. Unlike existing approaches that rely on attention-based token importance or evict tokens across different vLLM pages, PagedEviction introduces an efficient block-wise eviction algorithm tailored for paged memory layouts. Our method integrates seamlessly with PagedAttention without requiring any modifications to its CUDA attention kernels. We evaluate PagedEviction across Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models on the LongBench benchmark suite, demonstrating improved memory usage with better accuracy than baselines on long context tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-04T16:40:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.04377v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.04377v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and
  Multiple Level Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, Xiaowen Chu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study presents a comprehensive multi-level analysis of the NVIDIA Hopper GPU architecture, focusing on its performance characteristics and novel features. We benchmark Hopper's memory subsystem, highlighting improvements in the L2 partitioned cache and global memory access compared to Ampere and Ada Lovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the benefits of FP8 precision and asynchronous wgmma instructions for matrix operations. Additionally, we investigate the performance of DPX instructions for dynamic programming, distributed shared memory (DSM) for inter-SM communication, and the Tensor Memory Accelerator (TMA) for asynchronous data movement. Through multi-level evaluation, we discover that the Hopper architecture demonstrates significant acceleration potential in real-world applications. For instance, the asynchronous programming model supported by TMA achieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double the performance of FP16, and DPX instructions accelerate a computational biology algorithm by at least 4.75x. Our findings provide actionable insights for optimizing compute-intensive workloads, from AI training to bioinformatics, on Hopper GPUs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-04T15:21:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AR</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.12084v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.12084v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 InferLog: Accelerating LLM Inference for Online Log Parsing via
  ICL-oriented Prefix Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilun Wang, Pengfei Chen, Haiyu Huang, Zilong He, Gou Tan, Chuanfu Zhang, Jingkai He, Zibin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern software systems generate massive volumes of runtime logs, necessitating efficient and accurate log parsing to enable critical downstream tasks such as anomaly detection and root cause analysis. Recently, large language models (LLMs) have achieved advanced accuracy on log parsing, but their deployment in production environments faces two major limitations: (1) the privacy risks associated with commercial LLMs, driving the adoption of local deployment, and (2) the stringent latency and throughput requirements imposed by high-volume log streams, which existing LLM-based parsers fail to meet. Although recent efforts have reduced the number of LLM queries, they overlook the high latency of the LLM invocations, where concurrent log parsing requests can cause serve performance degradation of LLM inference system.   In this study, we present InferLog, the first LLM inference optimization method for online log parsing. Our key insight is that the inference efficiency emerges as the vital bottleneck in LLM-based online log parsing, rather than parsing accuracy. InferLog accelerates inference by designing (1) A Prefix-aware ICL Refinement policy to refine the examples and permutation of in-context learning to improve the prefix caching efficiency. (2) A rapid and task-specific configuration tuning pipeline based on meta-learning to find the optimal LLM scheduling-related configuration for dynamic log parsing workloads. The experimental results based on Loghub dataset and vLLM demonstrate that InferLog significantly outperforms existing inference optimization methods and markedly accelerates the state-of-the-art LLM-based log parser without compromising parsing accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-04T13:14:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08523v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08523v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Set Block Decoding is a Language Model Inference Accelerator</h2>
                <div class="authors">
                    <strong>Authors:</strong> Itai Gat, Heli Ben-Hamu, Marton Havasi, Daniel Haziza, Jeremy Reizenstein, Gabriel Synnaeve, David Lopez-Paz, Brian Karrer, Yaron Lipman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-04T13:02:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.04185v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.04185v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer
  Vision</h2>
                <div class="authors">
                    <strong>Authors:</strong> Safouane El Ghazouali, Umberto Michelucci
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. This hybrid approach employs CLIP combined with pre-trained detectors like Ultralytics models for common classes and zero-shot models such as Grounding DINO for custom labels, generating initial annotations with low-confidence thresholding to maximize recall. Through this framework, when tested on COCO-type of classes, initial prediction have been proven to be mostly correct though the users can refine these via interactive tools supporting bounding boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has on-the-fly segmentation powered by Segment Anything accelerated through WebGPU for browser-side efficiency. The tool supports multiple export formats (YOLO, COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort through benchmarks on diverse datasets, while maintaining high annotation accuracy via clustering of connected CLIP-based disambiguate components and IoU-graph for redundant detection suppression. VisioFirm can be accessed from \href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-04T12:54:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.04180v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.04180v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Spotlight Attention: Towards Efficient LLM Generation via Non-linear
  Hashing-based KV Cache Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-04T09:08:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19740v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19740v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and
  Lessons Learned</h2>
                <div class="authors">
                    <strong>Authors:</strong> Olivier Adjonyo, Sebastien Bardin, Emanuele Bellini, Gilbert Ndollane Dione, Mahmudul Faisal Al Ameen, Robert Merget, Frederic Recoules, Yanis Sellami
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The PQDSS standardization process requires cryptographic primitives to be free from vulnerabilities, including timing and cache side-channels. Resistance to timing leakage is therefore an essential property, and achieving this typically relies on software implementations that follow constant-time principles. Moreover, ensuring that all implementations are constant-time is crucial for fair performance comparisons, as secure implementations often incur additional overhead. Such analysis also helps identify scheme proposals that are inherently difficult to implement in constant time. Because constant-time properties can be broken during compilation, it is often necessary to analyze the compiled binary directly. Since manual binary analysis is extremely challenging, automated analysis becomes highly important. Although several tools exist to assist with such analysis, they often have usability limitations and are difficult to set up correctly. To support the developers besides the NIST committee in verifying candidates, we developed a toolchain that automates configuration, execution, and result analysis for several widely used constant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify constant-time policy compliance at the binary level, and dudect and RTLF to detect side-channel vulnerabilities through statistical analysis of execution time behavior. We demonstrate its effectiveness and practicability by evaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26 issues in total to the respective developers, and 5 of them have already been fixed. We also discuss our different findings, as well as the benefits of shortcomings of the different tools.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-04T08:41:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.04010v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.04010v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 IC-Cache: Efficient Large Language Model Serving via In-context Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Yu, Yu Gan, Nikhil Sarda, Lillian Tsai, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Henry M. Levy, David Culler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 70% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge transfer among requests. However, naively caching and reusing past responses leads to a big quality drop. In this paper, we introduce IC-Cache, a caching system that enables live LLM capability augmentation to improve serving efficiency: by leveraging historical request-response pairs from larger models as in-context examples, IC-Cache empowers small LLMs to imitate and even exceed the compositional abilities (e.g., reasoning) of their larger counterparts, enabling selective offloading of requests to reduce cost and latency. Achieving this live augmentation at scale introduces intricate trade-offs between response quality, latency, and system throughput. For a new request, IC-Cache efficiently selects similar, high-utility examples to prepend them to the new request's input. At scale, it adaptively routes requests across LLMs of varying capabilities, accounting for response quality and serving loads. IC-Cache employs a cost-aware cache replay mechanism that refines example quality offline to maximize online cache utility and efficiency. Evaluations on millions of realistic requests demonstrate that IC-Cache improves LLM serving throughput by 1.4-5.9x and reduces latency by 28-71% without hurting response quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-04T06:20:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3731569.3764829' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.12689v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.12689v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline
  Co-Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Qiao, Shu Anzai, Shan Yu, Haoran Ma, Shuo Yang, Yang Wang, Miryung Kim, Yongji Wu, Yang Zhou, Jiarong Xing, Joseph E. Gonzalez, Ion Stoica, Harry Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) serving demands low latency and high throughput, but high load variability makes it challenging to achieve high GPU utilization. In this paper, we identify a synergetic but overlooked opportunity to co-serve latency-critical online requests alongside latency-tolerant offline tasks such as model benchmarking. While promising, existing serving systems fail to co-serve them efficiently, as their coarse-grained resource management at the request or iteration level cannot harvest millisecond-level GPU idle cycles without introducing interference that violates online latency objectives. ConServe is a new LLM co-serving system that achieves high throughput and strong online latency guarantees by managing resources at finer granularities. ConServe introduces three techniques: (1) a latency-aware token-level scheduler that precisely sizes offline batches and tokens to fit within online latency objectives; (2) sub-iteration, layer-wise preemption that allows offline tasks to yield to online load spikes; and (3) incremental KV cache management that enables preempting and resuming offline requests at near-zero cost. Evaluations with Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe delivers an average of 2.2$\times$ higher throughput and reduces online serving tail latency by 2.9$\times$ on average compared to state-of-the-art systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-03T20:54:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.01228v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.01228v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 CloudFormer: An Attention-based Performance Prediction for Public Clouds
  with Unknown Workload</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cloud platforms are increasingly relied upon to host diverse, resource-intensive workloads due to their scalability, flexibility, and cost-efficiency. In multi-tenant cloud environments, virtual machines are consolidated on shared physical servers to improve resource utilization. While virtualization guarantees resource partitioning for CPU, memory, and storage, it cannot ensure performance isolation. Competition for shared resources such as last-level cache, memory bandwidth, and network interfaces often leads to severe performance degradation. Existing management techniques, including VM scheduling and resource provisioning, require accurate performance prediction to mitigate interference. However, this remains challenging in public clouds due to the black-box nature of VMs and the highly dynamic nature of workloads. To address these limitations, we propose CloudFormer, a dual-branch Transformer-based model designed to predict VM performance degradation in black-box environments. CloudFormer jointly models temporal dynamics and system-level interactions, leveraging 206 system metrics at one-second resolution across both static and dynamic scenarios. This design enables the model to capture transient interference effects and adapt to varying workload conditions without scenario-specific tuning. Complementing the methodology, we provide a fine-grained dataset that significantly expands the temporal resolution and metric diversity compared to existing benchmarks. Experimental results demonstrate that CloudFormer consistently outperforms state-of-the-art baselines across multiple evaluation metrics, achieving robust generalization across diverse and previously unseen workloads. Notably, CloudFormer attains a mean absolute error (MAE) of just 7.8%, representing a substantial improvement in predictive accuracy and outperforming existing methods at least by 28%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-03T15:15:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.03394v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.03394v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, Xinran Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75% more requests.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-03T14:56:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.00079v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.00079v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based
  Sequence Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiuying Wei, Anunay Yadav, Razvan Pascanu, Caglar Gulcehre
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation suffers from memory degradation in long contexts and limits fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a 7x improvement in training speed with 100K token sequences and 9x in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at https://github.com/CLAIRE-Labo/RAT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-03T14:28:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04416v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04416v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 A Cegar-centric Bounded Reachability Analysis for Compositional Affine
  Hybrid Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Atanu Kundu, Pratyay Sarkar, Rajarshi Ray
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reachability analysis of compositional hybrid systems, where individual components are modeled as hybrid automata, poses unique challenges. In addition to preserving the compositional semantics while computing system behaviors, algorithms have to cater to the explosion in the number of locations in the parallel product automaton. In this paper, we propose a bounded reachability analysis algorithm for compositional hybrid systems with piecewise affine dynamics, based on the principle of counterexample guided abstraction refinement (CEGAR). In particular, the algorithm searches for a counterexample in the discrete abstraction of the composition model, without explicitly computing a product automaton. When a counterexample is discovered in the abstraction, its validity is verified by a refinement of the state-space guided by the abstract counterexample. The state-space refinement is through a symbolic reachability analysis, particularly using a state-of-the-art algorithm with support functions as the continuous state representation. In addition, the algorithm mixes different semantics of composition with the objective of improved efficiency. Step compositional semantics is followed while exploring the abstract (discrete) state-space, while shallow compositional semantics is followed during state-space refinement with symbolic reachability analysis. Optimizations such as caching the results of the symbolic reachability analysis, which can be later reused, have been proposed. We implement this algorithm in the tool SAT-Reach and demonstrate the scalability benefits.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-03T11:23:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.03560v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.03560v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Adaptive KV-Cache Compression without Manually Setting Budget</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenxia Tang, Jianchun Liu, Hongli Xu, Liusheng Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) inference relies heavily on KV-caches to accelerate autoregressive decoding, but the resulting memory footprint grows rapidly with sequence length, posing significant efficiency challenges. Current KV-cache compression methods suffer from a Procrustes' bed problem: they force diverse workloads into fixed compression ratios, leading to suboptimal resource allocation and inference performance. To this end, we present GVote, an adaptive KV-cache compression scheme that eliminates manual budget specification while achieving superior accuracy-efficiency trade-offs. GVote operates on the principle that the important keys are the aggregation of keys required by future queries. The method predicts future query attention demands by Monte-Carlo style sampling potential queries and aggregating selected keys to determine the optimal cache budget without manual specification. Experimental evaluation demonstrates GVote's effectiveness across multiple benchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote exhibits 2$\times$ memory reduction while the accuracy maintains higher or comparable.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-03T08:38:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.03136v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.03136v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 FastCache: Fast Caching for Diffusion Transformer Through Learnable
  Linear Approximation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dong Liu, Yanxuan Yu, Jiayi Zhang, Yifan Li, Ben Lengerich, Ying Nian Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at https://github.com/NoakLiu/FastCache-xDiT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-03T06:56:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CV</span><span>cs.MM</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.20353v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.20353v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Digital Network Twins for Next-generation Wireless: Creation,
  Optimization, and Challenges</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zifan Zhang, Zhiyuan Peng, Hanzhi Yu, Mingzhe Chen, Yuchen Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Digital network twins (DNTs), by representing a physical network using a virtual model, offer significant benefits such as streamlined network development, enhanced productivity, and cost reduction for next-generation (nextG) communication infrastructure. Existing works mainly describe the deployment of DNT technologies in various service sections.The full life cycle of DNTs for telecommunication has not yet been comprehensively studied, particularly in the aspects of fine-grained creation, real-time adaptation, resource-efficient deployment, and security protection. This article presents an in-depth overview of DNTs, exploring their concrete integration into networks and communication, covering the fundamental designs, the emergent applications, and critical challenges in multiple dimensions. We also include two detailed case studies to illustrate how DNTs can be applied in real-world scenarios such as wireless traffic forecasting and edge caching. Additionally, a forward-looking vision of the research opportunities in tackling the challenges of DNTs is provided, aiming to fully maximize the benefits of DNTs in nextG networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T18:10:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.18002v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.18002v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device
  Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rashid Ummer N. T., K. K. Krishnan Namboodiri, B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Device-to-device (D2D) communication is one of the most promising techniques for future wireless cellular communication systems. This paper considers coded caching in a partially cooperative wireless D2D network, where only a subset of users transmit during delivery, while all users request files. The non-transmitting users are referred to as selfish users. All existing schemes that do not require knowledge of the identity of selfish users before content placement are limited to the high-memory regime, particularly when the number of selfish users is large. We propose a novel coded caching scheme for a partially cooperative D2D network that operates in all feasible memory regimes, regardless of the number of selfish users. We also derive a lower bound on the transmission load of a partially cooperative D2D coded caching scheme. Using this bound, the proposed scheme is shown to be optimal in the high-memory regime.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T17:35:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.02532v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.02532v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and
  Failure Mitigation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tommaso Bonato, Abdul Kabbani, Ahmad Ghalayini, Michael Papamichael, Mohammad Dohadwala, Lukas Gianinazzi, Mikhail Khalilov, Elias Achermann, Daniele De Sensi, Torsten Hoefler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Next-generation datacenters require highly efficient network load balancing to manage the growing scale of artificial intelligence (AI) training and general datacenter traffic. However, existing Ethernet-based solutions, such as Equal Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to maintain high network utilization due to both increasing traffic demands and the expanding scale of datacenter topologies, which also exacerbate network failures. To address these limitations, we propose REPS, a lightweight decentralized per-packet adaptive load balancing algorithm designed to optimize network utilization while ensuring rapid recovery from link failures. REPS adapts to network conditions by caching good-performing paths. In case of a network failure, REPS re-routes traffic away from it in less than 100 microseconds. REPS is designed to be deployed with next-generation out-of-order transports, such as Ultra Ethernet, and uses less than 25 bytes of per-connection state regardless of the topology size. We extensively evaluate REPS in large-scale simulations and FPGA-based NICs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T16:39:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.21625v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.21625v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to
  Break the GPU Memory Wall</h2>
                <div class="authors">
                    <strong>Authors:</strong> Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Training LLMs larger than the aggregated memory of multiple GPUs is increasingly necessary due to the faster growth of LLM sizes compared to GPU memory. To this end, multi-tier host memory or disk offloading techniques are proposed by state of art. Despite advanced asynchronous multi-tier read/write strategies, such offloading strategies result in significant I/O overheads in the critical path of training, resulting in slower iterations. To this end, we propose MLP-Offload, a novel multi-level, multi-path offloading engine specifically designed for optimizing LLM training on resource-constrained setups by mitigating I/O bottlenecks. We make several key observations that drive the design of MLP-Offload, such as I/O overheads during the update dominate the iteration time; I/O bandwidth of the third-level remote storage tier remains unutilized; and, contention due to concurrent offloading amplifies I/O bottlenecks. Driven by these insights, we design and implement MLP-Offload to offload the optimizer states across multiple tiers in a cache-efficient and concurrency-controlled fashion to mitigate I/O bottlenecks during the backward and update phases. Evaluations on models up to 280B parameters shows that MLP-Offload achieves 2.5$\times$ faster iterations compared to the state-of-the-art LLM training runtimes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T16:30:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.LG</span><span>H.2.0; E.2; I.2.11</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3712285.3759864' target='_blank'>doi</a><a href='http://arxiv.org/abs/2509.02480v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.02480v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Cache Management for Mixture-of-Experts LLMs -- extended version</h2>
                <div class="authors">
                    <strong>Authors:</strong> Spyros Angelopoulos, Loris Marchal, Adrien Obrecht, Bertrand Simon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable capabilities across a variety of tasks. One of the main challenges towards the successful deployment of LLMs is memory management, since they typically involve billions of parameters. To this end, architectures based on Mixture-of-Experts have been proposed, which aim to reduce the size of the parameters that are activated when producing a token. This raises the equally critical issue of efficiently managing the limited cache of the system, in that frequently used experts should be stored in the fast cache rather than in the slower secondary memory.   In this work, we introduce and study a new paging problem that models expert management optimization. Our formulation captures both the layered architecture of LLMs and the requirement that experts are cached efficiently. We first present lower bounds on the competitive ratio of both deterministic and randomized algorithms, which show that under mild assumptions, LRU-like policies have good theoretical competitive performance. We then propose a layer-based extension of LRU that is tailored to the problem at hand.   Extensive simulations on both synthetic datasets and actual traces of MoE usage show that our algorithm outperforms policies for the classic paging problem, such as the standard LRU.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T15:19:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/978-3-031-99872-0_2' target='_blank'>doi</a><a href='http://arxiv.org/abs/2509.02408v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.02408v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Leveraging Approximate Caching for Faster Retrieval-Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shai Bergman, Zhang Ji, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) improves the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, substantially reducing reliance on expensive vector database lookups. To scale efficiently, Proximity employs a locality-sensitive hashing (LSH) scheme that enables fast cache lookups while preserving retrieval accuracy. We evaluate Proximity using the MMLU and MedRAG question answering benchmarks. Our experiments demonstrate that Proximity with our LSH scheme and a realistically skewed MedRAG workload reduces database calls by 78.9% while maintaining database recall and test accuracy. We experiment with different similarity tolerances and cache capacities, and show that the time spent within the Proximity cache remains low and constant (4.8 microseconds) even as the cache grows substantially in size. Our work highlights that approximate caching is a viable and effective strategy for optimizing RAG-based systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T13:09:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3721146.3721941' target='_blank'>doi</a><a href='http://arxiv.org/abs/2503.05530v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.05530v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Efficient Geometry Compression and Communication for 3D Gaussian
  Splatting Point Clouds</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liang Xie, Yanting Li, Luyang Tang, Wei Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Storage and transmission challenges in dynamic 3D scene representation based on the i3DV platform, With increasing scene complexity, the explosive growth of 3D Gaussian data volume causes excessive storage space occupancy. To address this issue, we propose adopting the AVS PCRM reference software for efficient compression of Gaussian point cloud geometry data. The strategy deeply integrates the advanced encoding capabilities of AVS PCRM into the i3DV platform, forming technical complementarity with the original rate-distortion optimization mechanism based on binary hash tables. On one hand, the hash table efficiently caches inter-frame Gaussian point transformation relationships, which allows for high-fidelity transmission within a 40 Mbps bandwidth constraint. On the other hand, AVS PCRM performs precise compression on geometry data. Experimental results demonstrate that the joint framework maintains the advantages of fast rendering and high-quality synthesis in 3D Gaussian technology while achieving significant 10\%-25\% bitrate savings on universal test sets. It provides a superior rate-distortion tradeoff solution for the storage, transmission, and interaction of 3D volumetric video.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T11:58:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3680207.3765659' target='_blank'>doi</a><a href='http://arxiv.org/abs/2509.02232v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.02232v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache
  Channel Pruning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T11:29:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15212v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15212v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Batch Query Processing and Optimization for Agentic Workflows</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junyi Shen, Noppanat Wadlom, Yao Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) in agentic workflows combine multi-step reasoning, tool use, and collaboration across multiple specialized agents. Existing LLM serving engines optimize individual calls in isolation, while multi-agent frameworks focus on orchestration without system-level performance planning. As a result, repeated prompts, overlapping contexts, and concurrent executions create substantial redundancy and poor GPU utilization, especially in batch analytics scenarios. We introduce Halo, a system that brings batch query processing and optimization into agentic LLM workflows. Halo represents each workflow as a structured query plan DAG and constructs a consolidated graph for batched queries that exposes shared computation. Guided by a cost model that jointly considers prefill and decode costs, cache reuse, and GPU placement, Halo performs plan-level optimization to minimize redundant execution. Its runtime integrates adaptive batching, KV-cache sharing and migration, along with compute-communication overlap to maximize hardware efficiency. Evaluation across six benchmarks shows that Halo achieves up to 18.6x speedup for batch inference and 4.7x throughput improvement under online serving, scaling to workloads of tens of thousands of queries and complex graphs. These gains are achieved without compromising output quality. By unifying query optimization with LLM serving, Halo enables efficient agentic workflows in data analytics and decision-making applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T09:17:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.02121v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.02121v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Augmented Shuffle Differential Privacy Protocols for Large-Domain
  Categorical and Key-Value Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Takao Murakami, Yuichi Sei, Reo Eriguchi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy by introducing a shuffler who randomly shuffles data in a distributed system. However, most shuffle DP protocols are vulnerable to two attacks: collusion attacks by the data collector and users and data poisoning attacks. A recent study addresses this issue by introducing an augmented shuffle DP protocol, where users do not add noise and the shuffler performs random sampling and dummy data addition. However, it focuses on frequency estimation over categorical data with a small domain and cannot be applied to a large domain due to prohibitively high communication and computational costs.   In this paper, we fill this gap by introducing a novel augmented shuffle DP protocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME protocol uses a hash function to filter out unpopular items and then accurately calculates frequencies for popular items. To perform this within one round of interaction between users and the shuffler, our protocol carefully communicates within a system using multiple encryption. We also apply our FME protocol to more advanced KV (Key-Value) statistics estimation with an additional technique to reduce bias. For both categorical and KV data, we prove that our protocol provides computational DP, high robustness to the above two attacks, accuracy, and efficiency. We show the effectiveness of our proposals through comparisons with twelve existing protocols.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-02T06:40:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.02004v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.02004v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure
  HBM Accelerators</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yitong Guo, Hongbo Chen, Haobin Hiroki Chen, Yukui Luo, XiaoFeng Wang, Chenghong Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Trusted Execution Environments provide a strong foundation for secure cloud computing, they remain vulnerable to access pattern leakages. Oblivious Maps (OMAPs) mitigate this by fully hiding access patterns but suffer from high overhead due to randomized remapping and worst-case padding. We argue these costs are not fundamental. Modern accelerators featuring High-Bandwidth Memory (HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that eavesdropping on HBM is difficult -- even for physical attackers -- as its memory channels are sealed together with processor cores inside the same physical package. Later, Hunt et al. [NSDI'20] show that, with proper isolation, HBM can be turned into an unobservable region where both data and memory traces are hidden. This motivates a rethink of OMAP design with HBM-backed solutions to finally overcome their traditional performance limits. Building on these insights, we present BOLT, a Bandwidth Optimized, Lightning-fast OMAP accelerator that, for the first time, achieves O(1) + O(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations: (i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache to accelerate oblivious access to large host memory; (ii) a self-hosted architecture that offloads execution and memory control from the host to mitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs that maximize resource efficiency. We implement a prototype BOLT on a Xilinx U55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in initialization and query time, respectively, over state-of-the-art OMAPs, including an industry implementation from Facebook.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-09T00:15:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.01742v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.01742v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 LLMs cannot spot math errors, even when allowed to peek into the
  solution</h2>
                <div class="authors">
                    <strong>Authors:</strong> KV Aditya Srivatsa, Kaushal Kumar Maurya, Ekaterina Kochmar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) demonstrate remarkable performance on math word problems, yet they have been shown to struggle with meta-reasoning tasks such as identifying errors in student solutions. In this work, we investigate the challenge of locating the first error step in stepwise solutions using two error reasoning datasets: VtG and PRM800K. Our experiments show that state-of-the-art LLMs struggle to locate the first error step in student solutions even when given access to the reference solution. To that end, we propose an approach that generates an intermediate corrected student solution, aligning more closely with the original student's solution, which helps improve performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-01T11:41:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.01395v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.01395v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating
  Rotation and Learnable Non-uniform Quantizer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Euntae Choi, Sumin Song, Woosang Lim, Sungjoo Yoo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose Rotate, Clip, and Partition (RCP), a quantization-aware training (QAT) approach that first realizes extreme compression of LLMs with W2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP integrates recent rotation techniques with a novel non-uniform weight quantizer design, by quantitatively analyzing the impact of random rotation on 2-bit weight quantization. Our weight quantizer features Learnable Direct Partitioning (LDP), which introduces learnable parameters to directly learn non-uniform intervals jointly with LLM weights. We also present a specialized GPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP can compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and 5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging mobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and MetaMath-7B with no critical problems such as convergence failure and repetition. Code is available at https://github.com/ songsm921/RCP.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-01T07:26:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.15779v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.15779v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 ProMoE: Fast MoE-based LLM Serving using Proactive Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoniu Song, Zihang Zhong, Rong Chen, Haibo Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The promising applications of large language models are often limited by the constrained GPU memory capacity available on edge devices. Mixture-of-Experts (MoE) models help address this issue by activating only a subset of the model's parameters during computation. This approach allows the unused parameters to be offloaded to host memory, thereby reducing the overall GPU memory demand. However, existing cache-based offloading solutions handle cache misses reactively, which significantly impacts system performance. In this paper, we introduce ProMoE, a novel proactive caching system that utilizes intermediate results to predict subsequent expert usage. By proactively fetching experts in advance, ProMoE eliminates passive cache misses, removes loading time from the critical path, and reduces the performance overhead associated with offloading. Our evaluations demonstrate that ProMoE achieves an average speedup of 2.20x (up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages, respectively, compared to existing offloading solutions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-01T03:51:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.22134v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.22134v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 REFRAG: Rethinking RAG based Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoqiang Lin, Aritra Ghosh, Bryan Kian Hsiang Low, Anshumali Shrivastava, Vijai Mohan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive external knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-augmented generation (RAG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing latency for long-context inputs is a primary objective for LLMs, we contend that RAG require specialized consideration. In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting the sparsity structure, we demonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to previous work) without loss in perplexity. In addition, our optimization framework for large context enables REFRAG to extend the context size of LLMs by 16. We provide rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm that REFRAG delivers substantial speedup with no loss in accuracy compared to LLaMA models and other state-of-the-art baselines across various context sizes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-01T03:31:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.01092v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.01092v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Bidirectional Sparse Attention for Faster Video Diffusion Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenlu Zhan, Wen Li, Chuyu Shen, Jun Zhang, Suhui Wu, Hao Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-01T03:16:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.01085v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.01085v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 LLM Serving Optimization with Variable Prefill and Decode Lengths</h2>
                <div class="authors">
                    <strong>Authors:</strong> Meixuan Wang, Yinyu Ye, Zijie Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We study the problem of serving LLM (Large Language Model) requests where each request has heterogeneous prefill and decode lengths. In LLM serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the KV cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the KV cache memory usage by one unit. Given a set of n requests, our goal is to schedule and process them to minimize the total completion time. We show that this problem is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios scale up sublinearly with the memory limit-a significant drawback in real-world settings where memory demand is large. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. Finally, we develop and evaluate a few algorithm variants inspired by this approach, including dynamic programming variants, local search methods, and an LP-based scheduler, demonstrating through comprehensive simulations that they outperform standard baselines while maintaining computational efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-31T15:09:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.OC</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06133v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06133v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Accelerating Latency-Critical Applications with AI-Powered
  Semi-Automatic Fine-Grained Parallelization on SMT Processors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Denis Los, Igor Petushkov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Latency-critical applications tend to show low utilization of functional units due to frequent cache misses and mispredictions during speculative execution in high-performance superscalar processors. However, due to significant impact on single-thread performance, Simultaneous Multithreading (SMT) technology is rarely used with heavy threads of latency-critical applications. In this paper, we explore utilization of SMT technology to support fine-grained parallelization of latency-critical applications. Following the advancements in the development of Large Language Models (LLMs), we introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we extend AI Coding Agent in Cursor IDE with additional tools connected through Model Context Protocol, enabling end-to-end AI Agent for parallelization. Additional connected tools enable LLM-guided hotspot detection, collection of dynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance simulation to estimate performance gains. We apply Aira with Relic parallel framework for fine-grained task parallelism on SMT cores to parallelize latency-critical benchmarks representing real-world applications used in industry. We show 17% geomean performance gain from parallelization of latency-critical benchmarks using Aira with Relic framework.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-31T14:51:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00883v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00883v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based
  Side-Channel Attacks on Fully Associative Randomized Caches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chris Cao, Gururaj Saileshwar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent work presented at USENIX Security 2025 (SEC'25) claims that occupancy-based attacks can recover AES keys from the MIRAGE randomized cache. In this paper, we examine these claims and find that they arise from a modeling flaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of MIRAGE uses a constant seed to initialize the random number generator used for global evictions in MIRAGE, causing every AES encryption they trace to evict the same deterministic sequence of cache lines. This artificially creates a highly repeatable timing pattern that is not representative of a realistic implementation of MIRAGE, where eviction sequences vary randomly between encryptions. When we instead randomize the eviction seed for each run, reflecting realistic operation, the correlation between AES T-table accesses and attacker runtimes disappears, and the attack fails. These findings show that the reported leakage is an artifact of incorrect modeling, and not an actual vulnerability in MIRAGE.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-31T05:43:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10431v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10431v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 NetGent: Agent-Based Automation of Network Application Workflows</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jaber Daneshamooz, Eugene Vuong, Laasya Koduru, Sanjay Chandrasekaran, Arpit Gupta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web applications. However, using existing browser automation tools that are diverse, repeatable, realistic, and efficient remains fragile and costly. NetGent addresses this challenge by allowing users to specify workflows as natural-language rules that define state-dependent actions. These abstract specifications are compiled into nondeterministic finite automata (NFAs), which a state synthesis component translates into reusable, executable code. This design enables deterministic replay, reduces redundant LLM calls through state caching, and adapts quickly when application interfaces change. In experiments, NetGent automated more than 50+ workflows spanning video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces while remaining robust to UI variability. By combining the flexibility of language-based agents with the reliability of compiled execution, NetGent provides a scalable foundation for generating the diverse, repeatable datasets needed to advance ML in networking.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-30T22:47:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00625v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00625v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for
  KV Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bo Jiang, Taolue Yang, Youyuan Liu, Chengming Zhang, Xubin He, Sian Jin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based large language models (LLMs) demonstrate impressive potential in various practical applications. However, long context inference poses a significant challenge due to the enormous memory requirements of the key-value (KV) cache, which can scale to multiple gigabytes as sequence length and batch size increase. In this paper, we present KVComp, a generic and efficient KV cache management framework optimized for long-text generation that synergistically works with both latency-critical and throughput-critical inference systems. KVComp employs novel lossy compression techniques specifically designed for KV cache data characteristics, featuring careful co-design of compression algorithms and system architecture. Our approach maintains compatibility with the growing nature of KV cache while preserving high computational efficiency. Experimental results show that KVComp achieves on average 47\% and up to 83\% higher memory reduction rate compared to existing methods with little/no model accuracy degradation. Furthermore, KVComp achieves extremely high execution throughput, effectively reducing decompression overhead and, in some cases, even accelerating the matrix-vector multiplication operation and outperform cuBLAS-based attention kernels with less data movement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-30T18:25:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00579v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00579v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Discrete and Continuous Caching Games</h2>
                <div class="authors">
                    <strong>Authors:</strong> √Åron J√°nosik, Csenge Mikl√≥s, D√°niel G. Simon, Krist√≥f Z√≥lomy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate a discrete search game called the Multiple Caching Game where the searcher's aim is to find all of a set of $d$ treasures hidden in $n$ locations. Allowed queries are sets of locations of size $k$, and the searcher wins if in all $d$ queries, at least one treasure is hidden in one of the $k$ picked locations. P\'alv\"olgyi showed that the value of the game is at most $\frac{k^d}{\binom{n+d-1}{d}}$, with equality for large enough $n$. We conjecture the exact cases of equality. We also investigate variants of the game and show an example where their values are different, answering a question of P\'alv\"olgyi.   This game is closely related to a continuous variant, Alpern's Caching Game, based on which we define other continous variants of the multiple caching game and examine their values.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-30T14:49:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.OC</span><span>math.CO</span><span>91A05</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1142/S0219198925500057' target='_blank'>doi</a><a href='http://arxiv.org/abs/2310.13777v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.13777v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 DiffKV: Differentiated Memory Management for Large Language Models with
  Parallel KV Compaction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanqi Zhang, Yuwei Hu, Runyuan Zhao, John C. S. Lui, Haibo Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) demonstrate remarkable capabilities but face substantial serving costs due to their high memory demands, with the key-value (KV) cache being a primary bottleneck. State-of-the-art KV cache compression techniques, such as quantization and pruning, apply uniform treatment to both keys and values, and discard unimportant tokens entirely, overlooking the fine-grained distinctions in the significance of individual KV cache components. To address such limitations, we introduce \textit{DiffKV}, a novel framework for efficient KV cache compression that exploits three levels of differentiation in the KV cache: (1) the differing impact of keys and values on attention computation, (2) the varying importance of tokens, and (3) the diverse dynamic sparsity patterns across attention heads. These levels of differentiation introduce irregular memory usage patterns across different requests and attention heads, posing significant scalability challenges for memory management. To address these challenges, DiffKV proposes an on-GPU memory manager that compacts fragmented free memory list into contiguous regions in parallel, effectively translating sparsity in the KV cache into performance gains. We evaluate DiffKV on several mainstream LLMs, including the emerging thinking models that generate extended chains of thought. DiffKV is able to compress the KV cache by $2.7\times$ to $5.7\times$ with near-lossless accuracy on complex workloads requiring sophisticated reasoning and long-generation capabilities, and enhances throughput by $1.9\times$ to $5.4\times$. Source codes of DiffKV are available at https://github.com/zyqCSL/DiffKV.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-30T09:35:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.03131v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.03131v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging
  and KV Cache Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lianyu Hu, Fanhua Shang, Wei Feng, Liang Wan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce LightVLM, a simple but effective method that can be seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly accelerate the inference process in a training-free manner. We divide the inference procedure of VLMs into two stages, i.e., encoding and decoding, and propose to simultaneously accelerate VLMs in both stages to largely improve model efficiency. During encoding, we propose pyramid token merging to reduce tokens of different LLM layers in a hierarchical manner by finally only keeping a few dominant tokens to achieve high efficiency. During decoding, aimed at reducing the high latency of outputting long sequences, we propose KV Cache compression to remove unnecessary caches to increase the network throughput. Experimental results show that LightVLM successfully retains 100% performance when only preserving 35% image tokens, and maintains around 98% performance when keeping only 3% image tokens. LightVLM could 2.02$\times$ the network throughput and reduce the prefilling time by 3.65$\times$. LightVLM also makes large VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to infer faster than significantly smaller models (e.g., InternVL2.5 8B), hopefully facilitating the real-world deployment. When generating long text sequences (e.g., 4096 tokens), LightVLM could reduce the inference time by 3.21$\times$, largely outperforming existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-30T08:57:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00419v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00419v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV
  Cache Eviction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xuelin Li, Xiangqi Jin, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient Key-Value (KV) cache management is essential for processing long text sequences in large language models (LLMs), where memory constraints often limit performance. Conventional KV eviction strategies, such as top-k selection based on attention scores, depend on static heuristics that fail to capture the evolving implicit dependencies among tokens during inference. To overcome this, we propose GraphKV, a graph-based framework that redefines token selection for KV cache compression. In GraphKV, tokens are modeled as nodes with importance scores, and edges represent their similarity relationships. Through a decay-signal-propagation mechanism, token importance is dynamically updated by propagating information across the graph, enabling adaptive retention of the most contextually significant tokens. GraphKV can be seamlessly utilized in existing KV cache eviction methods such as SnapKV and PyramidKV in a plug-and-play manner. Codes will be released on Github.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-30T06:56:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00388v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00388v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Robust Containment Queries over Collections of Trimmed NURBS Surfaces
  via Generalized Winding Numbers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jacob Spainhour, Kenneth Weiss
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose a containment query that is robust to the watertightness of regions bound by trimmed NURBS surfaces, as this property is difficult to guarantee for in-the-wild CAD models. Containment is determined through the generalized winding number (GWN), a mathematical construction that is indifferent to the arrangement of surfaces in the shape. Applying contemporary techniques for the 3D GWN to trimmed NURBS surfaces requires some form of geometric discretization, introducing computational inefficiency to the algorithm and even risking containment misclassifications near the surface. In contrast, our proposed method uses a novel reformulation of the relevant surface integral based on Stokes' theorem, which operates on the boundary and trimming curves as provided through rapidly converging adaptive quadrature. Batches of queries are further accelerated by memoizing (i.e.\ caching and reusing) quadrature node positions and tangents as they are evaluated. We demonstrate that our GWN method is robust to complex trimming geometry in a CAD model, and is accurate up to arbitrary precision at arbitrary distances from the surface. The derived containment query is therefore robust to model non-watertightness while respecting all curved features of the input shape.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-29T20:39:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>cs.CG</span><span>cs.NA</span><span>math.NA</span><span>68U05</span><span>I.3.5</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.11435v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.11435v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer
  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhongpan Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although the Transformer has become the cornerstone of modern AI, its autoregressive inference suffers from a linearly growing KV Cache and a computational complexity of O(N^2 d), severely hindering its ability to process ultra-long sequences. To overcome this limitation, this paper introduces the TConstFormer architecture, building upon our previous work, TLinFormer. TConstFormer employs an innovative periodic state update mechanism to achieve a truly constant-size O(1) KV Cache. The computational complexity of this mechanism is also O(1) in an amortized sense: it performs purely constant-time computations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single linear-time global information synchronization only on the $k$-th step. Theoretical calculations and experimental results demonstrate that TConstFormer exhibits an overwhelming advantage over baseline models in terms of speed, memory efficiency, and overall performance on long-text inference tasks. This breakthrough paves the way for efficient and robust streaming language model applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-29T19:23:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00202v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00202v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Democratizing Agentic AI with Fast Test-Time Scaling on the Edge</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Mark Chen, Zhiwen Mo, Guanxi Lu, Shuang Liang, Lingxiao Ma, Wayne Luk, Hongxiang Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deploying agentic AI on edge devices is crucial for privacy and responsiveness, but memory constraints typically relegate these systems to smaller Large Language Models (LLMs) with inferior reasoning capabilities. Test-Time Scaling (TTS) can bridge this reasoning gap by dedicating more compute during inference, but existing methods incur prohibitive overhead on edge hardware. To overcome this, we introduce FlashTTS, a serving system that makes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces three synergistic optimizations: (i) Speculative Beam Extension to mitigate system stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model Memory Allocation to dynamically balance memory between generation and verification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache reuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on a single consumer GPU (24 GB) to match the accuracy and latency of large cloud models. Our evaluation demonstrates that FlashTTS achieves an average 2.2x higher goodput and reduces latency by 38%-68% compared to a vLLM baseline, paving the way for democratized, high-performance agentic AI on edge devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-29T19:12:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00195v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00195v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Towards Compute-Optimal Many-Shot In-Context Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shahriar Golchin, Yanfei Chen, Rujun Han, Manan Gandhi, Tianli Yu, Swaroop Mishra, Mihai Surdeanu, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-29T18:45:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.16217v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.16217v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Neural Visibility Cache for Real-Time Light Sampling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jakub Bok≈°ansk√Ω, Daniel Meister
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Direct illumination with many lights is an inherent component of physically-based rendering, remaining challenging, especially in real-time scenarios. We propose an online-trained neural cache that stores visibility between lights and 3D positions. We feed light visibility to weighted reservoir sampling (WRS) to sample a light source. The cache is implemented as a fully-fused multilayer perceptron (MLP) with multi-resolution hash-grid encoding, enabling online training and efficient inference on modern GPUs in real-time frame rates. The cache can be seamlessly integrated into existing rendering frameworks and can be used in combination with other real-time techniques such as spatiotemporal reservoir sampling (ReSTIR).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-29T09:58:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.05930v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.05930v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting
  Framework for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zishuai Zhang, Hainan zhang, Weihua Li, Qinnan zhang, jin Dong, Yongxin Tong, Zhiming Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-29T07:40:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.15683v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.15683v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode
  Disaggregation in Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Zhang, Mengsi Lyu, Yulong Ao, Yonghua Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a novel pruning method for PD disaggregation inference, enabling more precise and efficient block and KV Cache pruning. Our approach constructs pruning and distillation sets to perform iterative block removal independently for the prefill and decode stages, obtaining better pruning solutions. Moreover, we introduce a token-aware cache pruning mechanism that retains all KV Cache in the prefill stage but selectively reuses entries for the first and last token sequences in selected layers during decode, reducing communication costs with minimal overhead. Extensive experiments demonstrate that our approach consistently achieves strong performance in both PD disaggregation and PD unified settings without disaggregation. Under the default settings, our method achieves a 20.56% inference speedup and a 4.95 times reduction in data transmission bandwidth consumption.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-29T02:29:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.04467v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.04467v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Deep Multiple Quantization Network on Long Behavior Sequence for
  Click-Through Rate Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuoxing Wei, Qi Liu, Qingchen Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Click-Through Rate (CTR) prediction, the long behavior sequence, comprising the user's long period of historical interactions with items has a vital influence on assessing the user's interest in the candidate item. Existing approaches strike efficiency and effectiveness through a two-stage paradigm: first retrieving hundreds of candidate-related items and then extracting interest intensity vector through target attention. However, we argue that the discrepancy in target attention's relevance distribution between the retrieved items and the full long behavior sequence inevitably leads to a performance decline. To alleviate the discrepancy, we propose the Deep Multiple Quantization Network (DMQN) to process long behavior sequence end-to-end through compressing the long behavior sequence. Firstly, the entire spectrum of long behavior sequence will be quantized into multiple codeword sequences based on multiple independent codebooks. Hierarchical Sequential Transduction Unit is incorporated to facilitate the interaction of reduced codeword sequences. Then, attention between the candidate and multiple codeword sequences will output the interest vector. To enable online serving, intermediate representations of the codeword sequences are cached, significantly reducing latency. Our extensive experiments on both industrial and public datasets confirm the effectiveness and efficiency of DMQN. The A/B test in our advertising system shows that DMQN improves CTR by 3.5% and RPM by 2.0%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:58:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3726302.3730177' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20865v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20865v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yang Xiang, Fernando Garc√≠a-Redondo, Arvind Sharma, Van Dai Nguyen, Andrea Fantini, Philippe Matagne, Siddharth Rao, Subhali Subhechha, Lynn Verschueren, Mohammed Aftab Baig, Marie Garcia Bardon, Geert Hellings
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work explores the cross-node scaling potential of SOT-MRAM for last-level caches (LLCs) under heterogeneous system scaling paradigm. We perform extensive Design-Technology Co-Optimization (DTCO) exercises to evaluate the bitcell footprint for different cell configurations at a representative 7 nm technology and to assess their implications on read and write power-performance. We crucially identify the MTJ routing struggle in conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary bitcell area scaling challenge and propose to use BEOL read selectors (BEOL RSs) that enable (10 -- 40) % bitcell area reduction and eventually match sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet the required SOT switching current, provided the magnetic free layer properties be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This is particularly to attribute to their (i) more available Si fins for write transistor and (ii) lower bitline resistance at reduced cell width. We nevertheless underscore the read tradeoff associated with BEOL RSs, with the low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost relative to 2T1R. This article thus highlights the realistic prospects and hurdles of BEOL RSs towards holistic power-performance-area scaling of SOT-MRAM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:49:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18250v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18250v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport
  Equation Solver for Fast Scatter Correction in Multi-Spectral CT</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guoxi Zhu, Li Zhang, Zhiqiang Chen, Hewei Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> X-ray scatter has been a serious concern in computed tomography (CT), leading to image artifacts and distortion of CT values. The linear Boltzmann transport equation (LBTE) is recognized as a fast and accurate approach for scatter estimation. However, for multi-spectral CT, it is cumbersome to compute multiple scattering components for different spectra separately when applying LBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum Decomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute X-ray scatter distributions from CT acquisitions at two or more different spectra simultaneously, in a unified framework with no sacrifice in accuracy and nearly no increase in computation in theory. First, a matrixed-spectrum solver of LBTE is obtained by introducing an additional label dimension to expand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a principle of selection of basis using the QR decomposition, along with the above solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter correction method can be established for multi-spectral CT. We validate the effectiveness and accuracy of our method by comparing it with the Monte Carlo method, including the computational time. We also evaluate the scatter correction performance using two different phantoms for fast-kV switching based dual-energy CT, and using an elliptical phantom in a numerical simulation for kV-modulation enabled CT scans, validating that our proposed method can significantly reduce the computational cost at multiple spectra and effectively reduce scatter artifact in reconstructed CT images.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:05:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.med-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20524v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20524v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content
  Caching in Emerging Mega-Constellations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoyang Shi, Xing Zhang, Sitong Li, Minghang Li, Xinming Lu, Shaoxiang Xu, Guoquan Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Significant latency in global content delivery primarily arises from insufficient terrestrial infrastructure. Deploying space-based content delivery networks within emerging mega-constellations provides an effective means to bridge the digital divide. However, space-based caching faces constraints from physical-layer dynamics, including dynamic topologies, time-varying inter-satellite link conditions, and limited onboard energy. In addition, existing mechanisms often lack fine-grained content categorization and global optimization. This paper proposes MegaCacheX, a cost-effective hierarchical framework for collaborative content distribution that achieves "Earth-independence" by providing cloud services directly from space. Specifically, data centers in Sun-synchronous orbit act as primary content sources, while caching nodes in mega-constellations and ground stations collaboratively form a distributed edge layer. MegaCacheX optimizes caching strategies by integrating content popularity, regional user distribution, and satellite trajectory predictions. Multi-tier caching nodes serve as service anchors, enabling seamless content delivery with low latency. A prototype implemented on a microservices-based, containerized testbed demonstrates that MegaCacheX reduces global content access latency by about 36% compared to baseline approaches, while maintaining cost efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T05:22:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20433v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20433v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Breaking Diffusion with Cache: Exploiting Approximate Caches in
  Diffusion Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Desen Sun, Shuncheng Jie, Sihang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models are a powerful class of generative models that produce content, such as images, from user prompts, but they are computationally intensive. To mitigate this cost, recent academic and industry work has adopted approximate caching, which reuses intermediate states from similar prompts in a cache. While efficient, this optimization introduces new security risks by breaking isolation among users. This work aims to comprehensively assess new security vulnerabilities arising from approximate caching. First, we demonstrate a remote covert channel established with the cache, where a sender injects prompts with special keywords into the cache and a receiver can recover that even after days, to exchange information. Second, we introduce a prompt stealing attack using the cache, where an attacker can recover existing cached prompts based on cache hit prompts. Finally, we introduce a poisoning attack that embeds the attacker's logos into the previously stolen prompt, to render them in future user prompts that hit the cache. These attacks are all performed remotely through the serving system, which indicates severe security vulnerabilities in approximate caching.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T04:46:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20424v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20424v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full
  Context-Aware Linear Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhongpan Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Transformer architecture has become a cornerstone of modern artificial intelligence, but its core self-attention mechanism suffers from a complexity bottleneck that scales quadratically with sequence length, severely limiting its application in long-sequence tasks. To address this challenge, existing linear attention methods typically sacrifice model performance by relying on data-agnostic kernel approximations or restrictive context selection. This paper returns to the first principles of connectionism, starting from the topological structure of information flow, to introduce a novel linear attention architecture-\textbf{TLinFormer}. By reconfiguring neuron connection patterns, TLinFormer achieves strict linear complexity while computing exact attention scores and ensuring information flow remains aware of the full historical context. This design aims to bridge the performance gap prevalent between existing efficient attention methods and standard attention. Through a series of experiments, we systematically evaluate the performance of TLinFormer against a standard Transformer baseline on long-sequence inference tasks. The results demonstrate that TLinFormer exhibits overwhelming advantages in key metrics such as \textbf{inference latency}, \textbf{KV cache efficiency}, \textbf{memory footprint}, and \textbf{overall speedup}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T04:10:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20407v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20407v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 ASVD: Activation-aware Singular Value Decomposition for Compressing
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhihang Yuan, Yuzhang Shang, Yue Song, Dawei Yang, Qiang Wu, Yan Yan, Guangyu Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from (1) the distribution variance in the LLM activations and (2) the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T03:57:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.05821v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.05821v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Climber: Toward Efficient Scaling Laws for Large Recommendation Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Songpei Xu, Shijia Wang, Da Guo, Xianwen Guo, Qiang Xiao, Bin Huang, Guanlin Wu, Chuanjiang Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based generative models have achieved remarkable success across domains with various scaling law manifestations. However, our extensive experiments reveal persistent challenges when applying Transformer to recommendation systems: (1) Transformer scaling is not ideal with increased computational resources, due to structural incompatibilities with recommendation-specific features such as multi-source data heterogeneity; (2) critical online inference latency constraints (tens of milliseconds) that intensify with longer user behavior sequences and growing computational demands. We propose Climber, an efficient recommendation framework comprising two synergistic components: the model architecture for efficient scaling and the co-designed acceleration techniques. Our proposed model adopts two core innovations: (1) multi-scale sequence extraction that achieves a time complexity reduction by a constant factor, enabling more efficient scaling with sequence length; (2) dynamic temperature modulation adapting attention distributions to the multi-scenario and multi-behavior patterns. Complemented by acceleration techniques, Climber achieves a 5.15$\times$ throughput gain without performance degradation by adopting a "single user, multiple item" batched processing and memory-efficient Key-Value caching. Comprehensive offline experiments on multiple datasets validate that Climber exhibits a more ideal scaling curve. To our knowledge, this is the first publicly documented framework where controlled model scaling drives continuous online metric growth (12.19\% overall lift) without prohibitive resource costs. Climber has been successfully deployed on Netease Cloud Music, one of China's largest music streaming platforms, serving tens of millions of users daily.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T01:40:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746252.3761561' target='_blank'>doi</a><a href='http://arxiv.org/abs/2502.09888v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.09888v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and
  High-Quality Language Model Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shaoting Feng, Hanchen Li, Kuntai Du, Zhuohan Gu, Yuhan Liu, Jiayi Yao, Siddhant Ray, Samuel Shen, Yihua Cheng, Ganesh Ananthanarayanan, Junchen Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) applications often reuse previously processed context, such as chat history and documents, which introduces significant redundant computation. Existing LLM serving systems address such redundant computation by storing the KV caches of processed context and loading the corresponding KV cache when a new request reuses the context. Further, as these LLM applications scale, the total size of KV caches becomes excessively large and requires both DRAM and SSD for full storage.   However, prior work that stores KV caches in DRAM and SSD suffers from high loading delays, as most KV cache hits come from SSD, which is slow to load. To increase the KV cache hit rate on DRAM, we identify lossy KV cache compression as a promising approach. We design a lossy compression system that decides the compression algorithm, compression rate and device placement for each KV cache entry to maximise DRAM hits and minimise loading delay without significantly degrading generation quality. Compared to various static compression baselines across three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at the same quality and 6--55% quality improvements at the same delay.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T00:46:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00105v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00105v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource
  Allocation and Markov Decision Process in Named Data Networking (NDN)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fatemeh Roshanzadeh, Hamid Barati, Ali Barati
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Named Data Networking (NDN) represents a transformative shift in network architecture, prioritizing content names over host addresses to enhance data dissemination. Efficient queue and resource management are critical to NDN performance, especially under dynamic and high-traffic conditions. This paper introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR) algorithm. MDPF enables routers to intelligently predict optimal forwarding decisions based on key metrics such as bandwidth, delay, and the number of unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation among competing data flows. The proposed method models each router as a learning agent capable of adjusting its strategies through continuous feedback and probabilistic updates. Simulation results using ndnSIM demonstrate that DRR-MDPF significantly outperforms state-of-the-art strategies including SAF, RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and heavy traffic, offering enhanced adaptability and lower computational complexity due to its single-path routing design. Furthermore, its multi-metric decision-making capability enables more accurate interface selection, leading to optimized network performance. Overall, DRR-MDPF serves as an intelligent, adaptive, and scalable queue management solution for NDN, effectively addressing core challenges such as resource allocation, congestion control, and route optimization in dynamic networking environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T21:05:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20272v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20272v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 SpeedMalloc: Improving Multi-threaded Applications via a Lightweight
  Core for Memory Allocation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruihao Li, Qinzhe Wu, Krishna Kavi, Gayatri Mehta, Jonathan C. Beard, Neeraja J. Yadwadkar, Lizy K. John
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory allocation, though constituting only a small portion of the executed code, can have a "butterfly effect" on overall program performance, leading to significant and far-reaching impacts. Despite accounting for just approximately 5% of total instructions, memory allocation can result in up to a 2.7x performance variation depending on the allocator used. This effect arises from the complexity of memory allocation in modern multi-threaded multi-core systems, where allocator metadata becomes intertwined with user data, leading to cache pollution or increased cross-thread synchronization overhead. Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a potential direction to improve the allocator performance and mitigate cache pollution. However, these accelerators currently have limited support for multi-threaded applications, and synchronization between cores and accelerators remains a significant challenge.   We present SpeedMalloc, using a lightweight support-core to process memory allocation tasks in multi-threaded applications. The support-core is a lightweight programmable processor with efficient cross-core data synchronization and houses all allocator metadata in its own caches. This design minimizes cache conflicts with user data and eliminates the need for cross-core metadata synchronization. In addition, using a general-purpose core instead of domain-specific accelerators makes SpeedMalloc capable of adopting new allocator designs. We compare SpeedMalloc with state-of-the-art software and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on multithreaded workloads over these five allocators, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T20:18:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20253v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20253v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 MODE: Mixture of Document Experts for RAG</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rahul Anand
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) often relies on large vector databases and cross-encoders tuned for large-scale corpora, which can be excessive for small, domain-specific collections. We present MODE (Mixture of Document Experts), a lightweight alternative that replaces fine-grained nearest-neighbor search with cluster-and-route retrieval. Documents are embedded, grouped into semantically coherent clusters, and represented by cached centroids. At query time, we route to the top centroid(s) and retrieve context only within those clusters, eliminating external vector-database infrastructure and reranking while keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks, MODE matches or exceeds a dense-retrieval baseline in answer quality while reducing end-to-end retrieval time. Ablations show that cluster granularity and multi-cluster routing control the recall/precision trade-off, and that tighter clusters improve downstream accuracy. MODE offers a practical recipe for small and medium corpora where simplicity, speed, and topical focus matter.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:45:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00100v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00100v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Apple Intelligence Foundation Language Models: Tech Report 2025</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ethan Li, Anders Boesen Lindbo Larsen, Chen Zhang, Xiyou Zhou, Jun Qin, Dian Ang Yap, Narendran Raghavan, Xuankai Chang, Margit Bowler, Eray Yildiz, John Peebles, Hannah Gillis Coleman, Matteo Ronchi, Peter Gray, Keen You, Anthony Spalvieri-Kruse, Ruoming Pang, Reed Li, Yuli Yang, Emad Soroush, Zhiyun Lu, Crystal Xiao, Rong Situ, Jordan Huffaker, David Griffiths, Zaid Ahmed, Peng Zhang, Daniel Parilla, Asaf Liberman, Jennifer Mallalieu, Parsa Mazaheri, Qibin Chen, Manjot Bilkhu, Aonan Zhang, Eric Wang, Dave Nelson, Michael FitzMaurice, Thomas Voice, Jeremy Liu, Josh Shaffer, Shiwen Zhao, Prasanth Yadla, Farzin Rasteh, Pengsheng Guo, Arsalan Farooq, Jeremy Snow, Stephen Murphy, Tao Lei, Minsik Cho, George Horrell, Sam Dodge, Lindsay Hislop, Sumeet Singh, Alex Dombrowski, Aiswarya Raghavan, Sasha Sirovica, Mandana Saebi, Faye Lao, Max Lam, TJ Lu, Zhaoyang Xu, Karanjeet Singh, Marc Kirchner, David Mizrahi, Rajat Arora, Haotian Zhang, Henry Mason, Lawrence Zhou, Yi Hua, Ankur Jain, Felix Bai, Joseph Astrauskas, Floris Weers, Josh Gardner, Mira Chiang, Yi Zhang, Pulkit Agrawal, Tony Sun, Quentin Keunebroek, Matthew Hopkins, Bugu Wu, Tao Jia, Chen Chen, Xingyu Zhou, Nanzhu Wang, Peng Liu, Ruixuan Hou, Rene Rauch, Yuan Gao, Afshin Dehghan, Jonathan Janke, Zirui Wang, Cha Chen, Xiaoyi Ren, Feng Nan, Josh Elman, Dong Yin, Yusuf Goren, Jeff Lai, Yiran Fei, Syd Evans, Muyang Yu, Guoli Yin, Yi Qin, Erin Feldman, Isha Garg, Aparna Rajamani, Karla Vega, Walker Cheng, TJ Collins, Hans Han, Raul Rea Menacho, Simon Yeung, Sophy Lee, Phani Mutyala, Ying-Chang Cheng, Zhe Gan, Sprite Chu, Justin Lazarow, Alessandro Pappalardo, Federico Scozzafava, Jing Lu, Erik Daxberger, Laurent Duchesne, Jen Liu, David G√ºera, Stefano Ligas, Mary Beth Kery, Brent Ramerth, Ciro Sannino, Marcin Eichner, Haoshuo Huang, Rui Qian, Moritz Schwarzer-Becker, David Riazati, Mingfei Gao, Bailin Wang, Jack Cackler, Yang Lu, Ransen Niu, John Dennison, Guillaume Klein, Jeffrey Bigham, Deepak Gopinath, Navid Shiee, Darren Botten, Guillaume Tartavel, Alex Guillen Garcia, Sam Xu, Victoria M√∂nchJuan Haladjian, Zi-Yi Dou, Matthias Paulik, Adolfo Lopez Mendez, Zhen Li, Hong-You Chen, Chao Jia, Dhaval Doshi, Zhengdong Zhang, Raunak Manjani, Aaron Franklin, Zhile Ren, David Chen, Artsiom Peshko, Nandhitha Raghuram, Hans Hao, Jiulong Shan, Kavya Nerella, Ramsey Tantawi, Vivek Kumar, Saiwen Wang, Brycen Wershing, Bhuwan Dhingra, Dhruti Shah, Ob Adaranijo, Xin Zheng, Tait Madsen, Hadas Kotek, Chang Liu, Yin Xia, Hanli Li, Suma Jayaram, Yanchao Sun, Ahmed Fakhry, Vasileios Saveris, Dustin Withers, Yanghao Li, Alp Aygar, Andres Romero Mier Y Teran, Kaiwei Huang, Mark Lee, Xiujun Li, Yuhong Li, Tyler Johnson, Jay Tang, Joseph Yitan Cheng, Futang Peng, Andrew Walkingshaw, Lucas Guibert, Abhishek Sharma, Cheng Shen, Piotr Maj, Yasutaka Tanaka, You-Cyuan Jhang, Vivian Ma, Tommi Vehvilainen, Kelvin Zou, Jeff Nichols, Matthew Lei, David Qiu, Yihao Qian, Gokul Santhanam, Wentao Wu, Yena Han, Dominik Moritz, Haijing Fu, Mingze Xu, Vivek Rathod, Jian Liu, Louis D'hauwe, Qin Ba, Haitian Sun, Haoran Yan, Philipp Dufter, Anh Nguyen, Yihao Feng, Emma Wang, Keyu He, Rahul Nair, Sanskruti Shah, Jiarui Lu, Patrick Sonnenberg, Jeremy Warner, Yuanzhi Li, Bowen Pan, Ziyi Zhong, Joe Zhou, Sam Davarnia, Olli Saarikivi, Irina Belousova, Rachel Burger, Shang-Chen Wu, Di Feng, Bas Straathof, James Chou, Yuanyang Zhang, Marco Zuliani, Eduardo Jimenez, Abhishek Sundararajan, Xianzhi Du, Chang Lan, Nilesh Shahdadpuri, Peter Grasch, Sergiu Sima, Josh Newnham, Varsha Paidi, Jianyu Wang, Kaelen Haag, Alex Braunstein, Daniele Molinari, Richard Wei, Brenda Yang, Nicholas Lusskin, Joanna Arreaza-Taylor, Meng Cao, Nicholas Seidl, Simon Wang, Jiaming Hu, Yiping Ma, Mengyu Li, Kieran Liu, Hang Su, Sachin Ravi, Chong Wang, Xin Wang, Kevin Smith, Haoxuan You, Binazir Karimzadeh, Rui Li, Jinhao Lei, Wei Fang, Alec Doane, Sam Wiseman, Ismael Fernandez, Jane Li, Andrew Hansen, Javier Movellan, Christopher Neubauer, Hanzhi Zhou, Chris Chaney, Nazir Kamaldin, Valentin Wolf, Fernando Berm√∫dez-Medina, Joris Pelemans, Peter Fu, Howard Xing, Xiang Kong, Wayne Shan, Gabriel Jacoby-Cooper, Dongcai Shen, Tom Gunter, Guillaume Seguin, Fangping Shi, Shiyu Li, Yang Xu, Areeba Kamal, Dan Masi, Saptarshi Guha, Qi Zhu, Jenna Thibodeau, Changyuan Zhang, Rebecca Callahan, Charles Maalouf, Wilson Tsao, Boyue Li, Qingqing Cao, Naomy Sabo, Cheng Leong, Yi Wang, Anupama Mann Anupama, Colorado Reed, Kenneth Jung, Zhifeng Chen, Mohana Prasad Sathya Moorthy, Yifei He, Erik Hornberger, Devi Krishna, Senyu Tong, Michael, Lee, David Haldimann, Yang Zhao, Bowen Zhang, Chang Gao, Chris Bartels, Sushma Rao, Nathalie Tran, Simon Lehnerer, Co Giang, Patrick Dong, Junting Pan, Biyao Wang, Dongxu Li, Mehrdad Farajtabar, Dongseong Hwang, Grace Duanmu, Eshan Verma, Sujeeth Reddy, Qi Shan, Hongbin Gao, Nan Du, Pragnya Sridhar, Forrest Huang, Yingbo Wang, Nikhil Bhendawade, Diane Zhu, Sai Aitharaju, Fred Hohman, Lauren Gardiner, Chung-Cheng Chiu, Yinfei Yang, Alper Kokmen, Frank Chu, Ke Ye, Kaan Elgin, Oron Levy, John Park, Donald Zhang, Eldon Schoop, Nina Wenzel, Michael Booker, Hyunjik Kim, Chinguun Erdenebileg, Nan Dun, Eric Liang Yang, Priyal Chhatrapati, Vishaal Mahtani, Haiming Gang, Kohen Chia, Deepa Seshadri, Donghan Yu, Yan Meng, Kelsey Peterson, Zhen Yang, Yongqiang Wang, Carina Peng, Doug Kang, Anuva Agarwal, Albert Antony, Juan Lao Tebar, Albin Madappally Jose, Regan Poston, Andy De Wang, Gerard Casamayor, Elmira Amirloo, Violet Yao, Wojciech Kryscinski, Kun Duan, Lezhi L
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.   A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:34:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13575v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13575v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Re-thinking Memory-Bound Limitations in CGRAs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangfeng Liu, Zhe Jiang, Anzhen Zhu, Xiaomeng Han, Mingsong Lyu, Qingxu Deng, Nan Guan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators commonly employed to boost performance in workloads with iterative structures. Existing research typically focuses on compiler or architecture optimizations aimed at improving CGRA performance, energy efficiency, flexibility, and area utilization, under the idealistic assumption that kernels can access all data from Scratchpad Memory (SPM). However, certain complex workloads-particularly in fields like graph analytics, irregular database operations, and specialized forms of high-performance computing (e.g., unstructured mesh simulations)-exhibit irregular memory access patterns that hinder CGRA utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To address this challenge, we conduct a thorough analysis of the underlying causes of performance degradation, then propose a redesigned memory subsystem and refine the memory model. With both microarchitectural and theoretical optimization, our solution can effectively manage irregular memory accesses through CGRA-specific runahead execution mechanism and cache reconfiguration techniques. Our results demonstrate that we can achieve performance comparable to the original SPM-only system while requiring only 1.27% of the storage size. The runahead execution mechanism achieves an average 3.04x speedup (up to 6.91x), with cache reconfiguration technique providing an additional 6.02% improvement, significantly enhancing CGRA performance for irregular memory access patterns.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:13:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>B.3.0; B.6.0</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3760386' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.09570v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09570v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 ERTACache: Error Rectification and Timesteps Adjustment for Efficient
  Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xurui Peng, Hong Liu, Chenqian Yan, Rui Ma, Fangmin Chen, Xing Wang, Zhihua Wu, Songwei Liu, Mingbao Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models suffer from substantial computational overhead due to their inherently iterative inference process. While feature caching offers a promising acceleration strategy by reusing intermediate outputs across timesteps, naive reuse often incurs noticeable quality degradation. In this work, we formally analyze the cumulative error introduced by caching and decompose it into two principal components: feature shift error, caused by inaccuracies in cached outputs, and step amplification error, which arises from error propagation under fixed timestep schedules. To address these issues, we propose ERTACache, a principled caching framework that jointly rectifies both error types. Our method employs an offline residual profiling stage to identify reusable steps, dynamically adjusts integration intervals via a trajectory-aware correction coefficient, and analytically approximates cache-induced errors through a closed-form residual linearization model. Together, these components enable accurate and efficient sampling under aggressive cache reuse. Extensive experiments across standard image and video generation benchmarks show that ERTACache achieves up to 2x inference speedup while consistently preserving or even improving visual quality. Notably, on the state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x acceleration with minimal VBench degradation, effectively maintaining baseline fidelity while significantly improving efficiency. The code is available at https://github.com/bytedance/ERTACache.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:37:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21091v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21091v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed
  Criticality Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Diogo Costa, Jose Martins, Sandro Pinto
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate heterogeneous computing platforms, combining general-purpose processors with specialized accelerators such as AI engines, GPUs, and high-speed networking interfaces. This heterogeneity introduces challenges, as these accelerators and DMA-capable devices act as independent bus masters, directly accessing memory. Consequently, ensuring both security and timing predictability in such environments becomes critical. To address these concerns, the Input-Output Memory Management Unit (IOMMU) plays a key role in mediating and regulating memory access, preventing unauthorized transactions while enforcing isolation and access control policies. While prior work has explored IOMMU-related side-channel vulnerabilities from a security standpoint, its role in performance interference remains largely unexplored. Moreover, many of the same architectural properties that enable side-channel leakage, such as shared TLBs, caching effects, and translation overheads, can also introduce timing unpredictability. In this work, we analyze the contention effects within IOMMU structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how their shared nature introduce unpredictable delays. Our findings reveal that IOMMU-induced interference primarily affects small memory transactions, where translation overheads significantly impact execution time. Additionally, we hypothesize that contention effects arising from IOTLBs exhibit similar behavior across architectures due to shared caching principles, such as prefetching and hierarchical TLB structures. Notably, our experiments show that IOMMU interference can delay DMA transactions by up to 1.79x for lower-size transfers on the Arm SMMUv2 implementation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:30:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19670v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19670v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 FiRST: Finetuning Router-Selective Transformers for Input-Adaptive
  Latency Reduction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across different domains such as vision and language processing. However, due to sequential processing through a stack of transformer layers, autoregressive decoding faces significant computation/latency challenges, particularly in resource-constrained environments like mobile and edge devices. Existing approaches in literature that aim to improve latency via skipping layers have two distinct flavors - 1) Early exit, and 2) Input-agnostic heuristics where tokens exit at pre-determined layers irrespective of input sequence. Both the above strategies have limitations - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework and the latter does not capture the variation in layer importance across tasks or more generally, across input sequences. To address both limitations, we propose FiRST, an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence - the prompt (during the prefill stage) decides which layers will be skipped during decoding. FiRST preserves compatibility with KV caching enabling faster inference while being quality-aware. FiRST is model-agnostic and can be easily enabled on any pre-trained LLM. Our approach reveals that input adaptivity is critical - indeed, different task-specific middle layers play a crucial role in evolving hidden representations depending on tasks. Extensive experiments show that FiRST significantly reduces latency while outperforming other layer selection strategies in quality metics. It retains competitive performance to base model (without layer skipping) and in some cases, even improves upon it. FiRST is thus a promising and efficient solution for LLM deployment in low-resource environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T04:58:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.12513v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.12513v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D
  Space</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T17:59:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19247v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19247v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Enabling MoE on the Edge via Importance-Driven Expert Scheduling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T12:32:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18983v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18983v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Rethinking Caching for LLM Serving Systems: Beyond Traditional
  Heuristics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jungwoo Kim, Minsang Kim, Jaeheon Lee, Chanwoo Moon, Heejin Kim, Taeho Hwang, Woosuk Chung, Yeseong Kim, Sungjin Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serving Large Language Models (LLMs) at scale requires meeting strict Service Level Objectives (SLOs) under severe computational and memory constraints. Nevertheless, traditional caching strategies fall short: exact-matching and prefix caches neglect query semantics, while state-of-the-art semantic caches remain confined to traditional intuitions, offering little conceptual departure. Building on this, we present SISO, a semantic caching system that redefines efficiency for LLM serving. SISO introduces centroid-based caching to maximize coverage with minimal memory, locality-aware replacement to preserve high-value entries, and dynamic thresholding to balance accuracy and latency under varying workloads. Across diverse datasets, SISO delivers up to 1.71$\times$ higher hit ratios and consistently stronger SLO attainment compared to state-of-the-art systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T07:09:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18736v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18736v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Krul: Efficient State Restoration for Multi-turn Conversations with
  Dynamic Cross-layer KV Sharing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junyi Wen, Junyuan Liang, Zicong Hong, Wuhui Chen, Ting Cai, Zibin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge, primarily due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. To address this, existing approaches compress KV caches across adjacent layers with highly similar attention patterns. However, these methods often apply a fixed compression scheme across all conversations, selecting the same layer pairs for compression without considering conversation-specific attention dynamics. This static strategy overlooks variability in attention pattern similarity across different conversations, which can lead to noticeable accuracy degradation.   We present Krul, a multi-turn LLM inference system that enables accurate and efficient KV cache restoration. Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: 1) a preemptive compression strategy selector to preserve critical context for future conversation turns and selects a customized strategy for the conversation; 2) a token-wise heterogeneous attention similarity estimator to mitigate the attention similarity computation and storage overhead during model generation; 3) a bubble-free restoration scheduler to reduce potential bubbles brought by the imbalance of recomputing and loading stream due to compressed KV caches. Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T01:55:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08045v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08045v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuoping Yang, Jinming Zhuang, Xingzhen Chen, Alex K. Jones, Peipei Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> GPUs are critical for compute-intensive applications, yet emerging workloads such as recommender systems, graph analytics, and data analytics often exceed GPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as external memory, and the GPU-centric approach enables GPU threads to directly issue NVMe requests, further avoiding CPU intervention. However, current GPU-centric approaches adopt synchronous I/O, forcing threads to stall during long communication delays.   We propose AGILE, a lightweight asynchronous GPU-centric I/O library that eliminates deadlock risks and integrates a flexible HBM-based software cache. AGILE overlaps computation and I/O, improving performance by up to 1.88$\times$ across workloads with diverse computation-to-communication ratios. Compared to BaM on DLRM, AGILE achieves up to 1.75$\times$ speedup through efficient design and overlapping; on graph applications, AGILE reduces software cache overhead by up to 3.12$\times$ and NVMe I/O overhead by up to 2.85$\times$; AGILE also lowers per-thread register usage by up to 1.32$\times$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T01:45:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.19365v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.19365v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Strata: Hierarchical Context Caching for Long Context Language Model
  Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiqiang Xie, Ziyi Xu, Mark Zhao, Yuwei An, Vikram Sharma Mailthody, Scott Mahlke, Michael Garland, Christos Kozyrakis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) with expanding context windows face significant performance hurdles. While caching key-value (KV) states is critical for avoiding redundant computation, the storage footprint of long-context caches quickly exceeds GPU memory capacity, forcing production systems to adopt hierarchical caching across memory hierarchies. However, transferring large cached contexts back to the GPU introduces severe performance bottlenecks: fragmented I/O from paged layouts prevents full bandwidth utilization, and existing schedulers fail to account for cache-loading delays, leaving systems loading-bound rather than compute-bound. We present Strata, a hierarchical context caching framework designed for efficient long context LLM serving. Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling GPU and CPU memory layouts and employs cache-aware request scheduling to balance compute with I/O latency and overlapping unavoidable stalls with complementary tasks. Built on SGLang and deployed in production, Strata achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without degrading short-context performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T00:09:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18572v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18572v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Real-time 3D Visualization of Radiance Fields on Light Field Displays</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonghyun Kim, Cheng Sun, Michael Stengel, Matthew Chan, Andrew Russell, Jaehyun Jung, Wil Braithwaite, Shalini De Mello, David Luebke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Radiance fields have revolutionized photo-realistic 3D scene visualization by enabling high-fidelity reconstruction of complex environments, making them an ideal match for light field displays. However, integrating these technologies presents significant computational challenges, as light field displays require multiple high-resolution renderings from slightly shifted viewpoints, while radiance fields rely on computationally intensive volume rendering. In this paper, we propose a unified and efficient framework for real-time radiance field rendering on light field displays. Our method supports a wide range of radiance field representations, including NeRFs, 3D Gaussian Splatting, and Sparse Voxels, within a shared architecture based on a single-pass plane sweeping strategy and caching of shared, non-directional components. The framework generalizes across different scene formats without retraining, and avoids redundant computation across views. We further demonstrate a real-time interactive application on a Looking Glass display, achieving 200+ FPS at 512p across 45 views, enabling seamless, immersive 3D interaction. On standard benchmarks, our method achieves up to 22x speedup compared to independently rendering each view, while preserving image quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T22:21:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18540v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18540v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 DiskJoin: Large-scale Vector Similarity Join with SSD</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanqi Chen, Xiao Yan, Alexandra Meliou, Eric Lo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Similarity join--a widely used operation in data science--finds all pairs of items that have distance smaller than a threshold. Prior work has explored distributed computation methods to scale similarity join to large data volumes but these methods require a cluster deployment, and efficiency suffers from expensive inter-machine communication. On the other hand, disk-based solutions are more cost-effective by using a single machine and storing the large dataset on high-performance external storage, such as NVMe SSDs, but in these methods the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin, the first disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine. DiskJoin improves disk I/O by tailoring the data access patterns to avoid repetitive accesses and read amplification. It also uses main memory as a dynamic cache and carefully manages cache eviction to improve cache hit rate and reduce disk retrieval time. For further acceleration, we adopt a probabilistic pruning technique that can effectively prune a large number of vector pairs from computation. Our evaluation on real-world, large-scale datasets shows that DiskJoin significantly outperforms alternatives, achieving speedups from 50x to 1000x.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T21:07:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18494v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18494v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 MARM: Unlocking the Future of Recommendation Systems through Memory
  Augmentation and Scalable Complexity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiao Lv, Jiangxia Cao, Shijie Guan, Xiaoyou Zhou, Zhiguang Qi, Yaqiang Zang, Ming Li, Ben Wang, Kun Gai, Guorui Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling-law has guided the language model designing for past years, however, it is worth noting that the scaling laws of NLP cannot be directly applied to RecSys due to the following reasons: (1) The amount of training samples and model parameters is typically not the bottleneck for the model. Our recommendation system can generate over 50 billion user samples daily, and such a massive amount of training data can easily allow our model parameters to exceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the stability and robustness of the recommendation system, it is essential to control computational complexity FLOPs carefully. Considering the above differences with LLM, we can draw a conclusion that: for a RecSys model, compared to model parameters, the computational complexity FLOPs is a more expensive factor that requires careful control. In this paper, we propose our milestone work, MARM (Memory Augmented Recommendation Model), which explores a new cache scaling-laws successfully.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T15:48:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>N/A</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.09425v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.09425v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 ILRe: Intermediate Layer Retrieval for Context Compression in Causal
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate decoder layer offline, encodes context by streaming chunked prefill only up to that layer, and recalls tokens by the attention scores between the input query and full key cache in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance comparable to or better than the full context in the long context scenarios. Without additional post training or operator development, ILRe can process a single $1M$ tokens request in less than half a minute (speedup $\approx 180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T10:59:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17892v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17892v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 SuperGen: An Efficient Ultra-high-resolution Video Generation System
  with Sketching and Tiling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fanjiang Ye, Zepeng Zhao, Yi Mu, Jucheng Shen, Renjie Li, Kaijian Wang, Desen Sun, Saurabh Agarwal, Myungjin Lee, Triston Cao, Aditya Akella, Arvind Krishnamurthy, T. S. Eugene Ng, Zhengzhong Tu, Yuke Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SuperGen, an efficient tile-based framework for ultra-high-resolution video generation. SuperGen features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SuperGen incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SuperGen also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations demonstrate that SuperGen harvests the maximum performance gains while achieving high output quality across various benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T07:49:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17756v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17756v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 OmniCache: A Trajectory-Oriented Global Perspective on Training-Free
  Cache Reuse for Diffusion Transformer Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huanpeng Chu, Wei Wu, Guanyu Fen, Yutao Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure. In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction. Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T03:07:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16212v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16212v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters
  at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ge Shi, Hanieh Sadri, Qian Wang, Yu Zhang, Ying Xiong, Yong Zhang, Zhenan Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large language models to enhance their task-specific performance by selectively tuning the top-activated experts for the task. Serving these fine-tuned models at scale is challenging: deploying merged models in isolation is prohibitively resource-hungry, while existing multi-adapter serving systems with LoRA-style additive updates are incompatible with ESFT's expert-oriented paradigm. We present ExpertWeave, a system that serves multiple ESFT adapters concurrently over a single shared MoE base model, drastically reducing the memory footprint and improving resource utilization. To seamlessly integrate into existing inference pipelines for MoE models with non-intrusive modifications and minimal latency overhead, ExpertWeave introduces a virtual-memory-assisted expert weight manager that co-locates base-model and adapter experts without incurring memory overhead from fragmentation, and a fused kernel for batched rerouting to enable lightweight redirection of tokens to the appropriate experts at runtime. Our evaluations show that ExpertWeave can simultaneously serve multiple adapters of a 16B MoE model on a single accelerator where the baseline runs out of memory, or provides up to 94x more KV cache capacity and achieves up to 18% higher throughput while using comparable resources, all without compromising model accuracy. ExpertWeave maintains low overhead even when scaling to 20 adapters, with a 4-11% latency increase compared with serving the base model alone. Source code will be released soon.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T03:05:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17624v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17624v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Lightning Fast Caching-based Parallel Denoising Prediction for
  Accelerating Talking Head Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianzhi Long, Wenhao Sun, Rongcheng Tu, Dacheng Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based talking head models generate high-quality, photorealistic videos but suffer from slow inference, limiting practical applications. Existing acceleration methods for general diffusion models fail to exploit the temporal and spatial redundancies unique to talking head generation. In this paper, we propose a task-specific framework addressing these inefficiencies through two key innovations. First, we introduce Lightning-fast Caching-based Parallel denoising prediction (LightningCP), caching static features to bypass most model layers in inference time. We also enable parallel prediction using cached features and estimated noisy latents as inputs, efficiently bypassing sequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to further accelerate attention computations, exploiting the spatial decoupling in talking head videos to restrict attention to dynamic foreground regions. Additionally, we remove reference features in certain layers to bring extra speedup. Extensive experiments demonstrate that our framework significantly improves inference speed while preserving video quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T02:58:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.00052v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.00052v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated
  Prefill and Decode Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T02:24:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15881v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15881v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD
  NPUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aadesh Deshmukh, Venkata Yaswanth Raparti, Samuel Hsu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based deep learning models are increasingly deployed on energy, and DRAM bandwidth constrained devices such as laptops and gaming consoles, which presents significant challenges in meeting the latency requirements of the models. The industry is turning to neural processing units (NPUs) for superior performance-per-watt (perf/watt); however, efficiently mapping dynamic attention layers to the NPUs remains a challenging task. For optimizing perf/watt, AMD XDNA NPUs employ software managed caches and share system memory with host. This requires substantial engineering effort to unlock efficient tiling, buffer allocation, and data movement to extract the maximum efficiency from the device. This paper introduces Zen-Attention, a framework that optimizes DRAM bandwidth utilization in the attention layer of models by systematically exploring the complex design space of layer folding, tiling, and data-movement on the interconnect, and the tensor layouts to come up with an optimal solution. Our evaluation includes comparative analysis of end-to-end model latency and specific attention latency in each model. We demonstrate how the framework enhances mapping capabilities by varying input dimensions, which require padding and masking in the attention block. For representative transformer models, the Zen-Attention Framework achieves up to 4x improvement in the latency of the attention block and up to 32% improvement in end-to-end network latency compared to the baseline Unfolded- approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T01:33:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17593v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17593v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 RT-Cache: Training-Free Retrieval for Real-Time Manipulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: https://rt-cache.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T00:15:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.09040v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.09040v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 PRISM: Efficient Long-Range Reasoning With Short-Context LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-range tasks demand reasoning over long inputs. However, existing solutions are limited, e.g., long-context models require large compute budgets, parameter-efficient fine-tuning (PEFT) needs training data, and retrieval-augmented generation (RAG) entails complex task-specific designs. Though in-context approaches overcome many of these issues, methods with short-context LLMs are inefficient, trading context for processing more tokens. We introduce PRISM, a highly token-efficient in-context method based on structured schemas that outperforms baselines on diverse tasks with 4x shorter contexts. This approach produces concise outputs and efficiently leverages key-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny contexts without increasing costs or sacrificing quality, and generalizes to new tasks with minimal effort by generating schemas from task descriptions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T22:09:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.18914v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.18914v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Evaluating Compiler Optimization Impacts on zkVM Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thomas Gassmann, Stefanos Chaliasos, Thodoris Sotiropoulos, Zhendong Su
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Zero-knowledge proofs (ZKPs) are the cornerstone of programmable cryptography. They enable (1) privacy-preserving and verifiable computation across blockchains, and (2) an expanding range of off-chain applications such as credential schemes. Zero-knowledge virtual machines (zkVMs) lower the barrier by turning ZKPs into a drop-in backend for standard compilation pipelines. This lets developers write proof-generating programs in conventional languages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits. However, these VMs inherit compiler infrastructures tuned for traditional architectures rather than for proof systems. In particular, standard compiler optimizations assume features that are absent in zkVMs, including cache locality, branch prediction, or instruction-level parallelism. Therefore, their impact on proof generation is questionable.   We present the first systematic study of the impact of compiler optimizations on zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an unoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero and SP1). While standard LLVM optimization levels do improve zkVM performance (over 40\%), their impact is far smaller than on traditional CPUs, since their decisions rely on hardware features rather than proof constraints. Guided by a fine-grained pass-level analysis, we~\emph{slightly} refine a small set of LLVM passes to be zkVM-aware, improving zkVM execution time by up to 45\% (average +4.6\% on RISC Zero, +1\% on SP1) and achieving consistent proving-time gains. Our work highlights the potential of compiler-level optimizations for zkVM performance and opens new direction for zkVM-specific passes, backends, and superoptimizers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T20:51:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17518v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17518v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Practical Insertion-Only Convex Hull</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ivor van der Hoog, Henrik Reinst√§dtler, Eva Rotenberg
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Convex hull data structures are fundamental in computational geometry. We study insertion-only data structures, supporting various containment and intersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex hulls can be constructed in linear time using classical algorithms such as Graham scan. We investigate a variety of methods tailored to the insertion-only setting. We explore a broad selection of trade-offs involving robustness, memory access patterns, and space usage, providing an extensive evaluation of both existing and novel techniques. Logarithmic-time methods rely on pointer-based tree structures, which suffer in practice due to poor memory locality. Motivated by this, we develop a vector-based solution inspired by Overmars' logarithmic method. Our structure has worse asymptotic bounds, supporting queries in $O(\log^2 n)$ time, but stores data in $O(\log n)$ contiguous vectors, greatly improving cache performance.   Through empirical evaluation on real-world and synthetic data sets, we uncover surprising trends. Let $h$ denote the size of the convex hull. We show that a na\"ive $O(h)$ insertion-only algorithm based on Graham scan consistently outperforms both theoretical and practical state-of-the-art methods under realistic workloads, even on data sets with rather large convex hulls. While tree-based methods with $O(\log h)$ update times offer solid theoretical guarantees, they are never optimal in practice. In contrast, our vector-based logarithmic method, despite its theoretically inferior bounds, is highly competitive across all tested scenarios. It is optimal whenever the convex hull becomes large.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T19:28:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17496v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17496v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 TreePO: Bridging the Gap of Policy Optimization and Efficacy and
  Inference Efficiency with Heuristic Tree-based Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, Wenhao Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\% up to 43\% of the sampling design for the trained models, meanwhile showing up to 40\% reduction at trajectory-level and 35\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T16:52:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17445v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17445v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 TinySR: Pruning Diffusion for Real-World Image Super-Resolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Linwei Dong, Qingnan Fan, Yuhang Yu, Qi Zhang, Jinwei Chen, Yawei Luo, Changqing Zou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world image super-resolution (Real-ISR) focuses on recovering high-quality images from low-resolution inputs that suffer from complex degradations like noise, blur, and compression. Recently, diffusion models (DMs) have shown great potential in this area by leveraging strong generative priors to restore fine details. However, their iterative denoising process incurs high computational overhead, posing challenges for real-time applications. Although one-step distillation methods, such as OSEDiff and TSD-SR, offer faster inference, they remain fundamentally constrained by their large, over-parameterized model architectures. In this work, we present TinySR, a compact yet effective diffusion model specifically designed for Real-ISR that achieves real-time performance while maintaining perceptual quality. We introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy to facilitate more effective decision-making in depth pruning. We achieve VAE compression through channel pruning, attention removal and lightweight SepConv. We eliminate time- and prompt-related modules and perform pre-caching techniques to further speed up the model. TinySR significantly reduces computational cost and model size, achieving up to 5.68x speedup and 83% parameter reduction compared to its teacher TSD-SR, while still providing high quality results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T16:17:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17434v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17434v1' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose
  Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Shijian Lu, Nicu Sebe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:59:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06956v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06956v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Transforming Wearable Data into Personal Health Insights using Large
  Language Model Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mike A. Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory Y. McLean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, Xin Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:59:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.06464v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.06464v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for
  Dynamic Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahui Yang, Jason Jingzhou Liu, Yulong Li, Youssef Khaky, Kenneth Shaw, Deepak Pathak
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT's static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module. We evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. Video results and code available at https://deep-reactive-policy.com
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:59:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.CV</span><span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06953v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06953v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 On the Same Wavelength? Evaluating Pragmatic Reasoning in Language
  Models across Broad Concepts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Linlu Qiu, Cedegao E. Zhang, Joshua B. Tenenbaum, Yoon Kim, Roger P. Levy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Language use is shaped by pragmatics -- i.e., reasoning about communicative goals and norms in context. As language models (LMs) are increasingly used as conversational agents, it becomes ever more important to understand their pragmatic reasoning abilities. We propose an evaluation framework derived from Wavelength, a popular communication game where a speaker and a listener communicate about a broad range of concepts in a granular manner. We study a range of LMs on both language comprehension and language production using direct and Chain-of-Thought (CoT) prompting, and further explore a Rational Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM inference. We find that state-of-the-art LMs, but not smaller ones, achieve strong performance on language comprehension, obtaining similar-to-human accuracy and exhibiting high correlations with human judgments even without CoT prompting or RSA. On language production, CoT can outperform direct prompting, and using RSA provides significant improvements over both approaches. Our study helps identify the strengths and limitations in LMs' pragmatic reasoning abilities and demonstrates the potential for improving them with RSA, opening up future avenues for understanding conceptual representation, language understanding, and social reasoning in LMs and humans.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:59:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06952v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06952v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Revolutionizing Reinforcement Learning Framework for Diffusion Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:58:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06949v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06949v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liang Chen, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning (RL) has proven effective in incentivizing the reasoning abilities of large language models (LLMs), but suffers from severe efficiency challenges due to its trial-and-error nature. While the common practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this decoupled two-stage approach limits interaction between SFT and RL, thereby constraining overall effectiveness. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. By conditioning the SFT objective on the optimal RL policy, our approach enables SFT to meta-learn how to guide RL's optimization process. During training, the lower level performs RL updates while simultaneously receiving SFT supervision, and the upper level explicitly maximizes the cooperative gain-the performance advantage of joint SFT-RL training over RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:58:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06948v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06948v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Outcome-based Exploration for LLM Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuda Song, Julia Kempe, Remi Munos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. We analyze this phenomenon by viewing RL post-training as a sampling process and show that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. Our study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, we propose outcome-based exploration, which assigns exploration bonuses according to final outcomes. We introduce two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, we formalize the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:52:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06941v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06941v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 GWTC-4.0: Updating the Gravitational-Wave Transient Catalog with
  Observations from the First Part of the Fourth LIGO-Virgo-KAGRA Observing Run</h2>
                <div class="authors">
                    <strong>Authors:</strong> The LIGO Scientific Collaboration, the Virgo Collaboration, the KAGRA Collaboration, A. G. Abac, I. Abouelfettouh, F. Acernese, K. Ackley, C. Adamcewicz, S. Adhicary, D. Adhikari, N. Adhikari, R. X. Adhikari, V. K. Adkins, S. Afroz, A. Agapito, D. Agarwal, M. Agathos, N. Aggarwal, S. Aggarwal, O. D. Aguiar, I. -L. Ahrend, L. Aiello, A. Ain, P. Ajith, T. Akutsu, S. Albanesi, W. Ali, S. Al-Kershi, C. All√©n√©, A. Allocca, S. Al-Shammari, P. A. Altin, S. Alvarez-Lopez, W. Amar, O. Amarasinghe, A. Amato, F. Amicucci, C. Amra, A. Ananyeva, S. B. Anderson, W. G. Anderson, M. Andia, M. Ando, M. Andr√©s-Carcasona, T. Andriƒá, J. Anglin, S. Ansoldi, J. M. Antelis, S. Antier, M. Aoumi, E. Z. Appavuravther, S. Appert, S. K. Apple, K. Arai, A. Araya, M. C. Araya, M. Arca Sedda, J. S. Areeda, N. Aritomi, F. Armato, S. Armstrong, N. Arnaud, M. Arogeti, S. M. Aronson, K. G. Arun, G. Ashton, Y. Aso, L. Asprea, M. Assiduo, S. Assis de Souza Melo, S. M. Aston, P. Astone, F. Attadio, F. Aubin, K. AultONeal, G. Avallone, E. A. Avila, S. Babak, C. Badger, S. Bae, S. Bagnasco, L. Baiotti, R. Bajpai, T. Baka, A. M. Baker, K. A. Baker, T. Baker, G. Baldi, N. Baldicchi, M. Ball, G. Ballardin, S. W. Ballmer, S. Banagiri, B. Banerjee, D. Bankar, T. M. Baptiste, P. Baral, M. Baratti, J. C. Barayoga, B. C. Barish, D. Barker, N. Barman, P. Barneo, F. Barone, B. Barr, L. Barsotti, M. Barsuglia, D. Barta, A. M. Bartoletti, M. A. Barton, I. Bartos, A. Basalaev, R. Bassiri, A. Basti, M. Bawaj, P. Baxi, J. C. Bayley, A. C. Baylor, P. A. Baynard II, M. Bazzan, V. M. Bedakihale, F. Beirnaert, M. Bejger, D. Belardinelli, A. S. Bell, D. S. Bellie, L. Bellizzi, W. Benoit, I. Bentara, J. D. Bentley, M. Ben Yaala, S. Bera, F. Bergamin, B. K. Berger, S. Bernuzzi, M. Beroiz, C. P. L. Berry, D. Bersanetti, T. Bertheas, A. Bertolini, J. Betzwieser, D. Beveridge, G. Bevilacqua, N. Bevins, R. Bhandare, R. Bhatt, D. Bhattacharjee, S. Bhattacharyya, S. Bhaumik, V. Biancalana, A. Bianchi, I. A. Bilenko, G. Billingsley, A. Binetti, S. Bini, C. Binu, S. Biot, O. Birnholtz, S. Biscoveanu, A. Bisht, M. Bitossi, M. -A. Bizouard, S. Blaber, J. K. Blackburn, L. A. Blagg, C. D. Blair, D. G. Blair, N. Bode, N. Boettner, G. Boileau, M. Boldrini, G. N. Bolingbroke, A. Bolliand, L. D. Bonavena, R. Bondarescu, F. Bondu, E. Bonilla, M. S. Bonilla, A. Bonino, R. Bonnand, A. Borchers, S. Borhanian, V. Boschi, S. Bose, V. Bossilkov, Y. Bothra, A. Boudon, L. Bourg, M. Boyle, A. Bozzi, C. Bradaschia, P. R. Brady, A. Branch, M. Branchesi, I. Braun, T. Briant, A. Brillet, M. Brinkmann, P. Brockill, E. Brockmueller, A. F. Brooks, B. C. Brown, D. D. Brown, M. L. Brozzetti, S. Brunett, G. Bruno, R. Bruntz, J. Bryant, Y. Bu, F. Bucci, J. Buchanan, O. Bulashenko, T. Bulik, H. J. Bulten, A. Buonanno, K. Burtnyk, R. Buscicchio, D. Buskulic, C. Buy, R. L. Byer, G. S. Cabourn Davies, R. Cabrita, V. C√°ceres-Barbosa, L. Cadonati, G. Cagnoli, C. Cahillane, A. Calafat, T. A. Callister, E. Calloni, S. R. Callos, M. Canepa, G. Caneva Santoro, K. C. Cannon, H. Cao, L. A. Capistran, E. Capocasa, E. Capote, G. Capurri, G. Carapella, F. Carbognani, M. Carlassara, J. B. Carlin, T. K. Carlson, M. F. Carney, M. Carpinelli, G. Carrillo, J. J. Carter, G. Carullo, A. Casallas-Lagos, J. Casanueva Diaz, C. Casentini, S. Y. Castro-Lucas, S. Caudill, M. Cavagli√†, R. Cavalieri, A. Ceja, G. Cella, P. Cerd√°-Dur√°n, E. Cesarini, N. Chabbra, W. Chaibi, A. Chakraborty, P. Chakraborty, S. Chakraborty, S. Chalathadka Subrahmanya, J. C. L. Chan, M. Chan, K. Chang, S. Chao, P. Charlton, E. Chassande-Mottin, C. Chatterjee, Debarati Chatterjee, Deep Chatterjee, M. Chaturvedi, S. Chaty, K. Chatziioannou, A. Chen, A. H. -Y. Chen, D. Chen, H. Chen, H. Y. Chen, S. Chen, Yanbei Chen, Yitian Chen, H. P. Cheng, P. Chessa, H. T. Cheung, S. Y. Cheung, F. Chiadini, G. Chiarini, A. Chiba, A. Chincarini, M. L. Chiofalo, A. Chiummo, C. Chou, S. Choudhary, N. Christensen, S. S. Y. Chua, G. Ciani, P. Ciecielag, M. Cie≈õlar, M. Cifaldi, B. Cirok, F. Clara, J. A. Clark, T. A. Clarke, P. Clearwater, S. Clesse, F. Cleva, E. Coccia, E. Codazzo, P. -F. Cohadon, S. Colace, E. Colangeli, M. Colleoni, C. G. Collette, J. Collins, S. Colloms, A. Colombo, C. M. Compton, G. Connolly, L. Conti, T. R. Corbitt, I. Cordero-Carri√≥n, S. Corezzi, N. J. Cornish, I. Coronado, A. Corsi, R. Cottingham, M. W. Coughlin, A. Couineaux, P. Couvares, D. M. Coward, R. Coyne, A. Cozzumbo, J. D. E. Creighton, T. D. Creighton, P. Cremonese, S. Crook, R. Crouch, J. Csizmazia, J. R. Cudell, T. J. Cullen, A. Cumming, E. Cuoco, M. Cusinato, L. V. Da Concei√ß√£o, T. Dal Canton, S. Dal Pra, G. D√°lya, B. D'Angelo, S. Danilishin, S. D'Antonio, K. Danzmann, K. E. Darroch, L. P. Dartez, R. Das, A. Dasgupta, V. Dattilo, A. Daumas, N. Davari, I. Dave, A. Davenport, M. Davier, T. F. Davies, D. Davis, L. Davis, M. C. Davis, P. Davis, E. J. Daw, M. Dax, J. De Bolle, M. Deenadayalan, J. Degallaix, M. De Laurentis, F. De Lillo, S. Della Torre, W. Del Pozzo, A. Demagny, F. De Marco, G. Demasi, F. De Matteis, N. Demos, T. Dent, A. Depasse, N. DePergola, R. De Pietri, R. De Rosa, C. De Rossi, M. Desai, R. DeSalvo, A. DeSimone, R. De Simone, A. Dhani, R. Diab, M. C. D√≠az, M. Di Cesare, G. Dideron, T. Dietrich, L. Di Fiore, C. Di Fronzo, M. Di Giovanni, T. Di Girolamo, D. Diksha, J. Ding, S. Di Pace, I. Di Palma, D. Di Piero, F. Di Renzo, Divyajyoti, A. Dmitriev, J. P. Docherty, Z. Doctor, N. Doerksen, E. Dohmen, A. Doke, A. Domiciano De Souza, L. D'Onofrio, F. Donovan, K. L. Dooley, T. Dooney, S. Doravari, O. Dorosh, W. J. D. Doyle, M. Drago, J. C. Driggers, L. Dunn, U. Dupletsa, P. -A. Duverne, D. D'Urso, P. Dutta Roy, H. Duval, S. E. Dwyer, C. Eassa, M. Ebersold, T. Eckhardt, G. Eddolls, A. Effler, J. Eichholz, H. Einsle, M. Eisenmann, M. Emma, K. Endo, R. Enficiaud, L. Errico, R. Espinosa, M. Esposito, R. C. Essick, H. Estell√©s, T. Etzel, M. Evans, T. Evstafyeva, B. E. Ewing, J. M. Ezquiaga, F. Fabrizi, V. Fafone, S. Fairhurst, A. M. Farah, B. Farr, W. M. Farr, G. Favaro, M. Favata, M. Fays, M. Fazio, J. Feicht, M. M. Fejer, R. Felicetti, E. Fenyvesi, J. Fernandes, T. Fernandes, D. Fernando, S. Ferraiuolo, T. A. Ferreira, F. Fidecaro, P. Figura, A. Fiori, I. Fiori, M. Fishbach, R. P. Fisher, R. Fittipaldi, V. Fiumara, R. Flaminio, S. M. Fleischer, L. S. Fleming, E. Floden, H. Fong, J. A. Font, F. Fontinele-Nunes, C. Foo, B. Fornal, K. Franceschetti, F. Frappez, S. Frasca, F. Frasconi, J. P. Freed, Z. Frei, A. Freise, O. Freitas, R. Frey, W. Frischhertz, P. Fritschel, V. V. Frolov, G. G. Fronz√©, M. Fuentes-Garcia, S. Fujii, T. Fujimori, P. Fulda, M. Fyffe, B. Gadre, J. R. Gair, S. Galaudage, V. Galdi, R. Gamba, A. Gamboa, S. Gamoji, D. Ganapathy, A. Ganguly, B. Garaventa, J. Garc√≠a-Bellido, C. Garc√≠a-Quir√≥s, J. W. Gardner, K. A. Gardner, S. Garg, J. Gargiulo, X. Garrido, A. Garron, F. Garufi, P. A. Garver, C. Gasbarra, B. Gateley, F. Gautier, V. Gayathri, T. Gayer, G. Gemme, A. Gennai, V. Gennari, J. George, R. George, O. Gerberding, L. Gergely, Archisman Ghosh, Sayantan Ghosh, Shaon Ghosh, Shrobana Ghosh, Suprovo Ghosh, Tathagata Ghosh, J. A. Giaime, K. D. Giardina, D. R. Gibson, C. Gier, S. Gkaitatzis, J. Glanzer, F. Glotin, J. Godfrey, R. V. Godley, P. Godwin, A. S. Goettel, E. Goetz, J. Golomb, S. Gomez Lopez, B. Goncharov, G. Gonz√°lez, P. Goodarzi, S. Goode, A. W. Goodwin-Jones, M. Gosselin, R. Gouaty, D. W. Gould, K. Govorkova, A. Grado, V. Graham, A. E. Granados, M. Granata, V. Granata, S. Gras, P. Grassia, J. Graves, C. Gray, R. Gray, G. Greco, A. C. Green, L. Green, S. M. Green, S. R. Green, C. Greenberg, A. M. Gretarsson, H. K. Griffin, D. Griffith, H. L. Griggs, G. Grignani, C. Grimaud, H. Grote, S. Grunewald, D. Guerra, D. Guetta, G. M. Guidi, A. R. Guimaraes, H. K. Gulati, F. Gulminelli, H. Guo, W. Guo, Y. Guo, Anuradha Gupta, I. Gupta, N. C. Gupta, S. K. Gupta, V. Gupta, N. Gupte, J. Gurs, N. Gutierrez, N. Guttman, F. Guzman, D. Haba, M. Haberland, S. Haino, E. D. Hall, E. Z. Hamilton, G. Hammond, M. Haney, J. Hanks, C. Hanna, M. D. Hannam, O. A. Hannuksela, A. G. Hanselman, H. Hansen, J. Hanson, S. Hanumasagar, R. Harada, A. R. Hardison, S. Harikumar, K. Haris, I. Harley-Trochimczyk, T. Harmark, J. Harms, G. M. Harry, I. W. Harry, J. Hart, B. Haskell, C. J. Haster, K. Haughian, H. Hayakawa, K. Hayama, M. C. Heintze, J. Heinze, J. Heinzel, H. Heitmann, F. Hellman, A. F. Helmling-Cornell, G. Hemming, O. Henderson-Sapir, M. Hendry, I. S. Heng, M. H. Hennig, C. Henshaw, M. Heurs, A. L. Hewitt, J. Heynen, J. Heyns, S. Higginbotham, S. Hild, S. Hill, Y. Himemoto, N. Hirata, C. Hirose, D. Hofman, B. E. Hogan, N. A. Holland, I. J. Hollows, D. E. Holz, L. Honet, D. J. Horton-Bailey, J. Hough, S. Hourihane, N. T. Howard, E. J. Howell, C. G. Hoy, C. A. Hrishikesh, P. Hsi, H. -F. Hsieh, H. -Y. Hsieh, C. Hsiung, S. -H. Hsu, W. -F. Hsu, Q. Hu, H. Y. Huang, Y. Huang, Y. T. Huang, A. D. Huddart, B. Hughey, V. Hui, S. Husa, R. Huxford, L. Iampieri, G. A. Iandolo, M. Ianni, G. Iannone, J. Iascau, K. Ide, R. Iden, A. Ierardi, S. Ikeda, H. Imafuku, Y. Inoue, G. Iorio, P. Iosif, M. H. Iqbal, J. Irwin, R. Ishikawa, M. Isi, K. S. Isleif, Y. Itoh, M. Iwaya, B. R. Iyer, C. Jacquet, P. -E. Jacquet, T. Jacquot, S. J. Jadhav, S. P. Jadhav, M. Jain, T. Jain, A. L. James, K. Jani, J. Janquart, N. N. Janthalur, S. Jaraba, P. Jaranowski, R. Jaume, W. Javed, A. Jennings, M. Jensen, W. Jia, J. Jiang, H. -B. Jin, G. R. Johns, N. A. Johnson, M. C. Johnston, R. Johnston, N. Johny, D. H. Jones, D. I. Jones, R. Jones, H. E. Jose, P. Joshi, S. K. Joshi, G. Joubert, J. Ju, L. Ju, K. Jung, J. Junker, V. Juste, H. B. Kabagoz, T. Kajita, I. Kaku, V. Kalogera, M. Kalomenopoulos, M. Kamiizumi, N. Kanda, S. Kandhasamy, G. Kang, N. C. Kannachel, J. B. Kanner, S. A. KantiMahanty, S. J. Kapadia, D. P. Kapasi, M. Karthikeyan, M. Kasprzack, H. Kato, T. Kato, E. Katsavounidis, W. Katzman, R. Kaushik, K. Kawabe, R. Kawamoto, D. Keitel, L. J. Kemperman, J. Kennington, F. A. Kerkow, R. Kesharwani, J. S. Key, R. Khadela, S. Khadka, S. S. Khadkikar, F. Y. Khalili, F. Khan, T. Khanam, M. Khursheed, N. M. Khusid, W. Kiendrebeogo, N. Kijbunchoo, C. Kim, J. C. Kim, K. Kim, M. H. Kim, S. Kim, Y. -M. Kim, C. Kimball, K. Kimes, M. Kinnear, J. S. Kissel, S. Klimenko, A. M. Knee, E. J. Knox, N. Knust, K. Kobayashi, S. M. Koehlenbeck, G. Koekoek, K. Kohri, K. Kokeyama, S. Koley, P. Kolitsidou, A. E. Koloniari, K. Komori, A. K. H. Kong, A. Kontos, L. M. Koponen, M. Korobko, X. Kou, A. Koushik, N. Kouvatsos, M. Kovalam, T. Koyama, D. B. Kozak, S. L. Kranzhoff, V. Kringel, N. V. Krishnendu, S. Kroker, A. Kr√≥lak, K. Kruska, J. Kubisz, G. Kuehn, S. Kulkarni, A. Kulur Ramamohan, Achal Kumar, Anil Kumar, Praveen Kumar, Prayush Kumar, Rahul Kumar, Rakesh Kumar, J. Kume, K. Kuns, N. Kuntimaddi, S. Kuroyanagi, S. Kuwahara, K. Kwak, K. Kwan, S. Kwon, G. Lacaille, D. Laghi, A. H. Laity, E. Lalande, M. Lalleman, P. C. Lalremruati, M. Landry, B. B. Lane, R. N. Lang, J. Lange, R. Langgin, B. Lantz, I. La Rosa, J. Larsen, A. Lartaux-Vollard, P. D. Lasky, J. Lawrence, M. Laxen, C. Lazarte, A. Lazzarini, C. Lazzaro, P. Leaci, L. Leali, Y. K. Lecoeuche, H. M. Lee, H. W. Lee, J. Lee, K. Lee, R. -K. Lee, R. Lee, Sungho Lee, Sunjae Lee, Y. Lee, I. N. Legred, J. Lehmann, L. Lehner, M. Le Jean, A. Lema√Ætre, M. Lenti, M. Leonardi, M. Lequime, N. Leroy, M. Lesovsky, N. Letendre, M. Lethuillier, Y. Levin, K. Leyde, A. K. Y. Li, K. L. Li, T. G. F. Li, X. Li, Y. Li, Z. Li, A. Lihos, E. T. Lin, F. Lin, L. C. -C. Lin, Y. -C. Lin, C. Lindsay, S. D. Linker, A. Liu, G. C. Liu, Jian Liu, F. Llamas Villarreal, J. Llobera-Querol, R. K. L. Lo, J. -P. Locquet, S. C. G. Loggins, M. R. Loizou, L. T. London, A. Longo, D. Lopez, M. Lopez Portilla, M. Lorenzini, A. Lorenzo-Medina, V. Loriette, M. Lormand, G. Losurdo, E. Lotti, T. P. Lott IV, J. D. Lough, H. A. Loughlin, C. O. Lousto, N. Low, N. Lu, L. Lucchesi, H. L√ºck, D. Lumaca, A. P. Lundgren, A. W. Lussier, R. Macas, M. MacInnis, D. M. Macleod, I. A. O. MacMillan, A. Macquet, K. Maeda, S. Maenaut, S. S. Magare, R. M. Magee, E. Maggio, R. Maggiore, M. Magnozzi, M. Mahesh, M. Maini, S. Majhi, E. Majorana, C. N. Makarem, D. Malakar, J. A. Malaquias-Reis, U. Mali, S. Maliakal, A. Malik, L. Mallick, A. -K. Malz, N. Man, M. Mancarella, V. Mandic, V. Mangano, B. Mannix, G. L. Mansell, M. Manske, M. Mantovani, M. Mapelli, C. Marinelli, F. Marion, A. S. Markosyan, A. Markowitz, E. Maros, S. Marsat, F. Martelli, I. W. Martin, R. M. Martin, B. B. Martinez, D. A. Martinez, M. Martinez, V. Martinez, A. Martini, J. C. Martins, D. V. Martynov, E. J. Marx, L. Massaro, A. Masserot, M. Masso-Reid, S. Mastrogiovanni, T. Matcovich, M. Matiushechkina, L. Maurin, N. Mavalvala, N. Maxwell, G. McCarrol, R. McCarthy, D. E. McClelland, S. McCormick, L. McCuller, S. McEachin, C. McElhenny, G. I. McGhee, J. McGinn, K. B. M. McGowan, J. McIver, A. McLeod, I. McMahon, T. McRae, R. McTeague, D. Meacher, B. N. Meagher, R. Mechum, Q. Meijer, A. Melatos, C. S. Menoni, F. Mera, R. A. Mercer, L. Mereni, K. Merfeld, E. L. Merilh, J. R. M√©rou, J. D. Merritt, M. Merzougui, C. Messick, B. Mestichelli, M. Meyer-Conde, F. Meylahn, A. Mhaske, A. Miani, H. Miao, C. Michel, Y. Michimura, H. Middleton, D. P. Mihaylov, A. L. Miller, S. J. Miller, M. Millhouse, E. Milotti, V. Milotti, Y. Minenkov, E. M. Minihan, Ll. M. Mir, L. Mirasola, M. Miravet-Ten√©s, C. -A. Miritescu, A. Mishra, C. Mishra, T. Mishra, A. L. Mitchell, J. G. Mitchell, S. Mitra, V. P. Mitrofanov, K. Mitsuhashi, R. Mittleman, O. Miyakawa, S. Miyoki, A. Miyoko, G. Mo, L. Mobilia, S. R. P. Mohapatra, S. R. Mohite, M. Molina-Ruiz, M. Mondin, M. Montani, C. J. Moore, D. Moraru, A. More, S. More, C. Moreno, E. A. Moreno, G. Moreno, A. Moreso Serra, S. Morisaki, Y. Moriwaki, G. Morras, A. Moscatello, M. Mould, B. Mours, C. M. Mow-Lowry, L. Muccillo, F. Muciaccia, D. Mukherjee, Samanwaya Mukherjee, Soma Mukherjee, Subroto Mukherjee, Suvodip Mukherjee, N. Mukund, A. Mullavey, H. Mullock, J. Mundi, C. L. Mungioli, M. Murakoshi, P. G. Murray, D. Nabari, S. L. Nadji, A. Nagar, N. Nagarajan, K. Nakagaki, K. Nakamura, H. Nakano, M. Nakano, D. Nanadoumgar-Lacroze, D. Nandi, V. Napolano, P. Narayan, I. Nardecchia, T. Narikawa, H. Narola, L. Naticchioni, R. K. Nayak, L. Negri, A. Nela, C. Nelle, A. Nelson, T. J. N. Nelson, M. Nery, A. Neunzert, S. Ng, L. Nguyen Quynh, S. A. Nichols, A. B. Nielsen, Y. Nishino, A. Nishizawa, S. Nissanke, W. Niu, F. Nocera, J. Noller, M. Norman, C. North, J. Novak, R. Nowicki, J. F. Nu√±o Siles, L. K. Nuttall, K. Obayashi, J. Oberling, J. O'Dell, E. Oelker, M. Oertel, G. Oganesyan, T. O'Hanlon, M. Ohashi, F. Ohme, R. Oliveri, R. Omer, B. O'Neal, M. Onishi, K. Oohara, B. O'Reilly, M. Orselli, R. O'Shaughnessy, S. O'Shea, S. Oshino, C. Osthelder, I. Ota, D. J. Ottaway, A. Ouzriat, H. Overmier, B. J. Owen, R. Ozaki, A. E. Pace, R. Pagano, M. A. Page, A. Pai, L. Paiella, A. Pal, S. Pal, M. A. Palaia, M. P√°lfi, P. P. Palma, C. Palomba, P. Palud, H. Pan, J. Pan, K. C. Pan, P. K. Panda, Shiksha Pandey, Swadha Pandey, P. T. H. Pang, F. Pannarale, K. A. Pannone, B. C. Pant, F. H. Panther, M. Panzeri, F. Paoletti, A. Paolone, A. Papadopoulos, E. E. Papalexakis, L. Papalini, G. Papigkiotis, A. Paquis, A. Parisi, B. -J. Park, J. Park, W. Parker, G. Pascale, D. Pascucci, A. Pasqualetti, R. Passaquieti, L. Passenger, D. Passuello, O. Patane, A. V. Patel, D. Pathak, A. Patra, B. Patricelli, B. G. Patterson, K. Paul, S. Paul, E. Payne, T. Pearce, M. Pedraza, A. Pele, F. E. Pe√±a Arellano, X. Peng, Y. Peng, S. Penn, M. D. Penuliar, A. Perego, Z. Pereira, C. P√©rigois, G. Perna, A. Perreca, J. Perret, S. Perri√®s, J. W. Perry, D. Pesios, S. Peters, S. Petracca, C. Petrillo, H. P. Pfeiffer, H. Pham, K. A. Pham, K. S. Phukon, H. Phurailatpam, M. Piarulli, L. Piccari, O. J. Piccinni, M. Pichot, M. Piendibene, F. Piergiovanni, L. Pierini, G. Pierra, V. Pierro, M. Pietrzak, M. Pillas, F. Pilo, L. Pinard, I. M. Pinto, M. Pinto, B. J. Piotrzkowski, M. Pirello, M. D. Pitkin, A. Placidi, E. Placidi, M. L. Planas, W. Plastino, C. Plunkett, R. Poggiani, E. Polini, J. Pomper, L. Pompili, J. Poon, E. Porcelli, E. K. Porter, C. Posnansky, R. Poulton, J. Powell, G. S. Prabhu, M. Pracchia, B. K. Pradhan, T. Pradier, A. K. Prajapati, K. Prasai, R. Prasanna, P. Prasia, G. Pratten, G. Principe, G. A. Prodi, P. Prosperi, P. Prosposito, A. C. Providence, A. Puecher, J. Pullin, P. Puppo, M. P√ºrrer, H. Qi, J. Qin, G. Qu√©m√©ner, V. Quetschke, P. J. Quinonez, N. Qutob, R. Rading, I. Rainho, S. Raja, C. Rajan, B. Rajbhandari, K. E. Ramirez, F. A. Ramis Vidal, M. Ramos Arevalo, A. Ramos-Buades, S. Ranjan, K. Ransom, P. Rapagnani, B. Ratto, A. Ravichandran, A. Ray, V. Raymond, M. Razzano, J. Read, T. Regimbau, S. Reid, C. Reissel, D. H. Reitze, A. I. Renzini, B. Revenu, A. Revilla Pe√±a, R. Reyes, L. Ricca, F. Ricci, M. Ricci, A. Ricciardone, J. Rice, J. W. Richardson, M. L. Richardson, A. Rijal, K. Riles, H. K. Riley, S. Rinaldi, J. Rittmeyer, C. Robertson, F. Robinet, M. Robinson, A. Rocchi, L. Rolland, J. G. Rollins, A. E. Romano, R. Romano, A. Romero, I. M. Romero-Shaw, J. H. Romie, S. Ronchini, T. J. Roocke, L. Rosa, T. J. Rosauer, C. A. Rose, D. Rosi≈Ñska, M. P. Ross, M. Rossello-Sastre, S. Rowan, S. K. Roy, S. Roy, D. Rozza, P. Ruggi, N. Ruhama, E. Ruiz Morales, K. Ruiz-Rocha, S. Sachdev, T. Sadecki, P. Saffarieh, S. Safi-Harb, M. R. Sah, S. Saha, T. Sainrat, S. Sajith Menon, K. Sakai, Y. Sakai, M. Sakellariadou, S. Sakon, O. S. Salafia, F. Salces-Carcoba, L. Salconi, M. Saleem, F. Salemi, M. Sall√©, S. U. Salunkhe, S. Salvador, A. Salvarese, A. Samajdar, A. Sanchez, E. J. Sanchez, L. E. Sanchez, N. Sanchis-Gual, J. R. Sanders, E. M. S√§nger, F. Santoliquido, F. Sarandrea, T. R. Saravanan, N. Sarin, P. Sarkar, A. Sasli, P. Sassi, B. Sassolas, B. S. Sathyaprakash, R. Sato, S. Sato, Yukino Sato, Yu Sato, O. Sauter, R. L. Savage, T. Sawada, H. L. Sawant, S. Sayah, V. Scacco, D. Schaetzl, M. Scheel, A. Schiebelbein, M. G. Schiworski, P. Schmidt, S. Schmidt, R. Schnabel, M. Schneewind, R. M. S. Schofield, K. Schouteden, B. W. Schulte, B. F. Schutz, E. Schwartz, M. Scialpi, J. Scott, S. M. Scott, R. M. Sedas, T. C. Seetharamu, M. Seglar-Arroyo, Y. Sekiguchi, D. Sellers, N. Sembo, A. S. Sengupta, E. G. Seo, J. W. Seo, V. Sequino, M. Serra, A. Sevrin, T. Shaffer, U. S. Shah, M. A. Shaikh, L. Shao, A. K. Sharma, Preeti Sharma, Prianka Sharma, Ritwik Sharma, S. Sharma Chaudhary, P. Shawhan, N. S. Shcheblanov, E. Sheridan, Z. -H. Shi, M. Shikauchi, R. Shimomura, H. Shinkai, S. Shirke, D. H. Shoemaker, D. M. Shoemaker, R. W. Short, S. ShyamSundar, A. Sider, H. Siegel, D. Sigg, L. Silenzi, L. Silvestri, M. Simmonds, L. P. Singer, Amitesh Singh, Anika Singh, D. Singh, N. Singh, S. Singh, A. M. Sintes, V. Sipala, V. Skliris, B. J. J. Slagmolen, D. A. Slater, T. J. Slaven-Blair, J. Smetana, J. R. Smith, L. Smith, R. J. E. Smith, W. J. Smith, S. Soares de Albuquerque Filho, M. Soares-Santos, K. Somiya, I. Song, S. Soni, V. Sordini, F. Sorrentino, H. Sotani, F. Spada, V. Spagnuolo, A. P. Spencer, P. Spinicelli, A. K. Srivastava, F. Stachurski, C. J. Stark, D. A. Steer, N. Steinle, J. Steinlechner, S. Steinlechner, N. Stergioulas, P. Stevens, M. StPierre, M. D. Strong, A. Strunk, A. L. Stuver, M. Suchenek, S. Sudhagar, Y. Sudo, N. Sueltmann, L. Suleiman, K. D. Sullivan, J. Sun, L. Sun, S. Sunil, J. Suresh, B. J. Sutton, P. J. Sutton, K. Suzuki, M. Suzuki, B. L. Swinkels, A. Syx, M. J. Szczepa≈Ñczyk, P. Szewczyk, M. Tacca, H. Tagoshi, K. Takada, H. Takahashi, R. Takahashi, A. Takamori, S. Takano, H. Takeda, K. Takeshita, I. Takimoto Schmiegelow, M. Takou-Ayaoh, C. Talbot, M. Tamaki, N. Tamanini, D. Tanabe, K. Tanaka, S. J. Tanaka, S. Tanioka, D. B. Tanner, W. Tanner, L. Tao, R. D. Tapia, E. N. Tapia San Mart√≠n, C. Taranto, A. Taruya, J. D. Tasson, J. G. Tau, D. Tellez, R. Tenorio, H. Themann, A. Theodoropoulos, M. P. Thirugnanasambandam, L. M. Thomas, M. Thomas, P. Thomas, J. E. Thompson, S. R. Thondapu, K. A. Thorne, E. Thrane, J. Tissino, A. Tiwari, Pawan Tiwari, Praveer Tiwari, S. Tiwari, V. Tiwari, M. R. Todd, M. Toffano, A. M. Toivonen, K. Toland, A. E. Tolley, T. Tomaru, V. Tommasini, T. Tomura, H. Tong, C. Tong-Yu, A. Torres-Forn√©, C. I. Torrie, I. Tosta e Melo, E. Tournefier, M. Trad Nery, K. Tran, A. Trapananti, R. Travaglini, F. Travasso, G. Traylor, M. Trevor, M. C. Tringali, A. Tripathee, G. Troian, A. Trovato, L. Trozzo, R. J. Trudeau, T. Tsang, S. Tsuchida, L. Tsukada, K. Turbang, M. Turconi, C. Turski, H. Ubach, N. Uchikata, T. Uchiyama, R. P. Udall, T. Uehara, K. Ueno, V. Undheim, L. E. Uronen, T. Ushiba, M. Vacatello, H. Vahlbruch, N. Vaidya, G. Vajente, A. Vajpeyi, J. Valencia, M. Valentini, S. A. Vallejo-Pe√±a, S. Vallero, V. Valsan, M. van Dael, E. Van den Bossche, J. F. J. van den Brand, C. Van Den Broeck, M. van der Sluys, A. Van de Walle, J. van Dongen, K. Vandra, M. VanDyke, H. van Haevermaet, J. V. van Heijningen, P. Van Hove, J. Vanier, M. VanKeuren, J. Vanosky, N. van Remortel, M. Vardaro, A. F. Vargas, V. Varma, A. N. Vazquez, A. Vecchio, G. Vedovato, J. Veitch, P. J. Veitch, S. Venikoudis, R. C. Venterea, P. Verdier, M. Vereecken, D. Verkindt, B. Verma, Y. Verma, S. M. Vermeulen, F. Vetrano, A. Veutro, A. Vicer√©, S. Vidyant, A. D. Viets, A. Vijaykumar, A. Vilkha, N. Villanueva Espinosa, V. Villa-Ortega, E. T. Vincent, J. -Y. Vinet, S. Viret, S. Vitale, H. Vocca, D. Voigt, E. R. G. von Reis, J. S. A. von Wrangel, W. E. Vossius, L. Vujeva, S. P. Vyatchanin, J. Wack, L. E. Wade, M. Wade, K. J. Wagner, L. Wallace, E. J. Wang, H. Wang, J. Z. Wang, W. H. Wang, Y. F. Wang, G. Waratkar, J. Warner, M. Was, T. Washimi, N. Y. Washington, D. Watarai, B. Weaver, S. A. Webster, N. L. Weickhardt, M. Weinert, A. J. Weinstein, R. Weiss, L. Wen, K. Wette, J. T. Whelan, B. F. Whiting, C. Whittle, E. G. Wickens, D. Wilken, A. T. Wilkin, B. M. Williams, D. Williams, M. J. Williams, N. S. Williams, J. L. Willis, B. Willke, M. Wils, L. Wilson, C. W. Winborn, J. Winterflood, C. C. Wipf, G. Woan, J. Woehler, N. E. Wolfe, H. T. Wong, I. C. F. Wong, K. Wong, T. Wouters, J. L. Wright, M. Wright, B. Wu, C. Wu, D. S. Wu, H. Wu, K. Wu, Q. Wu, Y. Wu, Z. Wu, E. Wuchner, D. M. Wysocki, V. A. Xu, Y. Xu, N. Yadav, H. Yamamoto, K. Yamamoto, T. S. Yamamoto, T. Yamamoto, R. Yamazaki, T. Yan, K. Z. Yang, Y. Yang, Z. Yarbrough, J. Yebana, S. -W. Yeh, A. B. Yelikar, X. Yin, J. Yokoyama, T. Yokozawa, S. Yuan, H. Yuzurihara, M. Zanolin, M. Zeeshan, T. Zelenova, J. -P. Zendri, M. Zeoli, M. Zerrad, M. Zevin, L. Zhang, N. Zhang, R. Zhang, T. Zhang, C. Zhao, Yue Zhao, Yuhang Zhao, Z. -C. Zhao, Y. Zheng, H. Zhong, H. Zhou, H. O. Zhu, Z. -H. Zhu, A. B. Zimmerman, L. Zimmermann, M. E. Zucker, J. Zweizig
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Version 4.0 of the Gravitational-Wave Transient Catalog (GWTC-4.0) adds new candidates detected by the LIGO, Virgo, and KAGRA observatories through the first part of the fourth observing run (O4a: 2023 May 24 15:00:00 to 2024 January 16 16:00:00 UTC) and a preceding engineering run. In this new data, we find 128 new compact binary coalescence candidates that are identified by at least one of our search algorithms with a probability of astrophysical origin $p_{\rm astro} \geq 0.5$ and that are not vetoed during event validation. We also provide detailed source property measurements for 86 of these that have a false alarm rate $< 1 \rm{yr}^{-1}$. Based on the inferred component masses, these new candidates are consistent with signals from binary black holes and neutron star-black hole binaries (GW230518_125908 and GW230529_181500). Median inferred component masses of binary black holes in the catalog now range from $5.79\,M_\odot$ (GW230627_015337) to $137\,M_\odot$ (GW231123_135430), while GW231123_135430 was probably produced by the most massive binary observed in the catalog. For the first time we have discovered binary black hole signals with network signal-to-noise ratio exceeding 30, GW230814_230901 and GW231226_01520, enabling high-fidelity studies of the waveforms and astrophysical properties of these systems. Combined with the 90 candidates included in GWTC-3.0, the catalog now contains 218 candidates with $p_{\rm astro} \geq 0.5$ and not otherwise vetoed, doubling the size of the catalog and further opening our view of the gravitational-wave Universe.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:49:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span><span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18082v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18082v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in
  Multi-Hop Arithmetic Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study investigates the incremental, internal problem-solving process of language models (LMs) with arithmetic multi-hop reasoning as a case study. We specifically investigate when LMs internally resolve sub/whole problems through first reading the problem statements, generating reasoning chains, and achieving the final answer to mechanistically interpret LMs' multi-hop problem-solving process. Our experiments reveal a systematic incremental reasoning strategy underlying LMs. They have not derived an answer at the moment they first read the problem; instead, they obtain (sub)answers while generating the reasoning chain. Therefore, the generated reasoning chains can be regarded as faithful reflections of the model's internal computation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:43:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.01113v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.01113v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Neutron Reflectometry by Gradient Descent</h2>
                <div class="authors">
                    <strong>Authors:</strong> Max D. ~Champneys, Andrew J. ~Parnell, Philipp Gutfreund, Maximilian W. A. Skoda, . Patrick A. Fairclough, Timothy J. ~Rogers, Stephanie L. ~Burg
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Neutron reflectometry (NR) is a powerful technique to probe surfaces and interfaces. NR is inherently an indirect measurement technique, access to the physical quantities of interest (layer thickness, scattering length density, roughness), necessitate the solution of an inverse modelling problem, that is inefficient for large amounts of data or complex multiplayer structures (e.g. lithium batteries / electrodes). Recently, surrogate machine learning models have been proposed as an alternative to existing optimisation routines. Although such approaches have been successful, physical intuition is lost when replacing governing equations with fast neural networks. Instead, we propose a novel and efficient approach; to optimise reflectivity data analysis by performing gradient descent on the forward reflection model itself. Herein, automatic differentiation techniques are used to evaluate exact gradients of the error function with respect to the parameters of interest. Access to these quantities enables users of neutron reflectometry to harness a host of powerful modern optimisation and inference techniques that remain thus far unexploited in the context of neutron reflectometry. This paper presents two benchmark case studies; demonstrating state-of-the-art performance on a thick oxide quartz film, and robust co-fitting performance in the high complexity regime of organic LED multilayer devices. Additionally, we provide an open-source library of differentiable reflectometry kernels in the python programming language so that gradient based approaches can readily be applied to other NR datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:38:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06924v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06924v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Staying in the Sweet Spot: Responsive Reasoning Evolution via
  Capability-Adaptive Hint Scaffolding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziheng Li, Zexu Sun, Jinman Zhao, Erxue Min, Yongcheng Zeng, Hui Wu, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen, Zhi-Hong Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:36:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06923v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06923v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Mitigating Eddington and Malmquist Biases in Latent-Inclination
  Inference of the Tully-Fisher Relation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hai Fu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Tully-Fisher relation is a vital distance indicator, but its precise inference is challenged by selection bias, statistical bias, and uncertain inclination corrections. This study presents a Bayesian framework that simultaneously addresses these issues. To eliminate the need for individual inclination corrections, inclination is treated as a latent variable with a known probability distribution. To correct for the distance-dependent Malmquist bias arising from sample selection, the model incorporates Gaussian scatter in the dependent variable, the distribution of the independent variable, and the observational selection function into the data likelihood. To mitigate the statistical bias -- termed the ``general Eddington bias'' -- caused by Gaussian scatter and the non-uniform distribution of the independent variable, two methods are introduced: (1) analytical bias corrections applied to the dependent variable before likelihood computation, and (2) a dual-scatter model that accounts for Gaussian scatter in the independent variable within the likelihood function. The effectiveness of these methods is demonstrated using simulated datasets. By rigorously addressing selection and statistical biases in a latent-variable regression analysis, this work provides a robust approach for unbiased distance estimates from standardizable candles, which is critical for improving the accuracy of Hubble constant determinations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:34:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>astro-ph.GA</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.10589v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.10589v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and
  Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haywood Gelman, John D. Hastings, David Kenley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including precision, recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:32:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span><span>cs.CY</span><span>C.2.0; I.2.7; K.4.1; H.3.3</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06920v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06920v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Neural CRNs: A Natural Implementation of Learning in Chemical Reaction
  Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rajiv Teja Nagipogu, John H. Reif
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Molecular circuits capable of autonomous learning could unlock novel applications in fields such as bioengineering and synthetic biology. To this end, existing chemical implementations of neural computing have mainly relied on emulating discrete-layered neural architectures using steady-state computations of mass action kinetics. In contrast, we propose an alternative dynamical systems-based approach in which neural computations are modeled as the time evolution of molecular concentrations. The analog nature of our framework naturally aligns with chemical kinetics-based computation, leading to more compact circuits. We present the advantages of our framework through three key demonstrations. First, we assemble an end-to-end supervised learning pipeline using only two sequential phases, the minimum required number for supervised learning. Then, we show (through appropriate simplifications) that both linear and nonlinear modeling circuits can be implemented solely using unimolecular and bimolecular reactions, avoiding the complexities of higher-order chemistries. Finally, we demonstrate that first-order gradient approximations can be natively incorporated into the framework, enabling nonlinear models to scale linearly rather than combinatorially with input dimensionality. All the circuit constructions are validated through training and inference simulations across various regression and classification tasks. Our work presents a viable pathway toward embedding learning behaviors in synthetic biochemical systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:30:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.00034v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.00034v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Probabilistic operator learning: generative modeling and uncertainty
  quantification for foundation models of differential equations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin J. Zhang, Siting Liu, Stanley J. Osher, Markos A. Katsoulakis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-context operator networks (ICON) are a class of operator learning methods based on the novel architectures of foundation models. Trained on a diverse set of datasets of initial and boundary conditions paired with corresponding solutions to ordinary and partial differential equations (ODEs and PDEs), ICON learns to map example condition-solution pairs of a given differential equation to an approximation of its solution operator. Here, we present a probabilistic framework that reveals ICON as implicitly performing Bayesian inference, where it computes the mean of the posterior predictive distribution over solution operators conditioned on the provided context, i.e., example condition-solution pairs. The formalism of random differential equations provides the probabilistic framework for describing the tasks ICON accomplishes while also providing a basis for understanding other multi-operator learning methods. This probabilistic perspective provides a basis for extending ICON to \emph{generative} settings, where one can sample from the posterior predictive distribution of solution operators. The generative formulation of ICON (GenICON) captures the underlying uncertainty in the solution operator, which enables principled uncertainty quantification in the solution predictions in operator learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:28:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.LG</span><span>cs.NA</span><span>math.NA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.05186v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.05186v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly
  Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Margarida Ferreira, Victor Nicolet, Luan Pham, Joey Dodds, Daniel Kroening, Ines Lynce, Ruben Martins
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose HyGLAD, a novel algorithm that automatically builds a set of interpretable patterns that model event data. These patterns can then be used to detect event-based anomalies in a stationary system, where any deviation from past behavior may indicate malicious activity. The algorithm infers equivalence classes of entities with similar behavior observed from the events, and then builds regular expressions that capture the values of those entities. As opposed to deep-learning approaches, the regular expressions are directly interpretable, which also translates to interpretable anomalies. We evaluate HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five datasets from real-world systems. The experimental results show that on average HyGLAD outperforms existing deep-learning methods while being an order of magnitude more efficient in training and inference (single CPU vs GPU). Precision improved by 1.2x and recall by 1.3x compared to the second-best baseline.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:25:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06911v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06911v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Amplifying Effective CXL Memory Bandwidth for LLM Inference via
  Transparent Near-Data Processing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) inference is bottlenecked by the limited bandwidth of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a transparent near-data processing architecture that amplifies effective CXL bandwidth without requiring changes to the CXL.mem interface or AI models. CXL-NDP integrates a precision-scalable bit-plane layout for dynamic quantization with transparent lossless compression of weights and KV caches directly within the CXL device. In end-to-end serving, CXL-NDP improves throughput by 43%, extends the maximum context length by 87%, and reduces the KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms its practicality with a modest silicon footprint, lowering the barrier for adopting efficient, scalable CXL-based memory in generative AI infrastructure.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:22:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.03377v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.03377v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers
  from LLMs via Claim Verification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aivin V. Solatorio
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:20:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CR</span><span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06902v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06902v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Probing the cosmic sterile-neutrino background with IceCube</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bhavesh Chauhan, Priyank Parashari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we take a close look at the interaction between the TeV--PeV energy astrophysical neutrinos and a hypothetical cosmic sterile-neutrino background. These interactions yield absorption features, also called ``dips", in the astrophysical neutrino spectrum, which are studied using the deposited energy distribution of high-energy starting events (HESE) in the IceCube detector. We improve upon the previous analysis by including the effects of regeneration and a realistic source distribution on the propagation of astrophysical neutrinos. We use the latest 7.5-year HESE dataset and include the observation of Glashow resonance in our analysis. We evaluate the impact of these dips on the inferred spectral index and overall normalization of the astrophysical neutrinos. We find a mild preference for dips in the 300--800 TeV range, and the best-fit parameters for the mass of sterile-neutrino and the mediator are 0.5 eV and 23 MeV, respectively. We find that the inclusion of these absorption features lowers the spectral index of astrophysical neutrinos to $2.60^{+0.19}_{-0.16}$. We show qualitatively that the lower spectral index from HESE sample can reduce the disagreement with the Northern Tracks sample. We also forecast the event spectrum for IceCube-Gen2 for the two different fits.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:20:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>hep-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1103/s6q7-48yf' target='_blank'>doi</a><a href='http://arxiv.org/abs/2409.12145v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.12145v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Learning from one graph: transductive learning guarantees via the
  geometry of small random worlds</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nils Detering, Luca Galimberti, Anastasis Kratsios, Giulia Livieri, A. Martina Neuman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Since their introduction by Kipf and Welling in $2017$, a primary use of graph convolutional networks is transductive node classification, where missing labels are inferred within a single observed graph and its feature matrix. Despite the widespread use of the network model, the statistical foundations of transductive learning remain limited, as standard inference frameworks typically rely on multiple independent samples rather than a single graph. In this work, we address these gaps by developing new concentration-of-measure tools that leverage the geometric regularities of large graphs via low-dimensional metric embeddings. The emergent regularities are captured using a random graph model; however, the methods remain applicable to deterministic graphs once observed. We establish two principal learning results. The first concerns arbitrary deterministic $k$-vertex graphs, and the second addresses random graphs that share key geometric properties with an Erd\H{o}s-R\'{e}nyi graph $\mathbf{G}=\mathbf{G}(k,p)$ in the regime $p \in \mathcal{O}((\log (k)/k)^{1/2})$. The first result serves as the basis for and illuminates the second. We then extend these results to the graph convolutional network setting, where additional challenges arise. Lastly, our learning guarantees remain informative even with a few labelled nodes $N$ and achieve the optimal nonparametric rate $\mathcal{O}(N^{-1/2})$ as $N$ grows.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:13:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.LG</span><span>math.MG</span><span>math.PR</span><span>math.ST</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06894v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06894v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Tensor Network based Gene Regulatory Network Inference for Single-Cell
  Transcriptomic Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Olatz Sanz Larrarte, Borja Aizpurua, Reza Dastbasteh, Ruben M. Otxoa, Josu Etxezarreta Martinez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deciphering complex gene-gene interactions remains challenging in transcriptomics as traditional methods often miss higher-order and nonlinear dependencies. This study introduces a quantum-inspired framework leveraging tensor networks (TNs) to optimally map expression data into a lower dimensional representation preserving biological locality. Using Quantum Mutual Information (QMI), a nonparametric measure natural for tensor networks, we quantify gene dependencies and establish statistical significance via permutation testing. This constructs robust interaction networks where the edges reflect biologically meaningful relationships that are resilient to random chance. The approach effectively distinguishes true regulatory patterns from experimental noise and biological stochasticity. To test the proposed method, we recover a gene regulatory network consisted of six pathway genes from single-cell RNA sequencing data comprising over $28.000$ lymphoblastoid cells. Furthermore, we unveil several triadic regulatory mechanisms. By merging quantum physics inspired techniques with computational biology, our method provides novel insights into gene regulation, with applications in disease mechanisms and precision medicine.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:11:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.MN</span><span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06891v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06891v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Intraoperative 2D/3D Registration via Spherical Similarity Learning and
  Inference-Time Differentiable Levenberg-Marquardt Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minheng Chen, Youyong Kong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Intraoperative 2D/3D registration aligns preoperative 3D volumes with real-time 2D radiographs, enabling accurate localization of instruments and implants. A recent fully differentiable similarity learning framework approximates geodesic distances on SE(3), expanding the capture range of registration and mitigating the effects of substantial disturbances, but existing Euclidean approximations distort manifold structure and slow convergence. To address these limitations, we explore similarity learning in non-Euclidean spherical feature spaces to better capture and fit complex manifold structure. We extract feature embeddings using a CNN-Transformer encoder, project them into spherical space, and approximate their geodesic distances with Riemannian distances in the bi-invariant SO(4) space. This enables a more expressive and geometrically consistent deep similarity metric, enhancing the ability to distinguish subtle pose differences. During inference, we replace gradient descent with fully differentiable Levenberg-Marquardt optimization to accelerate convergence. Experiments on real and synthetic datasets show superior accuracy in both patient-specific and patient-agnostic scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:10:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06890v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06890v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Barlow-Swin: Toward a novel siamese-based segmentation architecture
  using Swin-Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Morteza Kiani Haftlang, Mohammadhossein Malmir, Foroutan Parand, Umberto Michelucci, Safouane El Ghazouali
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoder's ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: https://github.com/mkianih/Barlow-Swin.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:05:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06885v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06885v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 LatticeWorld: A Multimodal Large Language Model-Empowered Framework for
  Interactive Complex World Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Yenan Lin, Hao Jiang, Kang Chen, Shuang Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:05:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.05263v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.05263v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Probabilistic Shapley Value Modeling and Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mert Ketenci, I√±igo Urteaga, Victor Alfonso Rodriguez, No√©mie Elhadad, Adler Perotte
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose probabilistic Shapley inference (PSI), a novel probabilistic framework to model and infer sufficient statistics of feature attributions in flexible predictive models, via latent random variables whose mean recovers Shapley values. PSI enables efficient, scalable inference over input-to-output attributions, and their uncertainty, via a variational objective that jointly trains a predictive (regression or classification) model and its attribution distributions. To address the challenge of marginalizing over variable-length input feature subsets in Shapley value calculation, we introduce a masking-based neural network architecture, with a modular training and inference procedure. We evaluate PSI on synthetic and real-world datasets, showing that it achieves competitive predictive performance compared to strong baselines, while learning feature attribution distributions -- centered at Shapley values -- that reveal meaningful attribution uncertainty across data modalities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:04:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.04211v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.04211v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joe Wilder, Nikhil Kadapala, Benji Xu, Mohammed Alsaadi, Aiden Parsons, Mitchell Rogers, Palash Agarwal, Adam Hassick, Laura Dietz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:02:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06883v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06883v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Deriving Duration Time from Occupancy Data -- A case study in the length
  of stay in Intensive Care Units for COVID-19 patients</h2>
                <div class="authors">
                    <strong>Authors:</strong> Martje Rave, G√∂ran Kauermann
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper focuses on drawing information on underlying processes, which are not directly observed in the data. In particular, we work with data in which only the total count of units in a system at a given time point is observed, but the underlying process of inflows, length of stay and outflows is not. The particular data example looked at in this paper is the occupancy of intensive care units (ICU) during the COVID-19 pandemic, where the aggregated numbers of occupied beds in ICUs on the district level (`Landkreis') are recorded, but not the number of incoming and outgoing patients. The Skellam distribution allows us to infer the number of incoming and outgoing patients from the occupancy in the ICUs. This paper goes a step beyond and approaches the question of whether we can also estimate the average length of stay of ICU patients. Hence, the task is to derive not only the number of incoming and outgoing units from a total net count but also to gain information on the duration time of patients on ICUs. We make use of a stochastic Expectation-Maximisation algorithm and additionally include exogenous information which are assumed to explain the intensity of inflow.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:00:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.02587v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.02587v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Not All Features Deserve Attention: Graph-Guided Dependency Learning for
  Tabular Data Generation with Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits sparse feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as LLMs' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates sparse dependency graphs into LLMs' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing LLM-based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:53:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.18504v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.18504v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional
  Reasoning and Voting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ana Rita Ortigoso, Gabriel Vieira, Daniel Fuentes, Luis Fraz√£o, Nuno Costa, Ant√≥nio Pereira
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents Project Riley, a novel multimodal and multi-model conversational AI architecture oriented towards the simulation of reasoning influenced by emotional states. Drawing inspiration from Pixar's Inside Out, the system comprises five distinct emotional agents - Joy, Sadness, Fear, Anger, and Disgust - that engage in structured multi-round dialogues to generate, criticise, and iteratively refine responses. A final reasoning mechanism synthesises the contributions of these agents into a coherent output that either reflects the dominant emotion or integrates multiple perspectives. The architecture incorporates both textual and visual large language models (LLMs), alongside advanced reasoning and self-refinement processes. A functional prototype was deployed locally in an offline environment, optimised for emotional expressiveness and computational efficiency. From this initial prototype, another one emerged, called Armando, which was developed for use in emergency contexts, delivering emotionally calibrated and factually accurate information through the integration of Retrieval-Augmented Generation (RAG) and cumulative context tracking. The Project Riley prototype was evaluated through user testing, in which participants interacted with the chatbot and completed a structured questionnaire assessing three dimensions: Emotional Appropriateness, Clarity and Utility, and Naturalness and Human-likeness. The results indicate strong performance in structured scenarios, particularly with respect to emotional alignment and communicative clarity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:40:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>I.2.7; I.2.1; H.5.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.20521v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.20521v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 The Majority is not always right: RL training for solution aggregation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenting Zhao, Pranjal Aggarwal, Swarnadeep Saha, Asli Celikyilmaz, Jason Weston, Ilia Kulikov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling up test-time compute, by generating multiple independent solutions and selecting or aggregating among them, has become a central paradigm for improving large language models (LLMs) on challenging reasoning tasks. While most prior work relies on simple majority voting or reward model ranking to aggregate solutions, these approaches may only yield limited benefits. In this work, we propose to learn aggregation as an explicit reasoning skill: given a set of candidate solutions, we train an aggregator model to review, reconcile, and synthesize a final, correct answer using reinforcement learning from verifiable rewards. A key ingredient is careful balancing of easy and hard training examples, allowing the model to learn both to recover minority-but-correct answers as well as easy majority-correct answers. Empirically, we find our method, AggLM, outperforms both strong rule-based and reward-model baselines, across multiple benchmarks. Furthermore, it generalizes effectively to solutions from differing models, including stronger ones than contained in the training data, all while requiring substantially fewer tokens than majority voting with larger numbers of solutions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:39:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06870v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06870v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Assistance or Disruption? Exploring and Evaluating the Design and
  Trade-offs of Proactive AI Programming Support</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kevin Pu, Daniel Lazaro, Ian Arawjo, Haijun Xia, Ziang Xiao, Tovi Grossman, Yan Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N=18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and interaction context support alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:34:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3706598.3713357' target='_blank'>doi</a><a href='http://arxiv.org/abs/2502.18658v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.18658v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Test-Time Scaling in Reasoning Models Is Not Effective for
  Knowledge-Intensive Tasks Yet</h2>
                <div class="authors">
                    <strong>Authors:</strong> James Xu Zhao, Bryan Hooi, See-Kiong Ng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:28:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06861v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06861v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Disentangling Interaction and Bias Effects in Opinion Dynamics of Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vincent C. Brockers, David A. Ehrlich, Viola Priesemann
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias toward the initiating agent's stance. Applying this framework to multi-step dialogues reveals that opinion trajectories tend to quickly converge to a shared attractor, with the influence of the interaction fading over time, and the impact of biases differing between LLMs. In addition, we fine-tune an LLM on different sets of strongly opinionated statements (incl. misinformation) and demonstrate that the opinion attractor shifts correspondingly. Exposing stark differences between LLMs and providing quantitative tools to compare them to human subjects in the future, our approach highlights both chances and pitfalls in using LLMs as proxies for human behavior.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:26:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.soc-ph</span><span>cs.AI</span><span>nlin.AO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06858v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06858v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Seeing the Forest Through the Trees: Knowledge Retrieval for
  Streamlining Particle Physics Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> James McGreivy, Blaise Delaney, Anja Beck, Mike Williams
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative Large Language Models (LLMs) are a promising approach to structuring knowledge contained within the corpora of research literature produced by large-scale and long-running scientific collaborations. Within experimental particle physics, such structured knowledge bases could expedite methodological and editorial review. Complementarily, within the broader scientific community, generative LLM systems grounded in published work could make for reliable companions allowing non-experts to analyze open-access data. Techniques such as Retrieval Augmented Generation (RAG) rely on semantically matching localized text chunks, but struggle to maintain coherent context when relevant information spans multiple segments, leading to a fragmented representation devoid of global cross-document information. Here, we utilize the hierarchical organization of experimental physics articles to build a tree representation of the corpus, and present the SciTreeRAG system that uses this structure to create contexts that are more focused and contextually rich than standard RAG. Additionally, we develop methods for using LLMs to transform the unstructured corpus into a structured knowledge graph representation. We then implement SciGraphRAG, a retrieval system that leverages this knowledge graph to access global cross-document relationships eluding standard RAG, thereby encapsulating domain-specific connections and expertise. We demonstrate proof-of-concept implementations using the corpus of the LHCb experiment at CERN.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:23:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>hep-ex</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06855v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06855v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with
  Domain-Specific Structured Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> E Fan, Kang Hu, Zhuowen Wu, Jiangyang Ge, Jiawei Miao, Yuzhi Zhang, He Sun, Weizong Wang, Tianhan Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Computational Fluid Dynamics (CFD) is essential for advancing scientific and engineering fields but is hindered by operational complexity, high expertise requirements, and limited accessibility. This paper introduces ChatCFD, an automated agent system for OpenFOAM simulations that processes multi-modal inputs (e.g., research papers, meshes) via an interactive interface, leveraging DeepSeek-R1 and DeepSeek-V3 large language models, a multi-agent architecture, and OpenFOAM knowledge. Its four-stage pipeline (Knowledge Base Construction, User Input Processing, Case File Generation, and Execution and Error Reflection) enables iterative trial-reflection-refinement for intricate setups, supporting diverse physical models and external meshes. Validation on 205 benchmark tutorial cases, 110 perturbed variants, and 2 literature-derived cases shows ChatCFD's 82.1 percent operational success rate on basic cases, outperforming MetaOpenFOAM (6.2 percent) and Foam-Agent (42.3 percent), and 60-80 percent on literature-derived complex cases. Turbulence model studies show a 40 percent success rate for common models versus 10 percent for rare ones like RNG k-epsilon. Physics coupling analyses reveal higher resource demands for multi-physics-coupled cases, while LLM bias toward simpler setups introduces persistent errors, such as dimensional inconsistency. Ablation studies highlight the efficacy of RAG-based modules and reflection mechanisms. By automating hypothesis testing and parameter exploration, ChatCFD accelerates scientific discovery in fluid mechanics and engineering, addressing LLM limitations through structured design and showing strong potential as a modular component in MCP-based agent networks for collaborative multi-agent systems, paving the way for scalable AI-driven CFD innovation. The code for ChatCFD is available at https://github.com/ConMoo/ChatCFD.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:15:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.02019v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.02019v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Universal Approximation with XL MIMO Systems: OTA Classification via
  Trainable Analog Combining</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kyriakos Stylianopoulos, George C. Alexandropoulos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we show that an eXtremely Large (XL) Multiple-Input Multiple-Output (MIMO) wireless system with appropriate analog combining components exhibits the properties of a universal function approximator, similar to a feedforward neural network. By treating the channel coefficients as the random nodes of a hidden layer and the receiver's analog combiner as a trainable output layer, we cast the XL MIMO system to the Extreme Learning Machine (ELM) framework, leading to a novel formulation for Over-The-Air (OTA) edge inference without requiring traditional digital processing nor pre-processing at the transmitter. Through theoretical analysis and numerical evaluation, we showcase that XL-MIMO-ELM enables near-instantaneous training and efficient classification, even in varying fading conditions, suggesting the paradigm shift of beyond massive MIMO systems as OTA artificial neural networks alongside their profound communications role. Compared to deep learning approaches and conventional ELMs, the proposed framework achieves on par performance with orders of magnitude lower complexity, making it highly attractive for inference tasks with ultra low power wireless devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:10:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.12758v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.12758v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad Reza Mirbagheri, Mohammad Mahdi Mirkamali, Zahra Motoshaker Arani, Ali Javeri, Amir Mahdi Sadeghzadeh, Rasool Jalili
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:08:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06838v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06838v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eugene Kwek, Wenpeng Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Making LLMs more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a key technique toward this goal. However, prior pruning methods are limited: width pruning often breaks the standard transformer layout or requires custom inference code, while depth pruning removes entire layers and can cause abrupt accuracy drops. In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT enjoys merits of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN pruning), training-free operation with competitive pruning time, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream task performance at similar or higher pruning ratios, with substantial reductions in parameters, GPU memory, and end-to-end latency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:07:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06836v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06836v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Self-Alignment: Improving Alignment of Cultural Values in LLMs via
  In-Context Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rochelle Choenni, Ekaterina Shutova
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Improving the alignment of Large Language Models (LLMs) with respect to the cultural values that they encode has become an increasingly important topic. In this work, we study whether we can exploit existing knowledge about cultural values at inference time to adjust model responses to cultural value probes. We present a simple and inexpensive method that uses a combination of in-context learning (ICL) and human survey data, and show that we can improve the alignment to cultural values across 5 models that include both English-centric and multilingual LLMs. Importantly, we show that our method could prove useful in test languages other than English and can improve alignment to the cultural values that correspond to a range of culturally diverse countries.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:06:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.16482v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.16482v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Unlocking 21cm Cosmology with SBI: A Beginner friendly NRE for Inference
  of Astrophysical Parameters</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bisweswar Sen, Abhirup Datta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The 21-cm line of neutral hydrogen is a promising probe of the early Universe, yet extracting astrophysical parameters from its power spectrum remains a major challenge. We present a beginner-friendly PyTorch pipeline for Marginal Neural Ratio Estimation (MNRE), a Simulation-Based Inference (SBI) method that bypasses explicit likelihoods. Using 21cmFAST simulations, we show that MNRE can recover key astrophysical parameters such as the ionizing efficiency $\zeta$ and X-ray luminosity $L_X$ directly from power spectra. Our implementation prioritizes transparency and accessibility, offering a practical entry point for new researchers in 21-cm cosmology.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:06:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span><span>astro-ph.GA</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06834v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06834v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 RAFFLES: Reasoning-based Attribution of Faults for LLM Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenyang Zhu, Spencer Hong, Jingyu Wu, Kushal Chawla, Charlotte Tang, Youbing Yin, Nathan Wolfe, Erin Babinsky, Daben Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We have reached a critical roadblock in the development and enhancement of long-horizon, multi-component LLM agentic systems: it is incredibly tricky to identify where these systems break down and why. Evaluation capabilities that currently exist today (e.g., single pass LLM-as-a-judge) are limited in that they often focus on individual metrics or capabilities, end-to-end outcomes, and are narrowly grounded on the preferences of humans. We argue that to match the agentic capabilities, evaluation frameworks must also be able to reason, probe, iterate, and understand the complex logic passing through these systems over long horizons. In this paper, we present RAFFLES - an evaluation architecture that incorporates reasoning and iterative refinement. Specifically, RAFFLES operates as an iterative, multi-component pipeline, using a central Judge to systematically investigate faults and a set of specialized Evaluators to assess not only the system's components but also the quality of the reasoning by the Judge itself, thereby building a history of hypotheses. We tested RAFFLES against several baselines on the Who&When dataset, a benchmark designed to diagnose the "who" (agent) and "when" (step) of a system's failure. RAFFLES outperforms these baselines, achieving an agent-step fault pair accuracy of over 43% on the Algorithmically-Generated dataset (a substantial increase from the previously published best of 16.6%) and over 20% on the Hand-Crafted dataset (surpassing the previously published best of 8.8%). These results demonstrate a key step towards introducing automated fault detection for autonomous systems over labor-intensive manual human review.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:57:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06822v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06822v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Green Learning for STAR-RIS mmWave Systems with Implicit CSI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu-Hsiang Huang, Po-Heng Chou, Wan-Jen Huang, Walid Saad, C. -C. Jay Kuo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, a green learning (GL)-based precoding framework is proposed for simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems. Motivated by the growing emphasis on environmental sustainability in future 6G networks, this work adopts a broadcasting transmission architecture for scenarios where multiple users share identical information, improving spectral efficiency and reducing redundant transmissions and power consumption. Different from conventional optimization methods, such as block coordinate descent (BCD) that require perfect channel state information (CSI) and iterative computation, the proposed GL framework operates directly on received uplink pilot signals without explicit CSI estimation. Unlike deep learning (DL) approaches that require CSI-based labels for training, the proposed GL approach also avoids deep neural networks and backpropagation, leading to a more lightweight design. Although the proposed GL framework is trained with supervision generated by BCD under full CSI, inference is performed in a fully CSI-free manner. The proposed GL integrates subspace approximation with adjusted bias (Saab), relevant feature test (RFT)-based supervised feature selection, and eXtreme gradient boosting (XGBoost)-based decision learning to jointly predict the STAR-RIS coefficients and transmit precoder. Simulation results show that the proposed GL approach achieves competitive spectral efficiency compared to BCD and DL-based models, while reducing floating-point operations (FLOPs) by over four orders of magnitude. These advantages make the proposed GL approach highly suitable for real-time deployment in energy- and hardware-constrained broadcasting scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:56:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06820v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06820v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Efficient Dynamic Clustering-Based Document Compression for
  Retrieval-Augmented-Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weitao Li, Kaiming Liu, Xiangyu Zhang, Xuanyu Lei, Weizhi Ma, Yang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach for knowledge injection during large language model (LLM) inference in recent years. However, due to their limited ability to exploit fine-grained inter-document relationships, current RAG implementations face challenges in effectively addressing the retrieved noise and redundancy content, which may cause error in the generation results. To address these limitations, we propose an Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG) that utilizes latent inter-document relationships while simultaneously removing irrelevant information and redundant content. We validate our approach, built upon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and Hallucination-Detection datasets. Experimental results show that our method achieves consistent performance improvements across various scenarios and experimental settings, demonstrating strong robustness and applicability. Our code and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:51:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.03165v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.03165v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Automatic Prompt Optimization with Prompt Distillation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ernest A. Dyagin, Nikita I. Kulin, Artur R. Khairullin, Viktor N. Zhuravlev, Alena N. Sitkina
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:50:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18992v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18992v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 A Comparative Benchmark of Large Language Models for Labelling Wind
  Turbine Maintenance Logs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Max Malyi, Jonathan Shek, Alasdair McDonald, Andre Biscaya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Effective Operation and Maintenance (O&M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&M data quality and downstream reliability analysis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:48:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06813v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06813v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in
  the TPTP Ecosystem</h2>
                <div class="authors">
                    <strong>Authors:</strong> Valentin Quesnel, Damien Sileo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available.   https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:43:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06809v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06809v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and
  Security</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanrui Du, Fenglei Fan, Sendong Zhao, Jiawei Cao, Ting Liu, Bing Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Large Language Models (LLMs) increasingly permeate human life, their security has emerged as a critical concern, particularly their ability to maintain harmless responses to malicious instructions. Although extensive methods have improved LLMs' security, they often lead to conservative, rejection-oriented responses that compromise practical usability. This presents a key challenge: how to advance the Pareto frontier between LLMs' usability and security, rather than necessitate a trade-off between them. To address this, we propose the MoGU framework, in which the intra-layer router dynamically allocates weights by sensing hidden states, thereby balancing the contributions of security-optimized and usability-optimized variants. Despite its initial potential, the MoGU framework faces limitations such as parameter redundancy and performance bottlenecks. To overcome these, we further propose an improved MoGU_v2 framework that establishes a tighter coupling between the routers and hidden states. In MoGU_v2, routers are embedded only in layers encoding highly classifiable security features, and backbone modules are activated during router optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong adaptability and stable improvements across various series of LLMs, including mainstream LLMs serving as brains in various applications, on-device LLMs optimized for resource-constrained scenarios, and reasoning LLMs tailored for user interpretability. Meanwhile, even facing risks introduced by Instruction Fine-tuning, MoGU_v2 can easily restore security without compromising the task performance gains via a simple data-mix strategy. These comprehensive improvements highlight MoGU_V2 as a robust and versatile solution for mitigating security risks in real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:39:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06807v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06807v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 MachineLearningLM: Continued Pretraining Language Models on Millions of
  Synthetic Tabular Prediction Tasks Scales In-Context ML</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:38:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06806v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06806v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Error Classification of Large Language Models on Math Word Problems: A
  Dynamically Adaptive Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhong Sun, Zhangyue Yin, Xuanjing Huang, Xipeng Qiu, Hui Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. Math Word Problems (MWPs) serve as a crucial benchmark for evaluating LLMs' reasoning abilities. While most research primarily focuses on improving accuracy, it often neglects understanding and addressing the underlying patterns of errors. Current error classification methods rely on static and predefined categories, which limit their ability to capture the full spectrum of error patterns in mathematical reasoning. To enable systematic error analysis, we collect error samples from 15 different LLMs of varying sizes across four distinct MWP datasets using multiple sampling strategies. Based on this extensive collection, we introduce MWPES-300K, a comprehensive dataset containing 304,865 error samples that cover diverse error patterns and reasoning paths. To reduce human bias and enable fine-grained analysis of error patterns, we propose a novel framework for automated dynamic error classification in mathematical reasoning. Experimental results demonstrate that dataset characteristics significantly shape error patterns, which evolve from basic to complex manifestations as model capabilities increase. With deeper insights into error patterns, we propose Error-Aware Prompting (EAP) that incorporates common error patterns as explicit guidance, leading to significant improvements in mathematical reasoning performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:34:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.15581v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15581v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Imitative Membership Inference Attack</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuntao Du, Yuetian Chen, Hanshen Xiao, Bruno Ribeiro, Ninghui Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A Membership Inference Attack (MIA) assesses how much a target machine learning model reveals about its training data by determining whether specific query instances were part of the training set. State-of-the-art MIAs rely on training hundreds of shadow models that are independent of the target model, leading to significant computational overhead. In this paper, we introduce Imitative Membership Inference Attack (IMIA), which employs a novel imitative training technique to strategically construct a small number of target-informed imitative models that closely replicate the target model's behavior for inference. Extensive experimental results demonstrate that IMIA substantially outperforms existing MIAs in various attack settings while only requiring less than 5% of the computational cost of state-of-the-art approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:27:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06796v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06796v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via
  Projection Constraint</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanrui Du, Fenglei Fan, Sendong Zhao, Jiawei Cao, Qika Lin, Kai He, Ting Liu, Bing Qin, Mengling Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Instruction Fine-Tuning (IFT) has been widely adopted as an effective post-training strategy to enhance various abilities of Large Language Models (LLMs). However, prior studies have shown that IFT can significantly compromise LLMs' safety, particularly their ability to refuse malicious instructions, raising significant concerns. Recent research into the internal mechanisms of LLMs has identified the refusal direction (r-direction) in the hidden states, which plays a pivotal role in governing refusal behavior. Building on this insight, our study reveals that the r-direction tends to drift during training, which we identify as one of the causes of the associated safety risks. To mitigate such drift, our proposed ProCon method introduces a projection-constrained loss term that regularizes the projection magnitude of each training sample's hidden state onto the r-direction. Our initial analysis shows that applying an appropriate constraint can effectively mitigate the refusal direction drift and associated safety risks, but remains limited by overall performance barriers. To overcome this barrier, informed by our observation of early-stage sharp drift and a data-driven perspective, we introduce a warm-up strategy that emphasizes early-stage strong constraints and broaden the data distribution to strengthen constraint signals, leading to an enhanced ProCon method. Experimental results under various datasets, scenarios, and LLMs demonstrate that our method can significantly mitigate safety risks posed by IFT while preserving task performance gains. Even compared with strong baselines, our method consistently delivers superior overall performance. Crucially, our analysis indicates that ProCon can contribute to stabilizing the r-direction during training, while such an interpretability-driven exploration of LLMs' internal mechanisms lays a solid foundation for future safety research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:24:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06795v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06795v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Oyster-I: Beyond Refusal -- Constructive Safety Alignment for
  Responsible Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, Yinpeng Dong, Yichi Zhang, Yuefeng Chen, Chongwen Wang, Xingjun Ma, Xingxing Wei, Yang Liu, Hang Su, Jun Zhu, Xinfeng Li, Yitong Sun, Jie Zhang, Jinzhao Hu, Sha Xu, Yitong Yang, Jialing Tao, Hui Xue
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:18:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.CY</span><span>cs.HC</span><span>cs.SC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.01909v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.01909v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Why Report Failed Interactions With Robots?! Towards Vignette-based
  Interaction Quality</h2>
                <div class="authors">
                    <strong>Authors:</strong> Agnes Axelsson, Merle Reimann, Ronald Cumbal, Hannah Pelikan, Divesh Lala
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although the quality of human-robot interactions has improved with the advent of LLMs, there are still various factors that cause systems to be sub-optimal when compared to human-human interactions. The nature and criticality of failures are often dependent on the context of the interaction and so cannot be generalized across the wide range of scenarios and experiments which have been implemented in HRI research. In this work we propose the use of a technique overlooked in the field of HRI, ethnographic vignettes, to clearly highlight these failures, particularly those that are rarely documented. We describe the methodology behind the process of writing vignettes and create our own based on our personal experiences with failures in HRI systems. We emphasize the strength of vignettes as the ability to communicate failures from a multi-disciplinary perspective, promote transparency about the capabilities of robots, and document unexpected behaviours which would otherwise be omitted from research reports. We encourage the use of vignettes to augment existing interaction evaluation methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:18:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10603v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10603v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Vanishing Stacked-Residual PINN for State Reconstruction of Hyperbolic
  Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Katayoun Eshkofti, Matthieu Barreau
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In a more connected world, modeling multi-agent systems with hyperbolic partial differential equations (PDEs) offers a compact, physics-consistent description of collective dynamics. However, classical control tools need adaptation for these complex systems. Physics-informed neural networks (PINNs) provide a powerful framework to fix this issue by inferring solutions to PDEs by embedding governing equations into the neural network. A major limitation of original PINNs is their inability to capture steep gradients and discontinuities in hyperbolic PDEs. To tackle this problem, we propose a stacked residual PINN method enhanced with a vanishing viscosity mechanism. Initially, a basic PINN with a small viscosity coefficient provides a stable, low-fidelity solution. Residual correction blocks with learnable scaling parameters then iteratively refine this solution, progressively decreasing the viscosity coefficient to transition from parabolic to hyperbolic PDEs. Applying this method to traffic state reconstruction improved results by an order of magnitude in relative $\mathcal{L}^2$ error, demonstrating its potential to accurately estimate solutions where original PINNs struggle with instability and low fidelity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:17:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.14222v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.14222v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study
  of ChatGPT and Claude</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yile Yan, Yuqi Zhu, Wentao Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study systematically evaluates how nine popular LLMs (both open-source and closed-source) respond to ethical dilemmas involving protected attributes. Across 50,400 trials spanning single and intersectional attribute combinations in four dilemma scenarios (protective vs. harmful), we assess models' ethical preferences, sensitivity, stability, and clustering patterns. Results reveal significant biases in protected attributes in all models, with differing preferences depending on model type and dilemma context. Notably, open-source LLMs show stronger preferences for marginalized groups and greater sensitivity in harmful scenarios, while closed-source models are more selective in protective situations and tend to favor mainstream groups. We also find that ethical behavior varies across dilemma types: LLMs maintain consistent patterns in protective scenarios but respond with more diverse and cognitively demanding decisions in harmful ones. Furthermore, models display more pronounced ethical tendencies under intersectional conditions than in single-attribute settings, suggesting that complex inputs reveal deeper biases. These findings highlight the need for multi-dimensional, context-aware evaluation of LLMs' ethical behavior and offer a systematic evaluation and approach to understanding and addressing fairness in LLM decision-making.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:16:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10484v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10484v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Bayesian Quantum Amplitude Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alexandra Ram√¥a, Luis Paulo Santos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present BAE, a problem-tailored and noise-aware Bayesian algorithm for quantum amplitude estimation. In a fault tolerant scenario, BAE is capable of saturating the Heisenberg limit; if device noise is present, BAE can dynamically characterize it and self-adapt. We further propose aBAE, an annealed variant of BAE drawing on methods from statistical inference, to enhance robustness. Our proposals are parallelizable in both quantum and classical components, offer tools for fast noise model assessment, and can leverage preexisting information. Additionally, they accommodate experimental limitations and preferred cost trade-offs. We propose a robust benchmark for amplitude estimation algorithms and use it to test BAE against other approaches, demonstrating its competitive performance in both noisy and noiseless scenarios. In both cases, it achieves lower error than any other algorithm as a function of the cost. In the presence of decoherence, it is capable of learning when other algorithms fail.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:14:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.04394v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.04394v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Emergence of the Primacy Effect in Structured State-Space Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Takashi Morita
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Structured state-space models (SSMs) have been developed to offer more persistent memory retention than traditional recurrent neural networks, while maintaining real-time inference capabilities and addressing the time-complexity limitations of Transformers. Despite this intended persistence, the memory mechanism of canonical SSMs is theoretically designed to decay monotonically over time, meaning that more recent inputs are expected to be retained more accurately than earlier ones. Contrary to this theoretical expectation, however, the present study reveals a counterintuitive finding: when trained and evaluated on a synthetic, statistically balanced memorization task, SSMs predominantly preserve the *initially* presented data in memory. This pattern of memory bias, known as the *primacy effect* in psychology, presents a non-trivial challenge to the current theoretical understanding of SSMs and opens new avenues for future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:59:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.NE</span><span>q-bio.NC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.13729v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.13729v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 OpenCoderRank: AI-Driven Technical Assessments Made Easy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hridoy Sankar Dutta, Sana Ansari, Swati Kumari, Shounak Ravi Bhalerao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Organizations and educational institutions use time-bound assessment tasks to evaluate coding and problem-solving skills. These assessments measure not only the correctness of the solutions, but also their efficiency. Problem setters (educator/interviewer) are responsible for crafting these challenges, carefully balancing difficulty and relevance to create meaningful evaluation experiences. Conversely, problem solvers (student/interviewee) apply coding efficiency and logical thinking to arrive at correct solutions. In the era of Large Language Models (LLMs), LLMs assist problem setters in generating diverse and challenging questions, but they can undermine assessment integrity for problem solvers by providing easy access to solutions. This paper introduces OpenCoderRank, an easy-to-use platform designed to simulate technical assessments. It acts as a bridge between problem setters and problem solvers, helping solvers prepare for time constraints and unfamiliar problems while allowing setters to self-host assessments, offering a no-cost and customizable solution for technical assessments in resource-constrained environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:58:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06774v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06774v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Generative World Explorer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Taiming Lu, Tianmin Shu, Alan Yuille, Daniel Khashabi, Jieneng Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state. In contrast, humans can $\textit{imagine}$ unseen parts of the world through a mental exploration and $\textit{revise}$ their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the $\textit{Generative World Explorer (Genex)}$, an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train $\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) $\textit{Genex}$ can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:56:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.11844v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.11844v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM
  Prompting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shashidhar Reddy Javaji, Bhavul Gauri, Zining Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are now used in multi-turn workflows, but we still lack a clear way to measure when iteration helps and when it hurts. We present an evaluation framework for iterative refinement that spans ideation, code, and math. Our protocol runs controlled 12-turn conversations per task, utilizing a variety of prompts ranging from vague ``improve it'' feedback to targeted steering, and logs per-turn outputs. We score outcomes with domain-appropriate checks (unit tests for code; answer-equivalence plus reasoning-soundness for math; originality and feasibility for ideation) and track turn-level behavior with three families of metrics: semantic movement across turns, turn-to-turn change, and output size growth. Across models and tasks, gains are domain-dependent: they arrive early in ideas and code, but in math late turns matter when guided by elaboration. After the first few turns, vague feedback often plateaus or reverses correctness, while targeted prompts reliably shift the intended quality axis (novelty vs. feasibility in ideation; speed vs. readability in code; in math, elaboration outperforms exploration and drives late-turn gains). We also observe consistent domain patterns: ideation moves more in meaning across turns, code tends to grow in size with little semantic change, and math starts fixed but can break that path with late, elaborative iteration.Together, the framework and metrics make iteration measurable and comparable across models, and signal when to steer, stop, or switch strategies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:54:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06770v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06770v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiang Meng, Kayhan Behdin, Haoyue Wang, Rahul Mazumder
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:34:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.07831v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.07831v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Visual Structures Helps Visual Reasoning: Addressing the Binding Problem
  in VLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amirmohammad Izadi, Mohammad Ali Banayeeanzade, Fatemeh Askari, Ali Rahimiakbar, Mohammad Mahdi Vahedi, Hosein Hasani, Mahdieh Soleymani Baghshah
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite progress in Vision-Language Models (VLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current VLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces VISER (Visual Input Structure for Enhanced Reasoning), a simple yet effective intervention: augmenting visual inputs with low-level spatial structures and pairing this with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks. Specifically, VISER improves GPT-4o visual search accuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit distance error in scene description by 0.32, and enhances performance on spatial relationship tasks by 9.50% on a 2D synthetic dataset. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER enhances binding only with a single-query inference, underscoring the importance of visual input design over purely linguistically-based approaches. These findings suggest that low-level visual structuring is a powerful and underexplored direction for improving compositional visual reasoning and could serve as a general strategy for enhancing VLM performance on spatially grounded tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:34:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22146v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22146v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 SUDER: Self-Improving Unified Large Multimodal Models for Understanding
  and Generation with Dual Self-Rewards</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jixiang Hong, Yiran Zhang, Guanzhong Wang, Yi Liu, Ji-Rong Wen, Rui Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate vision-language alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are naturally inverse dual tasks, we propose \textbf{SUDER} (\textbf{S}elf-improving \textbf{U}nified LMMs with \textbf{D}ual s\textbf{E}lf-\textbf{R}ewards), a framework reinforcing the understanding and generation capabilities of LMMs with a self-supervised dual reward mechanism. SUDER leverages the inherent duality between understanding and generation tasks to provide self-supervised optimization signals for each other. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood within the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:31:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.07963v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.07963v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Data-driven discovery of dynamical models in biology</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bartosz Prokop, Lendert Gelens
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Dynamical systems theory describes how interacting quantities change over time and space, from molecular oscillators to large-scale biological patterns. Such systems often involve nonlinear feedbacks, delays, and interactions across scales. Classical modeling derives explicit governing equations, often systems of differential equations, by combining mechanistic assumptions, experimental observations, and known physical laws. The growing complexity of biological processes has, however, motivated complementary data-driven methods that aim to infer model structure directly from measurements, often without specifying equations a priori. In this review, we survey approaches for model discovery in biological dynamical systems, focusing on three methodological families: regression-based methods, network-based architectures, and decomposition techniques. We compare their ability to address three core goals: forecasting future states, identifying interactions, and characterizing system states. Representative methods are applied to a common benchmark, the Oregonator model, a minimal nonlinear oscillator that captures shared design principles of chemical and biological systems. By highlighting strengths, limitations, and interpretability, we aim to guide researchers in selecting tools for analyzing complex, nonlinear, and high-dimensional dynamics in the life sciences.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:28:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.QM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06735v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06735v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Intelligent Manufacturing Support: Specialized LLMs for Composite
  Material Processing and Equipment Operation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gunnika Kapoor, Komal Chawla, Tirthankar Ghosal, Kris Villez, Dan Coughlin, Tyden Rucker, Vincent Paquit, Soydan Ozcan, Seokpum Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Engineering educational curriculum and standards cover many material and manufacturing options. However, engineers and designers are often unfamiliar with certain composite materials or manufacturing techniques. Large language models (LLMs) could potentially bridge the gap. Their capacity to store and retrieve data from large databases provides them with a breadth of knowledge across disciplines. However, their generalized knowledge base can lack targeted, industry-specific knowledge. To this end, we present two LLM-based applications based on the GPT-4 architecture: (1) The Composites Guide: a system that provides expert knowledge on composites material and connects users with research and industry professionals who can provide additional support and (2) The Equipment Assistant: a system that provides guidance for manufacturing tool operation and material characterization. By combining the knowledge of general AI models with industry-specific knowledge, both applications are intended to provide more meaningful information for engineers. In this paper, we discuss the development of the applications and evaluate it through a benchmark and two informal user studies. The benchmark analysis uses the Rouge and Bertscore metrics to evaluate our model performance against GPT-4o. The results show that GPT-4o and the proposed models perform similarly or better on the ROUGE and BERTScore metrics. The two user studies supplement this quantitative evaluation by asking experts to provide qualitative and open-ended feedback about our model performance on a set of domain-specific questions. The results of both studies highlight a potential for more detailed and specific responses with the Composites Guide and the Equipment Assistant.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:28:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06734v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06734v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Leveraging Large Language Models for Accurate Sign Language Translation
  in Low-Resource Scenarios</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luana Bulla, Gabriele Tuccio, Misael Mongiov√¨, Aldo Gangemi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Translating natural languages into sign languages is a highly complex and underexplored task. Despite growing interest in accessibility and inclusivity, the development of robust translation systems remains hindered by the limited availability of parallel corpora which align natural language with sign language data. Existing methods often struggle to generalize in these data-scarce environments, as the few datasets available are typically domain-specific, lack standardization, or fail to capture the full linguistic richness of sign languages. To address this limitation, we propose Advanced Use of LLMs for Sign Language Translation (AulSign), a novel method that leverages Large Language Models via dynamic prompting and in-context learning with sample selection and subsequent sign association. Despite their impressive abilities in processing text, LLMs lack intrinsic knowledge of sign languages; therefore, they are unable to natively perform this kind of translation. To overcome this limitation, we associate the signs with compact descriptions in natural language and instruct the model to use them. We evaluate our method on both English and Italian languages using SignBank+, a recognized benchmark in the field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior performance compared to state-of-the-art models in low-data scenario. Our findings demonstrate the effectiveness of AulSign, with the potential to enhance accessibility and inclusivity in communication technologies for underrepresented linguistic communities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:25:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>I.2; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18183v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18183v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient
  LLM-based Scientific Peer Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Hayden Kwok-Hay So, Zhijiang Guo, Liya Zhu, Ngai Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Large Language Models (LLMs) have shown significant potential in assisting peer review, current methods often struggle to generate thorough and insightful reviews while maintaining efficiency. In this paper, we propose TreeReview, a novel framework that models paper review as a hierarchical and bidirectional question-answering process. TreeReview first constructs a tree of review questions by recursively decomposing high-level questions into fine-grained sub-questions and then resolves the question tree by iteratively aggregating answers from leaf to root to get the final review. Crucially, we incorporate a dynamic question expansion mechanism to enable deeper probing by generating follow-up questions when needed. We construct a benchmark derived from ICLR and NeurIPS venues to evaluate our method on full review generation and actionable feedback comments generation tasks. Experimental results of both LLM-based and human evaluation show that TreeReview outperforms strong baselines in providing comprehensive, in-depth, and expert-aligned review feedback, while reducing LLM token usage by up to 80% compared to computationally intensive approaches. Our code and benchmark dataset are available at https://github.com/YuanChang98/tree-review.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:22:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.07642v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.07642v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via
  Test-Time Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruicheng Zhang, Jun Zhou, Zunnan Xu, Zihao Liu, Jiehui Huang, Mingyang Zhang, Yu Sun, Xiu Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos that adhere to user-specified motion instructions. Existing methods typically rely on computationally expensive fine-tuning on scarce annotated datasets. Although some zero-shot methods attempt to trajectory control in the latent space, they may yield unrealistic motion by neglecting 3D perspective and creating a misalignment between the manipulated latents and the network's noise predictions. To address these challenges, we introduce Zo3T, a novel zero-shot test-time-training framework for trajectory-guided generation with three core innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging inferring scene depth to derive perspective-correct affine transformations for target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a mechanism that dynamically injects and optimizes ephemeral LoRA adapters into the denoising network alongside the latent state. Driven by a regional feature consistency loss, this co-adaptation effectively enforces motion constraints while allowing the pre-trained model to locally adapt its internal representations to the manipulated latent, thereby ensuring generative fidelity and on-manifold adherence. Finally, we develop Guidance Field Rectification, which refines the denoising evolutionary path by optimizing the conditional guidance field through a one-step lookahead strategy, ensuring efficient generative progression towards the target trajectory. Zo3T significantly enhances 3D realism and motion accuracy in trajectory-controlled I2V generation, demonstrating superior performance over existing training-based and zero-shot approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:21:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06723v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06723v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 A Spatial Gap in the Sky Distribution of Fast Radio Burst Detections
  Coinciding with Galactic Plasma Overdensities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Swarali Shivraj Patil, Robert A. Main, Emmanuel Fonseca, Kyle McGregor, B. M. Gaensler, Charanjot Brar, Amanda M. Cook, Alice P. Curtin, Gwendolyn Eadie, Ronniy Joseph, Lordrick Kahinga, Victoria Kaspi, Afrokk Khan, Bikash Kharel, Adam E. Lanman, Calvin Leung, Kiyoshi W. Masui, Mason Ng, Kenzie Nimmo, Ayush Pandhi, Aaron B. Pearlman, Ziggy Pleunis, Mawson W. Sammons, Ketan R. Sand, Paul Scholz, Kaitlyn Shin, Seth R. Siegel, Kendrick Smith
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We analyze the positional and morphological properties of about 3600 unique fast radio burst (FRB) sources reported in the second FRB catalog generated by the Canadian Hydrogen Intensity Mapping Experiment (CHIME) telescope. We find a two-dimensional dependence of FRB detections on sky position, and identify a significant absence of detections in a roughly circular region centered at Galactic coordinates (77.7$^\circ$, 0.9$^\circ$), spanning an area of 216.2 deg$^2$. This detection gap spatially coincides with the Cygnus X region $--$ a plasma-rich star-forming region in the Milky Way. This lack of FRB detections is most likely the result of increased sky temperature and strong multi-path scattering by turbulent ionized plasma, which broadens the FRB signals beyond detectability in the CHIME band. Our simulations yield a mean of 6 expected FRB detections within the gap when accounting for the elevated sky temperature in the direction of the detection gap. We infer that a lower limit of the maximum scattering timescale $\tau_{\rm sc,\, 1\,GHz} \geq 4.13$ ms is sufficient to suppress the observed signal-to-noise ratio of all FRBs. In addition to Cygnus X, a similar suppression is seen in Catalog 2 along other high-emission measure (EM) sightlines ( i.e., EM$\geq$2900 pc cm$^{-6}$), further supporting a broader trend of suppression due to Galactic scattering. Future very long baseline interferometry (VLBI) measurements of scattering disks with CHIME Outriggers could help confirm our interpretation. Our work highlights the substantial impact of the ionized and turbulent Galactic interstellar medium on the detectability of FRBs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:18:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06721v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06721v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Learning Load Balancing with GNN in MPTCP-Enabled Heterogeneous Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Han Ji, Xiping Wu, Zhihong Zeng, Chen Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks are a promising paradigm of heterogeneous network (HetNet), attributed to the complementary physical properties of optical spectra and radio frequency. However, the current development of such HetNets is mostly bottlenecked by the existing transmission control protocol (TCP), which restricts the user equipment (UE) to connecting one access point (AP) at a time. While the ongoing investigation on multipath TCP (MPTCP) can bring significant benefits, it complicates the network topology of HetNets, making the existing load balancing (LB) learning models less effective. Driven by this, we propose a graph neural network (GNN)-based model to tackle the LB problem for MPTCP-enabled HetNets, which results in a partial mesh topology. Such a topology can be modeled as a graph, with the channel state information and data rate requirement embedded as node features, while the LB solutions are deemed as edge labels. Compared to the conventional deep neural network (DNN), the proposed GNN-based model exhibits two key strengths: i) it can better interpret a complex network topology; and ii) it can handle various numbers of APs and UEs with a single trained model. Simulation results show that against the traditional optimisation method, the proposed learning model can achieve near-optimal throughput within a gap of 11.5%, while reducing the inference time by 4 orders of magnitude. In contrast to the DNN model, the new method can improve the network throughput by up to 21.7%, at a similar inference time level.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:13:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.17118v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.17118v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Efficiently Ranking Software Variants with Minimal Benchmarks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Th√©o Matricon, Mathieu Acher, Helge Spieker, Arnaud Gotlieb
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Benchmarking is a common practice in software engineering to assess the qualities and performance of software variants, coming from multiple competing systems or from configurations of the same system. Benchmarks are used notably to compare and understand variant performance, fine-tune software, detect regressions, or design new software systems. The execution of benchmarks to get a complete picture of software variants is highly costly in terms of computational resources and time. In this paper, we propose a novel approach for reducing benchmarks while maintaining stable rankings, using test suite optimization techniques. That is, we remove instances from the benchmarks while trying to keep the same rankings of the variants on all tests. Our method, BISection Sampling, BISS, strategically retains the most critical tests and applies a novel divide-and-conquer approach to efficiently sample among relevant remaining tests. We experiment with datasets and use cases from LLM leaderboards, SAT competitions, and configurable systems for performance modeling. Our results show that our method outperforms baselines even when operating on a subset of variants. Using BISS, we reduce the computational cost of the benchmarks on average to 44% and on more than half the benchmarks by up to 99% without loss in ranking stability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:11:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06716v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06716v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn
  Directly on Robotic Platforms</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zakariae El Asri, Ibrahim Laiche, Cl√©ment Rambour, Olivier Sigaud, Nicolas Thome
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning a controller directly on the robot requires extreme sample efficiency. Model-based reinforcement learning (RL) methods are the most sample efficient, but they often suffer from a too long inference time to meet the robot control frequency requirements. In this paper, we address the sample efficiency and inference time challenges with two contributions. First, we define a general framework to deal with inference delays where the slow inference robot controller provides a sequence of actions to feed the control-hungry robotic platform without execution gaps. Then, we compare several RL algorithms in the light of this framework and propose RT-HCP, an algorithm that offers an excellent trade-off between performance, sample efficiency and inference time. We validate the superiority of RT-HCP with experiments where we learn a controller directly on a simple but high frequency FURUTA pendulum platform. Code: github.com/elasriz/RTHCP
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:09:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06714v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06714v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Sequential Monte Carlo Squared for online inference in stochastic
  epidemic models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhorasso Temfack, Jason Wyse
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Effective epidemic modeling and surveillance require computationally efficient methods that can continuously update estimates as new data becomes available. This paper explores the application of an online variant of Sequential Monte Carlo Squared (O-SMC$^2$) to the stochastic Susceptible-Exposed-Infectious-Removed (SEIR) model for real-time epidemic tracking. The particularity of O-SMC$^2$ lies in its ability to update the parameters using a particle Metropolis-Hastings kernel, ensuring that the target distribution remains invariant while only utilizing a fixed window of recent observations. This feature enables timely parameter updates and significantly enhances computational efficiency compared to the standard SMC$^2$, which processes the entire dataset. First, we demonstrate the efficiency of O-SMC$^2$ on simulated data, where both the parameters and the observation process are known. We then apply the method to a real-world COVID-19 dataset from Ireland, successfully tracking the epidemic trajectory and estimating the time-dependent reproduction number of the disease. Our results show that O-SMC$^2$ provides highly accurate online estimates of both static and dynamic epidemiological parameters while substantially reducing computational costs. These findings highlight the potential of O-SMC$^2$ for real-time epidemic monitoring and supporting adaptive public health interventions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:02:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.PE</span><span>physics.soc-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1016/j.epidem.2025.100847' target='_blank'>doi</a><a href='http://arxiv.org/abs/2505.09761v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.09761v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Will Annotators Disagree? Identifying Subjectivity in Value-Laden
  Arguments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amir Homayounirad, Enrico Liscio, Tong Wang, Catholijn M. Jonker, Luciano C. Siebert
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Aggregating multiple annotations into a single ground truth label may hide valuable insights into annotator disagreement, particularly in tasks where subjectivity plays a crucial role. In this work, we explore methods for identifying subjectivity in recognizing the human values that motivate arguments. We evaluate two main approaches: inferring subjectivity through value prediction vs. directly identifying subjectivity. Our experiments show that direct subjectivity identification significantly improves the model performance of flagging subjective arguments. Furthermore, combining contrastive loss with binary cross-entropy loss does not improve performance but reduces the dependency on per-label subjectivity. Our proposed methods can help identify arguments that individuals may interpret differently, fostering a more nuanced annotation process.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:59:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06704v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06704v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Probabilistic Modeling of Latent Agentic Substructures in Deep Neural
  Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Su Hyeong Lee, Risi Kondor, Richard Ngo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We develop a theory of intelligent agency grounded in probabilistic modeling for neural models. Agents are represented as outcome distributions with epistemic utility given by log score, and compositions are defined through weighted logarithmic pooling that strictly improves every member's welfare. We prove that strict unanimity is impossible under linear pooling or in binary outcome spaces, but possible with three or more outcomes. Our framework admits recursive structure via cloning invariance, continuity, and openness, while tilt-based analysis rules out trivial duplication. Finally, we formalize an agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a manifest-then-suppress Waluigi strategy yields strictly larger first-order misalignment reduction than pure Luigi reinforcement alone. These results clarify how developing a principled mathematical framework for how subagents can coalesce into coherent higher-level entities provides novel implications for alignment in agentic AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:55:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06701v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06701v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Sovereign AI for 6G: Towards the Future of AI-Native Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Swarna Bindu Chetty, David Grace, Simon Saunders, Paul Harris, Eirini Eleni Tsiropoulou, Tony Quek, Hamed Ahmadi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The advent of Generative Artificial Intelligence (GenAI), Large Language Models (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile networks, especially as the telecom industry transitions from 5G's cloud-centric to AI-native 6G architectures. This transition unlocks unprecedented capabilities in real-time automation, semantic networking, and autonomous service orchestration. However, it introduces critical risks related to data sovereignty, security, explainability, and regulatory compliance especially when AI models are trained, deployed, or governed externally. This paper introduces the concept of `Sovereign AI' as a strategic imperative for 6G, proposing architectural, operational, and governance frameworks that enable national or operator-level control over AI development, deployment, and life-cycle management. Focusing on O-RAN architecture, we explore how sovereign AI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure policy-aligned control, secure model updates, and federated learning across trusted infrastructure. We analyse global strategies, technical enablers, and challenges across safety, talent, and model governance. Our findings underscore that Sovereign AI is not just a regulatory necessity but a foundational pillar for secure, resilient, and ethically-aligned 6G networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:54:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06700v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06700v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded
  Diffusion with Explicit Mask Alignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xichen Xu, Yanshu Wang, Jinbao Wang, Qunyi Zhang, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal role in enhancing the performance of downstream anomaly segmentation, as it provides an effective means of expanding abnormal data. However, existing SIAS methods face several critical limitations: (i) the synthesized anomalies often lack intricate texture details and fail to align precisely with the surrounding background, and (ii) they struggle to generate fine-grained, pixel-level anomalies. To address these challenges, we propose Segmentation-oriented Anomaly synthesis via Graded diffusion with Explicit mask alignment, termed STAGE. STAGE introduces a novel anomaly inference strategy that incorporates clean background information as a prior to guide the denoising distribution, enabling the model to more effectively distinguish and highlight abnormal foregrounds. Furthermore, it employs a graded diffusion framework with an anomaly-only branch to explicitly record local anomalies during both the forward and reverse processes, ensuring that subtle anomalies are not overlooked. Finally, STAGE incorporates the explicit mask alignment (EMA) strategy to progressively align the synthesized anomalies with the background, resulting in context-consistent and structurally coherent generations. Extensive experiments on the MVTec and BTAD datasets demonstrate that STAGE achieves state-of-the-art performance in SIAS, which in turn enhances downstream anomaly segmentation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:47:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06693v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06693v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ
  Bioprinting Monitoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Usman Haider, Lukasz Szemet, Daniel Kelly, Vasileios Sergis, Andrew C. Daly, Karl Mason
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:44:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.AR</span><span>N/A</span><span>I.2.9; I.2.10; I.4.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06690v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06690v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 When Cubic Law and Darcy Fail: Bayesian Correction of Model
  Misspecification in Fracture Conductivities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sarah Perez, Florian Doster, Julien Maes, Hannah Menke, Ahmed ElSheikh, Andreas Busch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Structural uncertainties and unresolved features in fault zones hinder the assessment of leakage risks in subsurface CO2 storage. Understanding multi-scale uncertainties in fracture network conductivity is crucial for mitigating risks and reliably modelling upscaled fault leakage rates. Conventional models, such as the Cubic Law, which is based on mechanical aperture measurements, often neglect fracture roughness, leading to model misspecifications and inaccurate conductivity estimates. Here, we develop a physics-informed, AI-driven correction of these model misspecifications by automatically integrating roughness effects and small-scale structural uncertainties. Using Bayesian inference combined with data-driven and geometric corrections, we reconstruct local hydraulic aperture fields that reliably estimate fracture conductivities. By leveraging interactions across scales, we improve upon traditional empirical corrections and provide a framework for propagating uncertainties from individual fractures to network scales. Our approach thereby supports robust calibration of conductivity ranges for fault leakage sensitivity analyses, offering a scalable solution for subsurface risk assessment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:40:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.geo-ph</span><span>physics.flu-dyn</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.20788v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.20788v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 The linearized translator equation and applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kyeongsu Choi, Robert Haslhofer, Or Hershkovits
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we consider the linearized translator equation $L_\phi u=f$, around entire convex translators $M=\textrm{graph}(\phi)\subset\mathbb{R}^4$, i.e. in the first dimension where the Bernstein property fails. Here, $L_\phi u=\mathrm{div} (a_\phi D u)+ b_\phi\cdot Du$ is a mean curvature type elliptic operator, whose coefficients degenerate as the slope tends to infinity. We derive two fundamental barrier estimates, specifically an upper-lower estimate and an inner-outer estimate, which allow to propagate $L^\infty$-control between different regions. Packaging these and further estimates together we then develop a Fredholm theory for $L_\phi$ between carefully designed weighted function spaces. Combined with Lyapunov-Schmidt reduction we infer that the space $\mathcal{S}$ of noncollapsed translators in $\mathbb{R}^4$ is a finite dimensional analytic variety and that the tip-curvature map $\kappa:\mathcal{S}\to\mathbb{R}$ is analytic. Together with the main result from our prior paper (Camb. J. Math. '23) this allows us to complete the classification of noncollapsed translators in $\mathbb{R}^4$. In particular, we conclude that the one-parameter family of translators constructed by Hoffman-Ilmanen-Martin-White is uniquely determined by the smallest principal curvature at the tip.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:26:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.DG</span><span>math.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06667v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06667v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Process-Supervised Reward Models for Verifying Clinical Note Generation:
  A Scalable Approach Guided by Domain Expertise</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanyin Wang, Chufan Gao, Qiping Xu, Bolun Liu, Guleid Hussein, Hariprasad Korsapati, Mohamad El Labban, Kingsley Iheasirim, Mohamed Hassan, Gokhan Anil, Brian Bartlett, Jimeng Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Process-supervised reward models (PRMs) excel at providing step-by-step verification for large language model (LLM) outputs in domains like mathematics and coding. However, their application to fields lacking ground-truth answers, such as clinical note generation, poses significant challenges. We introduce a novel framework for training PRMs to deliver step-level reward signals for LLM-generated clinical notes. By precisely defining meaningful "steps," injecting realistic "errors" informed by domain expertise, and leveraging LLMs to generate process supervision data at scale, we overcome previous limitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms proprietary reasoning and non-reasoning models, achieving state-of-the-art performance on two key evaluations: (1) distinguishing gold-standard from error-containing samples with 98.8% accuracy, and (2) selecting physician-preferred clinical notes with 56.2% accuracy. We investigate critical components for effective PRM training, including optimal loss functions and data selection strategies, and present a comprehensive physician reader study identifying predictors of downstream Best-of-N performance. Our study sheds light on unlocking the potential of PRMs for diverse generative tasks across domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:19:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12583v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12583v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized
  Self-Training and Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sining Zhoubian, Dan Zhang, Jie Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We conduct extensive experiments on coding problems to verify the validity of the proposed RL paradigm. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:12:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19576v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19576v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 An information metric for comparing and assessing informative interim
  decisions in sequential clinical trials</h2>
                <div class="authors">
                    <strong>Authors:</strong> G. Caruso, W. F. Rosenberger, P. Mozgunov, N. Flournoy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Group sequential designs enable interim analyses and potential early stopping for efficacy or futility. While these adaptations improve trial efficiency and ethical considerations, they also introduce bias into the adapted analyses. We demonstrate how failing to account for informative interim decisions in the analysis can substantially affect posterior estimates of the treatment effect, often resulting in overly optimistic credible intervals aligned with the stopping decision. Drawing on information theory, we use the Kullback-Leibler divergence to quantify this distortion and highlight its use for post-hoc evaluation of informative interim decisions, with a focus on end-of-study inference. Unlike pointwise comparisons, this measure provides an integrated summary of this distortion on the whole parameter space. By comparing alternative decision boundaries and prior specifications, we illustrate how this measure can improve the understanding of trial results and inform the planning of future adaptive studies. We also introduce an expected version of this metric to support clinicians in choosing decision boundaries. This guidance complements traditional strategies based on type-I error rate control by offering insights into the distortion introduced to the treatment effect at each interim phase. The use of this pre-experimental measure is finally illustrated in a group sequential trial for evaluating a treatment for central nervous system disorders.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:10:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>cs.IT</span><span>math.IT</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.04904v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.04904v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 IntrEx: A Dataset for Modeling Engagement in Educational Conversations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingwei Tan, Mahathi Parvatham, Chiara Gambi, Gabriele Pergola
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Engagement and motivation are crucial for second-language acquisition, yet maintaining learner interest in educational conversations remains a challenge. While prior research has explored what makes educational texts interesting, still little is known about the linguistic features that drive engagement in conversations. To address this gap, we introduce IntrEx, the first large dataset annotated for interestingness and expected interestingness in teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus (TSCC), IntrEx extends prior work by incorporating sequence-level annotations, allowing for the study of engagement beyond isolated turns to capture how interest evolves over extended dialogues. We employ a rigorous annotation process with over 100 second-language learners, using a comparison-based rating approach inspired by reinforcement learning from human feedback (RLHF) to improve agreement. We investigate whether large language models (LLMs) can predict human interestingness judgments. We find that LLMs (7B/8B parameters) fine-tuned on interestingness ratings outperform larger proprietary models like GPT-4o, demonstrating the potential for specialised datasets to model engagement in educational settings. Finally, we analyze how linguistic and cognitive factors, such as concreteness, comprehensibility (readability), and uptake, influence engagement in educational dialogues.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:07:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06652v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06652v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable
  Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Lin, Peitong Xie, Jingxue Chen, Jie Lin, Qingkun Tang, Qianchun Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval stage, particularly the coarse-ranking process. Existing coarse-ranking optimization approaches often struggle to balance domain-specific knowledge learning with query enhencement, resulting in suboptimal retrieval performance. To address this challenge, we propose MoLER, a domain-aware RAG method that uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of Losses (MoL) to balance domain-specific knowledge with general language capabilities, and a reinforcement learning (RL) phase leveraging Group Relative Policy Optimization (GRPO) to optimize query and passage generation for maximizing document recall. A key innovation is our Multi-query Single-passage Late Fusion (MSLF) strategy, which reduces computational overhead during RL training while maintaining scalable inference via Multi-query Multi-passage Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER achieves state-of-the-art performance, significantly outperforming baseline methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and scalable retrieval in specialized domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:04:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06650v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06650v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Hallucination Detection on a Budget: Efficient Bayesian Estimation of
  Semantic Entropy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kamil Ciosek, Nicol√≤ Felicioni, Sina Ghiassian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Detecting whether an LLM hallucinates is an important research challenge. One promising way of doing so is to estimate the semantic entropy (Farquhar et al., 2024) of the distribution of generated sequences. We propose a new algorithm for doing that, with two main advantages. First, due to us taking the Bayesian approach, we achieve a much better quality of semantic entropy estimates for a given budget of samples from the LLM. Second, we are able to tune the number of samples adaptively so that `harder' contexts receive more samples. We demonstrate empirically that our approach systematically beats the baselines, requiring only 53% of samples used by Farquhar et al. (2024) to achieve the same quality of hallucination detection as measured by AUROC. Moreover, quite counterintuitively, our estimator is useful even with just one sample from the LLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:59:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.03579v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.03579v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Full Integer Arithmetic Online Training for Spiking Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ismael Gomez, Guangzhi Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Spiking Neural Networks (SNNs) are promising for neuromorphic computing due to their biological plausibility and energy efficiency. However, training methods like Backpropagation Through Time (BPTT) and Real Time Recurrent Learning (RTRL) remain computationally intensive. This work introduces an integer-only, online training algorithm using a mixed-precision approach to improve efficiency and reduce memory usage by over 60%. The method replaces floating-point operations with integer arithmetic to enable hardware-friendly implementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs, RSNNs), showing versatility across architectures. Evaluations on MNIST and the Spiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models achieve accuracy comparable to or better than full-precision baselines using 16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in low-precision and deeper models, performance remains robust. In conclusion, the proposed integer-only online learning algorithm presents an effective solution for efficiently training SNNs, enabling deployment on resource-constrained neuromorphic hardware without sacrificing accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:54:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06636v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06636v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Guided Decoding and Its Critical Role in Retrieval-Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> √ñzg√ºr Uƒüur, Musa Yƒ±lmaz, Esra ≈ûavirdi, √ñzay Ezerceli, Mahmut El Huseyni, Selva Ta≈ü, Reyhan Bayraktar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:51:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/SIU66497.2025.11111950' target='_blank'>doi</a><a href='http://arxiv.org/abs/2509.06631v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06631v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and
  Context-Aware KGQA</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siyang Gao, Siwei Liu, Nan Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:44:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.00719v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.00719v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in
  Molecular Tumor Boards</h2>
                <div class="authors">
                    <strong>Authors:</strong> Noel Codella, Sam Preston, Hao Qiu, Leonardo Schettini, Wen-wai Yim, Mert √ñz, Shrey Jain, Matthew P. Lungren, Thomas Osborne
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology specialists collaboratively assess complex patient cases to determine optimal treatment strategies. A central element of this process is the patient summary, typically compiled by a medical oncologist, radiation oncologist, or surgeon, or their trained medical assistant, who distills heterogeneous medical records into a concise narrative to facilitate discussion. This manual approach is often labor-intensive, subjective, and prone to omissions of critical information. To address these limitations, we introduce the Healthcare Agent Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that coordinates a multi-agent clinical workflow to generate accurate and comprehensive patient summaries for MTBs. Evaluating predicted patient summaries against ground truth presents additional challenges due to stylistic variation, ordering, synonym usage, and phrasing differences, which complicate the measurement of both succinctness and completeness. To overcome these evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework designed to assess the comprehensiveness and succinctness of generated summaries. Using a benchmark dataset derived from de-identified tumor board discussions, we applied TBFact to evaluate our Patient History agent. Results show that the agent captured 94% of high-importance information (including partial entailments) and achieved a TBFact recall of 0.84 under strict entailment criteria. We further demonstrate that TBFact enables a data-free evaluation framework that institutions can deploy locally without sharing sensitive clinical data. Together, HAO and TBFact establish a robust foundation for delivering reliable and scalable support to MTBs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:15:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06602v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06602v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination
  Mitigation in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Tong, Zhi Lin, Jingya Wang, Bo Jin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:06:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06596v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06596v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Irdin Pekaric, Philipp Zech, Tom Mattson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are transforming human decision-making by acting as cognitive collaborators. Yet, this promise comes with a paradox: while LLMs can improve accuracy, they may also erode independent reasoning, promote over-reliance and homogenize decisions. In this paper, we investigate how LLMs shape human judgment in security-critical contexts. Through two exploratory focus groups (unaided and LLM-supported), we assess decision accuracy, behavioral resilience and reliance dynamics. Our findings reveal that while LLMs enhance accuracy and consistency in routine decisions, they can inadvertently reduce cognitive diversity and improve automation bias, which is especially the case among users with lower resilience. In contrast, high-resilience individuals leverage LLMs more effectively, suggesting that cognitive traits mediate AI benefit.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:06:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06595v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06595v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 IMAGGarment: Fine-Grained Garment Generation for Controllable Fashion
  Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fei Shen, Jian Yu, Cong Wang, Xin Jiang, Xiaoyu Du, Jinhui Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents IMAGGarment, a fine-grained garment generation (FGG) framework that enables high-fidelity garment synthesis with precise control over silhouette, color, and logo placement. Unlike existing methods that are limited to single-condition inputs, IMAGGarment addresses the challenges of multi-conditional controllability in personalized fashion design and digital apparel applications. Specifically, IMAGGarment employs a two-stage training strategy to separately model global appearance and local details, while enabling unified and controllable generation through end-to-end inference. In the first stage, we propose a global appearance model that jointly encodes silhouette and color using a mixed attention module and a color adapter. In the second stage, we present a local enhancement model with an adaptive appearance-aware module to inject user-defined logos and spatial constraints, enabling accurate placement and visual consistency. To support this task, we release GarmentBench, a large-scale dataset comprising over 180K garment samples paired with multi-level design conditions, including sketches, color references, logo placements, and textual prompts. Extensive experiments demonstrate that our method outperforms existing baselines, achieving superior structural stability, color fidelity, and local controllability performance. Code, models, and datasets are publicly available at https://github.com/muzishen/IMAGGarment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:05:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.13176v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.13176v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Steering LLM Reasoning Through Bias-Only Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Viacheslav Sinii, Alexey Gorbatovski, Artem Cherepanov, Boris Shaposhnikov, Nikita Balagansky, Daniil Gavrilov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We show that training a single $d$-dimensional steering vector per layer with reinforcement learning, while freezing all base weights, matches the accuracy of fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8 billion-parameter model this adds only $\approx 0.0016\%$ additional parameters and reproduces performance across a range of base models and mathematical-reasoning benchmarks. These results tighten the upper bound on the parameter budget required for high-level chain-of-thought reasoning, indicating that millions of adapter weights are unnecessary. The minimal trainable footprint reduces optimizer memory and inter-GPU communication, lowering the overall cost of fine-tuning. Moreover, a logit-lens analysis shows that the learned vectors amplify coherent token directions, providing clearer insight into the model's internal computations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:57:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.18706v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.18706v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Simulating Dispute Mediation with LLM-Based Agents for Legal Research</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junjie Chen, Haitao Li, Minghao Qin, Yujia Zhou, Yanxue Ren, Wuyue Wang, Yiqun Liu, Yueyue Wu, Qingyao Ai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Legal dispute mediation plays a crucial role in resolving civil disputes, yet its empirical study is limited by privacy constraints and complex multivariate interactions. To address this limitation, we present AgentMediation, the first LLM-based agent framework for simulating dispute mediation. It simulates realistic mediation processes grounded in real-world disputes and enables controlled experimentation on key variables such as disputant strategies, dispute causes, and mediator expertise. Our empirical analysis reveals patterns consistent with sociological theories, including Group Polarization and Surface-level Consensus. As a comprehensive and extensible platform, AgentMediation paves the way for deeper integration of social science and AI in legal research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:56:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06586v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06586v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ryosuke Takata, Atsushi Masumori, Takashi Ikegami
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:56:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.04537v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.04537v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View
  Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Kong, Daniel Watson, Yannick Str√ºmpler, Michael Niemeyer, Federico Tombari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-view diffusion models have shown promise in 3D novel view synthesis, but most existing methods adopt a non-autoregressive formulation. This limits their applicability in world modeling, as they only support a fixed number of views and suffer from slow inference due to denoising all frames simultaneously. To address these limitations, we propose CausNVS, a multi-view diffusion model in an autoregressive setting, which supports arbitrary input-output view configurations and generates views sequentially. We train CausNVS with causal masking and per-frame noise, using pairwise-relative camera pose encodings (CaPE) for precise camera control. At inference time, we combine a spatially-aware sliding-window with key-value caching and noise conditioning augmentation to mitigate drift. Our experiments demonstrate that CausNVS supports a broad range of camera trajectories, enables flexible autoregressive novel view synthesis, and achieves consistently strong visual quality across diverse settings. Project page: https://kxhit.github.io/CausNVS.html.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:49:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06579v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06579v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on
  the MCP Ecosystem</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuli Zhao, Qinsheng Hou, Zihan Zhan, Yanhao Wang, Yuchong Xie, Yu Guo, Libo Chen, Shenghong Li, Zhi Xue
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are increasingly integrated with external systems through the Model Context Protocol (MCP), which standardizes tool invocation and has rapidly become a backbone for LLM-powered applications. While this paradigm enhances functionality, it also introduces a fundamental security shift: LLMs transition from passive information processors to autonomous orchestrators of task-oriented toolchains, expanding the attack surface, elevating adversarial goals from manipulating single outputs to hijacking entire execution flows. In this paper, we reveal a new class of attacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy Disclosure (MCP-UPD). These attacks require no direct victim interaction; instead, adversaries embed malicious instructions into external data sources that LLMs access during legitimate tasks. The malicious logic infiltrates the toolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection, and Privacy Disclosure, culminating in stealthy exfiltration of private data. Our root cause analysis reveals that MCP lacks both context-tool isolation and least-privilege enforcement, enabling adversarial instructions to propagate unchecked into sensitive tool invocations. To assess the severity, we design MCP-SEC and conduct the first large-scale security census of the MCP ecosystem, analyzing 12,230 tools across 1,360 servers. Our findings show that the MCP ecosystem is rife with exploitable gadgets and diverse attack methods, underscoring systemic risks in MCP platforms and the urgent need for defense mechanisms in LLM-integrated environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:35:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06572v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06572v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Contrastive Self-Supervised Network Intrusion Detection using Augmented
  Negative Pairs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jack Wilkie, Hanan Hindy, Christos Tachtatzis, Robert Atkinson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Network intrusion detection remains a critical challenge in cybersecurity. While supervised machine learning models achieve state-of-the-art performance, their reliance on large labelled datasets makes them impractical for many real-world applications. Anomaly detection methods, which train exclusively on benign traffic to identify malicious activity, suffer from high false positive rates, limiting their usability. Recently, self-supervised learning techniques have demonstrated improved performance with lower false positive rates by learning discriminative latent representations of benign traffic. In particular, contrastive self-supervised models achieve this by minimizing the distance between similar (positive) views of benign traffic while maximizing it between dissimilar (negative) views. Existing approaches generate positive views through data augmentation and treat other samples as negative. In contrast, this work introduces Contrastive Learning using Augmented Negative pairs (CLAN), a novel paradigm for network intrusion detection where augmented samples are treated as negative views - representing potentially malicious distributions - while other benign samples serve as positive views. This approach enhances both classification accuracy and inference efficiency after pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset demonstrates that the proposed method surpasses existing self-supervised and anomaly detection techniques in a binary classification task. Furthermore, when fine-tuned on a limited labelled dataset, the proposed approach achieves superior multi-class classification performance compared to existing self-supervised models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:04:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CR</span><span>cs.NI</span><span>I.2.6; K.6.5</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/CSR64739.2025.11129979' target='_blank'>doi</a><a href='http://arxiv.org/abs/2509.06550v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06550v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, Wenhu Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:03:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.03646v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.03646v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 Transforming Wearable Data into Personal Health Insights using Large
  Language Model Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mike A. Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory Y. McLean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, Xin Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:59:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.06464v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.06464v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Revolutionizing Reinforcement Learning Framework for Diffusion Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:58:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06949v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06949v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liang Chen, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning (RL) has proven effective in incentivizing the reasoning abilities of large language models (LLMs), but suffers from severe efficiency challenges due to its trial-and-error nature. While the common practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this decoupled two-stage approach limits interaction between SFT and RL, thereby constraining overall effectiveness. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. By conditioning the SFT objective on the optimal RL policy, our approach enables SFT to meta-learn how to guide RL's optimization process. During training, the lower level performs RL updates while simultaneously receiving SFT supervision, and the upper level explicitly maximizes the cooperative gain-the performance advantage of joint SFT-RL training over RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:58:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06948v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06948v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Outcome-based Exploration for LLM Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuda Song, Julia Kempe, Remi Munos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. We analyze this phenomenon by viewing RL post-training as a sampling process and show that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. Our study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, we propose outcome-based exploration, which assigns exploration bonuses according to final outcomes. We introduce two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, we formalize the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:52:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06941v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06941v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in
  Multi-Hop Arithmetic Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study investigates the incremental, internal problem-solving process of language models (LMs) with arithmetic multi-hop reasoning as a case study. We specifically investigate when LMs internally resolve sub/whole problems through first reading the problem statements, generating reasoning chains, and achieving the final answer to mechanistically interpret LMs' multi-hop problem-solving process. Our experiments reveal a systematic incremental reasoning strategy underlying LMs. They have not derived an answer at the moment they first read the problem; instead, they obtain (sub)answers while generating the reasoning chain. Therefore, the generated reasoning chains can be regarded as faithful reflections of the model's internal computation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:43:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.01113v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.01113v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 NeedForHeat DataGear: An Open Monitoring System to Accelerate the
  Residential Heating Transition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Henri ter Hofte, Nick van Ravenzwaaij
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce NeedForHeat DataGear: an open hardware and open software data collection system designed to accelerate the residential heating transition. NeedForHeat DataGear collects time series monitoring data in homes that have not yet undergone a heating transition, enabling assessment of real-life thermal characteristics, heating system efficiency, and residents' comfort needs. This paper outlines its architecture and functionalities, emphasizing its modularity, adaptability, and cost-effectiveness for field data acquisition. Unlike conventional domestic monitoring solutions focused on home automation, direct feedback, or post-installation heat pump monitoring, it prioritizes time series data we deemed essential to evaluate the current situation in existing homes before the heating transition. Designed for seamless deployment across diverse households, NeedForHeat DataGear combines openness, security, and privacy with a low-cost, user-friendly approach, making it a valuable tool for researchers, energy professionals, and energy coaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:39:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06927v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06927v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Data-driven solar forecasting enables near-optimal economic decisions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhixiang Dai, Minghao Yin, Xuanhong Chen, Alberto Carpentieri, Jussi Leinonen, Boris Bonev, Chengzhe Zhong, Thorsten Kurth, Jingan Sun, Ram Cherukuri, Yuzhou Zhang, Ruihua Zhang, Farah Hariri, Xiaodong Ding, Chuanxiang Zhu, Dake Zhang, Yaodan Cui, Yuxi Lu, Yue Song, Bin He, Jie Chen, Yixin Zhu, Chenheng Xu, Maofeng Liu, Zeyi Niu, Wanpeng Qi, Xu Shan, Siyuan Xian, Ning Lin, Kairui Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Solar energy adoption is critical to achieving net-zero emissions. However, it remains difficult for many industrial and commercial actors to decide on whether they should adopt distributed solar-battery systems, which is largely due to the unavailability of fast, low-cost, and high-resolution irradiance forecasts. Here, we present SunCastNet, a lightweight data-driven forecasting system that provides 0.05$^\circ$, 10-minute resolution predictions of surface solar radiation downwards (SSRD) up to 7 days ahead. SunCastNet, coupled with reinforcement learning (RL) for battery scheduling, reduces operational regret by 76--93\% compared to robust decision making (RDM). In 25-year investment backtests, it enables up to five of ten high-emitting industrial sectors per region to cross the commercial viability threshold of 12\% Internal Rate of Return (IRR). These results show that high-resolution, long-horizon solar forecasts can directly translate into measurable economic gains, supporting near-optimal energy operations and accelerating renewable deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:38:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.geo-ph</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06925v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06925v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Staying in the Sweet Spot: Responsive Reasoning Evolution via
  Capability-Adaptive Hint Scaffolding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziheng Li, Zexu Sun, Jinman Zhao, Erxue Min, Yongcheng Zeng, Hui Wu, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen, Zhi-Hong Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:36:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06923v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06923v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and
  Opportunities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Shouhuai Xu, Houbing Herbert Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit fundamental limitations: inadequate conceptual grounding leading to non-robustness against novel attacks; limited instructibility impeding analyst-guided adaptation; and misalignment with cybersecurity objectives. Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize cybersecurity AI. However, there is no systematic understanding of this emerging approach. These hybrid systems address critical cybersecurity challenges by combining neural pattern recognition with symbolic reasoning, enabling enhanced threat understanding while introducing concerning autonomous offensive capabilities that reshape threat landscapes. In this survey, we systematically characterize this field by analyzing 127 publications spanning 2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A) framework to evaluate these systems, focusing on both cyber defense and cyber offense across network security, malware analysis, and cyber operations. Our analysis shows advantages of multi-agent NeSy architectures and identifies critical implementation challenges including standardization gaps, computational complexity, and human-AI collaboration requirements that constrain deployment. We show that causal reasoning integration is the most transformative advancement, enabling proactive defense beyond correlation-based approaches. Our findings highlight dual-use implications where autonomous systems demonstrate substantial capabilities in zero-day exploitation while achieving significant cost reductions, altering threat dynamics. We provide insights and future research directions, emphasizing the urgent need for community-driven standardization frameworks and responsible development practices that ensure advancement serves defensive cybersecurity objectives while maintaining societal alignment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:33:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06921v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06921v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and
  Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haywood Gelman, John D. Hastings, David Kenley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including precision, recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:32:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span><span>cs.CY</span><span>C.2.0; I.2.7; K.4.1; H.3.3</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06920v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06920v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Amplifying Effective CXL Memory Bandwidth for LLM Inference via
  Transparent Near-Data Processing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) inference is bottlenecked by the limited bandwidth of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a transparent near-data processing architecture that amplifies effective CXL bandwidth without requiring changes to the CXL.mem interface or AI models. CXL-NDP integrates a precision-scalable bit-plane layout for dynamic quantization with transparent lossless compression of weights and KV caches directly within the CXL device. In end-to-end serving, CXL-NDP improves throughput by 43%, extends the maximum context length by 87%, and reduces the KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms its practicality with a modest silicon footprint, lowering the barrier for adopting efficient, scalable CXL-based memory in generative AI infrastructure.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:22:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.03377v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.03377v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers
  from LLMs via Claim Verification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aivin V. Solatorio
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:20:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CR</span><span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06902v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06902v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 BatStation: Toward In-Situ Radar Sensing on 5G Base Stations with
  Zero-Shot Template Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhihui Gao, Zhecun Liu, Tingjun Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The coexistence between incumbent radar signals and commercial 5G signals necessitates a versatile and ubiquitous radar sensing for efficient and adaptive spectrum sharing. In this context, leveraging the densely deployed 5G base stations (BS) for radar sensing is particularly promising, offering both wide coverage and immediate feedback to 5G scheduling. However, the targeting radar signals are superimposed with concurrent 5G uplink transmissions received by the BS, and practical deployment also demands a lightweight, portable radar sensing model. This paper presents BatStation, a lightweight, in-situ radar sensing framework seamlessly integrated into 5G BSs. BatStation leverages uplink resource grids to extract radar signals through three key components: (i) radar signal separation to cancel concurrent 5G transmissions and reveal the radar signals, (ii) resource grid reshaping to align time-frequency resolution with radar pulse characteristics, and (iii) zero-shot template correlation based on a portable model trained purely on synthetic data that supports detection, classification, and localization of radar pulses without fine-tuning using experimental data. We implement BatStation on a software-defined radio (SDR) testbed and evaluate its performance with real 5G traffic in the CBRS band. Results show robust performance across diverse radar types, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH), classification accuracy up to 97.00%, and median localization errors of 2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably, BatStation achieves this performance with a runtime latency of only 0.11/0.94 ms on GPU/CPU, meeting the real-time requirement of 5G networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:17:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06898v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06898v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Not All Samples Are Equal: Quantifying Instance-level Difficulty in
  Targeted Data Poisoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> William Xu, Yiwei Lu, Yihan Wang, Matthew Y. R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Targeted data poisoning attacks pose an increasingly serious threat due to their ease of deployment and high success rates. These attacks aim to manipulate the prediction for a single test sample in classification models. Unlike indiscriminate attacks that aim to decrease overall test performance, targeted attacks present a unique threat to individual test instances. This threat model raises a fundamental question: what factors make certain test samples more susceptible to successful poisoning than others? We investigate how attack difficulty varies across different test instances and identify key characteristics that influence vulnerability. This paper introduces three predictive criteria for targeted data poisoning difficulty: ergodic prediction accuracy (analyzed through clean training dynamics), poison distance, and poison budget. Our experimental results demonstrate that these metrics effectively predict the varying difficulty of real-world targeted poisoning attacks across diverse scenarios, offering practitioners valuable insights for vulnerability assessment and understanding data poisoning attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:14:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06896v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06896v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 UniSearch: Rethinking Search System with a Unified Generative
  Architecture</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahui Chen, Xiaoze Jiang, Zhibo Wang, Quanzhi Zhu, Junyao Zhao, Feng Hu, Kang Pan, Ao Xie, Maohua Pei, Zhiheng Qin, Hongjing Zhang, Zhixin Zhai, Xiaobo Guo, Runbin Zhou, Kefeng Wang, Mingyang Geng, Cheng Chen, Jingshan Lv, Yupeng Huang, Xiao Liang, Han Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern search systems play a crucial role in facilitating information acquisition. Traditional search engines typically rely on a cascaded architecture, where results are retrieved through recall, pre-ranking, and ranking stages. The complexity of designing and maintaining multiple modules makes it difficult to achieve holistic performance gains. Recent advances in generative recommendation have motivated the exploration of unified generative search as an alternative. However, existing approaches are not genuinely end-to-end: they typically train an item encoder to tokenize candidates first and then optimize a generator separately, leading to objective inconsistency and limited generalization. To address these limitations, we propose UniSearch, a unified generative search framework for Kuaishou Search. UniSearch replaces the cascaded pipeline with an end-to-end architecture that integrates a Search Generator and a Video Encoder. The Generator produces semantic identifiers of relevant items given a user query, while the Video Encoder learns latent item embeddings and provides their tokenized representations. A unified training framework jointly optimizes both components, enabling mutual enhancement and improving representation quality and generation accuracy. Furthermore, we introduce Search Preference Optimization (SPO), which leverages a reward model and real user feedback to better align generation with user preferences. Extensive experiments on industrial-scale datasets, together with online A/B testing in both short-video and live search scenarios, demonstrate the strong effectiveness and deployment potential of UniSearch. Notably, its deployment in live search yields the largest single-experiment improvement in recent years of our product's history, highlighting its practical value for real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:08:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06887v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06887v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Barlow-Swin: Toward a novel siamese-based segmentation architecture
  using Swin-Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Morteza Kiani Haftlang, Mohammadhossein Malmir, Foroutan Parand, Umberto Michelucci, Safouane El Ghazouali
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoder's ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: https://github.com/mkianih/Barlow-Swin.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:05:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06885v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06885v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 LatticeWorld: A Multimodal Large Language Model-Empowered Framework for
  Interactive Complex World Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Yenan Lin, Hao Jiang, Kang Chen, Shuang Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:05:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.05263v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.05263v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joe Wilder, Nikhil Kadapala, Benji Xu, Mohammed Alsaadi, Aiden Parsons, Mitchell Rogers, Palash Agarwal, Adam Hassick, Laura Dietz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T17:02:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06883v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06883v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Not All Features Deserve Attention: Graph-Guided Dependency Learning for
  Tabular Data Generation with Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits sparse feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as LLMs' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates sparse dependency graphs into LLMs' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing LLM-based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:53:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.18504v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.18504v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional
  Reasoning and Voting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ana Rita Ortigoso, Gabriel Vieira, Daniel Fuentes, Luis Fraz√£o, Nuno Costa, Ant√≥nio Pereira
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents Project Riley, a novel multimodal and multi-model conversational AI architecture oriented towards the simulation of reasoning influenced by emotional states. Drawing inspiration from Pixar's Inside Out, the system comprises five distinct emotional agents - Joy, Sadness, Fear, Anger, and Disgust - that engage in structured multi-round dialogues to generate, criticise, and iteratively refine responses. A final reasoning mechanism synthesises the contributions of these agents into a coherent output that either reflects the dominant emotion or integrates multiple perspectives. The architecture incorporates both textual and visual large language models (LLMs), alongside advanced reasoning and self-refinement processes. A functional prototype was deployed locally in an offline environment, optimised for emotional expressiveness and computational efficiency. From this initial prototype, another one emerged, called Armando, which was developed for use in emergency contexts, delivering emotionally calibrated and factually accurate information through the integration of Retrieval-Augmented Generation (RAG) and cumulative context tracking. The Project Riley prototype was evaluated through user testing, in which participants interacted with the chatbot and completed a structured questionnaire assessing three dimensions: Emotional Appropriateness, Clarity and Utility, and Naturalness and Human-likeness. The results indicate strong performance in structured scenarios, particularly with respect to emotional alignment and communicative clarity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:40:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>I.2.7; I.2.1; H.5.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.20521v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.20521v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 The Majority is not always right: RL training for solution aggregation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenting Zhao, Pranjal Aggarwal, Swarnadeep Saha, Asli Celikyilmaz, Jason Weston, Ilia Kulikov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling up test-time compute, by generating multiple independent solutions and selecting or aggregating among them, has become a central paradigm for improving large language models (LLMs) on challenging reasoning tasks. While most prior work relies on simple majority voting or reward model ranking to aggregate solutions, these approaches may only yield limited benefits. In this work, we propose to learn aggregation as an explicit reasoning skill: given a set of candidate solutions, we train an aggregator model to review, reconcile, and synthesize a final, correct answer using reinforcement learning from verifiable rewards. A key ingredient is careful balancing of easy and hard training examples, allowing the model to learn both to recover minority-but-correct answers as well as easy majority-correct answers. Empirically, we find our method, AggLM, outperforms both strong rule-based and reward-model baselines, across multiple benchmarks. Furthermore, it generalizes effectively to solutions from differing models, including stronger ones than contained in the training data, all while requiring substantially fewer tokens than majority voting with larger numbers of solutions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:39:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06870v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06870v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Assistance or Disruption? Exploring and Evaluating the Design and
  Trade-offs of Proactive AI Programming Support</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kevin Pu, Daniel Lazaro, Ian Arawjo, Haijun Xia, Ziang Xiao, Tovi Grossman, Yan Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N=18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and interaction context support alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:34:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3706598.3713357' target='_blank'>doi</a><a href='http://arxiv.org/abs/2502.18658v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.18658v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Disentangling Interaction and Bias Effects in Opinion Dynamics of Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vincent C. Brockers, David A. Ehrlich, Viola Priesemann
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias toward the initiating agent's stance. Applying this framework to multi-step dialogues reveals that opinion trajectories tend to quickly converge to a shared attractor, with the influence of the interaction fading over time, and the impact of biases differing between LLMs. In addition, we fine-tune an LLM on different sets of strongly opinionated statements (incl. misinformation) and demonstrate that the opinion attractor shifts correspondingly. Exposing stark differences between LLMs and providing quantitative tools to compare them to human subjects in the future, our approach highlights both chances and pitfalls in using LLMs as proxies for human behavior.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:26:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.soc-ph</span><span>cs.AI</span><span>nlin.AO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06858v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06858v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Seeing the Forest Through the Trees: Knowledge Retrieval for
  Streamlining Particle Physics Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> James McGreivy, Blaise Delaney, Anja Beck, Mike Williams
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative Large Language Models (LLMs) are a promising approach to structuring knowledge contained within the corpora of research literature produced by large-scale and long-running scientific collaborations. Within experimental particle physics, such structured knowledge bases could expedite methodological and editorial review. Complementarily, within the broader scientific community, generative LLM systems grounded in published work could make for reliable companions allowing non-experts to analyze open-access data. Techniques such as Retrieval Augmented Generation (RAG) rely on semantically matching localized text chunks, but struggle to maintain coherent context when relevant information spans multiple segments, leading to a fragmented representation devoid of global cross-document information. Here, we utilize the hierarchical organization of experimental physics articles to build a tree representation of the corpus, and present the SciTreeRAG system that uses this structure to create contexts that are more focused and contextually rich than standard RAG. Additionally, we develop methods for using LLMs to transform the unstructured corpus into a structured knowledge graph representation. We then implement SciGraphRAG, a retrieval system that leverages this knowledge graph to access global cross-document relationships eluding standard RAG, thereby encapsulating domain-specific connections and expertise. We demonstrate proof-of-concept implementations using the corpus of the LHCb experiment at CERN.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:23:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>hep-ex</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06855v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06855v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Reinforcement learning meets bioprocess control through behaviour
  cloning: Real-world deployment in an industrial photobioreactor</h2>
                <div class="authors">
                    <strong>Authors:</strong> Juan D. Gil, Ehecatl Antonio Del Rio Chanona, Jos√© L. Guzm√°n, Manuel Berenguel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The inherent complexity of living cells as production units creates major challenges for maintaining stable and optimal bioprocess conditions, especially in open Photobioreactors (PBRs) exposed to fluctuating environments. To address this, we propose a Reinforcement Learning (RL) control approach, combined with Behavior Cloning (BC), for pH regulation in open PBR systems. This represents, to the best of our knowledge, the first application of an RL-based control strategy to such a nonlinear and disturbance-prone bioprocess. Our method begins with an offline training stage in which the RL agent learns from trajectories generated by a nominal Proportional-Integral-Derivative (PID) controller, without direct interaction with the real system. This is followed by a daily online fine-tuning phase, enabling adaptation to evolving process dynamics and stronger rejection of fast, transient disturbances. This hybrid offline-online strategy allows deployment of an adaptive control policy capable of handling the inherent nonlinearities and external perturbations in open PBRs. Simulation studies highlight the advantages of our method: the Integral of Absolute Error (IAE) was reduced by 8% compared to PID control and by 5% relative to standard off-policy RL. Moreover, control effort decreased substantially-by 54% compared to PID and 7% compared to standard RL-an important factor for minimizing operational costs. Finally, an 8-day experimental validation under varying environmental conditions confirmed the robustness and reliability of the proposed approach. Overall, this work demonstrates the potential of RL-based methods for bioprocess control and paves the way for their broader application to other nonlinear, disturbance-prone systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:21:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.AI</span><span>cs.LG</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06853v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06853v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with
  Domain-Specific Structured Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> E Fan, Kang Hu, Zhuowen Wu, Jiangyang Ge, Jiawei Miao, Yuzhi Zhang, He Sun, Weizong Wang, Tianhan Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Computational Fluid Dynamics (CFD) is essential for advancing scientific and engineering fields but is hindered by operational complexity, high expertise requirements, and limited accessibility. This paper introduces ChatCFD, an automated agent system for OpenFOAM simulations that processes multi-modal inputs (e.g., research papers, meshes) via an interactive interface, leveraging DeepSeek-R1 and DeepSeek-V3 large language models, a multi-agent architecture, and OpenFOAM knowledge. Its four-stage pipeline (Knowledge Base Construction, User Input Processing, Case File Generation, and Execution and Error Reflection) enables iterative trial-reflection-refinement for intricate setups, supporting diverse physical models and external meshes. Validation on 205 benchmark tutorial cases, 110 perturbed variants, and 2 literature-derived cases shows ChatCFD's 82.1 percent operational success rate on basic cases, outperforming MetaOpenFOAM (6.2 percent) and Foam-Agent (42.3 percent), and 60-80 percent on literature-derived complex cases. Turbulence model studies show a 40 percent success rate for common models versus 10 percent for rare ones like RNG k-epsilon. Physics coupling analyses reveal higher resource demands for multi-physics-coupled cases, while LLM bias toward simpler setups introduces persistent errors, such as dimensional inconsistency. Ablation studies highlight the efficacy of RAG-based modules and reflection mechanisms. By automating hypothesis testing and parameter exploration, ChatCFD accelerates scientific discovery in fluid mechanics and engineering, addressing LLM limitations through structured design and showing strong potential as a modular component in MCP-based agent networks for collaborative multi-agent systems, paving the way for scalable AI-driven CFD innovation. The code for ChatCFD is available at https://github.com/ConMoo/ChatCFD.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:15:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.02019v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.02019v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad Reza Mirbagheri, Mohammad Mahdi Mirkamali, Zahra Motoshaker Arani, Ali Javeri, Amir Mahdi Sadeghzadeh, Rasool Jalili
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:08:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06838v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06838v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eugene Kwek, Wenpeng Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Making LLMs more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a key technique toward this goal. However, prior pruning methods are limited: width pruning often breaks the standard transformer layout or requires custom inference code, while depth pruning removes entire layers and can cause abrupt accuracy drops. In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT enjoys merits of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN pruning), training-free operation with competitive pruning time, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream task performance at similar or higher pruning ratios, with substantial reductions in parameters, GPU memory, and end-to-end latency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:07:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06836v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06836v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Self-Alignment: Improving Alignment of Cultural Values in LLMs via
  In-Context Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rochelle Choenni, Ekaterina Shutova
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Improving the alignment of Large Language Models (LLMs) with respect to the cultural values that they encode has become an increasingly important topic. In this work, we study whether we can exploit existing knowledge about cultural values at inference time to adjust model responses to cultural value probes. We present a simple and inexpensive method that uses a combination of in-context learning (ICL) and human survey data, and show that we can improve the alignment to cultural values across 5 models that include both English-centric and multilingual LLMs. Importantly, we show that our method could prove useful in test languages other than English and can improve alignment to the cultural values that correspond to a range of culturally diverse countries.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T16:06:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.16482v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.16482v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 RAFFLES: Reasoning-based Attribution of Faults for LLM Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenyang Zhu, Spencer Hong, Jingyu Wu, Kushal Chawla, Charlotte Tang, Youbing Yin, Nathan Wolfe, Erin Babinsky, Daben Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We have reached a critical roadblock in the development and enhancement of long-horizon, multi-component LLM agentic systems: it is incredibly tricky to identify where these systems break down and why. Evaluation capabilities that currently exist today (e.g., single pass LLM-as-a-judge) are limited in that they often focus on individual metrics or capabilities, end-to-end outcomes, and are narrowly grounded on the preferences of humans. We argue that to match the agentic capabilities, evaluation frameworks must also be able to reason, probe, iterate, and understand the complex logic passing through these systems over long horizons. In this paper, we present RAFFLES - an evaluation architecture that incorporates reasoning and iterative refinement. Specifically, RAFFLES operates as an iterative, multi-component pipeline, using a central Judge to systematically investigate faults and a set of specialized Evaluators to assess not only the system's components but also the quality of the reasoning by the Judge itself, thereby building a history of hypotheses. We tested RAFFLES against several baselines on the Who&When dataset, a benchmark designed to diagnose the "who" (agent) and "when" (step) of a system's failure. RAFFLES outperforms these baselines, achieving an agent-step fault pair accuracy of over 43% on the Algorithmically-Generated dataset (a substantial increase from the previously published best of 16.6%) and over 20% on the Hand-Crafted dataset (surpassing the previously published best of 8.8%). These results demonstrate a key step towards introducing automated fault detection for autonomous systems over labor-intensive manual human review.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:57:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06822v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06822v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Green Learning for STAR-RIS mmWave Systems with Implicit CSI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu-Hsiang Huang, Po-Heng Chou, Wan-Jen Huang, Walid Saad, C. -C. Jay Kuo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, a green learning (GL)-based precoding framework is proposed for simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems. Motivated by the growing emphasis on environmental sustainability in future 6G networks, this work adopts a broadcasting transmission architecture for scenarios where multiple users share identical information, improving spectral efficiency and reducing redundant transmissions and power consumption. Different from conventional optimization methods, such as block coordinate descent (BCD) that require perfect channel state information (CSI) and iterative computation, the proposed GL framework operates directly on received uplink pilot signals without explicit CSI estimation. Unlike deep learning (DL) approaches that require CSI-based labels for training, the proposed GL approach also avoids deep neural networks and backpropagation, leading to a more lightweight design. Although the proposed GL framework is trained with supervision generated by BCD under full CSI, inference is performed in a fully CSI-free manner. The proposed GL integrates subspace approximation with adjusted bias (Saab), relevant feature test (RFT)-based supervised feature selection, and eXtreme gradient boosting (XGBoost)-based decision learning to jointly predict the STAR-RIS coefficients and transmit precoder. Simulation results show that the proposed GL approach achieves competitive spectral efficiency compared to BCD and DL-based models, while reducing floating-point operations (FLOPs) by over four orders of magnitude. These advantages make the proposed GL approach highly suitable for real-time deployment in energy- and hardware-constrained broadcasting scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:56:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06820v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06820v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation
  Policies and Teleoperation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel San Jos√© Pro, Oliver Hausd√∂rfer, Ralf R√∂mer, Maximilian D√∂sch, Martin Schuck, Angela P. Sch√∂llig
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning-based controllers, such as diffusion policies and vision-language action models, often generate low-frequency or discontinuous robot state changes. Achieving smooth reference tracking requires a low-level controller that converts high-level targets commands into joint torques, enabling compliant behavior during contact interactions. We present CRISP, a lightweight C++ implementation of compliant Cartesian and joint-space controllers for the ROS2 control standard, designed for seamless integration with high-level learning-based policies as well as teleoperation. The controllers are compatible with any manipulator that exposes a joint-torque interface. Through our Python and Gymnasium interfaces, CRISP provides a unified pipeline for recording data from hardware and simulation and deploying high-level learning-based policies seamlessly, facilitating rapid experimentation. The system has been validated on hardware with the Franka Robotics FR3 and in simulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid integration, flexible deployment, and real-time performance, our implementation provides a unified pipeline for data collection and policy execution, lowering the barrier to applying learning-based methods on ROS2-compatible manipulators. Detailed documentation is available at the project website - https://utiasDSL.github.io/crisp_controllers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:55:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06819v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06819v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Efficient Dynamic Clustering-Based Document Compression for
  Retrieval-Augmented-Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weitao Li, Kaiming Liu, Xiangyu Zhang, Xuanyu Lei, Weizhi Ma, Yang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach for knowledge injection during large language model (LLM) inference in recent years. However, due to their limited ability to exploit fine-grained inter-document relationships, current RAG implementations face challenges in effectively addressing the retrieved noise and redundancy content, which may cause error in the generation results. To address these limitations, we propose an Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG) that utilizes latent inter-document relationships while simultaneously removing irrelevant information and redundant content. We validate our approach, built upon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and Hallucination-Detection datasets. Experimental results show that our method achieves consistent performance improvements across various scenarios and experimental settings, demonstrating strong robustness and applicability. Our code and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:51:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.03165v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.03165v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Automatic Prompt Optimization with Prompt Distillation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ernest A. Dyagin, Nikita I. Kulin, Artur R. Khairullin, Viktor N. Zhuravlev, Alena N. Sitkina
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:50:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18992v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18992v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep
  Learning-Based Automated Inspections of Class III Medical Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Julio Zanon Diaz, Tommy Brennan, Peter Corcoran
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As deep learning (DL) technologies advance, their application in automated visual inspection for Class III medical devices offers significant potential to enhance quality assurance and reduce human error. However, the adoption of such AI-based systems introduces new regulatory complexities-particularly under the EU Artificial Intelligence (AI) Act, which imposes high-risk system obligations that differ in scope and depth from established regulatory frameworks such as the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation (QSR). This paper presents a high-level technical assessment of the foreseeable challenges that manufacturers are likely to encounter when qualifying DL-based automated inspections -- specifically static models -- within the existing medical device compliance landscape. It examines divergences in risk management principles, dataset governance, model validation, explainability requirements, and post-deployment monitoring obligations. The discussion also explores potential implementation strategies and highlights areas of uncertainty, including data retention burdens, global compliance implications, and the practical difficulties of achieving statistical significance in validation with limited defect data. Disclaimer: This paper presents a technical perspective and does not constitute legal or regulatory advice.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:48:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20144v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20144v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 A Comparative Benchmark of Large Language Models for Labelling Wind
  Turbine Maintenance Logs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Max Malyi, Jonathan Shek, Alasdair McDonald, Andre Biscaya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Effective Operation and Maintenance (O&M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&M data quality and downstream reliability analysis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:48:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06813v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06813v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in
  the TPTP Ecosystem</h2>
                <div class="authors">
                    <strong>Authors:</strong> Valentin Quesnel, Damien Sileo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available.   https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:43:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06809v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06809v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and
  Security</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanrui Du, Fenglei Fan, Sendong Zhao, Jiawei Cao, Ting Liu, Bing Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Large Language Models (LLMs) increasingly permeate human life, their security has emerged as a critical concern, particularly their ability to maintain harmless responses to malicious instructions. Although extensive methods have improved LLMs' security, they often lead to conservative, rejection-oriented responses that compromise practical usability. This presents a key challenge: how to advance the Pareto frontier between LLMs' usability and security, rather than necessitate a trade-off between them. To address this, we propose the MoGU framework, in which the intra-layer router dynamically allocates weights by sensing hidden states, thereby balancing the contributions of security-optimized and usability-optimized variants. Despite its initial potential, the MoGU framework faces limitations such as parameter redundancy and performance bottlenecks. To overcome these, we further propose an improved MoGU_v2 framework that establishes a tighter coupling between the routers and hidden states. In MoGU_v2, routers are embedded only in layers encoding highly classifiable security features, and backbone modules are activated during router optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong adaptability and stable improvements across various series of LLMs, including mainstream LLMs serving as brains in various applications, on-device LLMs optimized for resource-constrained scenarios, and reasoning LLMs tailored for user interpretability. Meanwhile, even facing risks introduced by Instruction Fine-tuning, MoGU_v2 can easily restore security without compromising the task performance gains via a simple data-mix strategy. These comprehensive improvements highlight MoGU_V2 as a robust and versatile solution for mitigating security risks in real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:39:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06807v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06807v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 MachineLearningLM: Continued Pretraining Language Models on Millions of
  Synthetic Tabular Prediction Tasks Scales In-Context ML</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:38:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06806v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06806v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Error Classification of Large Language Models on Math Word Problems: A
  Dynamically Adaptive Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhong Sun, Zhangyue Yin, Xuanjing Huang, Xipeng Qiu, Hui Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. Math Word Problems (MWPs) serve as a crucial benchmark for evaluating LLMs' reasoning abilities. While most research primarily focuses on improving accuracy, it often neglects understanding and addressing the underlying patterns of errors. Current error classification methods rely on static and predefined categories, which limit their ability to capture the full spectrum of error patterns in mathematical reasoning. To enable systematic error analysis, we collect error samples from 15 different LLMs of varying sizes across four distinct MWP datasets using multiple sampling strategies. Based on this extensive collection, we introduce MWPES-300K, a comprehensive dataset containing 304,865 error samples that cover diverse error patterns and reasoning paths. To reduce human bias and enable fine-grained analysis of error patterns, we propose a novel framework for automated dynamic error classification in mathematical reasoning. Experimental results demonstrate that dataset characteristics significantly shape error patterns, which evolve from basic to complex manifestations as model capabilities increase. With deeper insights into error patterns, we propose Error-Aware Prompting (EAP) that incorporates common error patterns as explicit guidance, leading to significant improvements in mathematical reasoning performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:34:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.15581v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15581v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via
  Projection Constraint</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanrui Du, Fenglei Fan, Sendong Zhao, Jiawei Cao, Qika Lin, Kai He, Ting Liu, Bing Qin, Mengling Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Instruction Fine-Tuning (IFT) has been widely adopted as an effective post-training strategy to enhance various abilities of Large Language Models (LLMs). However, prior studies have shown that IFT can significantly compromise LLMs' safety, particularly their ability to refuse malicious instructions, raising significant concerns. Recent research into the internal mechanisms of LLMs has identified the refusal direction (r-direction) in the hidden states, which plays a pivotal role in governing refusal behavior. Building on this insight, our study reveals that the r-direction tends to drift during training, which we identify as one of the causes of the associated safety risks. To mitigate such drift, our proposed ProCon method introduces a projection-constrained loss term that regularizes the projection magnitude of each training sample's hidden state onto the r-direction. Our initial analysis shows that applying an appropriate constraint can effectively mitigate the refusal direction drift and associated safety risks, but remains limited by overall performance barriers. To overcome this barrier, informed by our observation of early-stage sharp drift and a data-driven perspective, we introduce a warm-up strategy that emphasizes early-stage strong constraints and broaden the data distribution to strengthen constraint signals, leading to an enhanced ProCon method. Experimental results under various datasets, scenarios, and LLMs demonstrate that our method can significantly mitigate safety risks posed by IFT while preserving task performance gains. Even compared with strong baselines, our method consistently delivers superior overall performance. Crucially, our analysis indicates that ProCon can contribute to stabilizing the r-direction during training, while such an interpretability-driven exploration of LLMs' internal mechanisms lays a solid foundation for future safety research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:24:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06795v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06795v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Oyster-I: Beyond Refusal -- Constructive Safety Alignment for
  Responsible Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, Yinpeng Dong, Yichi Zhang, Yuefeng Chen, Chongwen Wang, Xingjun Ma, Xingxing Wei, Yang Liu, Hang Su, Jun Zhu, Xinfeng Li, Yitong Sun, Jie Zhang, Jinzhao Hu, Sha Xu, Yitong Yang, Jialing Tao, Hui Xue
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:18:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.CY</span><span>cs.HC</span><span>cs.SC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.01909v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.01909v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Why Report Failed Interactions With Robots?! Towards Vignette-based
  Interaction Quality</h2>
                <div class="authors">
                    <strong>Authors:</strong> Agnes Axelsson, Merle Reimann, Ronald Cumbal, Hannah Pelikan, Divesh Lala
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although the quality of human-robot interactions has improved with the advent of LLMs, there are still various factors that cause systems to be sub-optimal when compared to human-human interactions. The nature and criticality of failures are often dependent on the context of the interaction and so cannot be generalized across the wide range of scenarios and experiments which have been implemented in HRI research. In this work we propose the use of a technique overlooked in the field of HRI, ethnographic vignettes, to clearly highlight these failures, particularly those that are rarely documented. We describe the methodology behind the process of writing vignettes and create our own based on our personal experiences with failures in HRI systems. We emphasize the strength of vignettes as the ability to communicate failures from a multi-disciplinary perspective, promote transparency about the capabilities of robots, and document unexpected behaviours which would otherwise be omitted from research reports. We encourage the use of vignettes to augment existing interaction evaluation methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:18:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10603v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10603v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study
  of ChatGPT and Claude</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yile Yan, Yuqi Zhu, Wentao Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study systematically evaluates how nine popular LLMs (both open-source and closed-source) respond to ethical dilemmas involving protected attributes. Across 50,400 trials spanning single and intersectional attribute combinations in four dilemma scenarios (protective vs. harmful), we assess models' ethical preferences, sensitivity, stability, and clustering patterns. Results reveal significant biases in protected attributes in all models, with differing preferences depending on model type and dilemma context. Notably, open-source LLMs show stronger preferences for marginalized groups and greater sensitivity in harmful scenarios, while closed-source models are more selective in protective situations and tend to favor mainstream groups. We also find that ethical behavior varies across dilemma types: LLMs maintain consistent patterns in protective scenarios but respond with more diverse and cognitively demanding decisions in harmful ones. Furthermore, models display more pronounced ethical tendencies under intersectional conditions than in single-attribute settings, suggesting that complex inputs reveal deeper biases. These findings highlight the need for multi-dimensional, context-aware evaluation of LLMs' ethical behavior and offer a systematic evaluation and approach to understanding and addressing fairness in LLM decision-making.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T15:16:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10484v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10484v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 OpenCoderRank: AI-Driven Technical Assessments Made Easy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hridoy Sankar Dutta, Sana Ansari, Swati Kumari, Shounak Ravi Bhalerao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Organizations and educational institutions use time-bound assessment tasks to evaluate coding and problem-solving skills. These assessments measure not only the correctness of the solutions, but also their efficiency. Problem setters (educator/interviewer) are responsible for crafting these challenges, carefully balancing difficulty and relevance to create meaningful evaluation experiences. Conversely, problem solvers (student/interviewee) apply coding efficiency and logical thinking to arrive at correct solutions. In the era of Large Language Models (LLMs), LLMs assist problem setters in generating diverse and challenging questions, but they can undermine assessment integrity for problem solvers by providing easy access to solutions. This paper introduces OpenCoderRank, an easy-to-use platform designed to simulate technical assessments. It acts as a bridge between problem setters and problem solvers, helping solvers prepare for time constraints and unfamiliar problems while allowing setters to self-host assessments, offering a no-cost and customizable solution for technical assessments in resource-constrained environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:58:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06774v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06774v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Generative World Explorer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Taiming Lu, Tianmin Shu, Alan Yuille, Daniel Khashabi, Jieneng Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state. In contrast, humans can $\textit{imagine}$ unseen parts of the world through a mental exploration and $\textit{revise}$ their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the $\textit{Generative World Explorer (Genex)}$, an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train $\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) $\textit{Genex}$ can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:56:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.11844v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.11844v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM
  Prompting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shashidhar Reddy Javaji, Bhavul Gauri, Zining Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are now used in multi-turn workflows, but we still lack a clear way to measure when iteration helps and when it hurts. We present an evaluation framework for iterative refinement that spans ideation, code, and math. Our protocol runs controlled 12-turn conversations per task, utilizing a variety of prompts ranging from vague ``improve it'' feedback to targeted steering, and logs per-turn outputs. We score outcomes with domain-appropriate checks (unit tests for code; answer-equivalence plus reasoning-soundness for math; originality and feasibility for ideation) and track turn-level behavior with three families of metrics: semantic movement across turns, turn-to-turn change, and output size growth. Across models and tasks, gains are domain-dependent: they arrive early in ideas and code, but in math late turns matter when guided by elaboration. After the first few turns, vague feedback often plateaus or reverses correctness, while targeted prompts reliably shift the intended quality axis (novelty vs. feasibility in ideation; speed vs. readability in code; in math, elaboration outperforms exploration and drives late-turn gains). We also observe consistent domain patterns: ideation moves more in meaning across turns, code tends to grow in size with little semantic change, and math starts fixed but can break that path with late, elaborative iteration.Together, the framework and metrics make iteration measurable and comparable across models, and signal when to steer, stop, or switch strategies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:54:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06770v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06770v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Raw2Event: Converting Raw Frame Camera into Event Camera</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zijie Ning, Enmin Lin, Sudarshan R. Iyengar, Patrick Vandewalle
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Event cameras offer unique advantages such as high temporal resolution, low latency, and high dynamic range, making them more and more popular for vision tasks under challenging light conditions. However, their high cost, limited resolution, and lack of features such as autofocus hinder their broad adoption, particularly for early-stage development and prototyping. In this work, we present Raw2Event, a complete hardware-software system that enables real-time event generation from low-cost raw frame-based cameras. By leveraging direct access to raw Bayer data and bypassing traditional image signal processors (ISP), our system is able to utilize the full potential of camera hardware, delivering higher dynamic range, higher resolution, and more faithful output than RGB-based frame-to-event converters.   Built upon the DVS-Voltmeter model, Raw2Event features a configurable simulation framework optimized for deployment on embedded platforms. We further design a data acquisition pipeline that supports synchronized recording of raw, RGB, and event streams, facilitating downstream evaluation and dataset creation. Experimental results show that Raw2Event can generate event streams closely resembling those from real event cameras, while benefiting from higher resolution and autofocus capabilities. The system also supports user-intuitive parameter tuning, enabling flexible adaptation to various application requirements. Finally, we deploy the system on a Raspberry Pi for real-time operation, providing a scalable and cost-effective solution for event-based vision research and early-stage system development.   The codes are available online: https://anonymous.4open.science/r/raw2event-BFF2/README.md.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:53:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06767v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06767v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiang Meng, Kayhan Behdin, Haoyue Wang, Rahul Mazumder
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:34:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.07831v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.07831v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 SUDER: Self-Improving Unified Large Multimodal Models for Understanding
  and Generation with Dual Self-Rewards</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jixiang Hong, Yiran Zhang, Guanzhong Wang, Yi Liu, Ji-Rong Wen, Rui Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate vision-language alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are naturally inverse dual tasks, we propose \textbf{SUDER} (\textbf{S}elf-improving \textbf{U}nified LMMs with \textbf{D}ual s\textbf{E}lf-\textbf{R}ewards), a framework reinforcing the understanding and generation capabilities of LMMs with a self-supervised dual reward mechanism. SUDER leverages the inherent duality between understanding and generation tasks to provide self-supervised optimization signals for each other. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood within the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:31:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.07963v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.07963v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Intelligent Manufacturing Support: Specialized LLMs for Composite
  Material Processing and Equipment Operation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gunnika Kapoor, Komal Chawla, Tirthankar Ghosal, Kris Villez, Dan Coughlin, Tyden Rucker, Vincent Paquit, Soydan Ozcan, Seokpum Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Engineering educational curriculum and standards cover many material and manufacturing options. However, engineers and designers are often unfamiliar with certain composite materials or manufacturing techniques. Large language models (LLMs) could potentially bridge the gap. Their capacity to store and retrieve data from large databases provides them with a breadth of knowledge across disciplines. However, their generalized knowledge base can lack targeted, industry-specific knowledge. To this end, we present two LLM-based applications based on the GPT-4 architecture: (1) The Composites Guide: a system that provides expert knowledge on composites material and connects users with research and industry professionals who can provide additional support and (2) The Equipment Assistant: a system that provides guidance for manufacturing tool operation and material characterization. By combining the knowledge of general AI models with industry-specific knowledge, both applications are intended to provide more meaningful information for engineers. In this paper, we discuss the development of the applications and evaluate it through a benchmark and two informal user studies. The benchmark analysis uses the Rouge and Bertscore metrics to evaluate our model performance against GPT-4o. The results show that GPT-4o and the proposed models perform similarly or better on the ROUGE and BERTScore metrics. The two user studies supplement this quantitative evaluation by asking experts to provide qualitative and open-ended feedback about our model performance on a set of domain-specific questions. The results of both studies highlight a potential for more detailed and specific responses with the Composites Guide and the Equipment Assistant.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:28:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06734v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06734v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Reinforcement Learning Foundations for Deep Research Systems: A Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenjun Li, Zhi Chen, Jingru Lin, Hannan Cao, Wei Han, Sheng Liang, Zhi Zhang, Kuicai Dong, Dexun Li, Chen Zhang, Yong Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:27:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06733v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06733v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Leveraging Large Language Models for Accurate Sign Language Translation
  in Low-Resource Scenarios</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luana Bulla, Gabriele Tuccio, Misael Mongiov√¨, Aldo Gangemi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Translating natural languages into sign languages is a highly complex and underexplored task. Despite growing interest in accessibility and inclusivity, the development of robust translation systems remains hindered by the limited availability of parallel corpora which align natural language with sign language data. Existing methods often struggle to generalize in these data-scarce environments, as the few datasets available are typically domain-specific, lack standardization, or fail to capture the full linguistic richness of sign languages. To address this limitation, we propose Advanced Use of LLMs for Sign Language Translation (AulSign), a novel method that leverages Large Language Models via dynamic prompting and in-context learning with sample selection and subsequent sign association. Despite their impressive abilities in processing text, LLMs lack intrinsic knowledge of sign languages; therefore, they are unable to natively perform this kind of translation. To overcome this limitation, we associate the signs with compact descriptions in natural language and instruct the model to use them. We evaluate our method on both English and Italian languages using SignBank+, a recognized benchmark in the field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior performance compared to state-of-the-art models in low-data scenario. Our findings demonstrate the effectiveness of AulSign, with the potential to enhance accessibility and inclusivity in communication technologies for underrepresented linguistic communities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:25:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>I.2; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18183v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18183v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient
  LLM-based Scientific Peer Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Hayden Kwok-Hay So, Zhijiang Guo, Liya Zhu, Ngai Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Large Language Models (LLMs) have shown significant potential in assisting peer review, current methods often struggle to generate thorough and insightful reviews while maintaining efficiency. In this paper, we propose TreeReview, a novel framework that models paper review as a hierarchical and bidirectional question-answering process. TreeReview first constructs a tree of review questions by recursively decomposing high-level questions into fine-grained sub-questions and then resolves the question tree by iteratively aggregating answers from leaf to root to get the final review. Crucially, we incorporate a dynamic question expansion mechanism to enable deeper probing by generating follow-up questions when needed. We construct a benchmark derived from ICLR and NeurIPS venues to evaluate our method on full review generation and actionable feedback comments generation tasks. Experimental results of both LLM-based and human evaluation show that TreeReview outperforms strong baselines in providing comprehensive, in-depth, and expert-aligned review feedback, while reducing LLM token usage by up to 80% compared to computationally intensive approaches. Our code and benchmark dataset are available at https://github.com/YuanChang98/tree-review.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:22:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.07642v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.07642v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Efficiently Ranking Software Variants with Minimal Benchmarks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Th√©o Matricon, Mathieu Acher, Helge Spieker, Arnaud Gotlieb
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Benchmarking is a common practice in software engineering to assess the qualities and performance of software variants, coming from multiple competing systems or from configurations of the same system. Benchmarks are used notably to compare and understand variant performance, fine-tune software, detect regressions, or design new software systems. The execution of benchmarks to get a complete picture of software variants is highly costly in terms of computational resources and time. In this paper, we propose a novel approach for reducing benchmarks while maintaining stable rankings, using test suite optimization techniques. That is, we remove instances from the benchmarks while trying to keep the same rankings of the variants on all tests. Our method, BISection Sampling, BISS, strategically retains the most critical tests and applies a novel divide-and-conquer approach to efficiently sample among relevant remaining tests. We experiment with datasets and use cases from LLM leaderboards, SAT competitions, and configurable systems for performance modeling. Our results show that our method outperforms baselines even when operating on a subset of variants. Using BISS, we reduce the computational cost of the benchmarks on average to 44% and on more than half the benchmarks by up to 99% without loss in ranking stability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T14:11:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06716v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06716v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Probabilistic Modeling of Latent Agentic Substructures in Deep Neural
  Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Su Hyeong Lee, Risi Kondor, Richard Ngo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We develop a theory of intelligent agency grounded in probabilistic modeling for neural models. Agents are represented as outcome distributions with epistemic utility given by log score, and compositions are defined through weighted logarithmic pooling that strictly improves every member's welfare. We prove that strict unanimity is impossible under linear pooling or in binary outcome spaces, but possible with three or more outcomes. Our framework admits recursive structure via cloning invariance, continuity, and openness, while tilt-based analysis rules out trivial duplication. Finally, we formalize an agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a manifest-then-suppress Waluigi strategy yields strictly larger first-order misalignment reduction than pure Luigi reinforcement alone. These results clarify how developing a principled mathematical framework for how subagents can coalesce into coherent higher-level entities provides novel implications for alignment in agentic AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:55:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06701v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06701v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Sovereign AI for 6G: Towards the Future of AI-Native Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Swarna Bindu Chetty, David Grace, Simon Saunders, Paul Harris, Eirini Eleni Tsiropoulou, Tony Quek, Hamed Ahmadi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The advent of Generative Artificial Intelligence (GenAI), Large Language Models (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile networks, especially as the telecom industry transitions from 5G's cloud-centric to AI-native 6G architectures. This transition unlocks unprecedented capabilities in real-time automation, semantic networking, and autonomous service orchestration. However, it introduces critical risks related to data sovereignty, security, explainability, and regulatory compliance especially when AI models are trained, deployed, or governed externally. This paper introduces the concept of `Sovereign AI' as a strategic imperative for 6G, proposing architectural, operational, and governance frameworks that enable national or operator-level control over AI development, deployment, and life-cycle management. Focusing on O-RAN architecture, we explore how sovereign AI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure policy-aligned control, secure model updates, and federated learning across trusted infrastructure. We analyse global strategies, technical enablers, and challenges across safety, talent, and model governance. Our findings underscore that Sovereign AI is not just a regulatory necessity but a foundational pillar for secure, resilient, and ethically-aligned 6G networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:54:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06700v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06700v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ
  Bioprinting Monitoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Usman Haider, Lukasz Szemet, Daniel Kelly, Vasileios Sergis, Andrew C. Daly, Karl Mason
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:44:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.AR</span><span>N/A</span><span>I.2.9; I.2.10; I.4.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06690v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06690v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable
  VANETs under Partial Observations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaolu Fu, Ziyuan Bao, Eiman Kanjo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vehicular ad hoc networks (VANETs) are a crucial component of intelligent transportation systems; however, routing remains challenging due to dynamic topologies, incomplete observations, and the limited resources of edge devices. Existing reinforcement learning (RL) approaches often assume fixed graph structures and require retraining when network conditions change, making them unsuitable for deployment on constrained hardware. We present TrajAware, an RL-based framework designed for edge AI deployment in VANETs. TrajAware integrates three components: (i) action space pruning, which reduces redundant neighbour options while preserving two-hop reachability, alleviating the curse of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to the global graph context, producing features that generalise across diverse network sizes; and (iii) trajectory-aware prediction, which uses historical routes and junction information to estimate real-time positions under partial observations. We evaluate TrajAware in the open-source SUMO simulator using real-world city maps with a leave-one-city-out setup. Results show that TrajAware achieves near-shortest paths and high delivery ratios while maintaining efficiency suitable for constrained edge devices, outperforming state-of-the-art baselines in both full and partial observation scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:24:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06665v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06665v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Process-Supervised Reward Models for Verifying Clinical Note Generation:
  A Scalable Approach Guided by Domain Expertise</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanyin Wang, Chufan Gao, Qiping Xu, Bolun Liu, Guleid Hussein, Hariprasad Korsapati, Mohamad El Labban, Kingsley Iheasirim, Mohamed Hassan, Gokhan Anil, Brian Bartlett, Jimeng Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Process-supervised reward models (PRMs) excel at providing step-by-step verification for large language model (LLM) outputs in domains like mathematics and coding. However, their application to fields lacking ground-truth answers, such as clinical note generation, poses significant challenges. We introduce a novel framework for training PRMs to deliver step-level reward signals for LLM-generated clinical notes. By precisely defining meaningful "steps," injecting realistic "errors" informed by domain expertise, and leveraging LLMs to generate process supervision data at scale, we overcome previous limitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms proprietary reasoning and non-reasoning models, achieving state-of-the-art performance on two key evaluations: (1) distinguishing gold-standard from error-containing samples with 98.8% accuracy, and (2) selecting physician-preferred clinical notes with 56.2% accuracy. We investigate critical components for effective PRM training, including optimal loss functions and data selection strategies, and present a comprehensive physician reader study identifying predictors of downstream Best-of-N performance. Our study sheds light on unlocking the potential of PRMs for diverse generative tasks across domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:19:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12583v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12583v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized
  Self-Training and Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sining Zhoubian, Dan Zhang, Jie Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We conduct extensive experiments on coding problems to verify the validity of the proposed RL paradigm. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:12:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19576v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19576v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 IntrEx: A Dataset for Modeling Engagement in Educational Conversations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingwei Tan, Mahathi Parvatham, Chiara Gambi, Gabriele Pergola
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Engagement and motivation are crucial for second-language acquisition, yet maintaining learner interest in educational conversations remains a challenge. While prior research has explored what makes educational texts interesting, still little is known about the linguistic features that drive engagement in conversations. To address this gap, we introduce IntrEx, the first large dataset annotated for interestingness and expected interestingness in teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus (TSCC), IntrEx extends prior work by incorporating sequence-level annotations, allowing for the study of engagement beyond isolated turns to capture how interest evolves over extended dialogues. We employ a rigorous annotation process with over 100 second-language learners, using a comparison-based rating approach inspired by reinforcement learning from human feedback (RLHF) to improve agreement. We investigate whether large language models (LLMs) can predict human interestingness judgments. We find that LLMs (7B/8B parameters) fine-tuned on interestingness ratings outperform larger proprietary models like GPT-4o, demonstrating the potential for specialised datasets to model engagement in educational settings. Finally, we analyze how linguistic and cognitive factors, such as concreteness, comprehensibility (readability), and uptake, influence engagement in educational dialogues.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T13:07:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06652v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06652v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Hallucination Detection on a Budget: Efficient Bayesian Estimation of
  Semantic Entropy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kamil Ciosek, Nicol√≤ Felicioni, Sina Ghiassian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Detecting whether an LLM hallucinates is an important research challenge. One promising way of doing so is to estimate the semantic entropy (Farquhar et al., 2024) of the distribution of generated sequences. We propose a new algorithm for doing that, with two main advantages. First, due to us taking the Bayesian approach, we achieve a much better quality of semantic entropy estimates for a given budget of samples from the LLM. Second, we are able to tune the number of samples adaptively so that `harder' contexts receive more samples. We demonstrate empirically that our approach systematically beats the baselines, requiring only 53% of samples used by Farquhar et al. (2024) to achieve the same quality of hallucination detection as measured by AUROC. Moreover, quite counterintuitively, our estimator is useful even with just one sample from the LLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:59:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.03579v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.03579v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Full Integer Arithmetic Online Training for Spiking Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ismael Gomez, Guangzhi Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Spiking Neural Networks (SNNs) are promising for neuromorphic computing due to their biological plausibility and energy efficiency. However, training methods like Backpropagation Through Time (BPTT) and Real Time Recurrent Learning (RTRL) remain computationally intensive. This work introduces an integer-only, online training algorithm using a mixed-precision approach to improve efficiency and reduce memory usage by over 60%. The method replaces floating-point operations with integer arithmetic to enable hardware-friendly implementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs, RSNNs), showing versatility across architectures. Evaluations on MNIST and the Spiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models achieve accuracy comparable to or better than full-precision baselines using 16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in low-precision and deeper models, performance remains robust. In conclusion, the proposed integer-only online learning algorithm presents an effective solution for efficiently training SNNs, enabling deployment on resource-constrained neuromorphic hardware without sacrificing accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:54:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06636v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06636v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Guided Decoding and Its Critical Role in Retrieval-Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> √ñzg√ºr Uƒüur, Musa Yƒ±lmaz, Esra ≈ûavirdi, √ñzay Ezerceli, Mahmut El Huseyni, Selva Ta≈ü, Reyhan Bayraktar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:51:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/SIU66497.2025.11111950' target='_blank'>doi</a><a href='http://arxiv.org/abs/2509.06631v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06631v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and
  Context-Aware KGQA</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siyang Gao, Siwei Liu, Nan Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:44:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.00719v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.00719v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in
  Molecular Tumor Boards</h2>
                <div class="authors">
                    <strong>Authors:</strong> Noel Codella, Sam Preston, Hao Qiu, Leonardo Schettini, Wen-wai Yim, Mert √ñz, Shrey Jain, Matthew P. Lungren, Thomas Osborne
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology specialists collaboratively assess complex patient cases to determine optimal treatment strategies. A central element of this process is the patient summary, typically compiled by a medical oncologist, radiation oncologist, or surgeon, or their trained medical assistant, who distills heterogeneous medical records into a concise narrative to facilitate discussion. This manual approach is often labor-intensive, subjective, and prone to omissions of critical information. To address these limitations, we introduce the Healthcare Agent Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that coordinates a multi-agent clinical workflow to generate accurate and comprehensive patient summaries for MTBs. Evaluating predicted patient summaries against ground truth presents additional challenges due to stylistic variation, ordering, synonym usage, and phrasing differences, which complicate the measurement of both succinctness and completeness. To overcome these evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework designed to assess the comprehensiveness and succinctness of generated summaries. Using a benchmark dataset derived from de-identified tumor board discussions, we applied TBFact to evaluate our Patient History agent. Results show that the agent captured 94% of high-importance information (including partial entailments) and achieved a TBFact recall of 0.84 under strict entailment criteria. We further demonstrate that TBFact enables a data-free evaluation framework that institutions can deploy locally without sharing sensitive clinical data. Together, HAO and TBFact establish a robust foundation for delivering reliable and scalable support to MTBs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:15:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06602v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06602v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination
  Mitigation in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Tong, Zhi Lin, Jingya Wang, Bo Jin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:06:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06596v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06596v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Irdin Pekaric, Philipp Zech, Tom Mattson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are transforming human decision-making by acting as cognitive collaborators. Yet, this promise comes with a paradox: while LLMs can improve accuracy, they may also erode independent reasoning, promote over-reliance and homogenize decisions. In this paper, we investigate how LLMs shape human judgment in security-critical contexts. Through two exploratory focus groups (unaided and LLM-supported), we assess decision accuracy, behavioral resilience and reliance dynamics. Our findings reveal that while LLMs enhance accuracy and consistency in routine decisions, they can inadvertently reduce cognitive diversity and improve automation bias, which is especially the case among users with lower resilience. In contrast, high-resilience individuals leverage LLMs more effectively, suggesting that cognitive traits mediate AI benefit.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:06:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06595v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06595v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific
  Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Meher V. R. Malladi, Tiziano Guadagnino, Luca Lobefaro, Cyrill Stachniss
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate odometry is a critical component in a robotic navigation stack, and subsequent modules such as planning and control often rely on an estimate of the robot's motion. Sensor-based odometry approaches should be robust across sensor types and deployable in different target domains, from solid-state LiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on handheld packages used in unstructured natural environments. In this paper, we propose a robust LiDAR-inertial odometry system that does not rely on sensor-specific modeling. Sensor fusion techniques for LiDAR and inertial measurement unit (IMU) data typically integrate IMU data iteratively in a Kalman filter or use pre-integration in a factor graph framework, combined with LiDAR scan matching often exploiting some form of feature extraction. We propose an alternative strategy that only requires a simplified motion model for IMU integration and directly registers LiDAR scans in a scan-to-map approach. Our approach allows us to impose a novel regularization on the LiDAR registration, improving the overall odometry performance. We detail extensive experiments on a number of datasets covering a wide array of commonly used robotic sensors and platforms. We show that our approach works with the exact same configuration in all these scenarios, demonstrating its robustness. We have open-sourced our implementation so that the community can build further on our work and use it in their navigation stacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:04:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06593v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06593v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT
  Denoising</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yichao Liu, YueYang Teng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low-dose computed tomography (LDCT) and positron emission tomography (PET) have emerged as safer alternatives to conventional imaging modalities by significantly reducing radiation exposure. However, this reduction often results in increased noise and artifacts, which can compromise diagnostic accuracy. Consequently, denoising for LDCT/PET has become a vital area of research aimed at enhancing image quality while maintaining radiation safety. In this study, we introduce a novel Hybrid Swin Attention Network (HSANet), which incorporates Efficient Global Attention (EGA) modules and a hybrid upsampling module. The EGA modules enhance both spatial and channel-wise interaction, improving the network's capacity to capture relevant features, while the hybrid upsampling module mitigates the risk of overfitting to noise. We validate the proposed approach using a publicly available LDCT/PET dataset. Experimental results demonstrate that HSANet achieves superior denoising performance compared to existing methods, while maintaining a lightweight model size suitable for deployment on GPUs with standard memory configurations. This makes our approach highly practical for real-world clinical applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T12:02:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06591v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06591v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Steering LLM Reasoning Through Bias-Only Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Viacheslav Sinii, Alexey Gorbatovski, Artem Cherepanov, Boris Shaposhnikov, Nikita Balagansky, Daniil Gavrilov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We show that training a single $d$-dimensional steering vector per layer with reinforcement learning, while freezing all base weights, matches the accuracy of fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8 billion-parameter model this adds only $\approx 0.0016\%$ additional parameters and reproduces performance across a range of base models and mathematical-reasoning benchmarks. These results tighten the upper bound on the parameter budget required for high-level chain-of-thought reasoning, indicating that millions of adapter weights are unnecessary. The minimal trainable footprint reduces optimizer memory and inter-GPU communication, lowering the overall cost of fine-tuning. Moreover, a logit-lens analysis shows that the learned vectors amplify coherent token directions, providing clearer insight into the model's internal computations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:57:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.18706v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.18706v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Simulating Dispute Mediation with LLM-Based Agents for Legal Research</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junjie Chen, Haitao Li, Minghao Qin, Yujia Zhou, Yanxue Ren, Wuyue Wang, Yiqun Liu, Yueyue Wu, Qingyao Ai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Legal dispute mediation plays a crucial role in resolving civil disputes, yet its empirical study is limited by privacy constraints and complex multivariate interactions. To address this limitation, we present AgentMediation, the first LLM-based agent framework for simulating dispute mediation. It simulates realistic mediation processes grounded in real-world disputes and enables controlled experimentation on key variables such as disputant strategies, dispute causes, and mediator expertise. Our empirical analysis reveals patterns consistent with sociological theories, including Group Polarization and Surface-level Consensus. As a comprehensive and extensible platform, AgentMediation paves the way for deeper integration of social science and AI in legal research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:56:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06586v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06586v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ryosuke Takata, Atsushi Masumori, Takashi Ikegami
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:56:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.04537v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.04537v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on
  the MCP Ecosystem</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuli Zhao, Qinsheng Hou, Zihan Zhan, Yanhao Wang, Yuchong Xie, Yu Guo, Libo Chen, Shenghong Li, Zhi Xue
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are increasingly integrated with external systems through the Model Context Protocol (MCP), which standardizes tool invocation and has rapidly become a backbone for LLM-powered applications. While this paradigm enhances functionality, it also introduces a fundamental security shift: LLMs transition from passive information processors to autonomous orchestrators of task-oriented toolchains, expanding the attack surface, elevating adversarial goals from manipulating single outputs to hijacking entire execution flows. In this paper, we reveal a new class of attacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy Disclosure (MCP-UPD). These attacks require no direct victim interaction; instead, adversaries embed malicious instructions into external data sources that LLMs access during legitimate tasks. The malicious logic infiltrates the toolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection, and Privacy Disclosure, culminating in stealthy exfiltration of private data. Our root cause analysis reveals that MCP lacks both context-tool isolation and least-privilege enforcement, enabling adversarial instructions to propagate unchecked into sensitive tool invocations. To assess the severity, we design MCP-SEC and conduct the first large-scale security census of the MCP ecosystem, analyzing 12,230 tools across 1,360 servers. Our findings show that the MCP ecosystem is rife with exploitable gadgets and diverse attack methods, underscoring systemic risks in MCP platforms and the urgent need for defense mechanisms in LLM-integrated environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:35:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06572v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06572v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in
  Panoramic Radiographs using Federated, Centralized and Local Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Johan Andreas Balle Rubak, Khuram Naveed, Sanyam Jain, Lukas Esterle, Alexandros Iosifidis, Ruben Pauwels
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Objectives: Federated learning (FL) may mitigate privacy constraints, heterogeneous data quality, and inconsistent labeling in dental diagnostic AI. We compared FL with centralized (CL) and local learning (LL) for tooth segmentation in panoramic radiographs across multiple data corruption scenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six institutions across four settings: baseline (unaltered data); label manipulation (dilated/missing annotations); image-quality manipulation (additive Gaussian noise); and exclusion of a faulty client with corrupted data. FL was implemented via the Flower AI framework. Per-client training- and validation-loss trajectories were monitored for anomaly detection and a set of metrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set. From these metrics significance results were reported through Wilcoxon signed-rank test. CL and LL served as comparators. Results: Baseline: FL achieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at 0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777). Label manipulation: FL maintained the best median Dice score at 0.94884 (ASSD: 1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD: 1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL scored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD: 1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD: 1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring reliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and outperforms LL across corruption scenarios while preserving privacy. Per-client loss trajectories provide an effective anomaly-detection mechanism and support FL as a practical, privacy-preserving approach for scalable clinical AI deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:07:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.IV</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06553v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06553v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Tackling Device Data Distribution Real-time Shift via Prototype-based
  Parameter Editing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheqi Lv, Wenqiao Zhang, Kairui Fu, Qi Tian, Shengyu Zhang, Jiajie Su, Jingyuan Chen, Kun Kuang, Fei Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The on-device real-time data distribution shift on devices challenges the generalization of lightweight on-device models. This critical issue is often overlooked in current research, which predominantly relies on data-intensive and computationally expensive fine-tuning approaches. To tackle this, we introduce Persona, a novel personalized method using a prototype-based, backpropagation-free parameter editing framework to enhance model generalization without post-deployment retraining. Persona employs a neural adapter in the cloud to generate a parameter editing matrix based on real-time device data. This matrix adeptly adapts on-device models to the prevailing data distributions, efficiently clustering them into prototype models. The prototypes are dynamically refined via the parameter editing matrix, facilitating efficient evolution. Furthermore, the integration of cross-layer knowledge transfer ensures consistent and context-aware multi-layer parameter changes and prototype assignment. Extensive experiments on vision task and recommendation task on multiple datasets confirm Persona's effectiveness and generality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:06:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CV</span><span>cs.DC</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06552v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06552v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, Wenhu Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T11:03:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.03646v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.03646v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Reasoning-enhanced Query Understanding through Decomposition and
  Interpretation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunfei Zhong, Jun Yang, Yixing Fan, Jiafeng Guo, Lixin Su, Maarten de Rijke, Ruqing Zhang, Dawei Yin, Xueqi Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate inference of user intent is crucial for enhancing document retrieval in modern search engines. While large language models (LLMs) have made significant strides in this area, their effectiveness has predominantly been assessed with short, keyword-based queries. As AI-driven search evolves, long-form queries with intricate intents are becoming more prevalent, yet they remain underexplored in the context of LLM-based query understanding (QU). To bridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query understanding through Decomposition and Interpretation. ReDI leverages the reasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i) it breaks down complex queries into targeted sub-queries to accurately capture user intent; (ii) it enriches each sub-query with detailed semantic interpretations to improve the query-document matching; and (iii) it independently retrieves documents for each sub-query and employs a fusion strategy to aggregate the results for the final ranking. We compiled a large-scale dataset of real-world complex queries from a major search engine and distilled the query understanding capabilities of teacher models into smaller models for practical application. Experiments on BRIGHT and BEIR demonstrate that ReDI consistently surpasses strong baselines in both sparse and dense retrieval paradigms, affirming its effectiveness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:58:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06544v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06544v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Navigating the Labyrinth: Evaluating LLMs' Ability to Reason About
  Search Problems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nasim Borazjanizadeh, Roei Herzig, Trevor Darrell, Rogerio Feris, Leonid Karlinsky
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have recently achieved impressive performance in math and reasoning benchmarks. However, they often struggle with logic problems and puzzles that are relatively easy for humans. To further investigate this, we introduce a new benchmark, SearchBench, which contains 11 unique search problems, each equipped with automated pipelines to generate an arbitrary number of instances and analyze the feasibility, correctness, and optimality of LLM-generated solutions. We show that using language-only reasoning, even the most advanced LLMs fail to solve SearchBench end-to-end, e.g., OpenAI's frontier models GPT4 and o1-preview solve only 1.4% and 18.6% of SearchBench problems, respectively. The reason is that SearchBench problems require considering multiple pathways to the solution and performing backtracking, posing a significant challenge to auto-regressive models. Instructing LLMs to generate code that solves the problem helps, but only slightly, e.g., GPT4's performance rises to 11.7%. Interestingly, we show that the current strongest baseline on SearchBench is obtained using in-context learning with A* algorithm implementations. We further show that this baseline can be further enhanced via a Multi-Stage-Multi-Try inference method, raising GPT4's performance above 57%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:55:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.12172v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.12172v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive
  Neural Predicate Modulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaixin Zhang, Hongzhi Wang, Ziqi Li, Yabin Lu, Yingze Li, Yu Yan, Yiming Guan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Research on learned cardinality estimation has made significant progress in recent years. However, existing methods still face distinct challenges that hinder their practical deployment in production environments. We define these challenges as the ``Trilemma of Cardinality Estimation'', where learned cardinality estimation methods struggle to balance generality, accuracy, and updatability. To address these challenges, we introduce DistJoin, a join cardinality estimator based on efficient distribution prediction using multi-autoregressive models. Our contributions are threefold: (1) We propose a method to estimate join cardinality by leveraging the probability distributions of individual tables in a decoupled manner. (2) To meet the requirements of efficiency for DistJoin, we develop Adaptive Neural Predicate Modulation (ANPM), a high-throughput distribution estimation model. (3) We demonstrate that an existing similar approach suffers from variance accumulation issues by formal variance analysis. To mitigate this problem, DistJoin employs a selectivity-based approach to infer join cardinality, effectively reducing variance. In summary, DistJoin not only represents the first data-driven method to support both equi and non-equi joins simultaneously but also demonstrates superior accuracy while enabling fast and flexible updates. The experimental results demonstrate that DistJoin achieves the highest accuracy, robustness to data updates, generality, and comparable update and inference speed relative to existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:55:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.08994v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.08994v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Towards No-Code Programming of Cobots: Experiments with Code Synthesis
  by Large Code Models for Conversational Programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there. ``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs. To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation. As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios. In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language. We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code. With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples. Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:41:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11041v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11041v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 SLiNT: Structure-aware Language Model with Injection and Contrastive
  Training for Knowledge Graph Completion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengxue Yang, Chun Yang, Jiaqi Zhu, Jiafan Li, Jingqi Zhang, Yuyang Li, Ying Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Link prediction in knowledge graphs requires integrating structural information and semantic context to infer missing entities. While large language models offer strong generative reasoning capabilities, their limited exploitation of structural signals often results in structural sparsity and semantic ambiguity, especially under incomplete or zero-shot settings. To address these challenges, we propose SLiNT (Structure-aware Language model with Injection and coNtrastive Training), a modular framework that injects knowledge-graph-derived structural context into a frozen LLM backbone with lightweight LoRA-based adaptation for robust link prediction. Specifically, Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive Learning (DHCL) introduces fine-grained supervision by interpolating hard positives and negatives to resolve entity-level ambiguity; and Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware intervention while preserving the core LLM parameters. Experiments on WN18RR and FB15k-237 show that SLiNT achieves superior or competitive performance compared with both embedding-based and generation-based baselines, demonstrating the effectiveness of structure-aware representation learning for scalable knowledge graph completion.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:36:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06531v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06531v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jian Wu, Hang Yu, Bingchang Liu, Wenjie Yang, Peng Di, Jianguo Li, Yue Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Adapting large language models (LLMs) to specific domains often faces a critical bottleneck: the scarcity of high-quality, human-curated data. While large volumes of unchecked data are readily available, indiscriminately using them for fine-tuning risks introducing noise and degrading performance. Strategic data selection is thus crucial, requiring a method that is both accurate and efficient. Existing approaches, categorized as similarity-based and direct optimization methods, struggle to simultaneously achieve these goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for domain-specific DAta Selection), a novel approach that leverages the pre-trained LLM itself as an implicit classifier, thereby bypassing explicit feature engineering and computationally intensive optimization process. LAMDAS reframes data selection as a one-class classification problem, identifying candidate data that "belongs" to the target domain defined by a small reference dataset. Extensive experimental results demonstrate that LAMDAS not only exceeds the performance of full-data training using a fraction of the data but also outperforms nine state-of-the-art (SOTA) baselines under various scenarios. Furthermore, LAMDAS achieves the most compelling balance between performance gains and computational efficiency compared to all evaluated baselines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:30:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06524v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06524v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Conversational Code Generation: a Case Study of Designing a Dialogue
  System for Generating Driving Scenarios for Testing Autonomous Vehicles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rimvydas Rubavicius, Antonio Valerio Miceli-Barone, Alex Lascarides, Subramanian Ramamoorthy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cyber-physical systems like autonomous vehicles are tested in simulation before deployment, using domain-specific programs for scenario specification. To aid the testing of autonomous vehicles in simulation, we design a natural language interface, using an instruction-following large language model, to assist a non-coding domain expert in synthesising the desired scenarios and vehicle behaviours. We show that using it to convert utterances to the symbolic program is feasible, despite the very small training dataset. Human experiments show that dialogue is critical to successful simulation generation, leading to a 4.5 times higher success rate than a generation without engaging in extended conversation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:30:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.09829v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.09829v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrei Baroian, Kasper Notebomer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based language models traditionally use uniform (isotropic) layer sizes, yet they ignore the diverse functional roles that different depths can play and their computational capacity needs. Building on Layer-Wise Scaling (LWS) and pruning literature, we introduce three new LWS variants - Framed, Reverse, and Crown - that redistribute FFN widths and attention heads via two or three-point linear interpolation in the pre-training stage. We present the first systematic ablation of LWS and its variants, on a fixed budget of 180M parameters, trained on 5B tokens. All models converge to similar losses and achieve better performance compared to an equal-cost isotropic baseline, without a substantial decrease in training throughput. This work represents an initial step into the design space of layer-wise architectures for pre-training, but future work should scale experiments to orders of magnitude more tokens and parameters to fully assess their potential.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:24:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06518v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06518v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated
  Chain-of-Thought-based Reinforced Fine-Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code is available at https://github.com/WNQzhu/CARFT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:20:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15868v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15868v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 When Code Crosses Borders: A Security-Centric Evaluation of LLM-based
  Code Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hailong Chang, Guozhu Meng, Shuhui Xiao, Kai Chen, Kun Sun, Yilin Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the growing demand for cross-language codebase migration, evaluating LLMs' security implications in translation tasks has become critical. Existing evaluations primarily focus on syntactic or functional correctness at the function level, neglecting the critical dimension of security.   To enable security evaluation, we construct STED (Security-centric Translation Evaluation Dataset), the first dataset specifically designed for evaluating the security implications of LLM-based code translation. It comprises 720 security-related code samples across five programming languages and nine high-impact CWE categories, sourced from CVE/NVD and manually verified for translation tasks. Our evaluation framework consists of two independent assessment modules: (1) rigorous evaluation by security researchers, and (2) automated analysis via LLM-as-a-judge. Together they evaluate three critical aspects: functional correctness, vulnerability preservation, and vulnerability introduction rates.   Our large-scale evaluation of five state-of-the-art LLMs across 6,000 translation instances reveals significant security degradation, with 28.6-45% of translations introducing new vulnerabilities--particularly for web-related flaws like input validation, where LLMs show consistent weaknesses. Furthermore, we develop a Retrieval-Augmented Generation (RAG)-based mitigation strategy that reduces translation-induced vulnerabilities by 32.8%, showing the potential of knowledge-enhanced prompting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:08:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06504v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06504v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 An AI system to help scientists write expert-level empirical software</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eser Ayg√ºn, Anastasiya Belyaeva, Gheorghe Comanici, Marc Coram, Hao Cui, Jake Garrison, Renee Johnston Anton Kast, Cory Y. McLean, Peter Norgaard, Zahra Shamsi, David Smalling, James Thompson, Subhashini Venugopalan, Brian P. Williams, Chujun He, Sarah Martinson, Martyna Plomecka, Lai Wei, Yuchen Zhou, Qian-Ze Zhu, Matthew Abraham, Erica Brand, Anna Bulanova, Jeffrey A. Cardille, Chris Co, Scott Ellsworth, Grace Joseph, Malcolm Kane, Ryan Krueger, Johan Kartiwa, Dan Liebling, Jan-Matthis Lueckmann, Paul Raccuglia, Xuefei, Wang, Katherine Chou, James Manyika, Yossi Matias, John C. Platt, Lizzie Dorfman, Shibl Mourad, Michael P. Brenner
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, we present an AI system that creates expert-level scientific software whose goal is to maximize a quality metric. The system uses a Large Language Model (LLM) and Tree Search (TS) to systematically improve the quality metric and intelligently navigate the large space of possible solutions. The system achieves expert-level results when it explores and integrates complex research ideas from external sources. The effectiveness of tree search is demonstrated across a wide range of benchmarks. In bioinformatics, it discovered 40 novel methods for single-cell data analysis that outperformed the top human-developed methods on a public leaderboard. In epidemiology, it generated 14 models that outperformed the CDC ensemble and all other individual models for forecasting COVID-19 hospitalizations. Our method also produced state-of-the-art software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting and numerical solution of integrals. By devising and implementing novel solutions to diverse tasks, the system represents a significant step towards accelerating scientific progress.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:08:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>q-bio.QM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06503v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06503v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Robust and Label-Efficient Deep Waste Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hassan Abid, Khan Muhammad, Muhammad Haris Khan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Effective waste sorting is critical for sustainable recycling, yet AI research in this domain continues to lag behind commercial systems due to limited datasets and reliance on legacy object detectors. In this work, we advance AI-driven waste detection by establishing strong baselines and introducing an ensemble-based semi-supervised learning framework. We first benchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on the real-world ZeroWaste dataset, demonstrating that while class-only prompts perform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy. Next, to address domain-specific limitations, we fine-tune modern transformer-based detectors, achieving a new baseline of 51.6 mAP. We then propose a soft pseudo-labeling strategy that fuses ensemble predictions using spatial and consensus-aware weighting, enabling robust semi-supervised training. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations achieve performance gains that surpass fully supervised training, underscoring the effectiveness of scalable annotation pipelines. Our work contributes to the research community by establishing rigorous baselines, introducing a robust ensemble-based pseudo-labeling pipeline, generating high-quality annotations for the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models under real-world waste sorting conditions. Our code is available at: https://github.com/h-abid97/robust-waste-detection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:07:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18799v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18799v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, Jiayuan Song, Zhengmao Zhu, Wenhu Chen, Pengyu Zhao, Junxian He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T10:07:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06501v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06501v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM
  Step-Provers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, Xia Xiao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T09:54:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06493v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06493v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, Peter Grasch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T09:39:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.13111v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.13111v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 From Offline to Inline Without Pain: A Practical Framework for
  Translating Offline MR Reconstructions to Inline Deployment Using the
  Gadgetron Platform</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zihan Ning, Yannick Brackenier, Sarah McElroy, Sara Neves Silva, Lucilio Cordero-Grande, Sam Rot, Liane S. Canas, Rebecca E Thornley, David Leit√£o, Davide Poccecai, Andrew Cantell, Rene Kerosi, Anthony N Price, Jon Cleary, Donald J Tournier, Jana Hutter, Philippa Bridgen, Pierluigi Di Cio, Michela Cleri, Inka Granlund, Lucy Billimoria, Yasmin Blunck, Shaihan Malik, Marc Modat, Claire J Steves, Joseph V Hajnal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Purpose: To develop and validate a practical framework to overcome common issues in inline deployment of established offline MR reconstruction, including (1) delay from lengthy reconstructions, (2) limited support for multi-scan input reconstructions, (3) the need to adapt scripts for different raw formats, and (4) limited guidance and experience in retaining scanner reconstructions and applying scanner-based post-processing to custom outputs. Methods: The framework builds upon the Gadgetron platform and includes: (1) an input converter to transform ISMRMRD format raw into a Siemens format raw structure, facilitating reuse of existing code; (2) an asynchronous trigger-and-retrieve mechanism enabling long reconstructions without delaying scanner processes; (3) resource-aware scheduling for parallel execution; (4) integrated file management to support multi-scan inputs; and (5) preservation of scanner-based reconstructions and post-processing. The framework was validated on 2 Siemens scanners for SENSE, AlignedSENSE, and NUFFT reconstructions, and in a large-cohort study. Results: Minimum code modification for inline deployment has been shown, and all reconstructions were successfully executed inline without disrupting scanner workflows. Images were retrieved via automated or retro-reconstruction, with scanner-based post-processing applied to custom outputs. Multi-scan input reconstructions were executed using GPU-aware scheduling, confirming feasibility for routine and large-scale applications. In 480 consecutive examinations, inline reconstructions were retrieved in 99% of cases without disruptions. Conclusion: The framework lowers the technical barrier to inline deployment of offline reconstructions, enabling robust, scalable, and post-processing-compatible integration. It is openly available with documentation and demonstration cases to support reproducibility and community adoption.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T09:37:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.med-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06473v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06473v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for
  Dynamic Retrieval and Reranking</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoxiang Jin, Ronghan Li, Zixiang Lu, Qiguang Miao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) often generate inaccurate responses (hallucinations) when faced with questions beyond their knowledge scope. Retrieval-Augmented Generation (RAG) addresses this by leveraging external knowledge, but a critical challenge remains: determining whether retrieved contexts effectively enhance the model`s ability to answer specific queries. This challenge underscores the importance of knowledge boundary awareness, which current methods-relying on discrete labels or limited signals-fail to address adequately, as they overlook the rich information in LLMs` continuous internal hidden states. To tackle this, we propose a novel post-retrieval knowledge filtering approach. First, we construct a confidence detection model based on LLMs` internal hidden states to quantify how retrieved contexts enhance the model`s confidence. Using this model, we build a preference dataset (NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts preferred by the downstream LLM during reranking. Additionally, we introduce Confidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval based on the LLM`s initial confidence in the original question, reducing knowledge conflicts and improving efficiency. Experimental results demonstrate significant improvements in accuracy for context screening and end-to-end RAG performance, along with a notable reduction in retrieval costs while maintaining competitive accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-09T08:54:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06472v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06472v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Interactive Shaping of Granular Media Using Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benedikt Kreis, Malte Mosbach, Anny Ripke, Muhammad Ehsan Ullah, Sven Behnke, Maren Bennewitz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous manipulation of granular media, such as sand, is crucial for applications in construction, excavation, and additive manufacturing. However, shaping granular materials presents unique challenges due to their high-dimensional configuration space and complex dynamics, where traditional rule-based approaches struggle without extensive engineering efforts. Reinforcement learning (RL) offers a promising alternative by enabling agents to learn adaptive manipulation strategies through trial and error. In this work, we present an RL framework that enables a robotic arm with a cubic end-effector and a stereo camera to shape granular media into desired target structures. We show the importance of compact observations and concise reward formulations for the large configuration space, validating our design choices with an ablation study. Our results demonstrate the effectiveness of the proposed approach for the training of visual policies that manipulate granular media including their real-world deployment, significantly outperforming two baseline approaches in terms of target shape accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-09T10:50:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06469v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06469v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Accelerate Scaling of LLM Alignment via Quantifying the Coverage and
  Depth of Instruction Set</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chengwei Wu, Li Du, Hanyu Zhao, Yiming Ju, Jiapu Wang, Tengfei Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the growing demand for applying large language models to downstream tasks, improving model alignment performance and efficiency has become crucial. Such a process involves selecting informative instructions from a candidate pool. However, due to the complexity of instruction set distributions, the key factors driving the performance of aligned models remain unclear. As a result, current instruction set refinement methods fail to improve performance as the instruction pool expands continuously. To address this issue, we first investigate the key factors that influence the relationship between instruction dataset distribution and aligned model performance. Based on these insights, we propose a novel instruction data selection method. We identify that the depth of instructions and the coverage of the semantic space are the crucial factors determining downstream performance, which could explain over 70\% of the model loss on the development set. We then design an instruction selection algorithm to simultaneously maximize the depth and semantic coverage of the selected instructions. Experimental results demonstrate that, compared to state-of-the-art baseline methods, it can sustainably improve model performance at a faster pace and thus achieve \emph{``Accelerated Scaling''}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T09:22:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2509.06463v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2509.06463v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Persona-driven Simulation of Voting Behavior in the European Parliament
  with Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maximilian Kreutner, Marlene Lutz, Markus Strohmaier
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at https://github.com/dess-mannheim/european_parliament_simulation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T09:13:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.11798v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.11798v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Coexistence of continuous-variable quantum key distribution and
  classical data over 120-km fiber</h2>
                <div class="authors">
                    <strong>Authors:</strong> Adnan A. E. Hajomer, Ivan Derkach, Vladyslav C. Usenko, Ulrik L. Andersen, Tobias Gehring
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Integrating quantum key distribution (QKD) with classical data transmission over the same fiber is crucial for scalable quantum-secured communication. However, noise from classical channels limits QKD distance. We demonstrate the longest-distance continuous-variable QKD (CV-QKD) over 120 km (20 dB loss) in the asymptotic regime, and over 100 km (17 dB loss) in the finite-size regime, both coexisting with a fully populated coarse wavelength division multiplexing system. Natural mode filtering of the local oscillator and phase noise mitigation enabled this without additional filtering or wavelength reallocation. Benchmarking against a commercial discrete-variable QKD system and considering finite-size effects confirms the feasibility of CV-QKD as a plug-and-play solution for typical 80-100 km long-haul optical networks. Our results set a record fiber distance for CV-QKD, showing its potential for cost-effective, large-scale deployment in existing network infrastructure.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T09:11:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1103/zy2d-m3ch' target='_blank'>doi</a><a href='http://arxiv.org/abs/2502.17388v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.17388v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via
  Open-ended Interaction Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yichen Wu, Xudong Pan, Geng Hong, Min Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As the general capabilities of large language models (LLMs) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce OpenDeception, a novel deception evaluation framework with an open-ended scenario dataset. OpenDeception jointly evaluates both the deception intention and capabilities of LLM-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where LLMs intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream LLMs on OpenDeception highlights the urgent need to address deception risks and security concerns in LLM-based agents: the deception intention ratio across the models exceeds 80%, while the deception success rate surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-09-08T09:05:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.13707v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.13707v2' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    