
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 Tame fields, Graded Rings and Finite Complete Sequences of Key
  Polynomials</h2>
                <div class="authors">
                    <strong>Authors:</strong> Caio Henrique Silva de Souza
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we present a criterion for $(K,v)$ to be henselian and defectless in terms of finite complete sequences of key polynomials. For this, we use the theory of Mac Lane-Vaqui\'e chains and abstract key polynomials. We then prove that a valued field $(K,v)$ is tame if and only if $vK$ is $p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$ admits a finite complete sequence of key polynomials. The properties $vK$ $p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on the associated graded ring. We also make considerations on simply defectless and algebraically maximal valued fields and purely inertial and purely ramified extensions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:11:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.AC</span><span>13A18</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.01030v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.01030v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Handover_Management_in_UAV_Networks_with_Blockages</h2>
                <div class="authors">
                    <strong>Authors:</strong> Neetu R R, Gourab Ghatak, Vivek Ashok Bohara
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate the performance of unmanned aerial vehicle (UAV)-based networks in urban environments characterized by blockages, focusing on their capability to support the service demands of mobile users. The UAV-base stations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson point process (MPPP), where the marks represent the altitude of each UAV-BS. Leveraging stochastic geometry, we analyze the impact of blockages on network reliability by studying the meta distribution (MD) of the signal-to-interference noise ratio (SINR) for a specific reliability threshold and the association probabilities for both line-of-sight (LoS) and non line-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile users, we propose a novel cache-based handover management strategy that dynamically selects the cell search time and delays the received signal strength (RSS)-based base station (BS) associations. This strategy aims to minimize unnecessary handovers (HOs) experienced by users by leveraging caching capabilities at user equipment (UE), thus reducing latency, ensuring seamless connectivity, and maintaining the quality of service (QoS). This study provides valuable insights into optimizing UAV network deployments to support the stringent requirements in the network, ensuring reliable, low-latency, and high-throughput communication for next-generation smart cities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T15:14:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.20433v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.20433v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State
  Drives</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shaobo Li, Yirui Eric Zhou, Hao Ren, Jian Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unlike non-volatile memory that resides on the processor memory bus, memory-semantic solid-state drives (SSDs) support both byte and block access granularity via PCIe or CXL interconnects. They provide scalable memory capacity using NAND flash at a much lower cost. In addition, they have different performance characteristics for their dual byte/block interface respectively, while offering essential memory semantics for upper-level software. Such a byte-accessible storage device provides new implications on the software system design.   In this paper, we develop a new file system, named ByteFS, by rethinking the design primitives of file systems and SSD firmware to exploit the advantages of both byte and block-granular data accesses. ByteFS supports byte-granular data persistence to retain the persistence nature of SSDs. It extends the core data structure of file systems by enabling dual byte/block-granular data accesses. To facilitate the support for byte-granular writes, \pname{} manages the internal DRAM of SSD firmware in a log-structured manner and enables data coalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also enables coordinated data caching between the host page cache and SSD cache for best utilizing the precious memory resource. We implement ByteFS on both a real programmable SSD and an emulated memory-semantic SSD for sensitivity study. Compared to state-of-the-art file systems for non-volatile memory and conventional SSDs, ByteFS outperforms them by up to 2.7$\times$, while preserving the essential properties of a file system. ByteFS also reduces the write traffic to SSDs by up to 5.1$\times$ by alleviating unnecessary writes caused by both metadata and data updates in file systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T06:18:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04993v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04993v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 TreeKV: Smooth Key-Value Cache Compression with Tree Structures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. It consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T06:00:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04987v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04987v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Optimal Oblivious Algorithms for Multi-way Joins</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiao Hu, Zhiang Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In cloud databases, cloud computation over sensitive data uploaded by clients inevitably causes concern about data security and privacy. Even when encryption primitives and trusted computing environments are integrated into query processing to safeguard the actual contents of the data, access patterns of algorithms can still leak private information about the data. Oblivious Random Access Memory (ORAM) and circuits are two generic approaches to address this issue, ensuring that access patterns of algorithms remain oblivious to the data. However, deploying these methods on insecure algorithms, particularly for multi-way join processing, is computationally expensive and inherently challenging.   In this paper, we propose a novel sorting-based algorithm for multi-way join processing that operates without relying on ORAM simulations or other security assumptions. Our algorithm is a non-trivial, provably oblivious composition of basic primitives, with time complexity matching the insecure worst-case optimal join algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic, with cache complexity matching the insecure lower bound, also up to a logarithmic factor. This clean and straightforward approach has the potential to be extended to other security settings and implemented in practical database systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T03:02:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04216v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04216v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Modern Hardware Security: A Review of Attacks and Countermeasures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jyotiprakash Mishra, Sanjay K. Sahay
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the exponential rise in the use of cloud services, smart devices, and IoT devices, advanced cyber attacks have become increasingly sophisticated and ubiquitous. Furthermore, the rapid evolution of computing architectures and memory technologies has created an urgent need to understand and address hardware security vulnerabilities. In this paper, we review the current state of vulnerabilities and mitigation strategies in contemporary computing systems. We discuss cache side-channel attacks (including Spectre and Meltdown), power side-channel attacks (such as Simple Power Analysis, Differential Power Analysis, Correlation Power Analysis, and Template Attacks), and advanced techniques like Voltage Glitching and Electromagnetic Analysis to help understand and build robust cybersecurity defense systems and guide further research. We also examine memory encryption, focusing on confidentiality, granularity, key management, masking, and re-keying strategies. Additionally, we cover Cryptographic Instruction Set Architectures, Secure Boot, Root of Trust mechanisms, Physical Unclonable Functions, and hardware fault injection techniques. The paper concludes with an analysis of the RISC-V architecture's unique security challenges. The comprehensive analysis presented in this paper is essential for building resilient hardware security solutions that can protect against both current and emerging threats in an increasingly challenging security landscape.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-08T10:14:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04394v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04394v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear
  Approximation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Samrat Mukhopadhyay, Debasmita Mukherjee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider the problem of \textit{online sparse linear approximation}, where one predicts the best sparse approximation of a sequence of measurements in terms of linear combination of columns of a given measurement matrix. Such online prediction problems are ubiquitous, ranging from medical trials to web caching to resource allocation. The inherent difficulty of offline recovery also makes the online problem challenging. In this letter, we propose Follow-The-Approximate-Sparse-Leader, an efficient online meta-policy to address this online problem. Through a detailed theoretical analysis, we prove that under certain assumptions on the measurement sequence, the proposed policy enjoys a data-dependent sublinear upper bound on the static regret, which can range from logarithmic to square-root. Numerical simulations are performed to corroborate the theoretical findings and demonstrate the efficacy of the proposed online policy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-07T17:32:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>math.OC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00799v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00799v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Not all tokens are created equal: Perplexity Attention Weighted Networks
  for AI generated text detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-07T17:00:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.03940v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.03940v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Parallel $k$d-tree with Batch Updates</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyang Men, Zheqi Shen, Yan Gu, Yihan Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The $k$d-tree is one of the most widely used data structures to manage multi-dimensional data. Due to the ever-growing data volume, it is imperative to consider parallelism in $k$d-trees. However, we observed challenges in existing parallel kd-tree implementations, for both constructions and updates.   The goal of this paper is to develop efficient in-memory $k$d-trees by supporting high parallelism and cache-efficiency. We propose the Pkd-tree (Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and in practice. The Pkd-tree supports parallel tree construction, batch update (insertion and deletion), and various queries including k-nearest neighbor search, range query, and range count. We proved that our algorithms have strong theoretical bounds in work (sequential time complexity), span (parallelism), and cache complexity. Our key techniques include 1) an efficient construction algorithm that optimizes work, span, and cache complexity simultaneously, and 2) reconstruction-based update algorithms that guarantee the tree to be weight-balanced. With the new algorithmic insights and careful engineering effort, we achieved a highly optimized implementation of the Pkd-tree.   We tested Pkd-tree with various synthetic and real-world datasets, including both uniform and highly skewed data. We compare the Pkd-tree with state-of-the-art parallel $k$d-tree implementations. In all tests, with better or competitive query performance, Pkd-tree is much faster in construction and updates consistently than all baselines. We released our code.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-06T23:16:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DS</span><span>cs.DB</span><span>cs.DC</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.09275v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.09275v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 The Power of Negative Zero: Datatype Customization for Quantized Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuzong Chen, Xilai Dai, Chi-chih Chang, Yash Akhauri, Mohamed S. Abdelfattah
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable performance across various machine learning tasks, quickly becoming one of the most prevalent AI workloads. Yet the substantial memory requirement of LLMs significantly hinders their deployment for end users. Post-training quantization (PTQ) serves as one of the most hardware-efficient methods to mitigate the memory and computational demands of LLMs. Although the traditional integer (INT) datatype has received widespread adoption in PTQ methods, floating-point (FP) quantization has emerged as a viable alternative thanks to its effectiveness in fitting LLM numerical distributions. However, the FP datatype in sign-magnitude binary representation contains both positive and negative zero, which constrains its representation capability, particularly under low precision (3 and 4 bits). In this paper, we extend the basic FP datatype to perform Redundant Zero Remapping (RaZeR), which remaps the negative zero FP encoding to a set of pre-defined special values to maximally utilize FP quantization encodings and to better fit LLM numerical distributions. Through careful selection of special values, RaZeR outperforms conventional asymmetric INT quantization while achieving high computational efficiency. We demonstrate that RaZeR can be seamlessly integrated with quantization algorithms for both weights and KV-cache, including advanced methods with clipping and transformations, and consistently achieve better model accuracy. Additionally, we implement a fast GEMV kernel with fused dequantization that efficiently converts the 4-bit RaZeR value to FP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows that RaZeR improves the GEMV speed by up to 7.56$\times$ compared to the FP16 implementation, while achieving up to 2.72$\times$ speedup in the LLM decoding throughput.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-06T22:40:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04052v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04052v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Twinkle: A GPU-based binary-lens microlensing code with contour
  integration method</h2>
                <div class="authors">
                    <strong>Authors:</strong> Suwei Wang, Lile Wang, Subo Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapidly increasing rate of microlensing planet detections, microlensing modeling software faces significant challenges in computation efficiency. Here, we develop the Twinkle code, an efficient and robust binary-lens modeling software suite optimized for heterogeneous computing devices, especially GPUs. Existing microlensing codes have the issue of catastrophic cancellation that undermines the numerical stability and precision, and Twinkle resolves them by refining the coefficients of the binary-lens equation. We also devise an improved method for robustly identifying ghost images, thereby enhancing computational reliability. We have advanced the state of the art by optimizing Twinkle specifically for heterogeneous computing devices by taking into account the unique task and cache memory dispatching patterns of GPUs, while the compatibility with the traditional computing architectures of CPUs is still maintained. Twinkle has demonstrated an acceleration of approximately 2 orders of magnitude (>~100 times) on contemporary GPUs. The enhancement in computational speed of Twinkle will translate to the delivery of accurate and highly efficient data analysis for ongoing and upcoming microlensing projects. Both GPU and CPU versions of Twinkle are open-source and publicly available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-06T19:00:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.IM</span><span>astro-ph.EP</span><span>astro-ph.GA</span><span>astro-ph.SR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.03322v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.03322v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Direct Comparison of Magnetic Penetration Depth in Kagome
  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Austin Kaczmarek, Andrea Capa Salinas, Stephen D. Wilson, Katja C. Nowack
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We report measurements of the local temperature-dependent penetration depth, $\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using scanning superconducting quantum interference device (SQUID) microscopy. Our results suggest that the superconducting order in all three compounds is fully gapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density, $\rho_s(T)$, shows deviations from the behavior expected for a single isotropic gap, but the data are well described by models incorporating either a single anisotropic gap or two isotropic gaps. Notably, the temperature dependences of $\lambda(T)$ and $\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are qualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with the superconducting phase reflecting features of the normal-state band structure. Our findings provide a direct comparison of the superconducting properties across the AV$_3$Sb$_5$ family.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-06T15:59:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.supr-con</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.19919v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.19919v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li, Sven Koenig
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-Agent Path Finding (MAPF), which focuses on finding collision-free paths for multiple robots, is crucial in autonomous warehouse operations. Lifelong MAPF (L-MAPF), where agents are continuously reassigned new targets upon completing their current tasks, offers a more realistic approximation of real-world warehouse scenarios. While cache storage systems can enhance efficiency and reduce operational costs, existing approaches primarily rely on expectations and mathematical models, often without adequately addressing the challenges of multi-robot planning and execution. In this paper, we introduce a novel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which integrates high-level cache storage with low-level path planning. We have involved a new type of map grid called cache for temporary item storage. Additionally, we involved a task assigner (TA) with a locking mechanism to bridge the gap between the new cache grid and L-MAPF algorithm. The TA dynamically allocates target locations to agents based on their status in various scenarios. We evaluated L-MAPF-CM using different cache replacement policies and task distributions. L-MAPF-CM has demonstrated performance improvements particularly with high cache hit rates and smooth traffic conditions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-06T06:44:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02803v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02803v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 From Slow Bidirectional to Fast Autoregressive Video Diffusion Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-06T01:26:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.07772v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.07772v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video
  Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video Large Language Models (VideoLLMs) have achieved remarkable progress in video understanding. However, existing VideoLLMs often inherit the limitations of their backbone LLMs in handling long sequences, leading to challenges for long video understanding. Common solutions either simply uniformly sample videos' frames or compress visual tokens, which focus primarily on low-level temporal visual redundancy, overlooking high-level knowledge redundancy. This limits the achievable compression rate with minimal loss. To this end. we introduce a training-free method, $\textbf{ReTaKe}$, containing two novel modules DPSelect and PivotKV, to jointly model and reduce both temporal visual redundancy and knowledge redundancy for long video understanding. Specifically, DPSelect identifies keyframes with local maximum peak distance based on their visual features, which are closely aligned with human video perception. PivotKV employs the obtained keyframes as pivots and conducts KV-Cache compression for the non-pivot tokens with low attention scores, which are derived from the learned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and LVBench, show that ReTaKe can support 4x longer video sequences with minimal performance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%, even surpassing or on par with much larger ones. Our code is available at https://github.com/SCZwangxiao/video-ReTaKe
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-05T14:11:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.20504v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.20504v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 A Full-System Simulation Framework for CXL-Based SSD Memory System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yaohui Wang, Zicong Wang, Fanfeng Meng, Yanjing Wang, Yang Ou, Lizhou Wu, Wentao Hong, Xuran Ge, Jijun Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Compute eXpress Link (CXL) is a promising technology for memory disaggregation and expansion. Especially, CXL makes it more effectively for large-capacity storage devices such as Solid State Drive (SSD) to be deployed in the memory pool. However, CXL-based SSDs are still in early stages, necessitating the development of reliable simulation tools. In this paper, we propose CXL-SSD-Sim, the first open-source full-system simulator designed to simulate CXL-based SSD memory system. Constructed on the foundation of gem5 and SimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along with the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM layer as a caching mechanism for the SSD, meticulously engineered to counteract latency issues inherent to CXL-based SSD memory access. Experiments are performed among five different memory devices with CXL-SSD-Sim in aspect of latency, bandwidth and real-world benchmark performance. These experiments serve to underscore the efficacy of our simulation tool in providing a comprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim simulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-05T12:51:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02524v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02524v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas
  and Ad-Hoc Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Atonu Ghosh, Sudip Misra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The minimal infrastructure requirements of LoRa make it suitable for deployments in remote and disaster-stricken areas. Concomitantly, the modern era is witnessing the proliferation of web applications in all aspects of human life, including IoT and other network services. Contemporary IoT and network solutions heavily rely on web applications to render services. However, despite the recent research and development pivoted around LoRa, there is still a lack of studies focusing on web application access over LoRa networks. Specifically, technical challenges like payload size limitation, low data rate, and contentions in multi-user setups limit the applicability of LoRa for web applications. Hence, we propose LoRaWeb, which enables web access over LoRa networks. The LoRaWeb hardware tethers a WiFi hotspot to which the client devices connect and access the web pages using a web browser. LoRa backbone of the network handles the web page transmission from the requester and receiver devices. LoRaWeb implements a synchronization procedure to address the aforementioned challenges for effective message exchange between requesters and responders. The system implements a caching mechanism to reduce latency and contention. Additionally, it implements a message-slicing mechanism in the application layer to overcome the hardware limitations on the message length. The actual hardware-based implementation results indicate seamless deployment, and the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and ~$6 S$ for a $10 KB$ size web page.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-05T07:41:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.CY</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02469v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02469v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Reciprocating Locks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dave Dice, Alex Kogan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-04T20:59:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>D.4.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02380v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02380v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 End-to-End Long Document Summarization using Gradient Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rohit Saxena, Hao Tang, Frank Keller
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Training transformer-based encoder-decoder models for long document summarization poses a significant challenge due to the quadratic memory consumption during training. Several approaches have been proposed to extend the input length at test time, but training with these approaches is still difficult, requiring truncation of input documents and causing a mismatch between training and test conditions. In this work, we propose CachED (Gradient $\textbf{Cach}$ing for $\textbf{E}$ncoder-$\textbf{D}$ecoder models), an approach that enables end-to-end training of existing transformer-based encoder-decoder models, using the entire document without truncation. Specifically, we apply non-overlapping sliding windows to input documents, followed by fusion in decoder. During backpropagation, the gradients are cached at the decoder and are passed through the encoder in chunks by re-computing the hidden vectors, similar to gradient checkpointing. In the experiments on long document summarization, we extend BART to CachED BART, processing more than 500K tokens during training and achieving superior performance without using any additional parameters.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-03T13:32:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.01805v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.01805v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Efficient LLM Inference with Activation Checkpointing and Hybrid Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sanghyeon Lee, Hongbeen Kim, Soojin Hwang, Guseul Heo, Minwoo Noh, Jaehyuk Huh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent large language models (LLMs) with enormous model sizes use many GPUs to meet memory capacity requirements incurring substantial costs for token generation. To provide cost-effective LLM inference with relaxed latency constraints, extensive research has focused on expanding GPU memory by leveraging the host memory. However, LLM inference engines that utilize the host memory often face underutilization of GPU compute units, as a considerable portion of inference time is spent in loading the model onto the GPU via host-GPU interconnect. To tackle these challenges of the host memory offloading for LLM, we introduce HybridServe, an LLM inference system with activation checkpointing based on activation caching. The activation cache stores activation checkpoints generated during intermediate inference stages, allowing the fast recomputation of KV cache while model parameters are transferred to GPU from host memory. Unlike conventional methods that recompute the KV cache from scratch using token IDs, the activation cache allows bypassing projection and FFN operations. To balance between the activation recomputation and parameter loading overhead, this study proposes a KV-activation hybrid caching scheme which finds the best ratio of the key-value and activation caches to adjust the recomputation time. Our system achieves 2.19x throughput improvement over the state-of-the-art prior work for offloading both model weights and KV cache.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-03T12:51:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.01792v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.01792v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Object-level Visual Prompts for Compositional Image Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel Cohen-Or, Kfir Aberman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce a method for composing object-level visual prompts within a text-to-image diffusion model. Our approach addresses the task of generating semantically coherent compositions across diverse scenes and styles, similar to the versatility and expressiveness offered by text prompts. A key challenge in this task is to preserve the identity of the objects depicted in the input visual prompts, while also generating diverse compositions across different images. To address this challenge, we introduce a new KV-mixed cross-attention mechanism, in which keys and values are learned from distinct visual representations. The keys are derived from an encoder with a small bottleneck for layout control, whereas the values come from a larger bottleneck encoder that captures fine-grained appearance details. By mixing keys and values from these complementary sources, our model preserves the identity of the visual prompts while supporting flexible variations in object arrangement, pose, and composition. During inference, we further propose object-level compositional guidance to improve the method's identity preservation and layout correctness. Results show that our technique produces diverse scene compositions that preserve the unique characteristics of each visual prompt, expanding the creative potential of text-to-image generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-02T18:59:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.GR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.01424v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.01424v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 MSWA: Refining Local Attention with Multi-ScaleWindow Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yixing Xu, Shivank Nag, Dong Li, Lu Tian, Emad Barsoum
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based LLMs have achieved exceptional performance across a wide range of NLP tasks. However, the standard self-attention mechanism suffers from quadratic time complexity and linearly increased cache size. Sliding window attention (SWA) solves this problem by restricting the attention range to a fixed-size local context window. Nevertheless, SWA employs a uniform window size for each head in each layer, making it inefficient in capturing context of varying scales. To mitigate this limitation, we propose Multi-Scale Window Attention (MSWA) which applies diverse window sizes across heads and layers in the Transformer. It not only allows for different window sizes among heads within the same layer but also progressively increases window size allocation from shallow to deep layers, thus enabling the model to capture contextual information with different lengths and distances. Experimental results on language modeling and common-sense reasoning tasks substantiate that MSWA outperforms traditional local attention in both effectiveness and efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-02T03:41:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.01039v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.01039v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 A Survey on Large Language Model Acceleration based on KV Cache
  Management</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-02T03:40:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.19442v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.19442v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 FlashInfer: Efficient and Customizable Attention Engine for LLM
  Inference Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-02T02:02:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.01005v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.01005v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant
  Computation Elimination in Diffusion Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Omid Saghatchian, Atiyeh Gh. Moghadam, Ahmad Nickabadi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have emerged as a promising approach for generating high-quality, high-dimensional images. Nevertheless, these models are hindered by their high computational cost and slow inference, partly due to the quadratic computational complexity of the self-attention mechanisms with respect to input size. Various approaches have been proposed to address this drawback. One such approach focuses on reducing the number of tokens fed into the self-attention, known as token merging (ToMe). In our method, which is called cached adaptive token merging(CA-ToMe), we calculate the similarity between tokens and then merge the r proportion of the most similar tokens. However, due to the repetitive patterns observed in adjacent steps and the variation in the frequency of similarities, we aim to enhance this approach by implementing an adaptive threshold for merging tokens and adding a caching mechanism that stores similar pairs across several adjacent steps. Empirical results demonstrate that our method operates as a training-free acceleration method, achieving a speedup factor of 1.24 in the denoising process while maintaining the same FID scores compared to existing approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-01T20:16:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00946v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00946v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 EdgeRAG: Online-Indexed RAG for Edge Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Korakit Seemakhupt, Sihang Liu, Samira Khan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge devices is challenging due to limited memory and processing power. In this work, we propose EdgeRAG which addresses the memory constraint by pruning embeddings within clusters and generating embeddings on-demand during retrieval. To avoid the latency of generating embeddings for large tail clusters, EdgeRAG pre-computes and stores embeddings for these clusters, while adaptively caching remaining embeddings to minimize redundant computations and further optimize latency. The result from BEIR suite shows that EdgeRAG offers significant latency reduction over the baseline IVF index, but with similar generation quality while allowing all of our evaluated datasets to fit into the memory.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-31T20:40:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.21023v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.21023v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Token Pruning for Caching Better: 9 Times Acceleration on Stable
  Diffusion for Free</h2>
                <div class="authors">
                    <strong>Authors:</strong> Evelyn Zhang, Bang Xiao, Jiayi Tang, Qianli Ma, Chang Zou, Xuefei Ning, Xuming Hu, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Stable Diffusion has achieved remarkable success in the field of text-to-image generation, with its powerful generative capabilities and diverse generation results making a lasting impact. However, its iterative denoising introduces high computational costs and slows generation speed, limiting broader adoption. The community has made numerous efforts to reduce this computational burden, with methods like feature caching attracting attention due to their effectiveness and simplicity. Nonetheless, simply reusing features computed at previous timesteps causes the features across adjacent timesteps to become similar, reducing the dynamics of features over time and ultimately compromising the quality of generated images. In this paper, we introduce a dynamics-aware token pruning (DaTo) approach that addresses the limitations of feature caching. DaTo selectively prunes tokens with lower dynamics, allowing only high-dynamic tokens to participate in self-attention layers, thereby extending feature dynamics across timesteps. DaTo combines feature caching with token pruning in a training-free manner, achieving both temporal and token-wise information reuse. Applied to Stable Diffusion on the ImageNet, our approach delivered a 9$\times$ speedup while reducing FID by 0.33, indicating enhanced image quality. On the COCO-30k, we observed a 7$\times$ acceleration coupled with a notable FID reduction of 2.17.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-31T09:56:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00375v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00375v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 RetrievalAttention: Accelerating Long-Context LLM Inference via Vector
  Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-31T07:11:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10516v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10516v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Performant Automatic BLAS Offloading on Unified Memory Architecture with
  OpenMP First-Touch Style Data Movement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junjie Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-31T05:24:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.MS</span><span>cs.PF</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00279v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00279v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained
  Image Recognition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Edwin Arkel Rios, Jansen Christopher Yuanda, Vincent Leon Ghanz, Cheng-Wei Yu, Bo-Cheng Lai, Min-Chun Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Ultra-fine-grained image recognition (UFGIR) is a challenging task that involves classifying images within a macro-category. While traditional FGIR deals with classifying different species, UFGIR goes beyond by classifying sub-categories within a species such as cultivars of a plant. In recent times the usage of Vision Transformer-based backbones has allowed methods to obtain outstanding recognition performances in this task but this comes at a significant cost in terms of computation specially since this task significantly benefits from incorporating higher resolution images. Therefore, techniques such as token reduction have emerged to reduce the computational cost. However, dropping tokens leads to loss of essential information for fine-grained categories, specially as the token keep rate is reduced. Therefore, to counteract the loss of information brought by the usage of token reduction we propose a novel Cross-Layer Aggregation Classification Head and a Cross-Layer Cache mechanism to recover and access information from previous layers in later locations. Extensive experiments covering more than 2000 runs across diverse settings including 5 datasets, 9 backbones, 7 token reduction methods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the proposed plug-and-play modules and allow us to push the boundaries of accuracy vs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to 10\% while maintaining a competitive accuracy to state-of-the-art models. Code is available at: \url{https://github.com/arkel23/CLCA}
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-31T03:19:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>I.2; I.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00243v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00243v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 MapQaTor: A System for Efficient Annotation of Map Query Datasets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mapping and navigation services like Google Maps, Apple Maps, Openstreet Maps, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, a web application that streamlines the creation of reproducible, traceable map-based QA datasets. With its plug-and-play architecture, MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/7_aV9Wmhs6Q.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-30T15:33:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.21015v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.21015v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 SepLLM: Accelerate Large Language Models by Compressing One Segment into
  One Separator</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-30T14:54:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12094v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12094v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Li, Hanbyul Kim, Xinbo Wang, Jianlin Luo, Simone Latini, Dongbin Shin, Jun-Ming Liu, Jing-Feng Li, Angel Rubio, Ce-Wen Nan, Qian Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coherent manipulation of lattice vibrations using ultrafast light pulses enables access to nonequilibrium 'hidden' phases with designed functionalities in quantum materials. However, expanding the understanding of nonlinear light-phonon interaction mechanisms remains crucial for developing new strategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3 driven by intense terahertz excitation. As the terahertz field increases, the system transitions from the quantum paraelectric (QPE) ground state to an intermediate ferroelectric phase, and then unexpectedly reverts to a QPE state above ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice dynamics compared to the initial phases, highlighting activated antiferrodistortive phonon modes. Aided by first-principles dynamical calculations, we identify the mechanism for these complex behaviors as a superposition of multiple coherently excited eigenstates of the polar soft mode. Our results reveal a previously uncharted quantum facet of SrTiO3 and open pathways for harnessing high-order excitations to engineer quantum materials in the ultrafast regime.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-30T11:54:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span><span>physics.optics</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.20887v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.20887v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have seen widespread adoption due to their remarkable performance across various applications, driving the accelerated development of a large number of diverse LLMs. However, these individual LLMs show limitations in generalization and performance on complex tasks due to inherent training biases, model size constraints, and the quality or diversity of pre-training datasets. A promising direction is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. To address these limitations, we introduce a novel LLM selection algorithm called SelectLLM, which efficiently directs input queries to the most suitable subset of LLMs from a large pool, ensuring that the selected models collectively provide accurate responses. SelectLLM employs a multi-label classifier and policy based on the classifier's predictions and confidence scores in selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings indicate that the proposed model outperforms existing ensemble-based baselines and achieves competitive performance with similarly sized top-performing LLMs while maintaining efficiency. Specifically, it achieves a huge reduction in inference latency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU, compared to the top-performing baselines. Also, we establish a theoretical upper bound by an oracle with LLMs and explore in-depth linguistic analysis to understand the performance gap between Oracle and SelectLLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-30T05:01:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.08545v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.08545v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Align Attention Heads Before Merging Them: An Effective Way for
  Converting MHA to GQA</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qingyun Jin, Xiaohui Song, Feng Zhou, Zengchang Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models have been shown to perform well on a variety of natural language processing problems. However, as the model size and the input sequence's length increase, the rapid increase of KV Cache significantly slows down inference speed. Therefore GQA model, as an alternative to MHA model, has been widely introduced into LLMs. In this work, we propose a low-cost method for pruning MHA models into GQA models with any compression ratio of key-value heads. Our method is based on $\mathit{L_0}$ masks to gradually remove redundant parameters. In addition, we apply orthogonal transformations to attention heads without changing the model to increase similarity between attention heads before pruning training, in order to further improve performance of the model. Our method can be compatible with rotary position embedding (RoPE), which means the model after training can be fully adapted to the mainstream standard GQA framework. Experiments demonstrate that our strategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model without too much performance degradation, just achieved through supervised fine-tuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-30T03:05:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.20677v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.20677v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Dynamic Optimization of Storage Systems Using Reinforcement Learning
  Techniques</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The exponential growth of data-intensive applications has placed unprecedented demands on modern storage systems, necessitating dynamic and efficient optimization strategies. Traditional heuristics employed for storage performance optimization often fail to adapt to the variability and complexity of contemporary workloads, leading to significant performance bottlenecks and resource inefficiencies. To address these challenges, this paper introduces RL-Storage, a novel reinforcement learning (RL)-based framework designed to dynamically optimize storage system configurations. RL-Storage leverages deep Q-learning algorithms to continuously learn from real-time I/O patterns and predict optimal storage parameters, such as cache size, queue depths, and readahead settings[1]. The proposed framework operates within the storage kernel, ensuring minimal latency and low computational overhead. Through an adaptive feedback mechanism, RL-Storage dynamically adjusts critical parameters, achieving efficient resource utilization across a wide range of workloads. Experimental evaluations conducted on a range of benchmarks, including RocksDB and PostgreSQL, demonstrate significant improvements, with throughput gains of up to 2.6x and latency reductions of 43% compared to baseline heuristics. Additionally, RL-Storage achieves these performance enhancements with a negligible CPU overhead of 0.11% and a memory footprint of only 5 KB, making it suitable for seamless deployment in production environments. This work underscores the transformative potential of reinforcement learning techniques in addressing the dynamic nature of modern storage systems. By autonomously adapting to workload variations in real time, RL-Storage provides a robust and scalable solution for optimizing storage performance, paving the way for next-generation intelligent storage infrastructures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-29T17:41:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00068v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00068v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Ns3 meets Sionna: Using Realistic Channels in Network Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anatolij Zubow, Yannik Pilz, Sascha Rösler, Falko Dressler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Network simulators are indispensable tools for the advancement of wireless network technologies, offering a cost-effective and controlled environment to simulate real-world network behavior. However, traditional simulators, such as the widely used ns-3, exhibit limitations in accurately modeling indoor and outdoor scenarios due to their reliance on simplified statistical and stochastic channel propagation models, which often fail to accurately capture physical phenomena like multipath signal propagation and shadowing by obstacles in the line-of-sight path. We present Ns3Sionna, which integrates a ray tracing-based channel model, implemented using the Sionna RT framework, within the ns-3 network simulator. It allows to simulate environment-specific and physically accurate channel realizations for a given 3D scene and wireless device positions. Additionally, a mobility model based on ray tracing was developed to accurately represent device movements within the simulated 3D space. Ns3Sionna provides more realistic path and delay loss estimates for both indoor and outdoor environments than existing ns-3 propagation models, particularly in terms of spatial and temporal correlation. Moreover, fine-grained channel state information is provided, which could be used for the development of sensing applications. Due to the significant computational demands of ray tracing, Ns3Sionna takes advantage of the parallel execution capabilities of modern GPUs and multi-core CPUs by incorporating intelligent pre-caching mechanisms that leverage the channel's coherence time to optimize runtime performance. This enables the efficient simulation of scenarios with a small to medium number of mobile nodes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-29T17:18:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.20524v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.20524v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Revisiting Cache Freshness for Emerging Real-Time Applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziming Mao, Rishabh Iyer, Scott Shenker, Ion Stoica
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Caching is widely used in industry to improve application performance by reducing data-access latency and taking the load off the backend infrastructure. TTLs have become the de-facto mechanism used to keep cached data reasonably fresh (i.e., not too out of date with the backend). However, the emergence of real-time applications requires tighter data freshness, which is impractical to achieve with TTLs. We discuss why this is the case, and propose a simple yet effective adaptive policy to achieve the desired freshness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-28T17:17:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.DC</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3696348.3696858' target='_blank'>doi</a><a href='http://arxiv.org/abs/2412.20221v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.20221v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, Jungwook Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The expansion of large language models (LLMs) with hundreds of billions of parameters presents significant challenges to computational resources, particularly data movement and memory bandwidth. Long-context LLMs, which process sequences of tens of thousands of tokens, further increase the demand on the memory system as the complexity in attention layers and key-value cache sizes is proportional to the context length. Processing-in-Memory (PIM) maximizes memory bandwidth by moving compute to the data and can address the memory bandwidth challenges; however, PIM is not necessarily scalable to accelerate long-context LLM because of limited per-module memory capacity and the inflexibility of fixed-functional unit PIM architecture and static memory management. In this work, we propose LoL-PIM which is a multi-node PIM architecture that accelerates long context LLM through hardware-software co-design. In particular, we propose how pipeline parallelism can be exploited across a multi-PIM module while a direct PIM access (DPA) controller (or DMA for PIM) is proposed that enables dynamic PIM memory management and results in efficient PIM utilization across a diverse range of context length. We developed an MLIR-based compiler for LoL-PIM extending a commercial PIM-based compiler where the software modifications were implemented and evaluated, while the hardware changes were modeled in the simulator. Our evaluations demonstrate that LoL-PIM significantly improves throughput and reduces latency for long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems (up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient deployment of LLMs in real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-28T14:38:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.20166v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.20166v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal
  Visual Token Trimming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiedong Zhuang, Lu Lu, Ming Dai, Rui Hu, Jian Chen, Qiang Liu, Haoji Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal large language models (MLLMs) enhance their perceptual capabilities by integrating visual and textual information. However, processing the massive number of visual tokens incurs a significant computational cost. Existing analysis of the MLLM attention mechanisms remains shallow, leading to coarse-grain token pruning strategies that fail to effectively balance speed and accuracy. In this paper, we conduct a comprehensive investigation of MLLM attention mechanisms with LLaVA. We find that numerous visual tokens and partial attention computations are redundant during the decoding process. Based on this insight, we propose Spatial-Temporal Visual Token Trimming ($\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without retraining. $\textbf{ST}^{3}$ consists of two primary components: 1) Progressive Visual Token Pruning (\textbf{PVTP}), which eliminates inattentive visual tokens across layers, and 2) Visual Token Annealing (\textbf{VTA}), which dynamically reduces the number of visual tokens in each layer as the generated tokens grow. Together, these techniques deliver around $\mathbf{2\times}$ faster inference with only about $\mathbf{30\%}$ KV cache memory compared to the original LLaVA, while maintaining consistent performance across various datasets. Crucially, $\textbf{ST}^{3}$ can be seamlessly integrated into existing pre-trained MLLMs, providing a plug-and-play solution for efficient inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-28T10:17:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.20105v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.20105v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 A Robust Federated Learning Framework for Undependable Devices at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shilong Wang, Jianchun Liu, Hongli Xu, Chunming Qiao, Huarong Deng, Qiuye Zheng, Jiantao Gong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In a federated learning (FL) system, many devices, such as smartphones, are often undependable (e.g., frequently disconnected from WiFi) during training. Existing FL frameworks always assume a dependable environment and exclude undependable devices from training, leading to poor model performance and resource wastage. In this paper, we propose FLUDE to effectively deal with undependable environments. First, FLUDE assesses the dependability of devices based on the probability distribution of their historical behaviors (e.g., the likelihood of successfully completing training). Based on this assessment, FLUDE adaptively selects devices with high dependability for training. To mitigate resource wastage during the training phase, FLUDE maintains a model cache on each device, aiming to preserve the latest training state for later use in case local training on an undependable device is interrupted. Moreover, FLUDE proposes a staleness-aware strategy to judiciously distribute the global model to a subset of devices, thus significantly reducing resource wastage while maintaining model performance. We have implemented FLUDE on two physical platforms with 120 smartphones and NVIDIA Jetson devices. Extensive experimental results demonstrate that FLUDE can effectively improve model performance and resource efficiency of FL training in undependable environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-28T03:28:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.19991v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.19991v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Multi-matrix Factorization Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-26T15:45:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.19255v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.19255v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Performance Characterization and Optimizations of Traditional ML
  Applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Harsh Kumar, R. Govindarajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Even in the era of Deep Learning based methods, traditional machine learning methods with large data sets continue to attract significant attention. However, we find an apparent lack of a detailed performance characterization of these methods in the context of large training datasets. In this work, we study the system's behavior of a number of traditional ML methods as implemented in popular free software libraries/modules to identify critical performance bottlenecks experienced by these applications. The performance characterization study reveals several interesting insights on the performance of these applications. Then we evaluate the performance benefits of applying some well-known optimizations at the levels of caches and the main memory. More specifically, we test the usefulness of optimizations such as (i) software prefetching to improve cache performance and (ii) data layout and computation reordering optimizations to improve locality in DRAM accesses. These optimizations are implemented as modifications to the well-known scikit-learn library, and hence can be easily leveraged by application programmers. We evaluate the impact of the proposed optimizations using a combination of simulation and execution on a real system. The software prefetching optimization results in performance benefits varying from 5.2%-27.1% on different ML applications while the data layout and computation reordering approaches yield 6.16%-28.0% performance improvement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-26T04:13:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.19051v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.19051v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 XRFlux: Virtual Reality Benchmark for Edge Caching Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nader Alfares, George Kesidis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality (VR) delivery systems using edge-cloud caching. As VR applications and systems progress, the need to meet strict latency and Quality of Experience (QoE) requirements is increasingly evident. In the context of VR, traditional cloud architectures (e.g., remote AWS S3 for content delivery) often struggle to meet these demands, especially for users of the same application in different locations. With edge computing, resources are brought closer to users in efforts to reduce latency and improve QoEs. However, VR's dynamic nature, with changing fields of view (FoVs) and user synchronization requirements, creates various challenges for edge caching. We address the lack of suitable benchmarks and propose a framework that simulates multiuser VR scenarios while logging users' interaction with objects within their actual and predicted FoVs. The benchmark's activity log can then be played back through an edge cache to assess the resulting QoEs. This tool fills a gap by supporting research in the optimization of edge caching (and other edge-cloud functions) for VR streaming.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-25T18:36:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.18960v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.18960v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With
  Structured Memories</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-range tasks require reasoning over long inputs. Existing solutions either need large compute budgets, training data, access to model weights, or use complex, task-specific approaches. We present PRISM, which alleviates these concerns by processing information as a stream of chunks, maintaining a structured in-context memory specified by a typed hierarchy schema. This approach demonstrates superior performance to baselines on diverse tasks while using at least 4x smaller contexts than long-context models. Moreover, PRISM is token-efficient. By producing short outputs and efficiently leveraging key-value (KV) caches, it achieves up to 54% cost reduction when compared to alternative short-context approaches. The method also scales down to tiny information chunks (e.g., 500 tokens) without increasing the number of tokens encoded or sacrificing quality. Furthermore, we show that it is possible to generate schemas to generalize our approach to new tasks with minimal effort.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-25T14:14:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.18914v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.18914v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Accelerating Diffusion Transformers with Dual Feature Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. However, on the one hand, aggressively reusing all the features cached in previous timesteps leads to a severe drop in generation quality. On the other hand, conservatively caching only the features in the redundant layers or tokens but still computing the important ones successfully preserves the generation quality but results in reductions in acceleration ratios. Observing such a tradeoff between generation quality and acceleration performance, this paper begins by quantitatively studying the accumulated error from cached features. Surprisingly, we find that aggressive caching does not introduce significantly more caching errors in the caching step, and the conservative feature caching can fix the error introduced by aggressive caching. Thereby, we propose a dual caching strategy that adopts aggressive and conservative caching iteratively, leading to significant acceleration and high generation quality at the same time. Besides, we further introduce a V-caching strategy for token-wise conservative caching, which is compatible with flash attention and requires no training and calibration data.   Our codes have been released in Github: \textbf{Code: \href{https://github.com/Shenyi-Z/DuCa}{\texttt{\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-25T14:00:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.18911v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.18911v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Aspect-oriented Programming with Julia</h2>
                <div class="authors">
                    <strong>Authors:</strong> Osamu Ishimura, Yoshihide Yoshimoto
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper proposes integrating Aspect-oriented Programming (AOP) into Julia, a language widely used in scientific and High-Performance Computing (HPC). AOP enhances software modularity by encapsulating cross-cutting concerns, such as logging, caching, and parallelizing, into separate, reusable aspects. Leveraging Julia's powerful metaprogramming and abstract syntax tree (AST) manipulation capabilities, we introduce AspectJulia, an AOP framework designed to operate within Julia's runtime environment as a package. AspectJulia enables developers to define and apply aspects seamlessly, leading to more modular, maintainable, and adaptable code. We detail the implementation of AspectJulia and present diverse use cases, ranging from HPC and scientific computing to business applications, demonstrating its effectiveness in managing cross-cutting concerns. This integration simplifies application development and improves the adaptability of existing Julia modules and packages, paving the way for more efficient and maintainable software systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-25T11:59:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.18885v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.18885v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Efficiently serving large multimedia models using EPD Disaggregation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gursimran Singh, Xinglu Wang, Ivan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step helps convert raw inputs into tokenized representations that inflate the token sequence for the prefill phase, negatively impacting key Service Level Objectives (SLOs) like time to first token (TTFT) and end-to-end throughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our disaggregation approach alleviates memory bottlenecks, mitigates synchronization delays, and supports flexible batching. Specifically, we employ a new caching mechanism for multimodal tokens, enabling asynchronous transfer of multimodal tokens and introduce an integrated module to find optimal config for EPD system and minimize resource usage while maximizing SLO-based performance metric. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15$\times$ lesser for encoding-stage GPUs), that supports upto 22$\times$ higher batch sizes, 10$\times$ more number of images/ request, 2.2$\times$ higher kv cache size. Further, it leads to significant improvements in end-to-end throughput (up to 57\% better), and latency metrics (TTFT up to 71\% lower), compared to systems that do not disaggregate. Our findings underscore the potential of EPD disaggregation to enable resource-efficient and high-performance multimodal inference at scale.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-25T10:11:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05460v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05460v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 HashEvict: A Pre-Attention KV Cache Eviction Strategy using
  Locality-Sensitive Hashing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minghui Liu, Tahseen Rabbani, Tony O'Halloran, Ananth Sankaralingam, Mary-Anne Hartley, Brian Gravelle, Furong Huang, Cornelia Fermüller, Yiannis Aloimonos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce HashEvict, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, HashEvict makes these decisions pre-attention, thereby reducing computational costs. Additionally, HashEvict is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that HashEvict can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-24T13:04:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>cs.DS</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.16187v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.16187v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Development and Application of a Decentralized Domain Name Service</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guang Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The current Domain Name System (DNS), as a core infrastructure of the internet, exhibits several shortcomings: its centralized architecture leads to censorship risks and single points of failure, making domain name resolution vulnerable to attacks. The lack of encryption in the resolution process exposes it to DNS hijacking and cache poisoning attacks. Additionally, the high operational costs limit participation and innovation among small to medium-sized users. To address these issues, this paper proposes a Decentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and distributed storage (IPFS). By leveraging the immutability of blockchain and the content verification of IPFS, the system achieves decentralized storage and distribution of domain name records, eliminating the centralized dependencies of traditional DNS. With a block time of 15 seconds, the system supports rapid broadcasting of domain name updates, significantly improving resolution efficiency. The DDNS aims to serve as a complement or backup to the existing DNS system, providing a pollution-resistant, censorship-resistant, high-performance, and low-cost domain name resolution solution, offering a new technical path for the security and stability of the internet.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-24T00:46:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.01959v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.01959v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Deliberation in Latent Space via Differentiable Cache Augmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, Arthur Szlam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Techniques enabling large language models (LLMs) to "think more" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-23T18:02:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.17747v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.17747v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 A Reproducible Method for Mapping Electricity Transmission
  Infrastructure for Space Weather Risk Assessment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Edward J. Oughton, Evan Alexander Peters, Dennies Bor, Noah Rivera, C. Trevor Gaunt, Robert Weigel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Space weather impact assessment is constrained by the lack of available asset information to undertake modeling of Geomagnetically Induced Currents (GICs) in Extra High Voltage electricity infrastructure networks. The U.S. National Space Weather Strategy and Action Plan identifies underutilized data as a central issue for improving risk assessment, motivating this research. Accurate GIC prediction is generally not possible without information on the electrical circuit, therefore we define a reproducible method based on open-source data, which enables risk analysts to collect their own substation component data. This process converts OpenStreetMap (OSM) substation locations to high-resolution, component-level mapping of electricity transmission assets by utilizing an innovative web-browser platform to facilitate component annotation. As a case study example, we convert an initial 1,313 high-voltage (>115 kV) substations to 52,273 substation components via Google Earth APIs utilizing low-altitude, satellite, and Streetview imagery. We find that a total of 41,642 substation components (79.6%) connect to the highest substation voltage levels (>345 kV) and are possibly susceptible to GIC, with a total of 7,949 transformers identified. Compared to the initial OSM baseline, we provide new detailed insights on voltage levels, line capacities, and substation configurations. Two validation workshops were undertaken to align the method and data with GIC assessment needs. The approach ensures consistency and rapid scalability, enabling users to quickly count components via a flexible web-browser application.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-23T16:11:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.geo-ph</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.17685v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.17685v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT
  in SAGIN</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qian Chen, Chenyu Wu, Shuai Han, Weixiao Meng, Tony Q. S. Quek
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid development of the aviation Internet of Things (IoT) has positioned in-flight connectivity (IFC) as one of its critical applications. Space-air-ground integrated networks (SAGIN) are essential for ensuring the performance of IFC by enabling seamless and reliable connectivity. However, most existing research treats satellites merely as transparent forwarding nodes and overlooks their potential caching capabilities to enhance IFC data rates. In this article, we explore an IFC-oriented SAGIN where satellites and ground stations (GSs) work together to transmit content to airborne passengers, thereby facilitating airborne communication. By categorizing files into cached (instantly accessible via satellites) and non-cached files (available only through GSs), this article pioneers the integration of multiple inter-satellite links (ISLs) into the IFC framework, thus innovating the content delivery process for both types of files. To minimize the average delay of content delivery, we formulate the corresponding optimization problems: 1) For cached files, we propose an exact penalty-based method to determine the satellite association scheme. 2) For non-cached files, we present an efficient algorithm based on alternating optimization to jointly optimize satellite association and GS bandwidth allocation. Our proposed framework is low in complexity, paving the way for high-speed Internet connectivity for aviation passengers. Finally, simulation results are provided to demonstrate the effectiveness of our proposed IFC framework for SAGIN.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-23T14:40:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.18919v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.18919v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 A Framework for Effective Invocation Methods of Various LLM Services</h2>
                <div class="authors">
                    <strong>Authors:</strong> Can Wang, Dianbo Sui, Bolin Zhang, Xiaoyu Liu, Jiabao Kang, Zhidong Qiao, Zhiying Tu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown impressive abilities in solving various natural language processing tasks and are now widely offered as services. LLM services enable users to accomplish tasks without requiring specialized knowledge, simply by paying service providers. However, numerous providers offer various LLM services with variations in pricing, latency, and performance. These factors are also affected by different invocation methods, such as the choice of context and the use of cache, which lead to unpredictable and uncontrollable service cost and quality. Consequently, utilizing various LLM services invocation methods to construct an effective (cost-saving, low-latency and high-performance) invocation strategy that best meets task demands becomes a pressing challenge. This paper provides a comprehensive overview of methods help LLM services to be invoked efficiently. Technically, we define the problem of constructing an effective LLM services invocation strategy, and based on this, propose a unified LLM service invocation framework. The framework classifies existing methods into four categories: input abstraction, semantic cache, solution design, and output enhancement, which can be used separately or jointly during the invocation life cycle. We discuss the methods in each category and compare them to provide valuable guidance for researchers. Finally, we emphasize the open challenges in this domain and shed light on future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-23T12:55:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.03408v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.03408v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 CALLIC: Content Adaptive Learning for Lossless Image Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daxin Li, Yuanchao Bai, Kai Wang, Junjun Jiang, Xianming Liu, Wen Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learned lossless image compression has achieved significant advancements in recent years. However, existing methods often rely on training amortized generative models on massive datasets, resulting in sub-optimal probability distribution estimation for specific testing images during encoding process. To address this challenge, we explore the connection between the Minimum Description Length (MDL) principle and Parameter-Efficient Transfer Learning (PETL), leading to the development of a novel content-adaptive approach for learned lossless image compression, dubbed CALLIC. Specifically, we first propose a content-aware autoregressive self-attention mechanism by leveraging convolutional gating operations, termed Masked Gated ConvFormer (MGCF), and pretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed to accelerate the coding process. During encoding, we decompose pre-trained layers, including depth-wise convolutions, using low-rank matrices and then adapt the incremental weights on testing image by Rate-guided Progressive Fine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are sorted in descending order by estimated entropy, optimizing learning process and reducing adaptation time. Extensive experiments across diverse datasets demonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless image compression.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-23T10:41:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.17464v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.17464v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Fast and Live Model Auto Scaling with O(1) Host Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dingyan Zhang, Haotian Wang, Yang Liu, Xingda Wei, Yizhou Shan, Rong Chen, Haibo Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Model autoscaling is the key mechanism to achieve serverless model-as-a-service, but it faces a fundamental trade-off between scaling speed and storage/memory usage to cache parameters, and cannot meet frequent scaling requirements across multiple hosts. The key problem is that data plane performance is slow, and scaled instances remain stopped while parameters are loading. We first show that data plane can be made fast with no/O(1) caching by loading parameters through the compute network between GPUs because: (1) its speed is comparable host cache and is underutilized; (2) scaling multiple instances requires no or O(1) caching with network-optimized multicast. Second, autoscaling can be made live by breaking the scaling abstraction from a coarse-grained instance-level to a fine-grained layer-level. This allows us to offload the layer computation from the overloaded serving instances to the scaled instance with cooperative execution, thus handles cases even when the compute network is not sufficiently fast. Our system BLITZSCALE reduces the serving tail latencies by up to 86% without caching, and we achieve comparable performance (or even better) to an optimal setup where all the parameters are cached at all the host for autoscaling.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-23T03:38:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.17246v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.17246v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Semi-Supervised Contrastive Learning for Controllable Video-to-Music
  Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shanti Stewart, Gouthaman KV, Lie Lu, Andrea Fanelli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Content creators often use music to enhance their videos, from soundtracks in movies to background music in video blogs and social media content. However, identifying the best music for a video can be a difficult and time-consuming task. To address this challenge, we propose a novel framework for automatically retrieving a matching music clip for a given video, and vice versa. Our approach leverages annotated music labels, as well as the inherent artistic correspondence between visual and music elements. Distinct from previous cross-modal music retrieval works, our method combines both self-supervised and supervised training objectives. We use self-supervised and label-supervised contrastive learning to train a joint embedding space between music and video. We show the effectiveness of our approach by using music genre labels for the supervised training component, and our framework can be generalized to other music annotations (e.g., emotion, instrument, etc.). Furthermore, our method enables fine-grained control over how much the retrieval process focuses on self-supervised vs. label information at inference time. We evaluate the learned embeddings through a variety of video-to-music and music-to-video retrieval tasks. Our experiments show that the proposed approach successfully combines self-supervised and supervised objectives and is effective for controllable music-video retrieval.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-23T02:52:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.05831v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.05831v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Agile TLB Prefetching and Prediction Replacement Policy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Melkamu Mersha, Tsion Abay, Mingziem Bitewa, Gedare Bloom
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Virtual-to-physical address translation is a critical performance bottleneck in paging-based virtual memory systems. The Translation Lookaside Buffer (TLB) accelerates address translation by caching frequently accessed mappings, but TLB misses lead to costly page walks. Hardware and software techniques address this challenge. Hardware approaches enhance TLB reach through system-level support, while software optimizations include TLB prefetching, replacement policies, superpages, and page size adjustments. Prefetching Page Table Entries (PTEs) for future accesses reduces bottlenecks but may incur overhead from incorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP optimizes performance by leveraging page table locality and dynamically identifying essential free PTEs during page walks. Predictive replacement policies further improve TLB performance. Traditional LRU replacement is limited to near-instant references, while advanced policies like SRRIP, GHRP, SHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies. CHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control flow history to detect dead blocks, utilizing L2 TLB entries for learning instead of sampling. These integrated techniques collectively address key challenges in virtual memory management.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-23T00:46:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.17203v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.17203v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 MVREC: A General Few-shot Defect Classification Model Using Multi-View
  Region-Context</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuai Lyu, Fangjian Liao, Zeqi Ma, Rongchen Zhang, Dongmei Mo, Waikeung Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Few-shot defect multi-classification (FSDMC) is an emerging trend in quality control within industrial manufacturing. However, current FSDMC research often lacks generalizability due to its focus on specific datasets. Additionally, defect classification heavily relies on contextual information within images, and existing methods fall short of effectively extracting this information. To address these challenges, we propose a general FSDMC framework called MVREC, which offers two primary advantages: (1) MVREC extracts general features for defect instances by incorporating the pre-trained AlphaCLIP model. (2) It utilizes a region-context framework to enhance defect features by leveraging mask region input and multi-view context augmentation. Furthermore, Few-shot Zip-Adapter(-F) classifiers within the model are introduced to cache the visual features of the support set and perform few-shot classification. We also introduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes 1228 defect images with instance-level mask annotations and 46 defect types. Extensive experiments conducted on MVTec-FS and four additional datasets demonstrate its effectiveness in general defect classification and its ability to incorporate contextual information to improve classification performance. Code: https://github.com/ShuaiLYU/MVREC
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-22T07:14:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.16897v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.16897v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 MemServe: Context Caching for Disaggregated LLM Serving with Elastic
  Memory Pool</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, Yizhou Shan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) serving has transformed from stateless to stateful systems, utilizing techniques like context caching and disaggregated inference. These optimizations extend the lifespan and domain of the KV cache, necessitating a new architectural approach. We present MemServe, a unified system that integrates both inter-request and intra-request optimizations. MemServe introduces MemPool, an elastic memory pool managing distributed memory and KV caches across serving instances. Using MemPool APIs, MemServe combines context caching with disaggregated inference for the first time, supported by a global scheduler that enhances cache reuse through a global prompt tree-based locality-aware policy. Tests show that MemServe significantly improves job completion time and time-to-first-time.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-21T13:55:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.17565v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.17565v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Parameterized Complexity of Caching in Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Robert Ganian, Fionn Mc Inerney, Dimitra Tsigkari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The fundamental caching problem in networks asks to find an allocation of contents to a network of caches with the aim of maximizing the cache hit rate. Despite the problem's importance to a variety of research areas -- including not only content delivery, but also edge intelligence and inference -- and the extensive body of work on empirical aspects of caching, very little is known about the exact boundaries of tractability for the problem beyond its general NP-hardness. We close this gap by performing a comprehensive complexity-theoretic analysis of the problem through the lens of the parameterized complexity paradigm, which is designed to provide more precise statements regarding algorithmic tractability than classical complexity. Our results include algorithmic lower and upper bounds which together establish the conditions under which the caching problem becomes tractable.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-21T11:20:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.CC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.16585v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.16585v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Yi-Lightning Technical Report</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alan Wake, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Fan Zhou, Feng Hu, Guoyin Wang, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiming Yang, Shiyong Li, Tianhang Zhu, Wen Xie, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang, Zonghong Dai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-21T02:36:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.01253v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.01253v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 SYMPHONY: Improving Memory Management for LLM Inference Workloads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Saurabh Agarwal, Anyong Mao, Aditya Akella, Shivaram Venkataraman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly being deployed in applications such as chatbots, code editors, and conversational agents. A key feature of LLMs is their ability to engage in multi-turn interactions with humans or external tools, enabling a wide range of tasks. Each new request in a multi-turn interaction depends on the intermediate state, specifically the key-value (K,V) caches, from previous requests in the ongoing interaction. Existing serving engines either recompute the K,V caches or offload them to main memory. Profiling reveals that recomputation can result in over 99% of processed tokens being redundant. On the other hand, offloading K,V caches from GPU memory makes inference serving stateful, leading to load imbalances across the cluster. To address these challenges, we developed SYMPHONY. SYMPHONY leverages the observation that multi-turn work loads provide additional hints that allow K,V caches to be migrated off the critical serving path. By utilizing these hints, SYMPHONY dynamically migrates K,V caches to enable finegrained scheduling of inference requests. Our experiments demonstrate that SYMPHONY can handle over 8x the number of requests compared to state-of-the-art baselines, with a similar latency profile.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-21T01:48:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.16434v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.16434v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Multi-Strided Access Patterns to Boost Hardware Prefetching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Miguel O. Blom, Kristian F. D. Rietveld, Rob V. van Nieuwpoort
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Important memory-bound kernels, such as linear algebra, convolutions, and stencils, rely on SIMD instructions as well as optimizations targeting improved vectorized data traversal and data re-use to attain satisfactory performance. On on temporary CPU architectures, the hardware prefetcher is of key importance for efficient utilization of the memory hierarchy. In this paper, we demonstrate that transforming a memory access pattern consisting of a single stride to one that concurrently accesses multiple strides, can boost the utilization of the hardware prefetcher, and in turn improves the performance of memory-bound kernels significantly. Using a set of micro-benchmarks, we establish that accessing memory in a multi-strided manner enables more cache lines to be concurrently brought into the cache, resulting in improved cache hit ratios and higher effective memory bandwidth without the introduction of costly software prefetch instructions. Subsequently, we show that multi-strided variants of a collection of six memory-bound dense compute kernels outperform state-of-the-art counterparts on three different micro-architectures. More specifically, for kernels among which Matrix Vector Multiplication, Convolution Stencil and kernels from PolyBench, we achieve significant speedups of up to 12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and 1.87x over OpenCV. The code transformation to take advantage of multi-strided memory access is a natural extension of the loop unroll and loop interchange techniques, allowing this method to be incorporated into compiler pipelines in the future.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-20T15:51:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.16001v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.16001v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Towards Projected and Incremental Pseudo-Boolean Model Counting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Suwei Yang, Kuldeep S. Meel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Model counting is a fundamental task that involves determining the number of satisfying assignments to a logical formula, typically in conjunctive normal form (CNF). While CNF model counting has received extensive attention over recent decades, interest in Pseudo-Boolean (PB) model counting is just emerging partly due to the greater flexibility of PB formulas. As such, we observed feature gaps in existing PB counters such as a lack of support for projected and incremental settings, which could hinder adoption. In this work, our main contribution is the introduction of the PB model counter PBCount2, the first exact PB model counter with support for projected and incremental model counting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree (LOW-MD) computation ordering heuristic to support projected model counting and a cache mechanism to enable incremental model counting. In our evaluations, PBCount2 completed at least 1.40x the number of benchmarks of competing methods for projected model counting and at least 1.18x of competing methods in incremental model counting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-20T15:18:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.14485v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.14485v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Don't Do RAG: When Cache-Augmented Generation is All You Need for
  Knowledge Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Brian J Chan, Chao-Ting Chen, Jui-Hung Cheng, Hen-Hsen Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) has gained traction as a powerful approach for enhancing language models by integrating external knowledge sources. However, RAG introduces challenges such as retrieval latency, potential errors in document selection, and increased system complexity. With the advent of large language models (LLMs) featuring significantly extended context windows, this paper proposes an alternative paradigm, cache-augmented generation (CAG) that bypasses real-time retrieval. Our method involves preloading all relevant resources, especially when the documents or knowledge for retrieval are of a limited and manageable size, into the LLM's extended context and caching its runtime parameters. During inference, the model utilizes these preloaded parameters to answer queries without additional retrieval steps. Comparative analyses reveal that CAG eliminates retrieval latency and minimizes retrieval errors while maintaining context relevance. Performance evaluations across multiple benchmarks highlight scenarios where long-context LLMs either outperform or complement traditional RAG pipelines. These findings suggest that, for certain applications, particularly those with a constrained knowledge base, CAG provide a streamlined and efficient alternative to RAG, achieving comparable or superior results with reduced complexity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-20T06:58:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.15605v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.15605v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM
  Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhan Liu, Yuyang Huang, Jiayi Yao, Zhuohan Gu, Kuntai Du, Hanchen Li, Yihua Cheng, Junchen Jiang, Shan Lu, Madan Musuvathi, Esha Choukse
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly employed in complex workflows, where different LLMs and fine-tuned variants collaboratively address complex tasks. However, these systems face significant inefficiencies due to redundant context processing of the shared context. We propose DroidSpeak, a framework that optimizes context sharing between fine-tuned LLMs derived from the same foundational model. DroidSpeak identifies critical layers in the KV cache and selectively recomputes them, enabling effective reuse of intermediate data while maintaining high accuracy.   Our approach balances computational efficiency and task fidelity, significantly reducing inference latency and throughput bottlenecks. Experiments on diverse datasets and model pairs demonstrate that DroidSpeak achieves up to 3x higher throughputs and 2.6x faster prefill times with negligible accuracy loss compared to full recomputation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-19T23:52:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.02820v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.02820v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Exposing Shadow Branches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chrysanthos Pepi, Bhargav Reddy Godala, Krishnam Tibrewala, Gino Chacon, Paul V. Gratz, Daniel A. Jiménez, Gilles A. Pokam, David I. August
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern processors implement a decoupled front-end in the form of Fetch Directed Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is driven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and branch target tracking structures to speculatively fetch instructions into the Instruction Cache (L1I). As data center applications become more complex, their code footprints also grow, resulting in an increase in Branch Target Buffer (BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB miss, the BPU may not identify the current instruction as a branch to FDIP. This can prevent FDIP from prefetching or cause it to speculate down the wrong path, further polluting the L1I cache. We observe that the vast majority, 75%, of BTB-missing, unidentified branches are actually present in instruction cache lines that FDIP has previously fetched but, these missing branches have not yet been decoded and inserted into the BTB. This is because the instruction line is decoded from an entry point (which is the target of the previous taken branch) till an exit point (the taken branch). Branch instructions present in the ignored portion of the cache line we call them "Shadow Branches". Here we present Skeia, a novel shadow branch decoding technique that identifies and decodes unused bytes in cache lines fetched by FDIP, inserting them into a Shadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB, allowing FDIP to speculate despite a BTB miss. With a minimal storage state of 12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB) and ~2% versus adding an equal amount of state to the BTB across 16 front-end bound applications. Since many branches stored in the SBB are unique compared to those in a similarly sized BTB, we consistently observe greater performance gains with Skeia across all examined sizes until saturation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-19T22:34:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.12592v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.12592v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context
  LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient KV cache management in LLMs is crucial for long-context tasks like RAG and summarization. Existing KV cache compression methods enforce a fixed pattern, neglecting task-specific characteristics and reducing the retention of essential information. However, we observe distinct activation patterns across layers in various tasks, highlighting the need for adaptive strategies tailored to each task's unique demands. Based on this insight, we propose DynamicKV, a method that dynamically optimizes token retention by adjusting the number of tokens retained at each layer to adapt to the specific task. DynamicKV establishes global and per-layer maximum KV cache budgets, temporarily retaining the maximum budget for the current layer, and periodically updating the KV cache sizes of all preceding layers during inference. Our method retains only 1.7% of the KV cache size while achieving ~85% of the Full KV cache performance on LongBench. Notably, even under extreme compression (0.9%), DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be released.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-19T13:28:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.14838v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.14838v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Accelerating Diffusion Transformers with Token-wise Feature Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and reusing them in the following timesteps. However, previous caching methods ignore that different tokens exhibit different sensitivities to feature caching, and feature caching on some tokens may lead to 10$\times$ more destruction to the overall generation quality compared with other tokens. In this paper, we introduce token-wise feature caching, allowing us to adaptively select the most suitable tokens for caching, and further enable us to apply different caching ratios to neural layers in different types and depths. Extensive experiments on PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image and video generation with no requirements for training. For instance, 2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and PixArt-$\alpha$ with almost no drop in generation quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-19T12:38:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.05317v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.05317v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure
  Integration in Machine Learning Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dongfang Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Machine learning (ML) systems that guarantee security and privacy often rely on Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling computations on encrypted data without exposing sensitive information. However, a critical limitation of FHE is its computational inefficiency, making it impractical for large-scale applications. In this work, we propose \textit{Nemesis}, a framework that accelerates FHE-based systems without compromising accuracy or security. The design of Nemesis is inspired by Rache (SIGMOD'23), which introduced a caching mechanism for encrypted integers and scalars. Nemesis extends this idea with more advanced caching techniques and mathematical tools, enabling efficient operations over multi-slot FHE schemes and overcoming Rache's limitations to support general plaintext structures. We formally prove the security of Nemesis under standard cryptographic assumptions and evaluate its performance extensively on widely used datasets, including MNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis significantly reduces the computational overhead of FHE-based ML systems, paving the way for broader adoption of privacy-preserving technologies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T22:52:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.14392v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.14392v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 ResQ: Mixed-Precision Quantization of Large Language Models with
  Low-Rank Residuals</h2>
                <div class="authors">
                    <strong>Authors:</strong> Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers. We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33% lower perplexity on Wikitext than the next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code is available at https://github.com/utkarsh-dmx/project-resq.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T22:01:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.14363v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.14363v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Optimizing ML Concurrent Computation and Communication with GPU DMA
  Engines</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anirudha Agrawal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Concurrent computation and communication (C3) is a pervasive paradigm in ML and other domains, making its performance optimization crucial. In this paper, we carefully characterize C3 in ML on GPUs, which are most widely deployed for ML training and inference. We observe that while C3 leads to performance uplifts, the uplifts are far lower than ideal speedups (serial computation and communication versus maximum of computation or communication; all times from isolated executions). C3 on average achieves only 21% of ideal speedup, this is due to known challenges of compute and memory interference between concurrent GPU kernels (that is, sharing of GPU's compute units, caches and HBM).   To attain better performance for C3, first, we evaluate dual strategies of schedule prioritization and careful resource partitioning of compute units on GPUs to push performance attained with C3 (on average 42% of ideal speedup). We also provide heuristics that can guide a runtime while employing these strategies. To further enhance C3 performance, we propose to mitigate C3 interference by offloading communication tasks to the GPU's DMA engines. To this end, we build Concurrent Communication CoLlectives (ConCCL) proof-of-concepts that harness DMA engines for communication. We show how ConCCL considerably closes the gap between realized and ideal speedup for C3 (on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall, our work makes a strong case for GPU DMA engine advancements to better support C3 on GPUs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T21:09:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.14335v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.14335v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 MagicPIG: LSH Sampling for Efficient LLM Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at https://github.com/Infini-AI-Lab/MagicPIG.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T17:36:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.16179v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.16179v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Rehearsal-Free Continual Federated Learning with Synergistic
  Regularization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yichen Li, Yuying Wang, Tianzhe Xiao, Haozhao Wang, Yining Qi, Ruixuan Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding knowledge forgetting of previously seen tasks. To tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. Considering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named FedSSI, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. FedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. Extensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T12:16:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.13779v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.13779v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Semantic Convergence: Harmonizing Recommender Systems via Two-Stage
  Alignment and Behavioral Semantic Tokenization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guanghan Li, Xun Zhang, Yufei Zhang, Yifan Yin, Guojun Yin, Wei Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs), endowed with exceptional reasoning capabilities, are adept at discerning profound user interests from historical behaviors, thereby presenting a promising avenue for the advancement of recommendation systems. However, a notable discrepancy persists between the sparse collaborative semantics typically found in recommendation systems and the dense token representations within LLMs. In our study, we propose a novel framework that harmoniously merges traditional recommendation models with the prowess of LLMs. We initiate this integration by transforming ItemIDs into sequences that align semantically with the LLMs space, through the proposed Alignment Tokenization module. Additionally, we design a series of specialized supervised learning tasks aimed at aligning collaborative signals with the subtleties of natural language semantics. To ensure practical applicability, we optimize online inference by pre-caching the top-K results for each user, reducing latency and improving effciency. Extensive experimental evidence indicates that our model markedly improves recall metrics and displays remarkable scalability of recommendation systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T12:07:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.13771v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.13771v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 DyCoke: Dynamic Compression of Tokens for Fast Video Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video large language models (VLLMs) have significantly advanced recently in processing complex video content, yet their inference efficiency remains constrained because of the high computational cost stemming from the thousands of visual tokens generated from the video inputs. We empirically observe that, unlike single image inputs, VLLMs typically attend visual tokens from different frames at different decoding iterations, making a one-shot pruning strategy prone to removing important tokens by mistake. Motivated by this, we present DyCoke, a training-free token compression method to optimize token representation and accelerate VLLMs. DyCoke incorporates a plug-and-play temporal compression module to minimize temporal redundancy by merging redundant tokens across frames, and applies dynamic KV cache reduction to prune spatially redundant tokens selectively. It ensures high-quality inference by dynamically retaining the critical tokens at each decoding step. Extensive experimental results demonstrate that DyCoke can outperform the prior SoTA counterparts, achieving 1.5X inference speedup, 1.4X memory reduction against the baseline VLLM, while still improving the performance, with no training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T09:47:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.15024v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.15024v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jialong Wu, Zhenglin Wang, Linhai Zhang, Yilong Lai, Yulan He, Deyu Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T09:27:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.13649v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.13649v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 ZipVL: Efficient Large Vision-Language Models with Dynamic Token
  Sparsification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, Bohan Zhuang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform sparse attention mechanism solely on those important tokens, reducing the latency in the prefill phase. Tokens deemed less important will be discarded to reduce KV cache size, alleviating the memory bottleneck in the decoding phase. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.3$\times$ and improve decoding throughput by 2.8$\times$, with a minimal accuracy reduction of only 0.5\% on VQAv2 benchmark over LLaVA-Next-13B model, effectively enhancing the generation efficiency of LVLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T07:45:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.08584v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.08584v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data
  Presentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunqi Guo, Kaiyuan Hou, Heming Fu, Hongkai Chen, Zhenyu Yan, Guoliang Xing, Xiaofan Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding sensor data can be challenging for non-experts because of the complexity and unique semantic meanings of sensor modalities. This calls for intuitive and effective methods to present sensor information. However, creating intuitive sensor data visualizations presents three key challenges: the variability of sensor readings, gaps in domain comprehension, and the dynamic nature of sensor data. To address these issues, we develop Vivar, a novel AR system that integrates multi-modal sensor data and presents 3D volumetric content for visualization. In particular, we introduce a cross-modal embedding approach that maps sensor data into a pre-trained visual embedding space through barycentric interpolation. This allows for accurate and continuous integration of multi-modal sensor information. Vivar also incorporates sensor-aware AR scene generation using foundation models and 3D Gaussian Splatting (3DGS) without requiring domain expertise. In addition, Vivar leverages latent reuse and caching strategies to accelerate 2D and AR content generation. Our extensive experiments demonstrate that our system achieves 11$\times$ latency reduction without compromising quality. A user study involving over 485 participants, including domain experts, demonstrates Vivar's effectiveness in accuracy, consistency, and real-world applicability, paving the way for more intuitive sensor data visualization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T05:16:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.13509v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.13509v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Boosting Long-Context Management via Query-Guided Activation Refilling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, Defu Lian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Processing long contexts poses a significant challenge for large language models (LLMs) due to their inherent context-window limitations and the computational burden of extensive key-value (KV) activations, which severely impact efficiency. For information-seeking tasks, full context perception is often unnecessary, as a query's information needs can dynamically range from localized details to a global perspective, depending on its complexity. However, existing methods struggle to adapt effectively to these dynamic information needs.   In the paper, we propose a method for processing long-context information-seeking tasks via query-guided Activation Refilling (ACRE). ACRE constructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache compactly captures global information, and the layer-2 (L2) cache provides detailed and localized information. ACRE establishes a proxying relationship between the two caches, allowing the input query to attend to the L1 cache and dynamically refill it with relevant entries from the L2 cache. This mechanism integrates global understanding with query-specific local details, thus improving answer decoding. Experiments on a variety of long-context information-seeking datasets demonstrate ACRE's effectiveness, achieving improvements in both performance and efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T05:08:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12486v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12486v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 FlexCache: Flexible Approximate Cache System for Video Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Desen Sun, Henry Tian, Tim Lu, Sihang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Text-to-Video applications receive increasing attention from the public. Among these, diffusion models have emerged as the most prominent approach, offering impressive quality in visual content generation. However, it still suffers from substantial computational complexity, often requiring several minutes to generate a single video. While prior research has addressed the computational overhead in text-to-image diffusion models, the techniques developed are not directly suitable for video diffusion models due to the significantly larger cache requirements and enhanced computational demands associated with video generation.   We present FlexCache, a flexible approximate cache system that addresses the challenges in two main designs. First, we compress the caches before saving them to storage. Our compression strategy can reduce 6.7 times consumption on average. Then we find that the approximate cache system can achieve higher hit rate and computation savings by decoupling the object and background. We further design a tailored cache replacement policy to support the two techniques mentioned above better. Through our evaluation, FlexCache reaches 1.26 times higher throughput and 25% lower cost compared to the state-of-the-art diffusion approximate cache system.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-18T00:35:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04012v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04012v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic
  Vision-language Context Sparsification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaosheng Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Shaohui Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens during decoding, directly affecting the efficacy of MLLMs. Existing methods attempt to reduce the vision context redundancy to achieve efficient MLLMs. Unfortunately, the efficiency benefits of the vision context reduction in the prefill stage gradually diminish during the decoding stage. To address this problem, we proposed a dynamic vision-language context sparsification framework Dynamic-LLaVA, which dynamically reduces the redundancy of vision context in the prefill stage and decreases the memory and computation overhead of the generated language context during decoding. Dynamic-LLaVA designs a tailored sparsification inference scheme for different inference modes, i.e., prefill, decoding with and without KV cache, to achieve efficient inference of MLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by $\sim$75\% in the prefill stage. Meanwhile, throughout the entire generation process of MLLMs, Dynamic-LLaVA reduces the $\sim$50\% computation consumption under decoding without KV cache, while saving $\sim$50\% GPU memory overhead when decoding with KV cache, due to the vision-language context sparsification. Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficient inference for MLLMs with negligible understanding and generation ability degradation or even performance gains compared to the full-context inference baselines. Code is available at https://github.com/Osilly/dynamic_llava .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-17T14:45:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.00876v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.00876v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Efficient Diffusion Transformer Policies with Mixture of Expert
  Denoisers for Multitask Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Moritz Reuss, Jyothish Pari, Pulkit Agrawal, Rudolf Lioutikov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Policies have become widely used in Imitation Learning, offering several appealing properties, such as generating multimodal and discontinuous behavior. As models are becoming larger to capture more complex capabilities, their computational demands increase, as shown by recent scaling laws. Therefore, continuing with the current architectures will present a computational roadblock. To address this gap, we propose Mixture-of-Denoising Experts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current state-of-the-art Transformer-based Diffusion Policies while enabling parameter-efficient scaling through sparse experts and noise-conditioned routing, reducing both active parameters by 40% and inference costs by 90% via expert caching. Our architecture combines this efficient scaling with noise-conditioned self-attention mechanism, enabling more effective denoising across different noise levels. MoDE achieves state-of-the-art performance on 134 tasks in four established imitation learning benchmarks (CALVIN and LIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01 on CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and Transformer Diffusion Policies by an average of 57% across 4 benchmarks, while using 90% fewer FLOPs and fewer active parameters compared to default Diffusion Transformer architectures. Furthermore, we conduct comprehensive ablations on MoDE's components, providing insights for designing efficient and scalable Transformer architectures for Diffusion Policies. Code and demonstrations are available at https://mbreuss.github.io/MoDE_Diffusion_Policy/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-17T14:34:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12953v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12953v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance
  Segmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shiqi Huang, Shuting He, Bihan Wen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Instance segmentation algorithms in remote sensing are typically based on conventional methods, limiting their application to seen scenarios and closed-set predictions. In this work, we propose a novel task called zero-shot remote sensing instance segmentation, aimed at identifying aerial objects that are absent from training data. Challenges arise when classifying aerial categories with high inter-class similarity and intra-class variance. Besides, the domain gap between vision-language models' pretraining datasets and remote sensing datasets hinders the zero-shot capabilities of the pretrained model when it is directly applied to remote sensing images. To address these challenges, we propose a $\textbf{Z}$ero-Sh$\textbf{o}$t $\textbf{R}$emote Sensing $\textbf{I}$nstance Segmentation framework, dubbed $\textbf{ZoRI}$. Our approach features a discrimination-enhanced classifier that uses refined textual embeddings to increase the awareness of class disparities. Instead of direct fine-tuning, we propose a knowledge-maintained adaptation strategy that decouples semantic-related information to preserve the pretrained vision-language alignment while adjusting features to capture remote sensing domain-specific visual cues. Additionally, we introduce a prior-injected prediction with cache bank of aerial visual prototypes to supplement the semantic richness of text embeddings and seamlessly integrate aerial representations, adapting to the remote sensing domain. We establish new experimental protocols and benchmarks, and extensive experiments convincingly demonstrate that ZoRI achieves the state-of-art performance on the zero-shot remote sensing instance segmentation task. Our code is available at https://github.com/HuangShiqi128/ZoRI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-17T11:00:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12798v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12798v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 More Tokens, Lower Precision: Towards the Optimal Token-Precision
  Trade-off in KV Cache Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiebin Zhang, Dawei Zhu, Yifan Song, Wenhao Wu, Chuqiao Kuang, Xiaoguang Li, Lifeng Shang, Qun Liu, Sujian Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) process increasing context windows, the memory usage of KV cache has become a critical bottleneck during inference. The mainstream KV compression methods, including KV pruning and KV quantization, primarily focus on either token or precision dimension and seldom explore the efficiency of their combination. In this paper, we comprehensively investigate the token-precision trade-off in KV cache compression. Experiments demonstrate that storing more tokens in the KV cache with lower precision, i.e., quantized pruning, can significantly enhance the long-context performance of LLMs. Furthermore, in-depth analysis regarding token-precision trade-off from a series of key aspects exhibit that, quantized pruning achieves substantial improvements in retrieval-related tasks and consistently performs well across varying input lengths. Moreover, quantized pruning demonstrates notable stability across different KV pruning methods, quantization strategies, and model scales. These findings provide valuable insights into the token-precision trade-off in KV cache compression. We plan to release our code in the near future.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-17T09:20:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12706v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12706v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 FiRST: Finetuning Router-Selective Transformers for Input-Adaptive
  Latency Reduction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across different domains such as vision and language processing. However, due to sequential processing through a stack of transformer layers, autoregressive decoding faces significant computation/latency challenges, particularly in resource-constrained environments like mobile and edge devices. Existing approaches in literature that aim to improve latency via skipping layers have two distinct flavors - 1) Early exit, and 2) Input-agnostic heuristics where tokens exit at pre-determined layers irrespective of input sequence. Both the above strategies have limitations - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework and the latter does not capture the variation in layer importance across tasks or more generally, across input sequences. To address both limitations, we propose FiRST, an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence - the prompt (during the prefill stage) decides which layers will be skipped during decoding. FiRST preserves compatibility with KV caching enabling faster inference while being quality-aware. FiRST is model-agnostic and can be easily enabled on any pre-trained LLM. Our approach reveals that input adaptivity is critical - indeed, different task-specific middle layers play a crucial role in evolving hidden representations depending on tasks. Extensive experiments show that FiRST significantly reduces latency while outperforming other layer selection strategies in quality metics. It retains competitive performance to base model (without layer skipping) and in some cases, even improves upon it. FiRST is thus a promising and efficient solution for LLM deployment in low-resource environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-17T09:11:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.12513v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.12513v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 TurboAttention: Efficient Attention Approximation For High Throughputs
  LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanism. While techniques, such as quantization and acceleration algorithms, like FlashAttention, have improved efficiency of the overall inference, they address different aspects of the problem: quantization focuses on weight-activation operations, while FlashAttention improves execution but requires high-precision formats. Recent Key-value (KV) cache quantization reduces memory bandwidth but still needs floating-point dequantization for attention operation.   We present TurboAttention, a comprehensive approach to enable quantized execution of attention that simultaneously addresses both memory and computational efficiency. Our solution introduces two key innovations: FlashQ, a headwise attention quantization technique that enables both compression of KV cache and quantized execution of activation-activation multiplication, and Sparsity-based Softmax Approximation (SAS), which eliminates the need for dequantization to FP32 during exponentiation operation in attention. Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup in attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x maximum throughput over the FP16 baseline while outperforming state-of-the-art quantization and compression techniques across various datasets and models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-17T05:40:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.08585v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.08585v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Personalized Federated Deep Reinforcement Learning for Heterogeneous
  Edge Content Caching Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhen Li, Tan Li, Hai Liu, Tse-Tin Chan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Proactive caching is essential for minimizing latency and improving Quality of Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement Learning (FDRL) is a promising approach for developing cache policies tailored to dynamic content requests. However, FDRL faces challenges such as an expanding caching action space due to increased content numbers and difficulty in adapting global information to heterogeneous edge environments. In this paper, we propose a Personalized Federated Deep Reinforcement Learning framework for Caching, called PF-DRL-Ca, with the aim to maximize system utility while satisfying caching capability constraints. To manage the expanding action space, we employ a new DRL algorithm, Multi-head Deep Q-Network (MH-DQN), which reshapes the action output layers of DQN into a multi-head structure where each head generates a sub-dimensional action. We next integrate the proposed MH-DQN into a personalized federated training framework, employing a layer-wise approach for training to derive a personalized model that can adapt to heterogeneous environments while exploiting the global information to accelerate learning convergence. Our extensive experimental results demonstrate the superiority of MH-DQN over traditional DRL algorithms on a single server, as well as the advantages of the personal federated training architecture compared to other frameworks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-17T05:09:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12543v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12543v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 A System for Microserving of LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongyi Jin, Ruihang Lai, Charlie F. Ruan, Yingcheng Wang, Todd C. Mowry, Xupeng Miao, Zhihao Jia, Tianqi Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The recent advances in LLMs bring a strong demand for efficient system support to improve overall serving efficiency. As LLM inference scales towards multiple GPUs and even multiple compute nodes, various coordination patterns, such as prefill-decode disaggregation and context migration, arise in serving systems. Most inference services today expose a coarse-grained request-level API with a pre-configured coordination strategy, limiting the ability to customize and dynamically reconfigure the coordination. In this paper, we propose LLM microserving, a multi-level architecture for structuring and programming LLM inference services. We introduces simple yet effective microserving APIs to support fine-grained sub-request level actions. A programmable router transforms user requests into sub-request calls, enabling the dynamic reconfiguration of serving patterns. To support diverse execution patterns, we develop a unified KV cache interface that handles various KV compute, transfer, and reuse scenarios. Our evaluation shows that LLM microserving can be reconfigured to support multiple disaggregation orchestration strategies in a few lines of Python code while maintaining state-of-the-art performance for LLM inference tasks. Additionally, it allows us to explore new strategy variants that reduce up to 47% of job completion time compared to the existing strategies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-17T02:44:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12488v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12488v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Yanyu Li, Yifan Gong, Kai Zhang, Hao Tan, Jason Kuen, Henghui Ding, Zhihao Shu, Wei Niu, Pu Zhao, Yanzhi Wang, Jiuxiang Gu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers have emerged as the preeminent models for a wide array of generative tasks, demonstrating superior performance and efficacy across various applications. The promising results come at the cost of slow inference, as each denoising step requires running the whole transformer model with a large amount of parameters. In this paper, we show that performing the full computation of the model at each diffusion step is unnecessary, as some computations can be skipped by lazily reusing the results of previous steps. Furthermore, we show that the lower bound of similarity between outputs at consecutive steps is notably high, and this similarity can be linearly approximated using the inputs. To verify our demonstrations, we propose the \textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached results from earlier steps to skip redundant computations. Specifically, we incorporate lazy learning layers into the model, effectively trained to maximize laziness, enabling dynamic skipping of redundant computations. Experimental results show that LazyDiT outperforms the DDIM sampler across multiple diffusion transformer models at various resolutions. Furthermore, we implement our method on mobile devices, achieving better performance than DDIM with similar latency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-17T01:12:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12444v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12444v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 The Selection Problem in Multi-Query Optimization: a Comprehensive
  Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sergey Zinchenko, Denis Ponomaryov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> View materialization, index selection, and plan caching are well-known techniques for optimization of query processing in database systems. The essence of these tasks is to select and save a subset of the most useful candidates (views/indexes/plans) for reuse within given space/time budget constraints. In this paper, based on the View Selection Problem, we propose a unified view on these problems. We identify the root causes of the complexity of these selection problems and provide a detailed analysis of techniques to cope with them. Our survey provides a modern classification of selection algorithms known in the literature, including the latest ones based on Machine Learning. We provide a ground for the reuse of the selection techniques between different optimization scenarios and highlight challenges and promising directions in the field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-16T14:49:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.DM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11828v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11828v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongxuan Zhang, Yao Zhao, Jiaqi Zheng, Chenyi Zhuang, Jinjie Gu, Guihai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The emergence of long-context text applications utilizing large language models (LLMs) has presented significant scalability challenges, particularly in memory footprint. The linear growth of the Key-Value (KV) cache responsible for storing attention keys and values to minimize redundant computations can lead to substantial increases in memory consumption, potentially causing models to fail to serve with limited memory resources. To address this issue, we propose a novel approach called Cache Sparse Representation (CSR), which converts the KV cache by transforming the dense Key-Value cache tensor into sparse indexes and weights, offering a more memory-efficient representation during LLM inference. Furthermore, we introduce NeuralDict, a novel neural network-based method for automatically generating the dictionary used in our sparse representation. Our extensive experiments demonstrate that CSR achieves performance comparable to state-of-the-art KV cache quantization algorithms while maintaining robust functionality in memory-constrained environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-16T13:01:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11741v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11741v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric
  Reduction and Restoration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenhao Sun, Rong-Cheng Tu, Jingyi Liao, Zhao Jin, Dacheng Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video Diffusion Transformers (DiTs) have demonstrated significant potential for generating high-fidelity videos but are computationally intensive. Existing acceleration methods include distillation, which requires costly retraining, and feature caching, which is highly sensitive to network architecture. Recent token reduction methods are training-free and architecture-agnostic, offering greater flexibility and wider applicability. However, they enforce the same sequence length across different components, constraining their acceleration potential. We observe that intra-sequence redundancy in video DiTs varies across features, blocks, and denoising timesteps. Building on this observation, we propose Asymmetric Reduction and Restoration (AsymRnR), a training-free approach to accelerate video DiTs. It offers a flexible and adaptive strategy that reduces the number of tokens based on their redundancy to enhance both acceleration and generation quality. We further propose matching cache to facilitate faster processing. Integrated into state-of-the-art video DiTs, AsymRnR achieves a superior speedup without compromising the quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-16T12:28:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11706v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11706v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite
  Pixel Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingchi Chen, Zhuoran Zheng, Xuerui Li, Yuying Chen, Shu Wang, Wenqi Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the continuous improvement of device imaging resolution, the popularity of Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing methods for fusing multi-exposure images in dynamic scenes are designed for low-resolution images, which makes them inefficient for generating high-quality UHD images on a resource-constrained device. To alleviate the limitations of extremely long-sequence inputs, inspired by the Large Language Model (LLM) for processing infinitely long texts, we propose a novel learning paradigm to achieve UHD multi-exposure dynamic scene image fusion on a single consumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our approach comes from three key components: The first step is to slice the input sequences to relieve the pressure generated by the model processing the data stream; Second, we develop an attention cache technique, which is similar to KV cache for infinite data stream processing; Finally, we design a method for attention cache compression to alleviate the storage burden of the cache on the device. In addition, we provide a new UHD benchmark to evaluate the effectiveness of our method. Extensive experimental results show that our method maintains high-quality visual performance while fusing UHD dynamic multi-exposure images in real-time (>40fps) on a single consumer-grade GPU.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-16T11:55:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11685v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11685v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 The "Huh?" Button: Improving Understanding in Educational Videos with
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Boris Ruf, Marcin Detyniecki
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose a simple way to use large language models (LLMs) in education. Specifically, our method aims to improve individual comprehension by adding a novel feature to online videos. We combine the low threshold for interactivity in digital experiences with the benefits of rephrased and elaborated explanations typical of face-to-face interactions, thereby supporting to close knowledge gaps at scale. To demonstrate the technical feasibility of our approach, we conducted a proof-of-concept experiment and implemented a prototype which is available for testing online. Through the use case, we also show how caching can be applied in LLM-powered applications to reduce their carbon footprint.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-15T21:02:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.14201v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.14201v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated
  Memory (Extended Version)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yupeng Tang, Seung-seob Lee, Abhishek Bhattacharjee, Anurag Khandelwal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Caches at CPU nodes in disaggregated memory architectures amortize the high data access latency over the network. However, such caches are fundamentally unable to improve performance for workloads requiring pointer traversals across linked data structures. We argue for accelerating these pointer traversals closer to disaggregated memory in a manner that preserves expressiveness for supporting various linked structures, ensures energy efficiency and performance, and supports distributed execution. We design PULSE, a distributed pointer-traversal framework for rack-scale disaggregated memory to meet all the above requirements. Our evaluation of PULSE shows that it enables low-latency, high-throughput, and energy-efficient execution for a wide range of pointer traversal workloads on disaggregated memory that fare poorly with caching alone.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-15T03:29:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2305.02388v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2305.02388v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained
  Reconfigurable Array</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaobing Ni, Mengke Ge, Jiaheng Ruan, Song Chen, Yi Kang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Streaming coarse-grained reconfgurable array (CGRA) is a promising architecture for data/computing-intensive applications because of its fexibility, high throughput and efcient memory system. However,when accelerating sparse CNNs, the irregular input data demands inside sparse CNNs would cause excessive caching operations (COPs) and multi-cycle internal dependencies (MCIDs) between operations, declining the throughput of the streaming CGRA. We propose a mapping method for sparse CNNs onto streaming CGRA, SparseMap, which incorporates an efcient I/O data management along with operation scheduling and binding, to reduce the COPs and MCIDs, thereby ensuring the optimal throughput of streaming CGRA.The experimental results show SparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even smaller initiation interval (II) compared to previous works.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-15T02:30:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11021v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11021v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Accelerating Retrieval-Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Derrick Quinn, Mohammad Nouri, Neel Patel, John Salihu, Alireza Salemi, Sukhan Lee, Hamed Zamani, Mohammad Alian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG.   In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7-26.3x lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM, which is the most expensive component in today's servers, from being stranded.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-14T06:47:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.AR</span><span>cs.DC</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.15246v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.15246v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic
  Service Provisioning in Software-Defined SDM-EONs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Baljinder Singh Heera, Shrinivas Petale, Yatindra Nath Singh, Suresh Subramaniam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The implementation of 5G and the future deployment of 6G necessitate the utilization of optical networks that possess substantial capacity and exhibit minimal latency. The dynamic arrival and departure of connection requests in optical networks result in particular central links experiencing more traffic and congestion than non-central links. The occurrence of congested links leads to service blocking despite the availability of resources within the network, restricting the efficient utilization of network resources. The available algorithms in the literature that aim to balance load among network links offer a trade-off between blocking performance and algorithmic complexity, thus increasing service provisioning time. This work proposes a dynamic routing-based congestion-aware routing, modulation, core, and spectrum assignment (RMCSA) algorithm for space division multiplexing elastic optical networks (SDM-EONs). The algorithm finds alternative candidate paths based on real-time link occupancy metrics to minimize blocking due to link congestion under dynamic traffic scenarios. As a result, the algorithm reduces the formation of congestion hotspots in the network owing to link-betweenness centrality. We have performed extensive simulations using two realistic network topologies to compare the performance of the proposed algorithm with relevant RMCSA algorithms available in the literature. The simulation results verify the superior performance of our proposed algorithm compared to the benchmark Yen's K-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection blocking ratio and spectrum utilization efficiency. To expedite the route-finding process, we present a novel caching strategy that allows the proposed algorithm to demonstrate a much-reduced service delay time compared to the recently developed adaptive link weight-based load-balancing RMCSA algorithm.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-12-14T05:20:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.10685v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.10685v1' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, Salman Khan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reasoning is a fundamental capability for solving complex multi-step problems, particularly in visual contexts where sequential step-wise understanding is essential. Existing approaches lack a comprehensive framework for evaluating visual reasoning and do not emphasize step-wise problem-solving. To this end, we propose a comprehensive framework for advancing step-by-step visual reasoning in large language models (LMMs) through three key contributions. First, we introduce a visual reasoning benchmark specifically designed to evaluate multi-step reasoning tasks. The benchmark presents a diverse set of challenges with eight different categories ranging from complex visual perception to scientific reasoning with over 4k reasoning steps in total, enabling robust evaluation of LLMs' abilities to perform accurate and interpretable visual reasoning across multiple steps. Second, we propose a novel metric that assesses visual reasoning quality at the granularity of individual steps, emphasizing both correctness and logical coherence. The proposed metric offers deeper insights into reasoning performance compared to traditional end-task accuracy metrics. Third, we present a new multimodal visual reasoning model, named LlamaV-o1, trained using a multi-step curriculum learning approach, where tasks are progressively organized to facilitate incremental skill acquisition and problem-solving. The proposed LlamaV-o1 is designed for multi-step reasoning and learns step-by-step through a structured training paradigm. Extensive experiments show that our LlamaV-o1 outperforms existing open-source models and performs favorably against close-source proprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an average score of 67.3 with an absolute gain of 3.8\% across six benchmarks while being 5 times faster during inference scaling. Our benchmark, model, and code are publicly available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T18:59:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06186v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06186v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Decentralized Diffusion Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> David McAllister, Matthew Tancik, Jiaming Song, Angjoo Kanazawa
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large-scale AI model training divides work across thousands of GPUs, then synchronizes gradients across them at each step. This incurs a significant network burden that only centralized, monolithic clusters can support, driving up infrastructure costs and straining power systems. We propose Decentralized Diffusion Models, a scalable framework for distributing diffusion model training across independent clusters or datacenters by eliminating the dependence on a centralized, high-bandwidth networking fabric. Our method trains a set of expert diffusion models over partitions of the dataset, each in full isolation from one another. At inference time, the experts ensemble through a lightweight router. We show that the ensemble collectively optimizes the same objective as a single model trained over the whole dataset. This means we can divide the training burden among a number of "compute islands," lowering infrastructure costs and improving resilience to localized GPU failures. Decentralized diffusion models empower researchers to take advantage of smaller, more cost-effective and more readily available compute like on-demand GPU nodes rather than central integrated systems. We conduct extensive experiments on ImageNet and LAION Aesthetics, showing that decentralized diffusion models FLOP-for-FLOP outperform standard diffusion models. We finally scale our approach to 24 billion parameters, demonstrating that high-quality diffusion models can now be trained with just eight individual GPU nodes in less than a week.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T18:58:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05450v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05450v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 S2-Attention: Hardware-Aware Context Sharding Among Attention Heads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xihui Lin, Yunan Zhang, Suyu Ge, Liliang Ren, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse attention, which selectively attends to a subset of tokens in the context was supposed to be efficient. However, its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up over its dense attention counterparts due to the lack of hardware-aware optimizations like FlashAttention. Meanwhile, it remains unclear whether sparse attention can maintain the model's quality at a scale of today's large language models (LLMs) and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library that provides kernel optimization for sparse attention customizable at both per-head and per-context-range levels. S2-Attention enables the exploration of novel and high-performance sparse attention techniques, which we demonstrate through extensive ablations across a wide range of sparse attention designs at various model scales. From these insights, we present several basic guidelines to design sparse attention that can achieve not only practical efficiency improvements, but also strong downstream performance. To achieve high parallelization and optimized memory IO, sparse attention should shard the context heterogeneously across attention heads, where each head attends to a different subset of tokens while collectively covering the full context. Meanwhile, we find hybrid architectures combining sparse and dense attention particularly beneficial in practice. S2-Attention achieves wall-clock speedup of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with strong downstream performance on-par with full attention and perfect retrieval performance at a 128k context length. At inference, for 7B models, our model, with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to dense counterparts. S2-Attention is released with easy-to-customize APIs for direct usage in Megatron and vLLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T18:45:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.17678v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.17678v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Endogenous Heteroskedasticity in Linear Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Javier Alejo, Antonio F. Galvao, Julian Martinez-Iriarte, Gabriel Montes-Rojas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Linear regressions with endogeneity are widely used to estimate causal effects. This paper studies a framework that has two common issues, endogeneity of the regressors, and heteroskedasticity that is allowed to depend on endogenous regressors, i.e., endogenous heteroskedasticity. We show that the presence of such conditional heteroskedasticity in the structural regression renders the two-stages least squares estimator inconsistent. To solve this issue, we propose sufficient conditions together with a control function approach to identify and estimate the causal parameters of interest. We establish the limiting properties of the estimator, say consistency and asymptotic normality, and propose inference procedures. Monte Carlo simulations provide evidence of the finite sample performance of the proposed methods, and evaluate different implementation procedures. We revisit an empirical application about job training to illustrate the methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T18:24:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>econ.EM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.02767v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.02767v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Understanding what helium absorption tells us about atmospheric escape
  from exoplanets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giulia Ballabio, James E. Owen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Atmospheric escape is now considered the major contributing factor in shaping the demographic of detected exoplanets. However, inferences about the exoplanet populations strongly depend on the accuracy of the models. Direct observational tests of atmospheric models are still in their infancy. Helium escape from planetary atmospheres has rapidly become the primary observational probe, already observed in $\gtrsim$20 exoplanets. Grounding our understanding in the basic physics of atmospheric escape, we present a new theoretical model to predict the excess absorption from the helium absorption line. We constrain the atmosphere properties, such as mass-loss rates and outflow temperatures, by implementing a Parker wind solution with an energy limited evaporating outflow. Importantly, we self-consistently link the mass-loss rates and outflow temperatures, which are critical to understanding helium absorption as the triplet-level population is typically exponentially sensitive to temperature. Furthermore, helium absorption is typically optically thin and the absorption is dominated far from the planet. Therefore, the absorption depth is not a measure of the size of the helium outflow. Our results indicate that for planets with a detectable signal, typically the helium triplet population in the atmosphere rapidly approaches a statistical equilibrium between populations by recombination and depopulation caused by electron collisions. We suggest that excess helium absorption can be quantified by a scaled equivalent width, which is positively correlated with the mass loss rate. We also show that the helium absorption scales with incident radiation, particularly with the XEUV to FUV flux ratios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T18:19:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.EP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06149v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06149v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Supervision policies can shape long-term risk management in
  general-purpose AI models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manuel Cebrian, Emilia Gomez, David Fernandez Llorca
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid proliferation and deployment of General-Purpose AI (GPAI) models, including large language models (LLMs), present unprecedented challenges for AI supervisory entities. We hypothesize that these entities will need to navigate an emergent ecosystem of risk and incident reporting, likely to exceed their supervision capacity. To investigate this, we develop a simulation framework parameterized by features extracted from the diverse landscape of risk, incident, or hazard reporting ecosystems, including community-driven platforms, crowdsourcing initiatives, and expert assessments. We evaluate four supervision policies: non-prioritized (first-come, first-served), random selection, priority-based (addressing the highest-priority risks first), and diversity-prioritized (balancing high-priority risks with comprehensive coverage across risk types). Our results indicate that while priority-based and diversity-prioritized policies are more effective at mitigating high-impact risks, particularly those identified by experts, they may inadvertently neglect systemic issues reported by the broader community. This oversight can create feedback loops that amplify certain types of reporting while discouraging others, leading to a skewed perception of the overall risk landscape. We validate our simulation results with several real-world datasets, including one with over a million ChatGPT interactions, of which more than 150,000 conversations were identified as risky. This validation underscores the complex trade-offs inherent in AI risk supervision and highlights how the choice of risk management policies can shape the future landscape of AI risks across diverse GPAI models used in society.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T17:52:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CY</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06137v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06137v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented
  Conversational AI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuya Asano, Sabit Hassan, Paras Sharma, Anthony Sicilia, Katherine Atwell, Diane Litman, Malihe Alikhani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> General-purpose automatic speech recognition (ASR) systems do not always perform well in goal-oriented dialogue. Existing ASR correction methods rely on prior user data or named entities. We extend correction to tasks that have no prior user data and exhibit linguistic flexibility such as lexical and syntactic variations. We propose a novel context augmentation with a large language model and a ranking strategy that incorporates contextual information from the dialogue states of a goal-oriented conversational AI and its tasks. Our method ranks (1) n-best ASR hypotheses by their lexical and semantic similarity with context and (2) context by phonetic correspondence with ASR hypotheses. Evaluated in home improvement and cooking domains with real-world users, our method improves recall and F1 of correction by 34% and 16%, respectively, while maintaining precision and false positive rate. Users rated .8-1 point (out of 5) higher when our correction method worked properly, with no decrease due to false positives.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T17:35:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06129v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06129v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Benchmarking Different Application Types across Heterogeneous Cloud
  Compute Services</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nivedhitha Duggi, Masoud Rafiei, Mohsen Amini Salehi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Infrastructure as a Service (IaaS) clouds have become the predominant underlying infrastructure for the operation of modern and smart technology. IaaS clouds have proven to be useful for multiple reasons such as reduced costs, increased speed and efficiency, and better reliability and scalability. Compute services offered by such clouds are heterogeneous -- they offer a set of architecturally diverse machines that fit efficiently executing different workloads. However, there has been little study to shed light on the performance of popular application types on these heterogeneous compute servers across different clouds. Such a study can help organizations to optimally (in terms of cost, latency, throughput, consumed energy, carbon footprint, etc.) employ cloud compute services. At HPCC lab, we have focused on such benchmarks in different research projects and, in this report, we curate those benchmarks in a single document to help other researchers in the community using them. Specifically, we introduce our benchmarks datasets for three application types in three different domains, namely: Deep Neural Networks (DNN) Inference for industrial applications, Machine Learning (ML) Inference for assistive technology applications, and video transcoding for multimedia use cases.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T17:33:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06128v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06128v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Inferring High-Order Couplings with Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aurélien Decelle, Alfonso de Jesús Navas Gómez, Beatriz Seoane
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Maximum-entropy methods, rooted in the inverse Ising/Potts problem from statistical mechanics, have become indispensable tools for modeling pairwise interactions in disciplines such as bioinformatics, ecology, and neuroscience. Despite their remarkable success, these methods often overlook high-order interactions that may be crucial in complex systems. Conversely, while modern machine learning approaches can capture such interactions, existing interpretable frameworks are computationally expensive, making it impractical to assess the relevance of high-order interactions in real-world scenarios. Restricted Boltzmann Machines (RBMs) offer a computationally efficient alternative by encoding statistical correlations via hidden nodes in a bipartite neural network. Here, we present a method that maps RBMs exactly onto generalized Potts models with interactions of arbitrary high order. This approach leverages large-$N$ approximations, facilitated by the simple architecture of the RBM, to enable the efficient extraction of effective many-body couplings with minimal computational cost. This mapping also enables the development of a general formal framework for the extraction of effective higher-order interactions in arbitrarily complex probabilistic models. Additionally, we introduce a robust formalism for gauge fixing within the generalized Potts model. We validate our method by accurately recovering two- and three-body interactions from synthetic datasets. Additionally, applying our framework to protein sequence data demonstrates its effectiveness in reconstructing protein contact maps, achieving performance comparable to state-of-the-art inverse Potts models. These results position RBMs as a powerful and efficient tool for investigating high-order interactions in complex systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T17:01:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.dis-nn</span><span>cond-mat.stat-mech</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06108v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06108v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Positivity violations in marginal structural survival models with
  time-dependent confounding: a simulation study on IPTW-estimator performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Marta Spreafico
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In longitudinal observational studies, marginal structural models (MSMs) are a class of causal models used to analyse the effect of an exposure on the (time-to-event) outcome of interest, while accounting for exposure-affected time-dependent confounding. In the applied literature, inverse probability of treatment weighting (IPTW) has been widely adopted to estimate MSMs. An essential assumption for IPTW-based MSMs is the positivity assumption, which ensures that, for any combination of measured confounders among individuals, there is a non-zero probability of receiving each possible treatment strategy. Positivity is crucial for valid causal inference through IPTW-based MSMs, but is often overlooked compared to confounding bias. Positivity violations may also arise due to randomness, in situations where the assignment to a specific treatment is theoretically possible but is either absent or rarely observed in the data, leading to near violations. These situations are common in practical applications, particularly when the sample size is small, and they pose significant challenges for causal inference. This study investigates the impact of near-positivity violations on estimates from IPTW-based MSMs in survival analysis. Two algorithms are proposed for simulating longitudinal data from hazard-MSMs, accommodating near-positivity violations, a time-varying binary exposure, and a time-to-event outcome. Cases of near-positivity violations, where remaining unexposed is rare within certain confounder levels, are analysed across various scenarios and weight truncation (WT) strategies. This work aims to serve as a critical warning against overlooking the positivity assumption or naively applying WT in causal studies using longitudinal observational data and IPTW.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T16:57:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.19606v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.19606v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 From Conversation to Automation: Leveraging Large Language Models to
  Analyze Strategies in Problem Solving Therapy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Elham Aghakhani, Lu Wang, Karla T. Washington, George Demiris, Jina Huh-Yoo, Rezvaneh Rezapour
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Problem-solving therapy (PST) is a structured psychological approach that helps individuals manage stress and resolve personal issues by guiding them through problem identification, solution brainstorming, decision-making, and outcome evaluation. As mental health care increasingly integrates technologies like chatbots and large language models (LLMs), understanding how PST can be effectively automated is important. This study leverages anonymized therapy transcripts to analyze and classify therapeutic interventions using various LLMs and transformer-based models. Our results show that GPT-4o achieved the highest accuracy (0.76) in identifying PST strategies, outperforming other models. Additionally, we introduced a new dimension of communication strategies that enhances the current PST framework, offering deeper insights into therapist-client interactions. This research demonstrates the potential of LLMs to automate complex therapeutic dialogue analysis, providing a scalable, efficient tool for mental health interventions. Our annotation framework can enhance the accessibility, effectiveness, and personalization of PST, supporting therapists in real-time with more precise, targeted interventions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T16:54:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06101v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06101v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Molecular Communication-Inspired Particle Collector-Transmitter (PaCoT)
  for Heavy Metal Removal from Human Circulatory System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hilal Esra Yaldiz, Ozgur B. Akan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study proposes a novel molecular communication (MC)-inspired nanomachine, PArticle COllector-Transmitter (PaCoT), to remove toxic heavy metals from the human circulatory system. PaCoT collects these toxic metals and transmits them to release nodes, such as lymph capillaries, before they reach critical organs. The design incorporates key physical parameters and operates through particle reception and release mechanisms. In the reception process, described as ligand-receptor binding reactions, modeled as a continuous-time Markov process (CTMP), PaCoT uses metallothionein proteins as receptors and heavy metals (e.g., Zn, Pb, Cd) as ligands. We assume that the toxicity condition (toxic (bit-1), non-toxic (bit-0)) is encoded into the concentration of heavy metal molecules. Thus, we consider that heavy metal concentration within the MC channel (e.g., human circulatory system) employs binary concentration shift keying (binary CSK). The concentration ratio of specific heavy metals is estimated to infer toxicity, i.e., a high ratio indicates toxicity and a low ratio suggests non-toxicity. Toxicity detection is achieved by monitoring the receptor bound duration in the presence of interferers and various types of heavy metals. After detecting and collecting toxic heavy metals, PaCoT securely retains them in a liquid medium (e.g., water) until release, employing two mechanisms: (1) a single-disc viscous micropump to regulate flow rate, and (2) Brownian motion to facilitate diffusion. PaCoT's performance is evaluated through MATLAB simulations, focusing on bit error probability (BEP) of the toxicity detection method, release time of molecules from PaCoT and energy consumption.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T16:45:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06092v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06092v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Explainable Federated Bayesian Causal Inference and Its Application in
  Advanced Manufacturing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaofeng Xiao, Khawlah Alharbi, Pengyu Zhang, Hantang Qin, Xubo Yue
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Causal inference has recently gained notable attention across various fields like biology, healthcare, and environmental science, especially within explainable artificial intelligence (xAI) systems, for uncovering the causal relationships among multiple variables and outcomes. Yet, it has not been fully recognized and deployed in the manufacturing systems. In this paper, we introduce an explainable, scalable, and flexible federated Bayesian learning framework, \texttt{xFBCI}, designed to explore causality through treatment effect estimation in distributed manufacturing systems. By leveraging federated Bayesian learning, we efficiently estimate posterior of local parameters to derive the propensity score for each client without accessing local private data. These scores are then used to estimate the treatment effect using propensity score matching (PSM). Through simulations on various datasets and a real-world Electrohydrodynamic (EHD) printing data, we demonstrate that our approach outperforms standard Bayesian causal inference methods and several state-of-the-art federated learning benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T16:14:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06077v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06077v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Distilling Calibration via Conformalized Credal Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy resources, and ensuring reliable performance in sensitive decision-making tasks. One way to enhance reliability is through uncertainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud-based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets -- ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Inference (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:57:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06066v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06066v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Bayesian inference for the Markov-modulated Poisson process with an
  outcome process</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu Luo, Chris Sherlock
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In medical research, understanding changes in outcome measurements is crucial for inferring shifts in health conditions. However, traditional methods often struggle with large, irregularly longitudinal data and fail to account for the tendency of individuals in poorer health to interact more frequently with the healthcare system. Additionally, clinical data can lack information on terminating events like death. To address these challenges, we start from the continuous-time hidden Markov model which models observed data as outcomes influenced by latent health states. Our extension incorporates a point process to account for the impact of health states on observation timings and includes a "death" state to model unobserved terminating events through a Poisson process, where transition rates depend on the latent health state. This approach captures both the severity of the disease and the timing of healthcare interactions. We present an exact Gibbs sampler procedure that alternates between sampling the latent health state paths and the model parameters. By including the "death" state, we mitigate biases in parameter estimation that would arise from solely modelling "live" health states. Simulation studies demonstrate that the proposed Gibbs sampler performs effectively. We apply our method to Canadian healthcare data, offering valuable insights for healthcare management.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:47:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.AP</span><span>stat.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.15314v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.15314v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Personalized Language Model Learning on Text Data Without User
  Identifiers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yucheng Ding, Yangwenjian Tan, Xiangyu Liu, Chaoyue Niu, Fandong Meng, Jie Zhou, Ning Liu, Fan Wu, Guihai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In many practical natural language applications, user data are highly sensitive, requiring anonymous uploads of text data from mobile devices to the cloud without user identifiers. However, the absence of user identifiers restricts the ability of cloud-based language models to provide personalized services, which are essential for catering to diverse user needs. The trivial method of replacing an explicit user identifier with a static user embedding as model input still compromises data anonymization. In this work, we propose to let each mobile device maintain a user-specific distribution to dynamically generate user embeddings, thereby breaking the one-to-one mapping between an embedding and a specific user. We further theoretically demonstrate that to prevent the cloud from tracking users via uploaded embeddings, the local distributions of different users should either be derived from a linearly dependent space to avoid identifiability or be close to each other to prevent accurate attribution. Evaluation on both public and industrial datasets using different language models reveals a remarkable improvement in accuracy from incorporating anonymous user embeddings, while preserving real-time inference requirement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:46:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06062v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06062v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 FaceMe: Robust Blind Face Restoration with Personal Identification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Siyu Liu, Zheng-Peng Duan, Jia OuYang, Jiayi Fu, Hyunhee Park, Zikun Liu, Chun-Le Guo, Chongyi Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Blind face restoration is a highly ill-posed problem due to the lack of necessary context. Although existing methods produce high-quality outputs, they often fail to faithfully preserve the individual's identity. In this paper, we propose a personalized face restoration method, FaceMe, based on a diffusion model. Given a single or a few reference images, we use an identity encoder to extract identity-related features, which serve as prompts to guide the diffusion model in restoring high-quality and identity-consistent facial images. By simply combining identity-related features, we effectively minimize the impact of identity-irrelevant features during training and support any number of reference image inputs during inference. Additionally, thanks to the robustness of the identity encoder, synthesized images can be used as reference images during training, and identity changing during inference does not require fine-tuning the model. We also propose a pipeline for constructing a reference image training pool that simulates the poses and expressions that may appear in real-world scenarios. Experimental results demonstrate that our FaceMe can restore high-quality facial images while maintaining identity consistency, achieving excellent performance and robustness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:44:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05177v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05177v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Axon: A novel systolic array architecture for improved run time and
  energy efficient GeMM and Conv operation with on-chip im2col</h2>
                <div class="authors">
                    <strong>Authors:</strong> Md Mizanur Rahaman Nayan, Ritik Raj, Gouse Basha Shaik, Tushar Krishna, Azad J Naeemi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> General matrix multiplication (GeMM) is a core operation in virtually all AI applications. Systolic array (SA) based architectures have shown great promise as GeMM hardware accelerators thanks to their speed and energy efficiency. Unfortunately, SAs incur a linear delay in filling the operands, due to unidirectional propagation via pipeline latches. In this work, we propose a novel in-array data orchestration technique in SAs where we enable data feeding on the principal diagonal followed by bi-directional propagation. This improves the runtime by up to 2X at minimal hardware overhead. In addition, the proposed data orchestration enables convolution lowering (known as im2col) using a simple hardware support to fully exploit input feature map reuse opportunity and significantly lower the off-chip memory traffic resulting in 1.2X throughput improvement and 2.17X inference energy reduction during YOLOv3 and RESNET50 workload on average. In contrast, conventional data orchestration would require more elaborate hardware and control signals to implement im2col in hardware because of the data skew. We have synthesized and conducted place and route for 16X16 systolic arrays based on the novel and conventional orchestrations using ASAP 7nm PDK and found that our proposed approach results in 0.211% area and 1.6% power overheads.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:24:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06043v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06043v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 LUMIA: Linear probing for Unimodal and MultiModal Membership Inference
  Attacks leveraging internal LLM states</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux, Joaquin Garcia-Alfaro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly used in a variety of applications, but concerns around membership inference have grown in parallel. Previous efforts focus on black-to-grey-box models, thus neglecting the potential benefit from internal LLM information. To address this, we propose the use of Linear Probes (LPs) as a method to detect Membership Inference Attacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed LUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner workings. We test this method across several model architectures, sizes and datasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA achieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous techniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an increment of 46.80% against the state of the art. Furthermore, our approach reveals key insights, such as the model layers where MIAs are most detectable. In multimodal models, LPs indicate that visual inputs can significantly contribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:08:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.19876v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.19876v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Generate, Transduct, Adapt: Iterative Transduction with VLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Oindrila Saha, Logan Lawrence, Grant Van Horn, Subhransu Maji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transductive zero-shot learning with vision-language models leverages image-image similarities within the dataset to achieve better classification accuracy compared to the inductive setting. However, there is little work that explores the structure of the language space in this context. We propose GTA-CLIP, a novel technique that incorporates supervision from language models for joint transduction in language and vision spaces. Our approach is iterative and consists of three steps: (i) incrementally exploring the attribute space by querying language models, (ii) an attribute-augmented transductive inference procedure, and (iii) fine-tuning the language and vision encoders based on inferred labels within the dataset. Through experiments with CLIP encoders, we demonstrate that GTA-CLIP, yields an average performance improvement of 8.6% and 3.7% across 12 datasets and 3 encoders, over CLIP and transductive CLIP respectively in the zero-shot setting. We also observe similar improvements in a few-shot setting. We present ablation studies that demonstrate the value of each step and visualize how the vision and language spaces evolve over iterations driven by the transductive learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:07:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06031v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06031v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Doubly-Robust Functional Average Treatment Effect Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lorenzo Testa, Tobia Boschi, Francesca Chiaromonte, Edward H. Kennedy, Matthew Reimherr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding causal relationships in the presence of complex, structured data remains a central challenge in modern statistics and science in general. While traditional causal inference methods are well-suited for scalar outcomes, many scientific applications demand tools capable of handling functional data -- outcomes observed as functions over continuous domains such as time or space. Motivated by this need, we propose DR-FoS, a novel method for estimating the Functional Average Treatment Effect (FATE) in observational studies with functional outcomes. DR-FoS exhibits double robustness properties, ensuring consistent estimation of FATE even if either the outcome or the treatment assignment model is misspecified. By leveraging recent advances in functional data analysis and causal inference, we establish the asymptotic properties of the estimator, proving its convergence to a Gaussian process. This guarantees valid inference with simultaneous confidence bands across the entire functional domain. Through extensive simulations, we show that DR-FoS achieves robust performance under a wide range of model specifications. Finally, we illustrate the utility of DR-FoS in a real-world application, analyzing functional outcomes to uncover meaningful causal insights in the SHARE (Survey of Health, Aging and Retirement in Europe) dataset.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:01:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>math.ST</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06024v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06024v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Modern Bayesian Sampling Methods for Cosmological Inference: A
  Comparative Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Denitsa Staicova
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a comprehensive comparison of different Markov Chain Monte Carlo (MCMC) sampling methods, evaluating their performance on both standard test problems and cosmological parameter estimation. Our analysis includes traditional Metropolis-Hastings MCMC, Hamiltonian Monte Carlo (HMC), slice sampling, nested sampling as implemented in dynesty, and PolyChord. We examine samplers through multiple metrics including runtime, memory usage, effective sample size, and parameter accuracy, testing their scaling with dimension and response to different probability distributions. While all samplers perform well with simple Gaussian distributions, we find that HMC and nested sampling show advantages for more complex distributions typical of cosmological problems. Traditional MCMC and slice sampling become less efficient in higher dimensions, while nested methods maintain accuracy but at higher computational cost. In cosmological applications using BAO data, we observe similar patterns, with particular challenges arising from parameter degeneracies and poorly constrained parameters.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:01:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06022v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06022v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 "Stupid robot, I want to speak to a human!" User Frustration Detection
  in Task-Oriented Dialog Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mireia Hernandez Caralt, Ivan Sekulić, Filip Carević, Nghia Khau, Diana Nicoleta Popa, Bruna Guedes, Victor Guimarães, Zeyu Yang, Andre Manso, Meghana Reddy, Paolo Rosso, Roland Mathis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Detecting user frustration in modern-day task-oriented dialog (TOD) systems is imperative for maintaining overall user satisfaction, engagement, and retention. However, most recent research is focused on sentiment and emotion detection in academic settings, thus failing to fully encapsulate implications of real-world user data. To mitigate this gap, in this work, we focus on user frustration in a deployed TOD system, assessing the feasibility of out-of-the-box solutions for user frustration detection. Specifically, we compare the performance of our deployed keyword-based approach, open-source approaches to sentiment analysis, dialog breakdown detection methods, and emerging in-context learning LLM-based detection. Our analysis highlights the limitations of open-source methods for real-world frustration detection, while demonstrating the superior performance of the LLM-based approach, achieving a 16\% relative improvement in F1 score on an internal benchmark. Finally, we analyze advantages and limitations of our methods and provide an insight into user frustration detection task for industry practitioners.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:59:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.17437v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.17437v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 ViM-Disparity: Bridging the Gap of Speed, Accuracy and Memory for
  Disparity Map Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maheswar Bora, Tushar Anand, Saurabh Atreya, Aritra Mukherjee, Abhijit Das
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work we propose a Visual Mamba (ViM) based architecture, to dissolve the existing trade-off for real-time and accurate model with low computation overhead for disparity map generation (DMG). Moreover, we proposed a performance measure that can jointly evaluate the inference speed, computation overhead and the accurateness of a DMG model. The code implementation and corresponding models are available at: https://github.com/MBora/ViM-Disparity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:40:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.16745v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.16745v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Are We Done with MMLU?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, Pasquale Minervini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error annotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700 manually re-annotated questions across all 57 MMLU subjects. We estimate that 6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported. Our results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:31:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.04127v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.04127v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Addressing speaker gender bias in large scale speech translation systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shubham Bansal, Vikas Joshi, Harveen Chadha, Rupeshkumar Mehta, Jinyu Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study addresses the issue of speaker gender bias in Speech Translation (ST) systems, which can lead to offensive and inaccurate translations. The masculine bias often found in large-scale ST systems is typically perpetuated through training data derived from Machine Translation (MT) systems. Our approach involves two key steps. First, we employ Large Language Models (LLMs) to rectify translations based on the speaker's gender in a cost-effective manner. Second, we fine-tune the ST model with the corrected data, enabling the model to generate gender-specific translations directly from audio cues, without the need for explicit gender input. Additionally, we propose a three-mode fine-tuned model for scenarios where the speaker's gender is either predefined or should not be inferred from speech cues. We demonstrate a 70% improvement in translations for female speakers compared to our baseline and other large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHE test set.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:20:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05989v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05989v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Exploring LLMs for Automated Pre-Testing of Cross-Cultural Surveys</h2>
                <div class="authors">
                    <strong>Authors:</strong> Divya Mani Adhikari, Vikram Kamath Cannanure, Alexander Hartland, Ingmar Weber
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Designing culturally relevant questionnaires for ICTD research is challenging, particularly when adapting surveys for populations to non-western contexts. Prior work adapted questionnaires through expert reviews and pilot studies, which are resource-intensive and time-consuming. To address these challenges, we propose using large language models (LLMs) to automate the questionnaire pretesting process in cross-cultural settings. Our study used LLMs to adapt a U.S.-focused climate opinion survey for a South African audience. We then tested the adapted questionnaire with 116 South African participants via Prolific, asking them to provide feedback on both versions. Participants perceived the LLM-adapted questions as slightly more favorable than the traditional version. Our note opens discussions on the potential role of LLMs in adapting surveys and facilitating cross-cultural questionnaire design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:17:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05985v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05985v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Describing Hadronization via Histories and Observables for Monte-Carlo
  Event Reweighting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Christian Bierlich, Phil Ilten, Tony Menzo, Stephen Mrenna, Manuel Szewc, Michael K. Wilkinson, Ahmed Youssef, Jure Zupan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce a novel method for extracting a fragmentation model directly from experimental data without requiring an explicit parametric form, called Histories and Observables for Monte-Carlo Event Reweighting (HOMER), consisting of three steps: the training of a classifier between simulation and data, the inference of single fragmentation weights, and the calculation of the weight for the full hadronization chain. We illustrate the use of HOMER on a simplified hadronization problem, a $q\bar{q}$ string fragmenting into pions, and extract a modified Lund string fragmentation function $f(z)$. We then demonstrate the use of HOMER on three types of experimental data: (i) binned distributions of high level observables, (ii) unbinned event-by-event distributions of these observables, and (iii) full particle cloud information. After demonstrating that $f(z)$ can be extracted from data (the inverse of hadronization), we also show that, at least in this limited setup, the fidelity of the extracted $f(z)$ suffers only limited loss when moving from (i) to (ii) to (iii). Public code is available at https://gitlab.com/uchep/mlhad.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:16:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>hep-ph</span><span>hep-ex</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.06342v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.06342v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study
  of LLM Hallucination on North Korea</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eunjung Cho, Won Ik Cho, Soomin Seo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Hallucination in large language models (LLMs) remains a significant challenge for their safe deployment, particularly due to its potential to spread misinformation. Most existing solutions address this challenge by focusing on aligning the models with credible sources or by improving how models communicate their confidence (or lack thereof) in their outputs. While these measures may be effective in most contexts, they may fall short in scenarios requiring more nuanced approaches, especially in situations where access to accurate data is limited or determining credible sources is challenging. In this study, we take North Korea - a country characterised by an extreme lack of reliable sources and the prevalence of sensationalist falsehoods - as a case study. We explore and evaluate how some of the best-performing multilingual LLMs and specific language-based models generate information about North Korea in three languages spoken in countries with significant geo-political interests: English (United States, United Kingdom), Korean (South Korea), and Mandarin Chinese (China). Our findings reveal significant differences, suggesting that the choice of model and language can lead to vastly different understandings of North Korea, which has important implications given the global security challenges the country poses.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:08:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05981v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05981v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Model Inversion in Split Learning for Personalized LLMs: New Insights
  from Information Bottleneck Theory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunmeng Shu, Shaofeng Li, Tian Dong, Yan Meng, Haojin Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Personalized Large Language Models (LLMs) have become increasingly prevalent, showcasing the impressive capabilities of models like GPT-4. This trend has also catalyzed extensive research on deploying LLMs on mobile devices. Feasible approaches for such edge-cloud deployment include using split learning. However, previous research has largely overlooked the privacy leakage associated with intermediate representations transmitted from devices to servers. This work is the first to identify model inversion attacks in the split learning framework for LLMs, emphasizing the necessity of secure defense. For the first time, we introduce mutual information entropy to understand the information propagation of Transformer-based LLMs and assess privacy attack performance for LLM blocks. To address the issue of representations being sparser and containing less information than embeddings, we propose a two-stage attack system in which the first part projects representations into the embedding space, and the second part uses a generative model to recover text from these embeddings. This design breaks down the complexity and achieves attack scores of 38%-75% in various scenarios, with an over 60% improvement over the SOTA. This work comprehensively highlights the potential privacy risks during the deployment of personalized LLMs on the edge side.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:47:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05965v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05965v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Learning a Consensus Sub-Network with Polarization Regularization and
  One Pass Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoying Zhi, Varun Babbar, Rundong Liu, Pheobe Sun, Fran Silavong, Ruibo Shi, Sean Moran
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a polarizing loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and training simultaneously, which saves energy in both the training and inference phases and avoids extra computational overhead from gating modules at inference time. Our results on CIFAR-10, CIFAR-100, and Tiny Imagenet suggest that our scheme can remove 50% of connections in deep networks with <1% reduction in classification accuracy. Compared to other related pruning methods, our method demonstrates a lower drop in accuracy for equivalent reductions in computational cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:43:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2302.10798v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2302.10798v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Estimation and Restoration of Unknown Nonlinear Distortion using
  Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michal Švento, Eloi Moliner, Lauri Juvela, Alec Wright, Vesa Välimäki
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The restoration of nonlinearly distorted audio signals, alongside the identification of the applied memoryless nonlinear operation, is studied. The paper focuses on the difficult but practically important case in which both the nonlinearity and the original input signal are unknown. The proposed method uses a generative diffusion model trained unconditionally on guitar or speech signals to jointly model and invert the nonlinear system at inference time. Both the memoryless nonlinear function model and the restored audio signal are obtained as output. Successful example case studies are presented including inversion of hard and soft clipping, digital quantization, half-wave rectification, and wavefolding nonlinearities. Our results suggest that, out of the nonlinear functions tested here, the cubic Catmull-Rom spline is best suited to approximating these nonlinearities. In the case of guitar recordings, comparisons with informed and supervised methods show that the proposed blind method is at least as good as they are in terms of objective metrics. Experiments on distorted speech show that the proposed blind method outperforms general-purpose speech enhancement techniques and restores the original voice quality. The proposed method can be applied to audio effects modeling, restoration of music and speech recordings, and characterization of analog recording media.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:40:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05959v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05959v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 On Large Language Models in Mission-Critical IT Governance: Are We Ready
  Yet?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi, Davide Taibi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Context. The security of critical infrastructure has been a pressing concern since the advent of computers and has become even more critical in today's era of cyber warfare. Protecting mission-critical systems (MCSs), essential for national security, requires swift and robust governance, yet recent events reveal the increasing difficulty of meeting these challenges. Aim. Building on prior research showcasing the potential of Generative AI (GAI), such as Large Language Models, in enhancing risk analysis, we aim to explore practitioners' views on integrating GAI into the governance of IT MCSs. Our goal is to provide actionable insights and recommendations for stakeholders, including researchers, practitioners, and policymakers. Method. We designed a survey to collect practical experiences, concerns, and expectations of practitioners who develop and implement security solutions in the context of MCSs. Conclusions and Future Works. Our findings highlight that the safe use of LLMs in MCS governance requires interdisciplinary collaboration. Researchers should focus on designing regulation-oriented models and focus on accountability; practitioners emphasize data protection and transparency, while policymakers must establish a unified AI framework with global benchmarks to ensure ethical and secure LLMs-based MCS governance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:35:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.ET</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11698v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11698v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent
  Conversion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sho Inoue, Shuai Wang, Wanxing Wang, Pengcheng Zhu, Mengxiao Bi, Haizhou Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In accented voice conversion or accent conversion, we seek to convert the accent in speech from one another while preserving speaker identity and semantic content. In this study, we formulate a novel method for creating multi-accented speech samples, thus pairs of accented speech samples by the same speaker, through text transliteration for training accent conversion systems. We begin by generating transliterated text with Large Language Models (LLMs), which is then fed into multilingual TTS models to synthesize accented English speech. As a reference system, we built a sequence-to-sequence model on the synthetic parallel corpus for accent conversion. We validated the proposed method for both native and non-native English speakers. Subjective and objective evaluations further validate our dataset's effectiveness in accent conversion studies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:23:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09352v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09352v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary
  Empirical Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyang Song, Mao Zheng, Xuan Luo, Yue Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Utilizing Large Language Models (LLMs) as evaluators to assess the performance of LLMs has garnered attention. However, this kind of evaluation approach is affected by potential biases within LLMs, raising concerns about the accuracy and reliability of the evaluation results of LLMs. To address this problem, we propose and study two many-shot In-Context Learning (ICL) prompt templates to help LLM evaluators mitigate potential biases: Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the former utilizes in-context examples with model-generated evaluation rationales as references, while the latter does not include these references. Using these prompt designs, we investigate the impact of increasing the number of in-context examples on the consistency and quality of the evaluation results. Experimental results show that advanced LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot and few-shot regimes. Furthermore, when using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the prompt template performs better than MSoR.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:23:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.11629v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.11629v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Hierarchical Control of Emotion Rendering in Speech Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sho Inoue, Kun Zhou, Shuai Wang, Haizhou Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emotional text-to-speech synthesis (TTS) aims to generate realistic emotional speech from input text. However, quantitatively controlling multi-level emotion rendering remains challenging. In this paper, we propose a diffusion-based emotional TTS framework with a novel approach for emotion intensity modeling to facilitate fine-grained control over emotion rendering at the phoneme, word, and utterance levels. We introduce a hierarchical emotion distribution (ED) extractor that captures a quantifiable ED embedding across different speech segment levels. Additionally, we explore various acoustic features and assess their impact on emotion intensity modeling. During TTS training, the hierarchical ED embedding effectively captures the variance in emotion intensity from the reference audio and correlates it with linguistic and speaker information. The TTS model not only generates emotional speech during inference, but also quantitatively controls the emotion rendering over the speech constituents. Both objective and subjective evaluations demonstrate the effectiveness of our framework in terms of speech quality, emotional expressiveness, and hierarchical emotion control.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:21:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.12498v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.12498v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Reusable specimen-level inference in computational pathology</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jakub R. Kaczmarzyk, Rishul Sharma, Peter K. Koo, Joel H. Saltz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Foundation models for computational pathology have shown great promise for specimen-level tasks and are increasingly accessible to researchers. However, specimen-level models built on these foundation models remain largely unavailable, hindering their broader utility and impact. To address this gap, we developed SpinPath, a toolkit designed to democratize specimen-level deep learning by providing a zoo of pretrained specimen-level models, a Python-based inference engine, and a JavaScript-based inference platform. We demonstrate the utility of SpinPath in metastasis detection tasks across nine foundation models. SpinPath may foster reproducibility, simplify experimentation, and accelerate the adoption of specimen-level deep learning in computational pathology research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:15:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.IV</span><span>cs.CV</span><span>q-bio.TO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05945v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05945v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 LitSumm: Large language models for literature summarisation of
  non-coding RNAs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrew Green, Carlos Ribas, Nancy Ontiveros-Palacios, Sam Griffiths-Jones, Anton I. Petrov, Alex Bateman, Blake Sweeney
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Curation of literature in life sciences is a growing challenge. The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases. Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.   In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for non-coding RNAs using large language models (LLMs). We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality.   We apply our tool to a selection of over 4,600 ncRNAs and make the generated summaries available via the RNAcentral resource. We conclude that automated literature summarization is feasible with the current generation of LLMs, provided careful prompting and automated checking are applied.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:01:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.GN</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2311.03056v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2311.03056v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact
  Convolutional Transformers and SAM2</h2>
                <div class="authors">
                    <strong>Authors:</strong> Olivier Morelle, Justus Bisten, Maximilian W. M. Wintergerst, Robert P. Finger, Thomas Schultz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Weakly supervised segmentation has the potential to greatly reduce the annotation effort for training segmentation models for small structures such as hyper-reflective foci (HRF) in optical coherence tomography (OCT). However, most weakly supervised methods either involve a strong downsampling of input images, or only achieve localization at a coarse resolution, both of which are unsatisfactory for small structures. We propose a novel framework that increases the spatial resolution of a traditional attention-based Multiple Instance Learning (MIL) approach by using Layer-wise Relevance Propagation (LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with iterative inference. Moreover, we demonstrate that replacing MIL with a Compact Convolutional Transformer (CCT), which adds a positional encoding, and permits an exchange of information between different regions of the OCT image, leads to a further and substantial increase in segmentation accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T12:56:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05933v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05933v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 LLMs Reproduce Stereotypes of Sexual and Gender Minorities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruby Ostrow, Adam Lopez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A large body of research has found substantial gender bias in NLP systems. Most of this research takes a binary, essentialist view of gender: limiting its variation to the categories _men_ and _women_, conflating gender with sex, and ignoring different sexual identities. But gender and sexuality exist on a spectrum, so in this paper we study the biases of large language models (LLMs) towards sexual and gender minorities beyond binary categories. Grounding our study in a widely used psychological framework -- the Stereotype Content Model -- we demonstrate that English-language survey questions about social perceptions elicit more negative stereotypes of sexual and gender minorities from LLMs, just as they do from humans. We then extend this framework to a more realistic use case: text generation. Our analysis shows that LLMs generate stereotyped representations of sexual and gender minorities in this setting, raising concerns about their capacity to amplify representational harms in creative writing, a widely promoted use case.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T12:46:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05926v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05926v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Navigating Tomorrow: Reliably Assessing Large Language Models
  Performance on Future Event Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Petraq Nako, Adam Jatowt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Predicting future events is an important activity with applications across multiple fields and domains. For example, the capacity to foresee stock market trends, natural disasters, business developments, or political events can facilitate early preventive measures and uncover new opportunities. Multiple diverse computational methods for attempting future predictions, including predictive analysis, time series forecasting, and simulations have been proposed. This study evaluates the performance of several large language models (LLMs) in supporting future prediction tasks, an under-explored domain. We assess the models across three scenarios: Affirmative vs. Likelihood questioning, Reasoning, and Counterfactual analysis. For this, we create a dataset1 by finding and categorizing news articles based on entity type and its popularity. We gather news articles before and after the LLMs training cutoff date in order to thoroughly test and compare model performance. Our research highlights LLMs potential and limitations in predictive modeling, providing a foundation for future improvements.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T12:44:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05925v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05925v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Flexible inference in heterogeneous and attributed multilayer networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Martina Contisciani, Marius Hobbhahn, Eleanor A. Power, Philipp Hennig, Caterina De Bacco
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Networked datasets can be enriched by different types of information about individual nodes or edges. However, most existing methods for analyzing such datasets struggle to handle the complexity of heterogeneous data, often requiring substantial model-specific analysis. In this paper, we develop a probabilistic generative model to perform inference in multilayer networks with arbitrary types of information. Our approach employs a Bayesian framework combined with the Laplace matching technique to ease interpretation of inferred parameters. Furthermore, the algorithmic implementation relies on automatic differentiation, avoiding the need for explicit derivations. This makes our model scalable and flexible to adapt to any combination of input data. We demonstrate the effectiveness of our method in detecting overlapping community structures and performing various prediction tasks on heterogeneous multilayer data, where nodes and edges have different types of attributes. Additionally, we showcase its ability to unveil a variety of patterns in a social support network among villagers in rural India by effectively utilizing all input information in a meaningful way.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T12:15:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SI</span><span>physics.data-an</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.20918v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.20918v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Hybrid Fusion for 802.11ax Wi-Fi-based Passive Radars Exploiting
  Beamforming Feedbacks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Martin Willame, Hasan Can Yildirim, Laurent Storrer, Francois Horlin, Jerome Louveaux
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Passive Wi-Fi-based radars (PWRs) are devices that enable the localization of targets using Wi-Fi signals of opportunity transmitted by an access point. Unlike active radars that optimize their transmitted waveform for localization, PWRs align with the 802.11 amendments. Specifically, during the channel sounding session preceding a multi-user multiple-input multiple-output downlink transmission, an access point isotropically transmits a null data packet (NDP) with a known preamble. From these known symbols, client user equipments derive their channel state information and transmit an unencrypted beamforming feedback (BFF) back to the access point. The BFF comprises the right singular matrix of the channel and the corresponding stream gain for each subcarrier, which allows the computation of a beamforming matrix at the access point. In a classical PWR processing, only the preamble symbols from the NDP are exploited during the channel sounding session. In this study, we investigate multiple target localization by a PWR exploiting hybrid information sources. On one hand, the joint angle-of-departure and angle-of-arrival evaluated from the NDP. On another hand, the line-of-sight angle-of-departures inferred from the BFFs. The processing steps at the PWR are defined and an optimal hybrid fusion rule is derived in the maximum likelihood framework. Monte-Carlo simulations assess the enhanced accuracy of the proposed combination method compared to classical PWR processing based solely on the NDP, and compare the localisation performance between client and non-client targets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T12:14:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.14269v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.14269v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Prompt engineering and its implications on the energy consumption of
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Riccardo Rubei, Aicha Moussaid, Claudio di Sipio, Davide di Ruscio
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reducing the environmental impact of AI-based software systems has become critical. The intensive use of large language models (LLMs) in software engineering poses severe challenges regarding computational resources, data centers, and carbon emissions. In this paper, we investigate how prompt engineering techniques (PETs) can impact the carbon emission of the Llama 3 model for the code generation task. We experimented with the CodeXGLUE benchmark to evaluate both energy consumption and the accuracy of the generated code using an isolated testing environment. Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts. Even though a more in-depth evaluation is needed to confirm our findings, this work suggests that prompt engineering can reduce LLMs' energy consumption during the inference phase without compromising performance, paving the way for further investigations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:49:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05899v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05899v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Beyond Flat Text: Dual Self-inherited Guidance for Visual Text
  Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minxing Luo, Zixun Xia, Liaojun Chen, Zhenhang Li, Weichao Zeng, Jianye Wang, Wentao Cheng, Yaxing Wang, Yu Zhou, Jian Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In real-world images, slanted or curved texts, especially those on cans, banners, or badges, appear as frequently, if not more so, than flat texts due to artistic design or layout constraints. While high-quality visual text generation has become available with the advanced generative capabilities of diffusion models, these models often produce distorted text and inharmonious text background when given slanted or curved text layouts due to training data limitation. In this paper, we introduce a new training-free framework, STGen, which accurately generates visual texts in challenging scenarios (\eg, slanted or curved text layouts) while harmonizing them with the text background. Our framework decomposes the visual text generation process into two branches: (i) \textbf{Semantic Rectification Branch}, which leverages the ability in generating flat but accurate visual texts of the model to guide the generation of challenging scenarios. The generated latent of flat text is abundant in accurate semantic information related both to the text itself and its background. By incorporating this, we rectify the semantic information of the texts and harmonize the integration of the text with its background in complex layouts. (ii) \textbf{Structure Injection Branch}, which reinforces the visual text structure during inference. We incorporate the latent information of the glyph image, rich in glyph structure, as a new condition to further strengthen the text structure. To enhance image harmony, we also apply an effective combination method to merge the priors, providing a solid foundation for generation. Extensive experiments across a variety of visual text layouts demonstrate that our framework achieves superior accuracy and outstanding quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:44:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05892v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05892v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Affordably Fine-tuned LLMs Provide Better Answers to Course-specific
  MCQs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bianca Raimondi, Saverio Giallorenzo, Maurizio Gabbrielli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In education, the capability of generating human-like text of Large Language Models (LLMs) inspired work on how they can increase the efficiency of learning and teaching. We study the affordability of these models for educators and students by investigating how LLMs answer multiple-choice questions (MCQs) with respect to hardware constraints and refinement techniques. We explore this space by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of LLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming Languages (PL) -- the MCQ dataset is a contribution of this work, which we make publicly available. Specifically, we dissect how different factors, such as using readily-available material -- (parts of) the course's textbook -- for fine-tuning and quantisation (to decrease resource usage) can change the accuracy of the responses. The main takeaway is that smaller textbook-based fine-tuned models outperform generic larger ones (whose pre-training requires conspicuous resources), making the usage of LLMs for answering MCQs resource- and material-wise affordable.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:44:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05891v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05891v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Estimating variability power spectra of active galaxies from irregular
  time series</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mehdy Lefkir, Simon Vaughan, Daniela Huppenkothen, Phil Uttley, Vysakh Anilkumar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A common feature of Active Galactic Nuclei (AGN) is their random variations in brightness across the whole emission spectrum, from radio to $\gamma$-rays. Studying the nature and origin of these fluctuations is critical to characterising the underlying variability process of the accretion flow that powers AGN. Random timing fluctuations are often studied with the power spectrum; this quantifies how the amplitude of variations is distributed over temporal frequencies. Red noise variability -- when the power spectrum increases smoothly towards low frequencies -- is ubiquitous in AGN. The commonly used Fourier analysis methods, have significant challenges when applied to arbitrarily sampled light curves of red noise variability. Several time-domain methods exist to infer the power spectral shape in the case of irregular sampling but they suffer from biases which can be difficult to mitigate, or are computationally expensive. In this paper, we demonstrate a method infer the shape of broad-band power spectra for irregular time series, using a Gaussian process regression method scalable to large datasets. The power spectrum is modelled as a power-law model with one or two bends with flexible slopes. The method is fully Bayesian and we demonstrate its utility using simulated light curves. Finally, Ark 564, a well-known variable Seyfert 1 galaxy, is used as a test case and we find consistent results with the literature using independent X-ray data from XMM-Newton and Swift. We provide publicly available, documented and tested implementations in Python and Julia.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:39:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05886v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05886v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster
  Context Attention, Better Feature Fusion, and Hardware Acceleration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhifan Song, Yuan Zhang, Abd Al Rahman M. Abu Ebayyeh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Detecting small targets in drone imagery is challenging due to low resolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel edge-target detection framework built on an enhanced YOLOv10 architecture, optimized for real-time applications without post-processing. EDNet incorporates an XSmall detection head and a Cross Concat strategy to improve feature fusion and multi-scale context awareness for detecting tiny targets in diverse environments. Our unique C2f-FCA block employs Faster Context Attention to enhance feature extraction while reducing computational complexity. The WIoU loss function is employed for improved bounding box regression. With seven model sizes ranging from Tiny to XL, EDNet accommodates various deployment environments, enabling local real-time inference and ensuring data privacy. Notably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer parameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16 to 55 FPS, providing a scalable and efficient solution for edge-based object detection in challenging drone imagery. The source code and pre-trained models are available at: https://github.com/zsniko/EDNet.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:37:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/SWC62898.2024.00141' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.05885v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05885v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal
  LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dabing Cheng, Haosen Zhan, Xingchen Zhao, Guisheng Liu, Zemin Li, Jinghui Xie, Zhao Song, Weiguo Feng, Bingyue Peng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The exponential growth of short-video content has ignited a surge in the necessity for efficient, automated solutions to video editing, with challenges arising from the need to understand videos and tailor the editing according to user requirements. Addressing this need, we propose an innovative end-to-end foundational framework, ultimately actualizing precise control over the final video content editing. Leveraging the flexibility and generalizability of Multimodal Large Language Models (MLLMs), we defined clear input-output mappings for efficient video creation. To bolster the model's capability in processing and comprehending video content, we introduce a strategic combination of a denser frame rate and a slow-fast processing technique, significantly enhancing the extraction and understanding of both temporal and spatial video information. Furthermore, we introduce a text-to-edit mechanism that allows users to achieve desired video outcomes through textual input, thereby enhancing the quality and controllability of the edited videos. Through comprehensive experimentation, our method has not only showcased significant effectiveness within advertising datasets, but also yields universally applicable conclusions on public datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:35:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05884v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05884v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 TakuNet: an Energy-Efficient CNN for Real-Time Inference on Embedded UAV
  systems in Emergency Response Scenarios</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Rossi, Guido Borghi, Roberto Vezzani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Designing efficient neural networks for embedded devices is a critical challenge, particularly in applications requiring real-time performance, such as aerial imaging with drones and UAVs for emergency responses. In this work, we introduce TakuNet, a novel light-weight architecture which employs techniques such as depth-wise convolutions and an early downsampling stem to reduce computational complexity while maintaining high accuracy. It leverages dense connections for fast convergence during training and uses 16-bit floating-point precision for optimization on embedded hardware accelerators. Experimental evaluation on two public datasets shows that TakuNet achieves near-state-of-the-art accuracy in classifying aerial images of emergency situations, despite its minimal parameter count. Real-world tests on embedded devices, namely Jetson Orin Nano and Raspberry Pi, confirm TakuNet's efficiency, achieving more than 650 fps on the 15W Jetson board, making it suitable for real-time AI processing on resource-constrained platforms and advancing the applicability of drones in emergency scenarios. The code and implementation details are publicly released.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:32:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05880v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05880v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 A Neighbor-based Approach to Pitch Ownership Models in Soccer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tiago Mendes-Neves, Luís Meireles, João Mendes-Moreira
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Pitch ownership models allow many types of analysis in soccer and provide valuable assistance to tactical analysts in understanding the game's dynamics. The novelty they provide over event-based analysis is that tracking data incorporates context that event-based data does not possess, like player positioning. This paper proposes a novel approach to building pitch ownership models in soccer games using the K-Nearest Neighbors (KNN) algorithm. Our approach provides a fast inference mechanism that can model different approaches to pitch control using the same algorithm. Despite its flexibility, it uses only three hyperparameters to tune the model, facilitating the tuning process for different player skill levels. The flexibility of the approach allows for the emulation of different methods available in the literature by adjusting a small number of parameters, including adjusting for different levels of uncertainty. In summary, the proposed model provides a new and more flexible strategy for building pitch ownership models, extending beyond just replicating existing algorithms, and can provide valuable insights for tactical analysts and open up new avenues for future research. We thoroughly visualize several examples demonstrating the presented models' strengths and weaknesses. The code is available at github.com/nvsclub/KNNPitchControl.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:11:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05870v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05870v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Bayesian Transfer Learning for Artificially Intelligent Geospatial
  Systems: A Predictive Stacking Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luca Presicce, Sudipto Banerjee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Building artificially intelligent geospatial systems require rapid delivery of spatial data analysis at massive scales with minimal human intervention. Depending upon their intended use, data analysis may also entail model assessment and uncertainty quantification. This article devises transfer learning frameworks for deployment in artificially intelligent systems, where a massive data set is split into smaller data sets that stream into the analytical framework to propagate learning and assimilate inference for the entire data set. Specifically, we introduce Bayesian predictive stacking for multivariate spatial data and demonstrate its effectiveness in rapidly analyzing massive data sets. Furthermore, we make inference feasible in a reasonable amount of time, and without excessively demanding hardware settings. We illustrate the effectiveness of this approach in extensive simulation experiments and subsequently analyze massive data sets in climate science on sea surface temperatures and on vegetation index.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:03:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.09504v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.09504v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 ConSim: Measuring Concept-Based Explanations' Effectiveness with
  Automated Simulatability</h2>
                <div class="authors">
                    <strong>Authors:</strong> Antonin Poché, Alon Jacovi, Agustin Martin Picard, Victor Boutin, Fanny Jourdan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter. We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulator's ability to predict the explained model's outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at https://github.com/AnonymousConSim/ConSim
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:53:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05855v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05855v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junsheng Luan, Guangyuan Li, Lei Zhao, Wei Xing
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Virtual try-on methods based on diffusion models achieve realistic try-on effects. They use an extra reference network or an additional image encoder to process multiple conditional image inputs, which adds complexity pre-processing and additional computational costs. Besides, they require more than 25 inference steps, bringing longer inference time. In this work, with the development of diffusion transformer (DiT), we rethink the necessity of additional reference network or image encoder and introduce MC-VTON, which leverages DiT's intrinsic backbone to seamlessly integrate minimal conditional try-on inputs. Compared to existing methods, the superiority of MC-VTON is demonstrated in four aspects: (1) Superior detail fidelity. Our DiT-based MC-VTON exhibits superior fidelity in preserving fine-grained details. (2) Simplified network and inputs. We remove any extra reference network or image encoder. We also remove unnecessary conditions like the long prompt, pose estimation, human parsing, and depth map. We require only the masked person image and the garment image. (3) Parameter-efficient training. To process the try-on task, we fine-tune the FLUX.1-dev with only 39.7M additional parameters (0.33% of the backbone parameters). (4) Less inference steps. We apply distillation diffusion on MC-VTON and only need 8 steps to generate a realistic try-on image, with only 86.8M additional parameters (0.72% of the backbone parameters). Experiments show that MC-VTON achieves superior qualitative and quantitative results with fewer condition inputs, trainable parameters, and inference steps than baseline methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:45:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.03630v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.03630v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 VLM-driven Behavior Tree for Context-aware Task Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Kazuhiro Sasabuchi, Katsushi Ikeuchi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The use of Large Language Models (LLMs) for generating Behavior Trees (BTs) has recently gained attention in the robotics community, yet remains in its early stages of development. In this paper, we propose a novel framework that leverages Vision-Language Models (VLMs) to interactively generate and edit BTs that address visual conditions, enabling context-aware robot operations in visually complex environments. A key feature of our approach lies in the conditional control through self-prompted visual conditions. Specifically, the VLM generates BTs with visual condition nodes, where conditions are expressed as free-form text. Another VLM process integrates the text into its prompt and evaluates the conditions against real-world images during robot execution. We validated our framework in a real-world cafe scenario, demonstrating both its feasibility and limitations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:38:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.CV</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.03968v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.03968v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Long Story Short: Story-level Video Understanding from 20K Short Films</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, Ivan Laptev
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent developments in vision-language models have significantly advanced video understanding. Existing datasets and tasks, however, have notable limitations. Most datasets are confined to short videos with limited events and narrow narratives. For example, datasets with instructional and egocentric videos often depict activities of one person in a single scene. Although existing movie datasets offer richer content, they are often limited to short-term tasks, lack publicly available videos, and frequently encounter data leakage issues given the use of subtitles and other information about commercial movies during LLM pretraining. To address the above limitations, we propose Short-Films 20K (SF20K), the largest publicly available movie dataset. SF20K is composed of 20,143 amateur films and offers long-term video tasks in the form of multiple-choice and open-ended question answering. Our extensive analysis of SF20K reveals minimal data leakage, emphasizes the need for long-term reasoning, and demonstrates the strong performance of recent VLMs. Finally, we show that instruction tuning on the SF20K-Train set substantially improves model performance, paving the way for future progress in long-term video understanding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:36:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.10221v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.10221v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Benchmarking Uncertainty Quantification Methods for Large Language
  Models with LM-Polygraph</h2>
                <div class="authors">
                    <strong>Authors:</strong> Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Kirill Grishchenkov, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, Artem Shelmanov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid proliferation of large language models (LLMs) has stimulated researchers to seek effective and efficient approaches to deal with LLM hallucinations and low-quality outputs. Uncertainty quantification (UQ) is a key element of machine learning applications in dealing with such challenges. However, research to date on UQ for LLMs has been fragmented in terms of techniques and evaluation methodologies. In this work, we address this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines and offers an environment for controllable and consistent evaluation of novel UQ techniques over various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across eleven tasks, identifying the most effective approaches. Code: https://github.com/IINemo/lm-polygraph Benchmark: https://huggingface.co/LM-Polygraph
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:24:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.15627v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.15627v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Causal survival analysis, Estimation of the Average Treatment Effect
  (ATE): Practical Recommendations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Charlotte Voinot, Clément Berenfeld, Imke Mayer, Bernard Sebastien, Julie Josse
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Causal survival analysis combines survival analysis and causal inference to evaluate the effect of a treatment or intervention on a time-to-event outcome, such as survival time. It offers an alternative to relying solely on Cox models for assessing these effects. In this paper, we present a comprehensive review of estimators for the average treatment effect measured with the restricted mean survival time, including regression-based methods, weighting approaches, and hybrid techniques. We investigate their theoretical properties and compare their performance through extensive numerical experiments. Our analysis focuses on the finite-sample behavior of these estimators, the influence of nuisance parameter selection, and their robustness and stability under model misspecification. By bridging theoretical insights with practical evaluation, we aim to equip practitioners with both state-of-the-art implementations of these methods and practical guidelines for selecting appropriate estimators for treatment effect estimation. Among the approaches considered, G-formula two-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival forests emerge as particularly promising.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:23:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05836v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05836v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 MoColl: Agent-Based Specific and General Model Collaboration for Image
  Captioning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pu Yang, Bin Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Image captioning is a critical task at the intersection of computer vision and natural language processing, with wide-ranging applications across various domains. For complex tasks such as diagnostic report generation, deep learning models require not only domain-specific image-caption datasets but also the incorporation of relevant general knowledge to provide contextual accuracy. Existing approaches exhibit inherent limitations: specialized models excel in capturing domain-specific details but lack generalization, while vision-language models (VLMs) built on large language models (LLMs) leverage general knowledge but struggle with domain-specific adaptation. To address these limitations, this paper proposes a novel agent-enhanced model collaboration framework, which we call MoColl, designed to effectively integrate domain-specific and general knowledge. Specifically, our approach is to decompose complex image captioning tasks into a series of interconnected question-answer subtasks. A trainable visual question answering (VQA) model is employed as a specialized tool to focus on domain-specific visual analysis, answering task-specific questions based on image content. Concurrently, an LLM-based agent with general knowledge formulates these questions and synthesizes the resulting question-answer pairs into coherent captions. Beyond its role in leveraging the VQA model, the agent further guides its training to enhance its domain-specific capabilities. Experimental results on radiology report generation validate the effectiveness of the proposed framework, demonstrating significant improvements in the quality of generated reports.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:08:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.01834v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.01834v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Towards a Multimodal Large Language Model with Pixel-Level Insight for
  Biomedicine</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, Yehui Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In recent years, Multimodal Large Language Models (MLLM) have achieved notable advancements, demonstrating the feasibility of developing an intelligent biomedical assistant. However, current biomedical MLLMs predominantly focus on image-level understanding and restrict interactions to textual commands, thus limiting their capability boundaries and the flexibility of usage. In this paper, we introduce a novel end-to-end multimodal large language model for the biomedical domain, named MedPLIB, which possesses pixel-level understanding. Excitingly, it supports visual question answering (VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form shapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE) multi-stage training strategy, which divides MoE into separate training phases for a visual-language expert model and a pixel-grounding expert model, followed by fine-tuning using MoE. This strategy effectively coordinates multitask learning while maintaining the computational cost at inference equivalent to that of a single expert model. To advance the research of biomedical MLLMs, we introduce the Medical Complex Vision Question Answering Dataset (MeCoVQA), which comprises an array of 8 modalities for complex medical imaging question answering and image region understanding. Experimental results indicate that MedPLIB has achieved state-of-the-art outcomes across multiple medical visual language tasks. More importantly, in zero-shot evaluations for the pixel grounding task, MedPLIB leads the best small and large models by margins of 19.7 and 15.6 respectively on the mDice metric. The codes, data, and model checkpoints will be made publicly available at https://github.com/ShawnHuang497/MedPLIB.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:07:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.09278v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.09278v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Threshold Neuron: A Brain-inspired Artificial Neuron for Efficient
  On-device Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zihao Zheng, Yuanchun Li, Jiayu Chen, Peng Zhou, Xiang Chen, Yunxin Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Enhancing the computational efficiency of on-device Deep Neural Networks (DNNs) remains a significant challengein mobile and edge computing. As we aim to execute increasingly complex tasks with constrained computational resources, much of the research has focused on compressing neural network structures and optimizing systems. Although many studies have focused on compressing neural network structures and parameters or optimizing underlying systems, there has been limited attention on optimizing the fundamental building blocks of neural networks: the neurons. In this study, we deliberate on a simple but important research question: Can we design artificial neurons that offer greater efficiency than the traditional neuron paradigm? Inspired by the threshold mechanisms and the excitation-inhibition balance observed in biological neurons, we propose a novel artificial neuron model, Threshold Neurons. Using Threshold Neurons, we can construct neural networks similar to those with traditional artificial neurons, while significantly reducing hardware implementation complexity. Our extensive experiments validate the effectiveness of neural networks utilizing Threshold Neurons, achieving substantial power savings of 7.51x to 8.19x and area savings of 3.89x to 4.33x at the kernel level, with minimal loss in precision. Furthermore, FPGA-based implementations of these networks demonstrate 2.52x power savings and 1.75x speed enhancements at the system level. The source code will be made available upon publication.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T09:55:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.13902v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.13902v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and
  Improving Large Language Model Safety</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paul Röttger, Fabio Pernisi, Bertie Vidgen, Dirk Hovy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The last two years have seen a rapid growth in concerns around the safety of large language models (LLMs). Researchers and practitioners have met these concerns by creating an abundance of datasets for evaluating and improving LLM safety. However, much of this work has happened in parallel, and with very different goals in mind, ranging from the mitigation of near-term risks around bias and toxic content generation to the assessment of longer-term catastrophic risk potential. This makes it difficult for researchers and practitioners to find the most relevant datasets for their use case, and to identify gaps in dataset coverage that future work may fill. To remedy these issues, we conduct a first systematic review of open datasets for evaluating and improving LLM safety. We review 144 datasets, which we identified through an iterative and community-driven process over the course of several months. We highlight patterns and trends, such as a trend towards fully synthetic datasets, as well as gaps in dataset coverage, such as a clear lack of non-English and naturalistic datasets. We also examine how LLM safety datasets are used in practice -- in LLM release publications and popular LLM benchmarks -- finding that current evaluation practices are highly idiosyncratic and make use of only a small fraction of available datasets. Our contributions are based on SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we plan to update continuously as the field of LLM safety develops.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T09:54:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.05399v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.05399v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial
  Expressions Using Static Expression Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yin Chen, Jia Li, Yu Zhang, Zhenzhen Hu, Shiguang Shan, Meng Wang, Richang Hong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Dynamic facial expression recognition (DFER) infers emotions from the temporal evolution of expressions, unlike static facial expression recognition (SFER), which relies solely on a single snapshot. This temporal analysis provides richer information and promises greater recognition capability. However, current DFER methods often exhibit unsatisfied performance largely due to fewer training samples compared to SFER. Given the inherent correlation between static and dynamic expressions, we hypothesize that leveraging the abundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic (S4D), a unified dual-modal learning framework that integrates SFER data as a complementary resource for DFER. Specifically, S4D employs dual-modal self-supervised pre-training on facial images and videos using a shared Vision Transformer (ViT) encoder-decoder architecture, yielding improved spatiotemporal representations. The pre-trained encoder is then fine-tuned on static and dynamic expression datasets in a multi-task learning setup to facilitate emotional information interaction. Unfortunately, vanilla multi-task learning in our study results in negative transfer. To address this, we propose an innovative Mixture of Adapter Experts (MoAE) module that facilitates task-specific knowledge acquisition while effectively extracting shared knowledge from both static and dynamic expression data. Extensive experiments demonstrate that S4D achieves a deeper understanding of DFER, setting new state-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with weighted average recall (WAR) of 53.65\%, 58.44\%, and 76.68\%, respectively. Additionally, a systematic correlation analysis between SFER and DFER tasks is presented, which further elucidates the potential benefits of leveraging SFER.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T09:44:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.06154v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.06154v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust
  Scattering in the HH 30 disk</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ryo Tazaki, François Ménard, Gaspard Duchêne, Marion Villenave, Álvaro Ribas, Karl R. Stapelfeldt, Marshall D. Perrin, Christophe Pinte, Schuyler G. Wolff, Deborah L. Padgett, Jie Ma, Laurine Martinien, Maxime Roumesy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present near- and mid-infrared (IR) broadband imaging observations of the edge-on protoplanetary disk around HH 30 with the James Webb Space Telescope/Near Infrared Camera (NIRCam) and the Mid-Infrared Instrument (MIRI). We combine these observations with archival optical/near-IR scattered light images obtained with the Hubble Space Telescope (HST) and a millimeter-wavelength dust continuum image obtained with the Atacama Large Millimeter/submillimeter Array (ALMA) with the highest spatial resolution ever obtained for this target. Our multiwavelength images clearly reveal the vertical and radial segregation of micron-sized and sub-mm-sized grains in the disk. In the near- and mid-IR, the images capture not only bi-reflection nebulae separated by a dark lane but also diverse dynamical processes occurring in the HH 30 disk, such as spiral- and tail-like structures, a conical outflow, and a collimated jet. In contrast, the ALMA image reveals a flat dust disk in the disk midplane. By performing radiative transfer simulations, we show that grains of about 3 $\mu$m in radius or larger are fully vertically mixed to explain the observed mid-IR scattered light flux and its morphology, whereas millimeter-sized grains are settled into a layer with a scale height of $\gtrsim1$ au at $100$ au from the central star. We also find a tension in the disk inclination angle inferred from optical/near-IR and mm observations with the latter being closer to an exactly edge-on. Finally, we report the first detection of the proper motion of an emission knot associated with the mid-IR collimated jet detected by combining two epochs of our MIRI 12.8-$\mu$m observations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T09:23:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.EP</span><span>astro-ph.SR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.07523v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.07523v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Understanding Impact of Human Feedback via Influence Functions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Taywon Min, Haeone Lee, Hanho Ryu, Yongchan Kwon, Kimin Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses. Such feedback can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the performance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward models and large-scale preference datasets. In our experiments, we demonstrate two key applications of influence functions: (1) detecting common forms of labeler bias in human feedback datasets and (2) guiding labelers to refine their strategies to align more closely with expert feedback. By quantifying the impact of human feedback on reward models, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping labelers provide more accurate and consistent feedback. Source code is available at https://github.com/mintaywon/IF_RLHF
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T08:50:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.HC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05790v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05790v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 ResPanDiff: Diffusion Model for Pansharpening by Inferring Residual
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shiqi Cao, Liangjian Deng, Shangqi Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The implementation of diffusion-based pansharpening task is predominantly constrained by its slow inference speed, which results from numerous sampling steps. Despite the existing techniques aiming to accelerate sampling, they often compromise performance when fusing multi-source images. To ease this limitation, we introduce a novel and efficient diffusion model named Diffusion Model for Pansharpening by Inferring Residual Inference (ResPanDiff), which significantly reduces the number of diffusion steps without sacrificing the performance to tackle pansharpening task. In ResPanDiff, we innovatively propose a Markov chain that transits from noisy residuals to the residuals between the LRMS and HRMS images, thereby reducing the number of sampling steps and enhancing performance. Additionally, we design the latent space to help model extract more features at the encoding stage, Shallow Cond-Injection~(SC-I) to help model fetch cond-injected hidden features with higher dimensions, and loss functions to give a better guidance for the residual generation task. enabling the model to achieve superior performance in residual generation. Furthermore, experimental evaluations on pansharpening datasets demonstrate that the proposed method achieves superior outcomes compared to recent state-of-the-art~(SOTA) techniques, requiring only 15 sampling steps, which reduces over $90\%$ step compared with the benchmark diffusion models. Our experiments also include thorough discussions and ablation studies to underscore the effectiveness of our approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T08:43:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05091v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05091v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matthew Baas, Pieter Scholtz, Arnav Mehta, Elliott Dyson, Akshat Prakash, Herman Kamper
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T08:41:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05787v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05787v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 StructSR: Refuse Spurious Details in Real-World Image Super-Resolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yachao Li, Dong Liang, Tianyu Ding, Sheng-Jun Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based models have shown great promise in real-world image super-resolution (Real-ISR), but often generate content with structural errors and spurious texture details due to the empirical priors and illusions of these models. To address this issue, we introduce StructSR, a simple, effective, and plug-and-play method that enhances structural fidelity and suppresses spurious details for diffusion-based Real-ISR. StructSR operates without the need for additional fine-tuning, external model priors, or high-level semantic knowledge. At its core is the Structure-Aware Screening (SAS) mechanism, which identifies the image with the highest structural similarity to the low-resolution (LR) input in the early inference stage, allowing us to leverage it as a historical structure knowledge to suppress the generation of spurious details. By intervening in the diffusion inference process, StructSR seamlessly integrates with existing diffusion-based Real-ISR models. Our experimental results demonstrate that StructSR significantly improves the fidelity of structure and texture, improving the PSNR and SSIM metrics by an average of 5.27% and 9.36% on a synthetic dataset (DIV2K-Val) and 4.13% and 8.64% on two real-world datasets (RealSR and DRealSR) when integrated with four state-of-the-art diffusion-based Real-ISR methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T08:18:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05777v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05777v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qianxiong Xu, Cheng Long, Ziyue Li, Sijie Ruan, Rui Zhao, Zhishuai Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sensors are commonly deployed to perceive the environment. However, due to the high cost, sensors are usually sparsely deployed. Kriging is the tailored task to infer the unobserved nodes (without sensors) using the observed source nodes (with sensors). The essence of kriging task is transferability. Recently, several inductive spatio-temporal kriging methods have been proposed based on graph neural networks, being trained based on a graph built on top of observed nodes via pretext tasks such as masking nodes out and reconstructing them. However, the graph in training is inevitably much sparser than the graph in inference that includes all the observed and unobserved nodes. The learned pattern cannot be well generalized for inference, denoted as graph gap. To address this issue, we first present a novel Increment training strategy: instead of masking nodes (and reconstructing them), we add virtual nodes into the training graph so as to mitigate the graph gap issue naturally. Nevertheless, the empty-shell virtual nodes without labels could have bad-learned features and lack supervision signals. To solve these issues, we pair each virtual node with its most similar observed node and fuse their features together; to enhance the supervision signal, we construct reliable pseudo labels for virtual nodes. As a result, the learned pattern of virtual nodes could be safely transferred to real unobserved nodes for reliable kriging. We name our new Kriging model with Increment Training Strategy as KITS. Extensive experiments demonstrate that KITS consistently outperforms existing kriging methods by large margins, e.g., the improvement over MAE score could be as high as 18.33%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T08:01:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2311.02565v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2311.02565v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Large dark matter content and steep metallicity profile predicted for
  Ultra-Diffuse Galaxies formed in high-spin halos</h2>
                <div class="authors">
                    <strong>Authors:</strong> José A. Benavides, Laura V. Sales, Mario. G. Abadi, Mark Vogelsberger, Federico Marinacci, Lars Hernquist
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We study the stellar properties of a sample of simulated ultra-diffuse galaxies (UDGs) with stellar mass $\rm{M_\star=10^{7.5} - 10^{9} ~ M_{\odot}}$, selected from the TNG50 simulation, where UDGs form mainly in high-spin dwarf-mass halos. We divide our sample into star-forming and quenched UDGs, finding good agreement with the stellar assembly history measured in observations. Star-forming UDGs and quenched UDGs with $\rm{M_\star \geq 10^8 ~ M_\odot}$ in our sample are particularly inefficient at forming stars, having $2$ - $10$ times less stellar mass than non-UDGs for the same virial mass halo. These results are consistent with recent mass inferences in UDG samples and suggest that the most inefficient UDGs arise from a late assembly of the dark matter mass followed by a stellar growth that is comparatively slower (for star-forming UDGs) or that was interrupted due to environmental removal of the gas (for quenched UDGs). Regardless of efficiency, UDGs are $60\%$ poorer in [Fe/H] than the population of non-UDGs at a fixed stellar mass, with the most extreme objects having metal content consistent with the simulated mass-metallicity relation at $z \sim 2$. Quenched UDGs stop their star formation in shorter timescales than non-UDGs of similar mass and are, as a consequence, alpha-enhanced with respect to non-UDGs. We identify metallicity profiles in UDGs as a potential avenue to distinguish between different formation paths for these galaxies, where gentle formation as a result of high-spin halos would present well-defined declining metallicity radial profiles while powerful-outflows or tidal stripping formation models would lead to flatter or constant metallicity as a function of radius due to the inherent mixing of stellar orbits.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T08:00:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15938v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15938v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Controlling Large Language Models Through Concept Activation Vectors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanyu Zhang, Xiting Wang, Chengao Li, Xiang Ao, Qing He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) are widely deployed across various domains, the ability to control their generated outputs has become more critical. This control involves aligning LLMs outputs with human values and ethical principles or customizing LLMs on specific topics or styles for individual users. Existing controlled generation methods either require significant computational resources and extensive trial-and-error or provide coarse-grained control. In this paper, we propose Generation with Concept Activation Vector (GCAV), a lightweight model control framework that ensures accurate control without requiring resource-extensive fine-tuning. Specifically, GCAV first trains a concept activation vector for specified concepts to be controlled, such as toxicity. During inference, GCAV steers the concept vector in LLMs, for example, by removing the toxicity concept vector from the activation layers. Control experiments from different perspectives, including toxicity reduction, sentiment control, linguistic style, and topic control, demonstrate that our framework achieves state-of-the-art performance with granular control, allowing for fine-grained adjustments of both the steering layers and the steering magnitudes for individual samples.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T07:41:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05764v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05764v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion
  Model for Scalable and Controllable Scene Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, Guofeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in large reconstruction and generative models have significantly improved scene reconstruction and novel view generation. However, due to compute limitations, each inference with these large models is confined to a small area, making long-range consistent scene generation challenging. To address this, we propose StarGen, a novel framework that employs a pre-trained video diffusion model in an autoregressive manner for long-range scene generation. The generation of each video clip is conditioned on the 3D warping of spatially adjacent images and the temporally overlapping image from previously generated clips, improving spatiotemporal consistency in long-range scene generation with precise pose control. The spatiotemporal condition is compatible with various input conditions, facilitating diverse tasks, including sparse view interpolation, perpetual view generation, and layout-conditioned city generation. Quantitative and qualitative evaluations demonstrate StarGen's superior scalability, fidelity, and pose accuracy compared to state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T07:41:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05763v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05763v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 TabuLa: Harnessing Language Models for Tabular Data Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zilong Zhao, Robert Birke, Lydia Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tabular data synthesis is crucial for addressing privacy and security concerns in industries reliant on tabular data. While recent advancements adopt large language models (LLMs) for realistic tabular data generation, their long training times and limited reusability hinder practical applications. In this paper, we propose Tabula, a tabular data synthesizer that leverages the structure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data synthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained weights originally designed for natural language tasks, focusing instead on a tailored approach for tabular data. In addition, Tabula introduces a token sequence compression strategy that significantly reduces training time while maintaining data quality, alongside a novel token padding method that improves sequence alignment across training batches. Experiments on six datasets show that Tabula achieves superior synthetic data utility compared to current SOTA methods. Additionally, the results demonstrate that Tabula model trained on tabular datasets serves effectively as a foundational model for synthesizing new tabular datasets. Furthermore, the proposed padding method outperforms the conventional left and right padding strategies. Finally, the results highlight that Tabula averagely reduces training time per epoch by 46.2% compared to state-of-the-art LLM approaches while achieving higher data utility. Our code is available at https://github.com/zhao-zilong/Tabula
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T07:38:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.12746v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.12746v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 ViPOcc: Leveraging Visual Priors from Vision Foundation Models for
  Single-View 3D Occupancy Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Feng, Yu Han, Xijing Zhang, Tanghui Li, Yanting Zhang, Rui Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Inferring the 3D structure of a scene from a single image is an ill-posed and challenging problem in the field of vision-centric autonomous driving. Existing methods usually employ neural radiance fields to produce voxelized 3D occupancy, lacking instance-level semantic reasoning and temporal photometric consistency. In this paper, we propose ViPOcc, which leverages the visual priors from vision foundation models (VFMs) for fine-grained 3D occupancy prediction. Unlike previous works that solely employ volume rendering for RGB and depth image reconstruction, we introduce a metric depth estimation branch, in which an inverse depth alignment module is proposed to bridge the domain gap in depth distribution between VFM predictions and the ground truth. The recovered metric depth is then utilized in temporal photometric alignment and spatial geometric alignment to ensure accurate and consistent 3D occupancy prediction. Additionally, we also propose a semantic-guided non-overlapping Gaussian mixture sampler for efficient, instance-aware ray sampling, which addresses the redundant and imbalanced sampling issue that still exists in previous state-of-the-art methods. Extensive experiments demonstrate the superior performance of ViPOcc in both 3D occupancy prediction and depth estimation tasks on the KITTI-360 and KITTI Raw datasets. Our code is available at: \url{https://mias.group/ViPOcc}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T07:26:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11210v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11210v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 GridShow: Omni Visual Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cong Wan, Xiangyang Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Yuhang He, Yihong Gong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce GRID, a novel paradigm that reframes a broad range of visual generation tasks as the problem of arranging grids, akin to film strips. At its core, GRID transforms temporal sequences into grid layouts, enabling image generation models to process visual sequences holistically. To achieve both layout consistency and motion coherence, we develop a parallel flow-matching training strategy that combines layout matching and temporal losses, guided by a coarse-to-fine schedule that evolves from basic layouts to precise motion control. Our approach demonstrates remarkable efficiency, achieving up to 35 faster inference speeds while using 1/1000 of the computational resources compared to specialized models. Extensive experiments show that GRID exhibits exceptional versatility across diverse visual generation tasks, from Text-to-Video to 3D Editing, while maintaining its foundational image generation capabilities. This dual strength in both expanded applications and preserved core competencies establishes GRID as an efficient and versatile omni-solution for visual generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T07:20:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.10718v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.10718v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Semantic Exploration with Adaptive Gating for Efficient Problem Solving
  with Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sungjae Lee, Hyejin Park, Jaechang Kim, Jungseul Ok
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer from computational inefficiency and redundancy. First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks. Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths. To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method. SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method. Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy. Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T07:02:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05752v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05752v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Multi-objective Bayesian optimization for Likelihood-Free inference in
  sequential sampling models of decision making</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Chen, Xinwei Li, Eui-Jin Kim, Prateek Bansal, David Nott
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scientifically motivated statistical models can sometimes be defined by a generative process for simulating synthetic data. Models specified this way can have likelihoods which are intractable, and this is the case for many sequential sampling models (SSMs) widely used in psychology and consumer behavior modelling. Researchers have developed likelihood-free inference (LFI) methods to make Bayesian inferences on parameters in models with intractable likelihood. Extending a popular approach to simulation efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood-Free Inference (MOBOLFI) to estimate the parameters of SSMs calibrated using multi-source data, such as those based on response times and choice outcomes. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a discrepancy for each data source. Multi-objective Bayesian Optimization is then used to ensure simulation efficient approximation of the SSM likelihood. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling both the detection of conflicting information and a deeper understanding of the importance of different data sources in estimating individual SSM parameters. We illustrate the advantages of our approach in comparison with the use of a single discrepancy in a simple synthetic data example and an SSM example with real-world data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles. Although we focus on applications to SSMs, our approach applies to the likelihood-free calibration of other models using multi-source data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T06:45:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.01735v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.01735v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Covariate Dependent Mixture of Bayesian Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Roman Marchant, Dario Draca, Gilad Francis, Sahand Assadzadeh, Mathew Varidel, Frank Iorfino, Sally Cripps
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning the structure of Bayesian networks from data provides insights into underlying processes and the causal relationships that generate the data, but its usefulness depends on the homogeneity of the data population, a condition often violated in real-world applications. In such cases, using a single network structure for inference can be misleading, as it may not capture sub-population differences. To address this, we propose a novel approach of modelling a mixture of Bayesian networks where component probabilities depend on individual characteristics. Our method identifies both network structures and demographic predictors of sub-population membership, aiding personalised interventions. We evaluate our method through simulations and a youth mental health case study, demonstrating its potential to improve tailored interventions in health, education, and social policy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T06:21:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05745v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05745v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 BaThe: Defense against the Jailbreak Attack in Multimodal Large Language
  Models by Treating Harmful Instruction as Backdoor Trigger</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yulin Chen, Haoran Li, Yirui Zhang, Zihao Zheng, Yangqiu Song, Bryan Hooi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) have showcased impressive performance in a variety of multimodal tasks. On the other hand, the integration of additional image modality may allow the malicious users to inject harmful content inside the images for jailbreaking. Unlike text-based LLMs, where adversaries need to select discrete tokens to conceal their malicious intent using specific algorithms, the continuous nature of image signals provides a direct opportunity for adversaries to inject harmful intentions. In this work, we propose $\textbf{BaThe}$ ($\textbf{Ba}$ckdoor $\textbf{T}$rigger S$\textbf{h}$i$\textbf{e}$ld), a simple yet effective jailbreak defense mechanism. Our work is motivated by recent research on jailbreak backdoor attack and virtual prompt backdoor attack in generative language models. Jailbreak backdoor attack uses harmful instructions combined with manually crafted strings as triggers to make the backdoored model generate prohibited responses. We assume that harmful instructions can function as triggers, and if we alternatively set rejection responses as the triggered response, the backdoored model then can defend against jailbreak attacks. We achieve this by utilizing virtual rejection prompt, similar to the virtual prompt backdoor attack. We embed the virtual rejection prompt into the soft text embeddings, which we call ``wedge''. Our comprehensive experiments demonstrate that BaThe effectively mitigates various types of jailbreak attacks and is adaptable to defend against unseen attacks, with minimal impact on MLLMs' performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T06:04:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.09093v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.09093v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Element-wise Attention Is All You Need</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guoxin Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The self-attention (SA) mechanism has demonstrated superior performance across various domains, yet it suffers from substantial complexity during both training and inference. The next-generation architecture, aiming at retaining the competitive performance of SA while achieving low-cost inference and efficient long-sequence training, primarily focuses on three approaches: linear attention, linear RNNs, and state space models. Although these approaches achieve reduced complexity than SA, they all have built-in performance degradation factors, such as diminished “spikiness” and compression of historical information. In contrast to these approaches, we propose a novel element-wise attention mechanism, which uses the element-wise squared Euclidean distance, instead of the dot product operation, to compute similarity and approximates the quadratic complexity term $\exp(q_{ic}k_{jc})$ with a Taylor polynomial. This design achieves remarkable efficiency: during training, the element-wise attention has a complexity of $\mathcal{O}(tLD)$, making long-sequence training both computationally and memory efficient, where $L$ is the sequence length, $D$ is the feature dimension, and $t$ is the highest order of the polynomial; during inference, it can be reformulated as recurrent neural networks, achieving a inference complexity of $\mathcal{O}(tD)$. Furthermore, the element-wise attention circumvents the performance degradation factors present in these approaches and achieves performance comparable to SA in both causal and non-causal forms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:54:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05730v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05730v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Super-class guided Transformer for Zero-Shot Attribute Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sehyung Kim, Chanhyeong Yang, Jihwan Park, Taehoon Song, Hyunwoo J. Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Attribute classification is crucial for identifying specific characteristics within image regions. Vision-Language Models (VLMs) have been effective in zero-shot tasks by leveraging their general knowledge from large-scale datasets. Recent studies demonstrate that transformer-based models with class-wise queries can effectively address zero-shot multi-label classification. However, poor utilization of the relationship between seen and unseen attributes makes the model lack generalizability. Additionally, attribute classification generally involves many attributes, making maintaining the model's scalability difficult. To address these issues, we propose Super-class guided transFormer (SugaFormer), a novel framework that leverages super-classes to enhance scalability and generalizability for zero-shot attribute classification. SugaFormer employs Super-class Query Initialization (SQI) to reduce the number of queries, utilizing common semantic information from super-classes, and incorporates Multi-context Decoding (MD) to handle diverse visual cues. To strengthen generalizability, we introduce two knowledge transfer strategies that utilize VLMs. During training, Super-class guided Consistency Regularization (SCR) aligns SugaFormer's features with VLMs using region-specific prompts, and during inference, Zero-shot Retrieval-based Score Enhancement (ZRSE) refines predictions for unseen attributes. Extensive experiments demonstrate that SugaFormer achieves state-of-the-art performance across three widely-used attribute classification benchmarks under zero-shot, and cross-dataset transfer settings. Our code is available at https://github.com/mlvlab/SugaFormer.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:53:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05728v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05728v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Enabling Scalable Oversight via Self-Evolving Critic</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, Junyang Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite their remarkable performance, the development of Large Language Models (LLMs) faces a critical challenge in scalable oversight: providing effective feedback for tasks where human evaluation is difficult or where LLMs outperform humans. While there is growing interest in using LLMs for critique, current approaches still rely on human annotations or more powerful models, leaving the issue of enhancing critique capabilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that enables genuine self-evolution of critique abilities. Technically, SCRIT self-improves by training on synthetic data, generated by a contrastive-based self-critic that uses reference solutions for step-by-step critique, and a self-validation mechanism that ensures critique quality through correction outcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs, SCRIT achieves up to a 10.3\% improvement on critique-correction and error identification benchmarks. Our analysis reveals that SCRIT's performance scales positively with data and model size, outperforms alternative approaches, and benefits critically from its self-validation component.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:51:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05727v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05727v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 I Can't Share Code, but I need Translation -- An Empirical Study on Code
  Translation through Federated LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jahnavi Kumar, Venkata Lakshmana Sasaank Janapati, Mokshith Reddy Tanguturi, Sridhar Chimalakonda
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Owing to the rapid evolution of technologies and project requirements, organizations need to upgrade the code base in their software projects to a new version of the programming language or even translating to an entirely new one. However, code translation is resource-intensive and requires expertise in both the source and target languages. While researchers have made progress in automating translations between legacy and modern languages, recent work has increasingly turned to pre-trained Large Language Models (LLMs) to translate efficiently.   Given the proprietary nature of code, organizations prefer fine-tuning LLMs locally rather than relying on external APIs. This is one of the first empirical studies that proposes a Federated LLM-based approach for code translation. The proposed approach enables clients to jointly train a code translator without sharing sensitive data. This study demonstrates that participants can collaboratively develop a FedLLM for efficient code translation (particularly C\# to Java and vice-versa) with superior results (more than 40\% improvement in CodeLLaMA's CodeBLEU score) compared to individual client models. Our findings indicate that FedLLM offers a collaborative approach to code translation and could serve as a promising direction for future research in this field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:43:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05724v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05724v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Bayesian Joint Additive Factor Models for Multiview Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Niccolo Anceschi, Federico Ferrari, David B. Dunson, Himel Mallick
                </div>
                <div class="summary">
                    <strong>Summary:</strong> It is increasingly common in a wide variety of applied settings to collect data of multiple different types on the same set of samples. Our particular focus in this article is on studying relationships between such multiview features and responses. A motivating application arises in the context of precision medicine where multi-omics data are collected to correlate with clinical outcomes. It is of interest to infer dependence within and across views while combining multimodal information to improve the prediction of outcomes. The signal-to-noise ratio can vary substantially across views, motivating more nuanced statistical tools beyond standard late and early fusion. This challenge comes with the need to preserve interpretability, select features, and obtain accurate uncertainty quantification. We propose a joint additive factor regression model (JAFAR) with a structured additive design, accounting for shared and view-specific components. We ensure identifiability via a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide an efficient implementation via a partially collapsed Gibbs sampler and extend our approach to allow flexible feature and outcome distributions. Prediction of time-to-labor onset from immunome, metabolome, and proteome data illustrates performance gains against state-of-the-art competitors. Our open-source software (R package) is available at https://github.com/niccoloanceschi/jafar.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:35:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.AI</span><span>cs.LG</span><span>stat.CO</span><span>stat.ME</span><span>62F15</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.00778v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.00778v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Takin-VC: Expressive Zero-Shot Voice Conversion via Adaptive Hybrid
  Content Encoding and Enhanced Timbre Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuguang Yang, Yu Pan, Jixun Yao, Xiang Zhang, Jianhao Ye, Hongbin Zhou, Lei Xie, Lei Ma, Jianjun Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Expressive zero-shot voice conversion (VC) is a critical and challenging task that aims to transform the source timbre into an arbitrary unseen speaker while preserving the original content and expressive qualities. Despite recent progress in zero-shot VC, there remains considerable potential for improvements in speaker similarity and speech naturalness. Moreover, existing zero-shot VC systems struggle to fully reproduce paralinguistic information in highly expressive speech, such as breathing, crying, and emotional nuances, limiting their practical applicability. To address these issues, we propose Takin-VC, a novel expressive zero-shot VC framework via adaptive hybrid content encoding and memory-augmented context-aware timbre modeling. Specifically, we introduce an innovative hybrid content encoder that incorporates an adaptive fusion module, capable of effectively integrating quantized features of the pre-trained WavLM and HybridFormer in an implicit manner, so as to extract precise linguistic features while enriching paralinguistic elements. For timbre modeling, we propose advanced memory-augmented and context-aware modules to generate high-quality target timbre features and fused representations that seamlessly align source content with target timbre. To enhance real-time performance, we advocate a conditional flow matching model to reconstruct the Mel-spectrogram of the source speech. Experimental results show that our Takin-VC consistently surpasses state-of-the-art VC systems, achieving notable improvements in terms of speech naturalness, speech expressiveness, and speaker similarity, while offering enhanced inference speed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:35:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.01350v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.01350v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 How to Enable Effective Cooperation Between Humans and NLP Models: A
  Survey of Principles, Formalizations, and Beyond</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, Jimmy Xiangji Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:15:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05714v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05714v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Multi-Step Reasoning in Korean and the Emergent Mirage</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guijin Son, Hyunwoo Ko, Dasol Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark designed to evaluate large language models' ability to perform multi-step reasoning in culturally specific contexts, focusing on Korean. The questions are automatically generated via templates and algorithms, requiring LLMs to integrate Korean cultural knowledge into sequential reasoning steps. Consistent with prior observations on emergent abilities, our experiments reveal that models trained on fewer than \(2 \cdot 10^{25}\) training FLOPs struggle to solve any questions, showing near-zero performance. Beyond this threshold, performance improves sharply. State-of-the-art models (e.g., O1) still score under 50\%, underscoring the difficulty of our tasks. Notably, stepwise analysis suggests the observed emergent behavior may stem from compounding errors across multiple steps rather than reflecting a genuinely new capability. We publicly release the benchmark and commit to regularly updating the dataset to prevent contamination.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:07:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05712v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05712v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Plug-and-Play DISep: Separating Dense Instances for Scene-to-Pixel
  Weakly-Supervised Change Detection in High-Resolution Remote Sensing Images</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhenghui Zhao, Chen Wu, Lixiang Ru, Di Wang, Hongruixuan Chen, Cuiqun Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing Weakly-Supervised Change Detection (WSCD) methods often encounter the problem of "instance lumping" under scene-level supervision, particularly in scenarios with a dense distribution of changed instances (i.e., changed objects). In these scenarios, unchanged pixels between changed instances are also mistakenly identified as changed, causing multiple changes to be mistakenly viewed as one. In practical applications, this issue prevents the accurate quantification of the number of changes. To address this issue, we propose a Dense Instance Separation (DISep) method as a plug-and-play solution, refining pixel features from a unified instance perspective under scene-level supervision. Specifically, our DISep comprises a three-step iterative training process: 1) Instance Localization: We locate instance candidate regions for changed pixels using high-pass class activation maps. 2) Instance Retrieval: We identify and group these changed pixels into different instance IDs through connectivity searching. Then, based on the assigned instance IDs, we extract corresponding pixel-level features on a per-instance basis. 3) Instance Separation: We introduce a separation loss to enforce intra-instance pixel consistency in the embedding space, thereby ensuring separable instance feature representations. The proposed DISep adds only minimal training cost and no inference cost. It can be seamlessly integrated to enhance existing WSCD methods. We achieve state-of-the-art performance by enhancing {three Transformer-based and four ConvNet-based methods} on the LEVIR-CD, WHU-CD, DSIFN-CD, SYSU-CD, and CDD datasets. Additionally, our DISep can be used to improve fully-supervised change detection methods. Code is available at https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T04:43:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04934v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04934v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vighnesh Subramaniam, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Shuang Li, Igor Mordatch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A group of language models, all starting from the same base model, are independently specialized by updating each one using data generated through multiagent interactions among the models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to preserve diverse reasoning chains and autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T04:35:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05707v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05707v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Debugging Without Error Messages: How LLM Prompting Strategy Affects
  Programming Error Explanation Effectiveness</h2>
                <div class="authors">
                    <strong>Authors:</strong> Audrey Salmon, Katie Hammer, Eddie Antonio Santos, Brett A. Becker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Making errors is part of the programming process -- even for the most seasoned professionals. Novices in particular are bound to make many errors while learning. It is well known that traditional (compiler/interpreter) programming error messages have been less than helpful for many novices and can have effects such as being frustrating, containing confusing jargon, and being downright misleading. Recent work has found that large language models (LLMs) can generate excellent error explanations, but that the effectiveness of these error messages heavily depends on whether the LLM has been provided with context -- typically the original source code where the problem occurred. Knowing that programming error messages can be misleading and/or contain that serves little-to-no use (particularly for novices) we explore the reverse: what happens when GPT-3.5 is prompted for error explanations on just the erroneous source code itself -- original compiler/interpreter produced error message excluded. We utilized various strategies to make more effective error explanations, including one-shot prompting and fine-tuning. We report the baseline results of how effective the error explanations are at providing feedback, as well as how various prompting strategies might improve the explanations' effectiveness. Our results can help educators by understanding how LLMs respond to such prompts that novices are bound to make, and hopefully lead to more effective use of Generative AI in the classroom.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T04:32:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05706v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05706v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bowen Wang, Jiuyang Chang, Yiming Qian, Guoxin Chen, Junhao Chen, Zhouqiang Jiang, Jiahao Zhang, Yuta Nakashima, Hajime Nagahara
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 511 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability and that of human doctors, highlighting the critical need for models that can reason effectively in real-world clinical scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T04:09:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.01933v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.01933v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Human-In-the-Loop Software Development Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wannita Takerngsaksiri, Jirat Pasuksmit, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Ruixiong Zhang, Fan Jiang, Jing Li, Evan Cook, Kun Chen, Ming Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, Large Language Models (LLMs)-based multi-agent paradigms for software engineering are introduced to automatically resolve software development tasks (e.g., from a given issue to source code). However, existing work is evaluated based on historical benchmark datasets, rarely considers human feedback at each stage of the automated software development process, and has not been deployed in practice. In this paper, we introduce a Human-in-the-loop LLM-based Agents framework (HULA) for software development that allows software engineers to refine and guide LLMs when generating coding plans and source code for a given task. We design, implement, and deploy the HULA framework into Atlassian JIRA for internal uses. Through a multi-stage evaluation of the HULA framework, Atlassian software engineers perceive that HULA can minimize the overall development time and effort, especially in initiating a coding plan and writing code for straightforward tasks. On the other hand, challenges around code quality remain a concern in some cases. We draw lessons learned and discuss opportunities for future work, which will pave the way for the advancement of LLM-based agents in software development.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T03:55:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.HC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.12924v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.12924v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Two-Stage Super-Resolution Simulation Method of Three-Dimensional
  Street-Scale Atmospheric Flows for Real-Time Urban Micrometeorology
  Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuki Yasuda, Ryo Onishi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A two-stage super-resolution simulation method is proposed for street-scale air temperature and wind velocity, which considerably reduces computation time while maintaining accuracy. The first stage employs a convolutional neural network (CNN) to correct large-scale flows above buildings in the input low-resolution simulation results. The second stage uses another CNN to reconstruct small-scale flows between buildings from the output of the first stage, resulting in high-resolution inferences. The CNNs are trained using high-resolution simulation data for the second stage and their coarse-grained version for the first stage as the ground truth, where the high-resolution simulations are conducted independently of the low-resolution simulations used as input. This learning approach separates the spatial scales of inference in each stage. The effectiveness of the proposed method was evaluated using micrometeorological simulations in an actual urban area around Tokyo Station in Japan. The super-resolution simulation successfully inferred high-resolution atmospheric flows, reducing errors by approximately 50% compared to the low-resolution simulations. Furthermore, the two-stage approach enabled localized high-resolution inferences, reducing GPU memory usage to as low as 12% during training. The total wall-clock time for 60-min predictions was reduced to 6.83 min, which was 3.32% of the high-resolution simulation time.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T03:50:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.ao-ph</span><span>physics.flu-dyn</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.02631v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.02631v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series
  Forecasting with Regime Switching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiuqin Xu, Hanqiu Peng, Ying Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern time series data often display complex nonlinear dependencies along with irregular regime-switching behaviors. These features present technical challenges in modeling, inference, and in offering insightful understanding into the underlying stochastic phenomena. To tackle these challenges, we introduce a novel modeling framework known as the Deep Switching State Space Model (DS$^3$M). This framework is engineered to make accurate forecasts for such time series while adeptly identifying the irregular regimes hidden within the dynamics. These identifications not only have significant economic ramifications but also contribute to a deeper understanding of the underlying phenomena. In DS$^3$M, the architecture employs discrete latent variables to represent regimes and continuous latent variables to account for random driving factors. By melding a Recurrent Neural Network (RNN) with a nonlinear Switching State Space Model (SSSM), we manage to capture the nonlinear dependencies and irregular regime-switching behaviors, governed by a Markov chain and parameterized using multilayer perceptrons. We validate the effectiveness and regime identification capabilities of DS$^3$M through short- and long-term forecasting tests on a wide array of simulated and real-world datasets, spanning sectors such as healthcare, economics, traffic, meteorology, and energy. Experimental results reveal that DS$^3$M outperforms several state-of-the-art models in terms of forecasting accuracy, while providing meaningful regime identifications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T03:00:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2106.02329v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2106.02329v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Facilitate Collaboration between Large Language Model and Task-specific
  Model for Time Series Anomaly Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Feiyi Chen, Leilei Zhang, Guansong Pang, Roger Zimmermann, Shuiguang Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In anomaly detection, methods based on large language models (LLMs) can incorporate expert knowledge, while task-specific smaller models excel at extracting normal patterns and detecting value fluctuations. Inspired by the human nervous system, where the brain stores expert knowledge and the peripheral nervous system and spinal cord handle specific tasks like withdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to facilitate collaboration between LLMs and task-specific models, leveraging the strengths of both.   In this work, we first formulate the collaboration process and identify two key challenges in the collaboration between LLMs and task-specific models: (1) the misalignment between the expression domains of LLMs and smaller models, and (2) error accumulation arising from the predictions of both models.   To address these challenges, we introduce two key components in CoLLaTe: the alignment module and the collaborative loss function. Through theoretical analysis and experimental validation, we demonstrate that these components effectively mitigate the identified challenges and achieve better performance than LLM based methods and task-specific smaller model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T02:57:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05675v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05675v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Cascaded Self-Evaluation Augmented Training for Efficient Multimodal
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheqi Lv, Wenkai Wang, Jiawei Wang, Shengyu Zhang, Fei Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self-evaluation has improved their performance. However, limited parameters often hinder EMLLMs from effectively using self-evaluation during inference. Key challenges include synthesizing evaluation data, determining its quantity, optimizing training and inference strategies, and selecting appropriate prompts.   To address these issues, we introduce Self-Evaluation Augmented Training (SEAT). SEAT uses more powerful EMLLMs for CoT reasoning, data selection, and evaluation generation, then trains EMLLMs with the synthesized data. However, handling long prompts and maintaining CoT reasoning quality are problematic. Therefore, we propose Cascaded Self-Evaluation Augmented Training (Cas-SEAT), which breaks down lengthy prompts into shorter, task-specific cascaded prompts and reduces costs for resource-limited settings. During data synthesis, we employ open-source 7B-parameter EMLLMs and annotate a small dataset with short prompts.   Experiments demonstrate that Cas-SEAT significantly boosts EMLLMs' self-evaluation abilities, improving performance by 19.68%, 55.57%, and 46.79% on the MathVista, Math-V, and We-Math datasets, respectively. Additionally, our Cas-SEAT Dataset serves as a valuable resource for future research in enhancing EMLLM self-evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T02:28:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05662v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05662v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 The Oscars of AI Theater: A Survey on Role-Playing with Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nuo Chen, Yan Wang, Yang Deng, Jia Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially confined to simple persona consistency due to limited model capabilities, role-playing tasks have now expanded to embrace complex character portrayals involving character consistency, behavioral alignment, and overall attractiveness. We provide a comprehensive taxonomy of the critical components in designing these systems, including data, models and alignment, agent architecture and evaluation. This survey not only outlines the current methodologies and challenges, such as managing dynamic personal profiles and achieving high-level persona consistency but also suggests avenues for future research in improving the depth and realism of role-playing applications. The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement. Related resources and papers are available at https://github.com/nuochenpku/Awesome-Role-Play-Papers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T02:18:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.11484v9' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.11484v9' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Collaboration of Large Language Models and Small Recommendation Models
  for Device-Cloud Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheqi Lv, Tianyu Zhan, Wenjie Wang, Xinyu Lin, Shengyu Zhang, Wenqiao Zhang, Jiwei Li, Kun Kuang, Fei Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising research direction that has demonstrated exceptional performance in this field. However, its inability to capture real-time user preferences greatly limits the practical application of LLM4Rec because (i) LLMs are costly to train and infer frequently, and (ii) LLMs struggle to access real-time data (its large number of parameters poses an obstacle to deployment on devices). Fortunately, small recommendation models (SRMs) can effectively supplement these shortcomings of LLM4Rec diagrams by consuming minimal resources for frequent training and inference, and by conveniently accessing real-time data on devices.   In light of this, we designed the Device-Cloud LLM-SRM Collaborative Recommendation Framework (LSC4Rec) under a device-cloud collaboration setting. LSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the benefits of cloud and edge computing, achieving a complementary synergy. We enhance the practicability of LSC4Rec by designing three strategies: collaborative training, collaborative inference, and intelligent request. During training, LLM generates candidate lists to enhance the ranking ability of SRM in collaborative scenarios and enables SRM to update adaptively to capture real-time user interests. During inference, LLM and SRM are deployed on the cloud and on the device, respectively. LLM generates candidate lists and initial ranking results based on user behavior, and SRM get reranking results based on the candidate list, with final results integrating both LLM's and SRM's scores. The device determines whether a new candidate list is needed by comparing the consistency of the LLM's and SRM's sorted lists. Our comprehensive and extensive experimental analysis validates the effectiveness of each strategy in LSC4Rec.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T01:27:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CL</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3690624.3709335' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.05647v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05647v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 k-Sample inference via Multimarginal Optimal Transport</h2>
                <div class="authors">
                    <strong>Authors:</strong> Natalia Kravtsova
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for simultaneously comparing $k\geq 2$ measures supported on finite subsets of $\mathbb{R}^d$, $d \geq 1$. We derive asymptotic distributions of the optimal value of the empirical $MOT$ program under the null hypothesis that all $k$ measures are same, and the alternative hypothesis that at least two measures are different. We use these results to construct the test of the null hypothesis and provide consistency and power guarantees of this $k$-sample test. We consistently estimate asymptotic distributions using bootstrap, and propose a low complexity linear program to approximate the test cut-off. We demonstrate the advantages of our approach on synthetic and real datasets, including the real data on cancers in the United States in 2004 - 2020.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T01:17:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>stat.ME</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05645v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05645v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Consistency Checks for Language Model Forecasters</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters instantaneously? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on arbitrage: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60% probability of winning the 2024 US presidential election, an arbitrageur can trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T01:06:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.18544v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.18544v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, Salman Khan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reasoning is a fundamental capability for solving complex multi-step problems, particularly in visual contexts where sequential step-wise understanding is essential. Existing approaches lack a comprehensive framework for evaluating visual reasoning and do not emphasize step-wise problem-solving. To this end, we propose a comprehensive framework for advancing step-by-step visual reasoning in large language models (LMMs) through three key contributions. First, we introduce a visual reasoning benchmark specifically designed to evaluate multi-step reasoning tasks. The benchmark presents a diverse set of challenges with eight different categories ranging from complex visual perception to scientific reasoning with over 4k reasoning steps in total, enabling robust evaluation of LLMs' abilities to perform accurate and interpretable visual reasoning across multiple steps. Second, we propose a novel metric that assesses visual reasoning quality at the granularity of individual steps, emphasizing both correctness and logical coherence. The proposed metric offers deeper insights into reasoning performance compared to traditional end-task accuracy metrics. Third, we present a new multimodal visual reasoning model, named LlamaV-o1, trained using a multi-step curriculum learning approach, where tasks are progressively organized to facilitate incremental skill acquisition and problem-solving. The proposed LlamaV-o1 is designed for multi-step reasoning and learns step-by-step through a structured training paradigm. Extensive experiments show that our LlamaV-o1 outperforms existing open-source models and performs favorably against close-source proprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an average score of 67.3 with an absolute gain of 3.8\% across six benchmarks while being 5 times faster during inference scaling. Our benchmark, model, and code are publicly available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T18:59:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06186v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06186v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 S2-Attention: Hardware-Aware Context Sharding Among Attention Heads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xihui Lin, Yunan Zhang, Suyu Ge, Liliang Ren, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse attention, which selectively attends to a subset of tokens in the context was supposed to be efficient. However, its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up over its dense attention counterparts due to the lack of hardware-aware optimizations like FlashAttention. Meanwhile, it remains unclear whether sparse attention can maintain the model's quality at a scale of today's large language models (LLMs) and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library that provides kernel optimization for sparse attention customizable at both per-head and per-context-range levels. S2-Attention enables the exploration of novel and high-performance sparse attention techniques, which we demonstrate through extensive ablations across a wide range of sparse attention designs at various model scales. From these insights, we present several basic guidelines to design sparse attention that can achieve not only practical efficiency improvements, but also strong downstream performance. To achieve high parallelization and optimized memory IO, sparse attention should shard the context heterogeneously across attention heads, where each head attends to a different subset of tokens while collectively covering the full context. Meanwhile, we find hybrid architectures combining sparse and dense attention particularly beneficial in practice. S2-Attention achieves wall-clock speedup of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with strong downstream performance on-par with full attention and perfect retrieval performance at a 128k context length. At inference, for 7B models, our model, with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to dense counterparts. S2-Attention is released with easy-to-customize APIs for direct usage in Megatron and vLLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T18:45:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.17678v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.17678v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 MS-Temba : Multi-Scale Temporal Mamba for Efficient Temporal Action
  Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arkaprava Sinha, Monish Soundar Raj, Pu Wang, Ahmed Helmy, Srijan Das
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Action detection in real-world scenarios is particularly challenging due to densely distributed actions in hour-long untrimmed videos. It requires modeling both short- and long-term temporal relationships while handling significant intra-class temporal variations. Previous state-of-the-art (SOTA) Transformer-based architectures, though effective, are impractical for real-world deployment due to their high parameter count, GPU memory usage, and limited throughput, making them unsuitable for very long videos. In this work, we innovatively adapt the Mamba architecture for action detection and propose Multi-scale Temporal Mamba (MS-Temba), comprising two key components: Temporal Mamba (Temba) Blocks and the Temporal Mamba Fuser. Temba Blocks include the Temporal Local Module (TLM) for short-range temporal modeling and the Dilated Temporal SSM (DTS) for long-range dependencies. By introducing dilations, a novel concept for Mamba, TLM and DTS capture local and global features at multiple scales. The Temba Fuser aggregates these scale-specific features using Mamba to learn comprehensive multi-scale representations of untrimmed videos. MS-Temba is validated on three public datasets, outperforming SOTA methods on long videos and matching prior methods on short videos while using only one-eighth of the parameters.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T17:52:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06138v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06138v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Supervision policies can shape long-term risk management in
  general-purpose AI models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manuel Cebrian, Emilia Gomez, David Fernandez Llorca
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid proliferation and deployment of General-Purpose AI (GPAI) models, including large language models (LLMs), present unprecedented challenges for AI supervisory entities. We hypothesize that these entities will need to navigate an emergent ecosystem of risk and incident reporting, likely to exceed their supervision capacity. To investigate this, we develop a simulation framework parameterized by features extracted from the diverse landscape of risk, incident, or hazard reporting ecosystems, including community-driven platforms, crowdsourcing initiatives, and expert assessments. We evaluate four supervision policies: non-prioritized (first-come, first-served), random selection, priority-based (addressing the highest-priority risks first), and diversity-prioritized (balancing high-priority risks with comprehensive coverage across risk types). Our results indicate that while priority-based and diversity-prioritized policies are more effective at mitigating high-impact risks, particularly those identified by experts, they may inadvertently neglect systemic issues reported by the broader community. This oversight can create feedback loops that amplify certain types of reporting while discouraging others, leading to a skewed perception of the overall risk landscape. We validate our simulation results with several real-world datasets, including one with over a million ChatGPT interactions, of which more than 150,000 conversations were identified as risky. This validation underscores the complex trade-offs inherent in AI risk supervision and highlights how the choice of risk management policies can shape the future landscape of AI risks across diverse GPAI models used in society.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T17:52:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CY</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06137v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06137v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion
  Planning for Future Autonomous Mobility on Demand Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haichao Liu, Ruoyu Yao, Wenru Liu, Zhenmin Huang, Shaojie Shen, Jun Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The increasing demand for flexible and efficient urban transportation solutions has spotlighted the limitations of traditional Demand Responsive Transport (DRT) systems, particularly in accommodating diverse passenger needs and dynamic urban environments. Autonomous Mobility-on-Demand (AMoD) systems have emerged as a promising alternative, leveraging connected and autonomous vehicles (CAVs) to provide responsive and adaptable services. However, existing methods primarily focus on either vehicle scheduling or path planning, which often simplify complex urban layouts and neglect the necessity for simultaneous coordination and mutual avoidance among CAVs. This oversimplification poses significant challenges to the deployment of AMoD systems in real-world scenarios. To address these gaps, we propose CoDriveVLM, a novel framework that integrates high-fidelity simultaneous dispatching and cooperative motion planning for future AMoD systems. Our method harnesses Vision-Language Models (VLMs) to enhance multi-modality information processing, and this enables comprehensive dispatching and collision risk evaluation. The VLM-enhanced CAV dispatching coordinator is introduced to effectively manage complex and unforeseen AMoD conditions, thus supporting efficient scheduling decision-making. Furthermore, we propose a scalable decentralized cooperative motion planning method via consensus alternating direction method of multipliers (ADMM) focusing on collision risk evaluation and decentralized trajectory optimization. Simulation results demonstrate the feasibility and robustness of CoDriveVLM in various traffic conditions, showcasing its potential to significantly improve the fidelity and effectiveness of AMoD systems in future urban transportation networks. The code is available at https://github.com/henryhcliu/CoDriveVLM.git.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T17:44:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06132v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06132v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented
  Conversational AI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuya Asano, Sabit Hassan, Paras Sharma, Anthony Sicilia, Katherine Atwell, Diane Litman, Malihe Alikhani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> General-purpose automatic speech recognition (ASR) systems do not always perform well in goal-oriented dialogue. Existing ASR correction methods rely on prior user data or named entities. We extend correction to tasks that have no prior user data and exhibit linguistic flexibility such as lexical and syntactic variations. We propose a novel context augmentation with a large language model and a ranking strategy that incorporates contextual information from the dialogue states of a goal-oriented conversational AI and its tasks. Our method ranks (1) n-best ASR hypotheses by their lexical and semantic similarity with context and (2) context by phonetic correspondence with ASR hypotheses. Evaluated in home improvement and cooking domains with real-world users, our method improves recall and F1 of correction by 34% and 16%, respectively, while maintaining precision and false positive rate. Users rated .8-1 point (out of 5) higher when our correction method worked properly, with no decrease due to false positives.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T17:35:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06129v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06129v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Vehicle-in-Virtual-Environment (VVE) Based Autonomous Driving Function
  Development and Evaluation Methodology for Vulnerable Road User Safety</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haochong Chen, Xincheng Cao, Levent Guvenc, Bilin Aksun Guvenc
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traditional methods for developing and evaluating autonomous driving functions, such as model-in-the-loop (MIL) and hardware-in-the-loop (HIL) simulations, heavily depend on the accuracy of simulated vehicle models and human factors, especially for vulnerable road user safety systems. Continuation of development during public road deployment forces other road users including vulnerable ones to involuntarily participate in the development process, leading to safety risks, inefficiencies, and a decline in public trust. To address these deficiencies, the Vehicle-in-Virtual-Environment (VVE) method was proposed as a safer, more efficient, and cost-effective solution for developing and testing connected and autonomous driving technologies by operating the real vehicle and multiple other actors like vulnerable road users in different test areas while being immersed within the same highly realistic virtual environment. This VVE approach synchronizes real-world vehicle and vulnerable road user motion within the same virtual scenario, enabling the safe and realistic testing of various traffic situations in a safe and repeatable manner. In this paper, we propose a new testing pipeline that sequentially integrates MIL, HIL, and VVE methods to comprehensively develop and evaluate autonomous driving functions. The effectiveness of this testing pipeline will be demonstrated using an autonomous driving path-tracking algorithm with local deep reinforcement learning modification for vulnerable road user collision avoidance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T17:05:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06113v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06113v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 From Conversation to Automation: Leveraging Large Language Models to
  Analyze Strategies in Problem Solving Therapy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Elham Aghakhani, Lu Wang, Karla T. Washington, George Demiris, Jina Huh-Yoo, Rezvaneh Rezapour
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Problem-solving therapy (PST) is a structured psychological approach that helps individuals manage stress and resolve personal issues by guiding them through problem identification, solution brainstorming, decision-making, and outcome evaluation. As mental health care increasingly integrates technologies like chatbots and large language models (LLMs), understanding how PST can be effectively automated is important. This study leverages anonymized therapy transcripts to analyze and classify therapeutic interventions using various LLMs and transformer-based models. Our results show that GPT-4o achieved the highest accuracy (0.76) in identifying PST strategies, outperforming other models. Additionally, we introduced a new dimension of communication strategies that enhances the current PST framework, offering deeper insights into therapist-client interactions. This research demonstrates the potential of LLMs to automate complex therapeutic dialogue analysis, providing a scalable, efficient tool for mental health interventions. Our annotation framework can enhance the accessibility, effectiveness, and personalization of PST, supporting therapists in real-time with more precise, targeted interventions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T16:54:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06101v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06101v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Distilling Calibration via Conformalized Credal Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy resources, and ensuring reliable performance in sensitive decision-making tasks. One way to enhance reliability is through uncertainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud-based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets -- ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Inference (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:57:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06066v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06066v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Exploring the Use of Robots for Diary Studies</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael F. Xu, Bilge Mutlu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As interest in studying in-the-wild human-robot interaction grows, there is a need for methods to collect data over time and in naturalistic or potentially private environments. HRI researchers have increasingly used the diary method for these studies, asking study participants to self-administer a structured data collection instrument, i.e., a diary, over a period of time. Although the diary method offers a unique window into settings that researchers may not have access to, they also lack the interactivity and probing that interview-based methods offer. In this paper, we explore a novel data collection method in which a robot plays the role of an interactive diary. We developed the Diary Robot system and performed in-home deployments for a week to evaluate the feasibility and effectiveness of this approach. Using traditional text-based and audio-based diaries as benchmarks, we found that robots are able to effectively elicit the intended information. We reflect on our findings, and describe scenarios where the utilization of robots in diary studies as a data collection instrument may be especially applicable.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:47:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04860v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04860v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 LUMIA: Linear probing for Unimodal and MultiModal Membership Inference
  Attacks leveraging internal LLM states</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux, Joaquin Garcia-Alfaro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly used in a variety of applications, but concerns around membership inference have grown in parallel. Previous efforts focus on black-to-grey-box models, thus neglecting the potential benefit from internal LLM information. To address this, we propose the use of Linear Probes (LPs) as a method to detect Membership Inference Attacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed LUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner workings. We test this method across several model architectures, sizes and datasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA achieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous techniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an increment of 46.80% against the state of the art. Furthermore, our approach reveals key insights, such as the model layers where MIAs are most detectable. In multimodal models, LPs indicate that visual inputs can significantly contribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T15:08:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.19876v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.19876v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 "Stupid robot, I want to speak to a human!" User Frustration Detection
  in Task-Oriented Dialog Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mireia Hernandez Caralt, Ivan Sekulić, Filip Carević, Nghia Khau, Diana Nicoleta Popa, Bruna Guedes, Victor Guimarães, Zeyu Yang, Andre Manso, Meghana Reddy, Paolo Rosso, Roland Mathis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Detecting user frustration in modern-day task-oriented dialog (TOD) systems is imperative for maintaining overall user satisfaction, engagement, and retention. However, most recent research is focused on sentiment and emotion detection in academic settings, thus failing to fully encapsulate implications of real-world user data. To mitigate this gap, in this work, we focus on user frustration in a deployed TOD system, assessing the feasibility of out-of-the-box solutions for user frustration detection. Specifically, we compare the performance of our deployed keyword-based approach, open-source approaches to sentiment analysis, dialog breakdown detection methods, and emerging in-context learning LLM-based detection. Our analysis highlights the limitations of open-source methods for real-world frustration detection, while demonstrating the superior performance of the LLM-based approach, achieving a 16\% relative improvement in F1 score on an internal benchmark. Finally, we analyze advantages and limitations of our methods and provide an insight into user frustration detection task for industry practitioners.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:59:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.17437v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.17437v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Are We Done with MMLU?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, Pasquale Minervini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error annotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700 manually re-annotated questions across all 57 MMLU subjects. We estimate that 6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported. Our results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:31:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.04127v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.04127v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Addressing speaker gender bias in large scale speech translation systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shubham Bansal, Vikas Joshi, Harveen Chadha, Rupeshkumar Mehta, Jinyu Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study addresses the issue of speaker gender bias in Speech Translation (ST) systems, which can lead to offensive and inaccurate translations. The masculine bias often found in large-scale ST systems is typically perpetuated through training data derived from Machine Translation (MT) systems. Our approach involves two key steps. First, we employ Large Language Models (LLMs) to rectify translations based on the speaker's gender in a cost-effective manner. Second, we fine-tune the ST model with the corrected data, enabling the model to generate gender-specific translations directly from audio cues, without the need for explicit gender input. Additionally, we propose a three-mode fine-tuned model for scenarios where the speaker's gender is either predefined or should not be inferred from speech cues. We demonstrate a 70% improvement in translations for female speakers compared to our baseline and other large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHE test set.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:20:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05989v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05989v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Exploring LLMs for Automated Pre-Testing of Cross-Cultural Surveys</h2>
                <div class="authors">
                    <strong>Authors:</strong> Divya Mani Adhikari, Vikram Kamath Cannanure, Alexander Hartland, Ingmar Weber
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Designing culturally relevant questionnaires for ICTD research is challenging, particularly when adapting surveys for populations to non-western contexts. Prior work adapted questionnaires through expert reviews and pilot studies, which are resource-intensive and time-consuming. To address these challenges, we propose using large language models (LLMs) to automate the questionnaire pretesting process in cross-cultural settings. Our study used LLMs to adapt a U.S.-focused climate opinion survey for a South African audience. We then tested the adapted questionnaire with 116 South African participants via Prolific, asking them to provide feedback on both versions. Participants perceived the LLM-adapted questions as slightly more favorable than the traditional version. Our note opens discussions on the potential role of LLMs in adapting surveys and facilitating cross-cultural questionnaire design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:17:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05985v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05985v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study
  of LLM Hallucination on North Korea</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eunjung Cho, Won Ik Cho, Soomin Seo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Hallucination in large language models (LLMs) remains a significant challenge for their safe deployment, particularly due to its potential to spread misinformation. Most existing solutions address this challenge by focusing on aligning the models with credible sources or by improving how models communicate their confidence (or lack thereof) in their outputs. While these measures may be effective in most contexts, they may fall short in scenarios requiring more nuanced approaches, especially in situations where access to accurate data is limited or determining credible sources is challenging. In this study, we take North Korea - a country characterised by an extreme lack of reliable sources and the prevalence of sensationalist falsehoods - as a case study. We explore and evaluate how some of the best-performing multilingual LLMs and specific language-based models generate information about North Korea in three languages spoken in countries with significant geo-political interests: English (United States, United Kingdom), Korean (South Korea), and Mandarin Chinese (China). Our findings reveal significant differences, suggesting that the choice of model and language can lead to vastly different understandings of North Korea, which has important implications given the global security challenges the country poses.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T14:08:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05981v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05981v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Model Inversion in Split Learning for Personalized LLMs: New Insights
  from Information Bottleneck Theory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunmeng Shu, Shaofeng Li, Tian Dong, Yan Meng, Haojin Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Personalized Large Language Models (LLMs) have become increasingly prevalent, showcasing the impressive capabilities of models like GPT-4. This trend has also catalyzed extensive research on deploying LLMs on mobile devices. Feasible approaches for such edge-cloud deployment include using split learning. However, previous research has largely overlooked the privacy leakage associated with intermediate representations transmitted from devices to servers. This work is the first to identify model inversion attacks in the split learning framework for LLMs, emphasizing the necessity of secure defense. For the first time, we introduce mutual information entropy to understand the information propagation of Transformer-based LLMs and assess privacy attack performance for LLM blocks. To address the issue of representations being sparser and containing less information than embeddings, we propose a two-stage attack system in which the first part projects representations into the embedding space, and the second part uses a generative model to recover text from these embeddings. This design breaks down the complexity and achieves attack scores of 38%-75% in various scenarios, with an over 60% improvement over the SOTA. This work comprehensively highlights the potential privacy risks during the deployment of personalized LLMs on the edge side.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:47:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05965v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05965v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 On Large Language Models in Mission-Critical IT Governance: Are We Ready
  Yet?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi, Davide Taibi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Context. The security of critical infrastructure has been a pressing concern since the advent of computers and has become even more critical in today's era of cyber warfare. Protecting mission-critical systems (MCSs), essential for national security, requires swift and robust governance, yet recent events reveal the increasing difficulty of meeting these challenges. Aim. Building on prior research showcasing the potential of Generative AI (GAI), such as Large Language Models, in enhancing risk analysis, we aim to explore practitioners' views on integrating GAI into the governance of IT MCSs. Our goal is to provide actionable insights and recommendations for stakeholders, including researchers, practitioners, and policymakers. Method. We designed a survey to collect practical experiences, concerns, and expectations of practitioners who develop and implement security solutions in the context of MCSs. Conclusions and Future Works. Our findings highlight that the safe use of LLMs in MCS governance requires interdisciplinary collaboration. Researchers should focus on designing regulation-oriented models and focus on accountability; practitioners emphasize data protection and transparency, while policymakers must establish a unified AI framework with global benchmarks to ensure ethical and secure LLMs-based MCS governance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:35:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.ET</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11698v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11698v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent
  Conversion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sho Inoue, Shuai Wang, Wanxing Wang, Pengcheng Zhu, Mengxiao Bi, Haizhou Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In accented voice conversion or accent conversion, we seek to convert the accent in speech from one another while preserving speaker identity and semantic content. In this study, we formulate a novel method for creating multi-accented speech samples, thus pairs of accented speech samples by the same speaker, through text transliteration for training accent conversion systems. We begin by generating transliterated text with Large Language Models (LLMs), which is then fed into multilingual TTS models to synthesize accented English speech. As a reference system, we built a sequence-to-sequence model on the synthetic parallel corpus for accent conversion. We validated the proposed method for both native and non-native English speakers. Subjective and objective evaluations further validate our dataset's effectiveness in accent conversion studies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:23:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09352v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09352v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary
  Empirical Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyang Song, Mao Zheng, Xuan Luo, Yue Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Utilizing Large Language Models (LLMs) as evaluators to assess the performance of LLMs has garnered attention. However, this kind of evaluation approach is affected by potential biases within LLMs, raising concerns about the accuracy and reliability of the evaluation results of LLMs. To address this problem, we propose and study two many-shot In-Context Learning (ICL) prompt templates to help LLM evaluators mitigate potential biases: Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the former utilizes in-context examples with model-generated evaluation rationales as references, while the latter does not include these references. Using these prompt designs, we investigate the impact of increasing the number of in-context examples on the consistency and quality of the evaluation results. Experimental results show that advanced LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot and few-shot regimes. Furthermore, when using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the prompt template performs better than MSoR.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:23:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.11629v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.11629v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 LitSumm: Large language models for literature summarisation of
  non-coding RNAs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrew Green, Carlos Ribas, Nancy Ontiveros-Palacios, Sam Griffiths-Jones, Anton I. Petrov, Alex Bateman, Blake Sweeney
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Curation of literature in life sciences is a growing challenge. The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases. Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.   In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for non-coding RNAs using large language models (LLMs). We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality.   We apply our tool to a selection of over 4,600 ncRNAs and make the generated summaries available via the RNAcentral resource. We conclude that automated literature summarization is feasible with the current generation of LLMs, provided careful prompting and automated checking are applied.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T13:01:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.GN</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2311.03056v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2311.03056v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Environment Modeling for Service Robots From a Task Execution
  Perspective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ying Zhang, Guohui Tian, Cui-Hua Zhang, Changchun Hua, Weili Ding, Choon Ki Ahn
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Service robots are increasingly entering the home to provide domestic tasks for residents. However, when working in an open, dynamic, and unstructured home environment, service robots still face challenges such as low intelligence for task execution and poor long-term autonomy (LTA), which has limited their deployment. As the basis of robotic task execution, environment modeling has attracted significant attention. This integrates core technologies such as environment perception, understanding, and representation to accurately recognize environmental information. This paper presents a comprehensive survey of environmental modeling from a new task-executionoriented perspective. In particular, guided by the requirements of robots in performing domestic service tasks in the home environment, we systematically review the progress that has been made in task-execution-oriented environmental modeling in four respects: 1) localization, 2) navigation, 3) manipulation, and 4) LTA. Current challenges are discussed, and potential research opportunities are also highlighted.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T12:54:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/JAS.2025.125168' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.05931v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05931v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 LLMs Reproduce Stereotypes of Sexual and Gender Minorities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruby Ostrow, Adam Lopez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A large body of research has found substantial gender bias in NLP systems. Most of this research takes a binary, essentialist view of gender: limiting its variation to the categories _men_ and _women_, conflating gender with sex, and ignoring different sexual identities. But gender and sexuality exist on a spectrum, so in this paper we study the biases of large language models (LLMs) towards sexual and gender minorities beyond binary categories. Grounding our study in a widely used psychological framework -- the Stereotype Content Model -- we demonstrate that English-language survey questions about social perceptions elicit more negative stereotypes of sexual and gender minorities from LLMs, just as they do from humans. We then extend this framework to a more realistic use case: text generation. Our analysis shows that LLMs generate stereotyped representations of sexual and gender minorities in this setting, raising concerns about their capacity to amplify representational harms in creative writing, a widely promoted use case.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T12:46:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05926v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05926v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Navigating Tomorrow: Reliably Assessing Large Language Models
  Performance on Future Event Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Petraq Nako, Adam Jatowt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Predicting future events is an important activity with applications across multiple fields and domains. For example, the capacity to foresee stock market trends, natural disasters, business developments, or political events can facilitate early preventive measures and uncover new opportunities. Multiple diverse computational methods for attempting future predictions, including predictive analysis, time series forecasting, and simulations have been proposed. This study evaluates the performance of several large language models (LLMs) in supporting future prediction tasks, an under-explored domain. We assess the models across three scenarios: Affirmative vs. Likelihood questioning, Reasoning, and Counterfactual analysis. For this, we create a dataset1 by finding and categorizing news articles based on entity type and its popularity. We gather news articles before and after the LLMs training cutoff date in order to thoroughly test and compare model performance. Our research highlights LLMs potential and limitations in predictive modeling, providing a foundation for future improvements.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T12:44:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05925v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05925v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Prompt engineering and its implications on the energy consumption of
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Riccardo Rubei, Aicha Moussaid, Claudio di Sipio, Davide di Ruscio
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reducing the environmental impact of AI-based software systems has become critical. The intensive use of large language models (LLMs) in software engineering poses severe challenges regarding computational resources, data centers, and carbon emissions. In this paper, we investigate how prompt engineering techniques (PETs) can impact the carbon emission of the Llama 3 model for the code generation task. We experimented with the CodeXGLUE benchmark to evaluate both energy consumption and the accuracy of the generated code using an isolated testing environment. Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts. Even though a more in-depth evaluation is needed to confirm our findings, this work suggests that prompt engineering can reduce LLMs' energy consumption during the inference phase without compromising performance, paving the way for further investigations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:49:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05899v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05899v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Affordably Fine-tuned LLMs Provide Better Answers to Course-specific
  MCQs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bianca Raimondi, Saverio Giallorenzo, Maurizio Gabbrielli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In education, the capability of generating human-like text of Large Language Models (LLMs) inspired work on how they can increase the efficiency of learning and teaching. We study the affordability of these models for educators and students by investigating how LLMs answer multiple-choice questions (MCQs) with respect to hardware constraints and refinement techniques. We explore this space by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of LLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming Languages (PL) -- the MCQ dataset is a contribution of this work, which we make publicly available. Specifically, we dissect how different factors, such as using readily-available material -- (parts of) the course's textbook -- for fine-tuning and quantisation (to decrease resource usage) can change the accuracy of the responses. The main takeaway is that smaller textbook-based fine-tuned models outperform generic larger ones (whose pre-training requires conspicuous resources), making the usage of LLMs for answering MCQs resource- and material-wise affordable.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:44:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05891v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05891v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster
  Context Attention, Better Feature Fusion, and Hardware Acceleration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhifan Song, Yuan Zhang, Abd Al Rahman M. Abu Ebayyeh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Detecting small targets in drone imagery is challenging due to low resolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel edge-target detection framework built on an enhanced YOLOv10 architecture, optimized for real-time applications without post-processing. EDNet incorporates an XSmall detection head and a Cross Concat strategy to improve feature fusion and multi-scale context awareness for detecting tiny targets in diverse environments. Our unique C2f-FCA block employs Faster Context Attention to enhance feature extraction while reducing computational complexity. The WIoU loss function is employed for improved bounding box regression. With seven model sizes ranging from Tiny to XL, EDNet accommodates various deployment environments, enabling local real-time inference and ensuring data privacy. Notably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer parameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16 to 55 FPS, providing a scalable and efficient solution for edge-based object detection in challenging drone imagery. The source code and pre-trained models are available at: https://github.com/zsniko/EDNet.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:37:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/SWC62898.2024.00141' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.05885v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05885v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal
  LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dabing Cheng, Haosen Zhan, Xingchen Zhao, Guisheng Liu, Zemin Li, Jinghui Xie, Zhao Song, Weiguo Feng, Bingyue Peng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The exponential growth of short-video content has ignited a surge in the necessity for efficient, automated solutions to video editing, with challenges arising from the need to understand videos and tailor the editing according to user requirements. Addressing this need, we propose an innovative end-to-end foundational framework, ultimately actualizing precise control over the final video content editing. Leveraging the flexibility and generalizability of Multimodal Large Language Models (MLLMs), we defined clear input-output mappings for efficient video creation. To bolster the model's capability in processing and comprehending video content, we introduce a strategic combination of a denser frame rate and a slow-fast processing technique, significantly enhancing the extraction and understanding of both temporal and spatial video information. Furthermore, we introduce a text-to-edit mechanism that allows users to achieve desired video outcomes through textual input, thereby enhancing the quality and controllability of the edited videos. Through comprehensive experimentation, our method has not only showcased significant effectiveness within advertising datasets, but also yields universally applicable conclusions on public datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:35:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05884v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05884v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Bayesian Transfer Learning for Artificially Intelligent Geospatial
  Systems: A Predictive Stacking Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luca Presicce, Sudipto Banerjee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Building artificially intelligent geospatial systems require rapid delivery of spatial data analysis at massive scales with minimal human intervention. Depending upon their intended use, data analysis may also entail model assessment and uncertainty quantification. This article devises transfer learning frameworks for deployment in artificially intelligent systems, where a massive data set is split into smaller data sets that stream into the analytical framework to propagate learning and assimilate inference for the entire data set. Specifically, we introduce Bayesian predictive stacking for multivariate spatial data and demonstrate its effectiveness in rapidly analyzing massive data sets. Furthermore, we make inference feasible in a reasonable amount of time, and without excessively demanding hardware settings. We illustrate the effectiveness of this approach in extensive simulation experiments and subsequently analyze massive data sets in climate science on sea surface temperatures and on vegetation index.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T11:03:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.09504v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.09504v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 ConSim: Measuring Concept-Based Explanations' Effectiveness with
  Automated Simulatability</h2>
                <div class="authors">
                    <strong>Authors:</strong> Antonin Poché, Alon Jacovi, Agustin Martin Picard, Victor Boutin, Fanny Jourdan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter. We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulator's ability to predict the explained model's outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at https://github.com/AnonymousConSim/ConSim
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:53:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05855v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05855v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 VLM-driven Behavior Tree for Context-aware Task Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Kazuhiro Sasabuchi, Katsushi Ikeuchi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The use of Large Language Models (LLMs) for generating Behavior Trees (BTs) has recently gained attention in the robotics community, yet remains in its early stages of development. In this paper, we propose a novel framework that leverages Vision-Language Models (VLMs) to interactively generate and edit BTs that address visual conditions, enabling context-aware robot operations in visually complex environments. A key feature of our approach lies in the conditional control through self-prompted visual conditions. Specifically, the VLM generates BTs with visual condition nodes, where conditions are expressed as free-form text. Another VLM process integrates the text into its prompt and evaluates the conditions against real-world images during robot execution. We validated our framework in a real-world cafe scenario, demonstrating both its feasibility and limitations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:38:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.CV</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.03968v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.03968v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Long Story Short: Story-level Video Understanding from 20K Short Films</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, Ivan Laptev
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent developments in vision-language models have significantly advanced video understanding. Existing datasets and tasks, however, have notable limitations. Most datasets are confined to short videos with limited events and narrow narratives. For example, datasets with instructional and egocentric videos often depict activities of one person in a single scene. Although existing movie datasets offer richer content, they are often limited to short-term tasks, lack publicly available videos, and frequently encounter data leakage issues given the use of subtitles and other information about commercial movies during LLM pretraining. To address the above limitations, we propose Short-Films 20K (SF20K), the largest publicly available movie dataset. SF20K is composed of 20,143 amateur films and offers long-term video tasks in the form of multiple-choice and open-ended question answering. Our extensive analysis of SF20K reveals minimal data leakage, emphasizes the need for long-term reasoning, and demonstrates the strong performance of recent VLMs. Finally, we show that instruction tuning on the SF20K-Train set substantially improves model performance, paving the way for future progress in long-term video understanding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:36:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.10221v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.10221v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Applying Think-Aloud in ICTD: A Case Study of a Chatbot Use by Teachers
  in Rural Côte d'Ivoire</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vikram Kamath Cannanure, Sharon Wolf, Kaja Jasińska, Timothy X Brown, Amy Ogan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Think-alouds are a common HCI usability method where participants verbalize their thoughts while using interfaces. However, their utility in cross-cultural settings, particularly in the Global South, is unclear, where cultural differences impact user interactions. This paper investigates the usability challenges teachers in rural C\^ote d'Ivoire faced when using a chatbot designed to support an educational program. We conducted think-aloud sessions with 20 teachers two weeks after a chatbot deployment, analyzing their navigation, errors, and time spent on tasks. We discuss our approach and findings that helped us identify usability issues and challenging features for improving the chatbot designs. Our note summarizes our reflections on using think-aloud and contributes to discussions on its culturally sensitive adaptation in the Global South.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:29:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>H.5.2; K.3.1; K.4.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05840v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05840v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Benchmarking Uncertainty Quantification Methods for Large Language
  Models with LM-Polygraph</h2>
                <div class="authors">
                    <strong>Authors:</strong> Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Kirill Grishchenkov, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, Artem Shelmanov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid proliferation of large language models (LLMs) has stimulated researchers to seek effective and efficient approaches to deal with LLM hallucinations and low-quality outputs. Uncertainty quantification (UQ) is a key element of machine learning applications in dealing with such challenges. However, research to date on UQ for LLMs has been fragmented in terms of techniques and evaluation methodologies. In this work, we address this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines and offers an environment for controllable and consistent evaluation of novel UQ techniques over various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across eleven tasks, identifying the most effective approaches. Code: https://github.com/IINemo/lm-polygraph Benchmark: https://huggingface.co/LM-Polygraph
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:24:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.15627v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.15627v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 MoColl: Agent-Based Specific and General Model Collaboration for Image
  Captioning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pu Yang, Bin Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Image captioning is a critical task at the intersection of computer vision and natural language processing, with wide-ranging applications across various domains. For complex tasks such as diagnostic report generation, deep learning models require not only domain-specific image-caption datasets but also the incorporation of relevant general knowledge to provide contextual accuracy. Existing approaches exhibit inherent limitations: specialized models excel in capturing domain-specific details but lack generalization, while vision-language models (VLMs) built on large language models (LLMs) leverage general knowledge but struggle with domain-specific adaptation. To address these limitations, this paper proposes a novel agent-enhanced model collaboration framework, which we call MoColl, designed to effectively integrate domain-specific and general knowledge. Specifically, our approach is to decompose complex image captioning tasks into a series of interconnected question-answer subtasks. A trainable visual question answering (VQA) model is employed as a specialized tool to focus on domain-specific visual analysis, answering task-specific questions based on image content. Concurrently, an LLM-based agent with general knowledge formulates these questions and synthesizes the resulting question-answer pairs into coherent captions. Beyond its role in leveraging the VQA model, the agent further guides its training to enhance its domain-specific capabilities. Experimental results on radiology report generation validate the effectiveness of the proposed framework, demonstrating significant improvements in the quality of generated reports.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:08:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.01834v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.01834v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays</h2>
                <div class="authors">
                    <strong>Authors:</strong> José P. González-Coma, Santiago Fernández, F. Javier López-Martínez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modular Arrays (MAs) are a promising architecture to enable multi-user communications in next-generation multiple-input multiple-output (MIMO) systems based on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off an improved spatial resolution with characteristic interference patterns associated to grating lobes. In this work, we analyze whether MAs can outperform conventional collocated deployments, in terms of achievable sum-rate and served users in a multi-user downlink set-up. First, we provide a rigorous analytical characterization of the inter-user interference for modular gMIMO systems operating in the near field. Then, we leverage these results to optimize the user selection and precoding mechanisms, designing two algorithms that largely outperform existing alternatives in the literature, with different algorithmic complexities. Results show that the proposed algorithms yield over 70% improvements in achievable sum-spectral efficiencies compared to the state of the art. We also illustrate how MAs allow to serve a larger number of users thanks to their improved spatial resolution, compared to the collocated counterpart.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T09:59:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05820v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05820v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and
  Improving Large Language Model Safety</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paul Röttger, Fabio Pernisi, Bertie Vidgen, Dirk Hovy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The last two years have seen a rapid growth in concerns around the safety of large language models (LLMs). Researchers and practitioners have met these concerns by creating an abundance of datasets for evaluating and improving LLM safety. However, much of this work has happened in parallel, and with very different goals in mind, ranging from the mitigation of near-term risks around bias and toxic content generation to the assessment of longer-term catastrophic risk potential. This makes it difficult for researchers and practitioners to find the most relevant datasets for their use case, and to identify gaps in dataset coverage that future work may fill. To remedy these issues, we conduct a first systematic review of open datasets for evaluating and improving LLM safety. We review 144 datasets, which we identified through an iterative and community-driven process over the course of several months. We highlight patterns and trends, such as a trend towards fully synthetic datasets, as well as gaps in dataset coverage, such as a clear lack of non-English and naturalistic datasets. We also examine how LLM safety datasets are used in practice -- in LLM release publications and popular LLM benchmarks -- finding that current evaluation practices are highly idiosyncratic and make use of only a small fraction of available datasets. Our contributions are based on SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we plan to update continuously as the field of LLM safety develops.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T09:54:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.05399v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.05399v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 RIS Optimization Algorithms for Urban Wireless Scenarios in Sionna RT</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ahmet Esad Güneşer, Berkay Şekeroğlu, Sefa Kayraklık, Erhan Karakoca, İbrahim Hökelek, Sultan Aldirmaz-Colak, Ali Görçin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper evaluates the performance of reconfigurable intelligent surface (RIS) optimization algorithms, which utilize channel estimation methods, in ray tracing (RT) simulations within urban digital twin environments. Beyond Sionna's native capabilities, we implement and benchmark additional RIS optimization algorithms based on channel estimation, enabling an evaluation of RIS strategies under various deployment conditions. Coverage maps for RIS-assisted communication systems are generated through the integration of Sionna's RT simulations. Moreover, real-world experimentation underscores the necessity of validating algorithms in near-realistic simulation environments, as minor variations in measurement setups can significantly affect performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T09:44:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05817v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05817v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 CORD: Generalizable Cooperation via Role Diversity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kanefumi Matsuyama, Kefan Su, Jiangxing Wang, Deheng Ye, Zongqing Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cooperative multi-agent reinforcement learning (MARL) aims to develop agents that can collaborate effectively. However, most cooperative MARL methods overfit training agents, making learned policies not generalize well to unseen collaborators, which is a critical issue for real-world deployment. Some methods attempt to address the generalization problem but require prior knowledge or predefined policies of new teammates, limiting real-world applications. To this end, we propose a hierarchical MARL approach to enable generalizable cooperation via role diversity, namely CORD. CORD's high-level controller assigns roles to low-level agents by maximizing the role entropy with constraints. We show this constrained objective can be decomposed into causal influence in role that enables reasonable role assignment, and role heterogeneity that yields coherent, non-redundant role clusters. Evaluated on a variety of cooperative multi-agent tasks, CORD achieves better performance than baselines, especially in generalization tests. Ablation studies further demonstrate the efficacy of the constrained objective in generalizable cooperation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T09:26:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02221v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02221v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Understanding Impact of Human Feedback via Influence Functions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Taywon Min, Haeone Lee, Hanho Ryu, Yongchan Kwon, Kimin Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses. Such feedback can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the performance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward models and large-scale preference datasets. In our experiments, we demonstrate two key applications of influence functions: (1) detecting common forms of labeler bias in human feedback datasets and (2) guiding labelers to refine their strategies to align more closely with expert feedback. By quantifying the impact of human feedback on reward models, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping labelers provide more accurate and consistent feedback. Source code is available at https://github.com/mintaywon/IF_RLHF
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T08:50:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.HC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05790v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05790v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 rmlnomogram: An R package to construct an explainable nomogram for any
  machine learning algorithms</h2>
                <div class="authors">
                    <strong>Authors:</strong> Herdiantri Sufriyana, Emily Chia-Yu Su
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Background: Current nomogram can only be created for regression algorithm. Providing nomogram for any machine learning (ML) algorithms may accelerate model deployment in clinical settings or improve model availability. We developed an R package and web application to construct nomogram with model explainability of any ML algorithms. Methods: We formulated a function to transform an ML prediction model into a nomogram, requiring datasets with: (1) all possible combinations of predictor values; (2) the corresponding outputs of the model; and (3) the corresponding explainability values for each predictor (optional). Web application was also created. Results: Our R package could create 5 types of nomograms for categorical predictors and binary outcome without probability (1), categorical predictors and binary outcome with probability (2) or continuous outcome (3), and categorical with single numerical predictors and binary outcome with probability (4) or continuous outcome (5). Respectively, the first and remaining types optimally allowed maximum 15 and 5 predictors with maximum 3,200 combinations. Web application is provided with such limits. The explainability values were possible for types 2 to 5. Conclusions: Our R package and web application could construct nomogram with model explainability of any ML algorithms using a fair number of predictors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T08:07:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.ML</span><span>I.2.6; I.5.m; I.2.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05772v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05772v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Controlling Large Language Models Through Concept Activation Vectors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanyu Zhang, Xiting Wang, Chengao Li, Xiang Ao, Qing He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) are widely deployed across various domains, the ability to control their generated outputs has become more critical. This control involves aligning LLMs outputs with human values and ethical principles or customizing LLMs on specific topics or styles for individual users. Existing controlled generation methods either require significant computational resources and extensive trial-and-error or provide coarse-grained control. In this paper, we propose Generation with Concept Activation Vector (GCAV), a lightweight model control framework that ensures accurate control without requiring resource-extensive fine-tuning. Specifically, GCAV first trains a concept activation vector for specified concepts to be controlled, such as toxicity. During inference, GCAV steers the concept vector in LLMs, for example, by removing the toxicity concept vector from the activation layers. Control experiments from different perspectives, including toxicity reduction, sentiment control, linguistic style, and topic control, demonstrate that our framework achieves state-of-the-art performance with granular control, allowing for fine-grained adjustments of both the steering layers and the steering magnitudes for individual samples.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T07:41:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05764v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05764v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 TabuLa: Harnessing Language Models for Tabular Data Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zilong Zhao, Robert Birke, Lydia Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tabular data synthesis is crucial for addressing privacy and security concerns in industries reliant on tabular data. While recent advancements adopt large language models (LLMs) for realistic tabular data generation, their long training times and limited reusability hinder practical applications. In this paper, we propose Tabula, a tabular data synthesizer that leverages the structure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data synthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained weights originally designed for natural language tasks, focusing instead on a tailored approach for tabular data. In addition, Tabula introduces a token sequence compression strategy that significantly reduces training time while maintaining data quality, alongside a novel token padding method that improves sequence alignment across training batches. Experiments on six datasets show that Tabula achieves superior synthetic data utility compared to current SOTA methods. Additionally, the results demonstrate that Tabula model trained on tabular datasets serves effectively as a foundational model for synthesizing new tabular datasets. Furthermore, the proposed padding method outperforms the conventional left and right padding strategies. Finally, the results highlight that Tabula averagely reduces training time per epoch by 46.2% compared to state-of-the-art LLM approaches while achieving higher data utility. Our code is available at https://github.com/zhao-zilong/Tabula
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T07:38:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.12746v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.12746v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Semantic Exploration with Adaptive Gating for Efficient Problem Solving
  with Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sungjae Lee, Hyejin Park, Jaechang Kim, Jungseul Ok
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer from computational inefficiency and redundancy. First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks. Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths. To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method. SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method. Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy. Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T07:02:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05752v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05752v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 BaThe: Defense against the Jailbreak Attack in Multimodal Large Language
  Models by Treating Harmful Instruction as Backdoor Trigger</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yulin Chen, Haoran Li, Yirui Zhang, Zihao Zheng, Yangqiu Song, Bryan Hooi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) have showcased impressive performance in a variety of multimodal tasks. On the other hand, the integration of additional image modality may allow the malicious users to inject harmful content inside the images for jailbreaking. Unlike text-based LLMs, where adversaries need to select discrete tokens to conceal their malicious intent using specific algorithms, the continuous nature of image signals provides a direct opportunity for adversaries to inject harmful intentions. In this work, we propose $\textbf{BaThe}$ ($\textbf{Ba}$ckdoor $\textbf{T}$rigger S$\textbf{h}$i$\textbf{e}$ld), a simple yet effective jailbreak defense mechanism. Our work is motivated by recent research on jailbreak backdoor attack and virtual prompt backdoor attack in generative language models. Jailbreak backdoor attack uses harmful instructions combined with manually crafted strings as triggers to make the backdoored model generate prohibited responses. We assume that harmful instructions can function as triggers, and if we alternatively set rejection responses as the triggered response, the backdoored model then can defend against jailbreak attacks. We achieve this by utilizing virtual rejection prompt, similar to the virtual prompt backdoor attack. We embed the virtual rejection prompt into the soft text embeddings, which we call ``wedge''. Our comprehensive experiments demonstrate that BaThe effectively mitigates various types of jailbreak attacks and is adaptable to defend against unseen attacks, with minimal impact on MLLMs' performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T06:04:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.09093v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.09093v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Enabling Scalable Oversight via Self-Evolving Critic</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, Junyang Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite their remarkable performance, the development of Large Language Models (LLMs) faces a critical challenge in scalable oversight: providing effective feedback for tasks where human evaluation is difficult or where LLMs outperform humans. While there is growing interest in using LLMs for critique, current approaches still rely on human annotations or more powerful models, leaving the issue of enhancing critique capabilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that enables genuine self-evolution of critique abilities. Technically, SCRIT self-improves by training on synthetic data, generated by a contrastive-based self-critic that uses reference solutions for step-by-step critique, and a self-validation mechanism that ensures critique quality through correction outcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs, SCRIT achieves up to a 10.3\% improvement on critique-correction and error identification benchmarks. Our analysis reveals that SCRIT's performance scales positively with data and model size, outperforms alternative approaches, and benefits critically from its self-validation component.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:51:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05727v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05727v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 I Can't Share Code, but I need Translation -- An Empirical Study on Code
  Translation through Federated LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jahnavi Kumar, Venkata Lakshmana Sasaank Janapati, Mokshith Reddy Tanguturi, Sridhar Chimalakonda
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Owing to the rapid evolution of technologies and project requirements, organizations need to upgrade the code base in their software projects to a new version of the programming language or even translating to an entirely new one. However, code translation is resource-intensive and requires expertise in both the source and target languages. While researchers have made progress in automating translations between legacy and modern languages, recent work has increasingly turned to pre-trained Large Language Models (LLMs) to translate efficiently.   Given the proprietary nature of code, organizations prefer fine-tuning LLMs locally rather than relying on external APIs. This is one of the first empirical studies that proposes a Federated LLM-based approach for code translation. The proposed approach enables clients to jointly train a code translator without sharing sensitive data. This study demonstrates that participants can collaboratively develop a FedLLM for efficient code translation (particularly C\# to Java and vice-versa) with superior results (more than 40\% improvement in CodeLLaMA's CodeBLEU score) compared to individual client models. Our findings indicate that FedLLM offers a collaborative approach to code translation and could serve as a promising direction for future research in this field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:43:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05724v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05724v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 An Efficiency Firmware Verification Framework for Public Key
  Infrastructure with Smart Grid and Energy Storage System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jhih-Zen Shih, Cheng-Che Chuang, Hong-Sheng Huang, Hsuan-Tung Chen, Hung-Min Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As a critical component of electrical energy infrastructure, the smart grid system has become indispensable to the energy sector. However, the rapid evolution of smart grids has attracted numerous nation-state actors seeking to disrupt the power infrastructure of adversarial nations. This development underscores the urgent need to establish secure mechanisms for firmware updates, with firmware signing and verification serving as pivotal elements in safeguarding system integrity. In this work, we propose a digital signing and verification framework grounded in Public Key Infrastructure (PKI), specifically tailored for resource-constrained devices such as smart meters. The framework utilizes the Concise Binary Object Representation (CBOR) and Object Signing and Encryption (COSE) formats to achieve efficient da-ta encapsulation and robust security features. Our approach not only en-sures the secure deployment of firmware updates against the convergence of information technology (IT) and operational technology (OT) attacks but also addresses performance bottlenecks stemming from device limitations, thereby enhancing the overall reliability and stability of the smart grid sys-tem.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:43:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05722v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05722v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 How to Enable Effective Cooperation Between Humans and NLP Models: A
  Survey of Principles, Formalizations, and Beyond</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, Jimmy Xiangji Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:15:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05714v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05714v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Multi-Step Reasoning in Korean and the Emergent Mirage</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guijin Son, Hyunwoo Ko, Dasol Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark designed to evaluate large language models' ability to perform multi-step reasoning in culturally specific contexts, focusing on Korean. The questions are automatically generated via templates and algorithms, requiring LLMs to integrate Korean cultural knowledge into sequential reasoning steps. Consistent with prior observations on emergent abilities, our experiments reveal that models trained on fewer than \(2 \cdot 10^{25}\) training FLOPs struggle to solve any questions, showing near-zero performance. Beyond this threshold, performance improves sharply. State-of-the-art models (e.g., O1) still score under 50\%, underscoring the difficulty of our tasks. Notably, stepwise analysis suggests the observed emergent behavior may stem from compounding errors across multiple steps rather than reflecting a genuinely new capability. We publicly release the benchmark and commit to regularly updating the dataset to prevent contamination.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T05:07:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05712v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05712v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vighnesh Subramaniam, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Shuang Li, Igor Mordatch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A group of language models, all starting from the same base model, are independently specialized by updating each one using data generated through multiagent interactions among the models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to preserve diverse reasoning chains and autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T04:35:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05707v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05707v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Debugging Without Error Messages: How LLM Prompting Strategy Affects
  Programming Error Explanation Effectiveness</h2>
                <div class="authors">
                    <strong>Authors:</strong> Audrey Salmon, Katie Hammer, Eddie Antonio Santos, Brett A. Becker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Making errors is part of the programming process -- even for the most seasoned professionals. Novices in particular are bound to make many errors while learning. It is well known that traditional (compiler/interpreter) programming error messages have been less than helpful for many novices and can have effects such as being frustrating, containing confusing jargon, and being downright misleading. Recent work has found that large language models (LLMs) can generate excellent error explanations, but that the effectiveness of these error messages heavily depends on whether the LLM has been provided with context -- typically the original source code where the problem occurred. Knowing that programming error messages can be misleading and/or contain that serves little-to-no use (particularly for novices) we explore the reverse: what happens when GPT-3.5 is prompted for error explanations on just the erroneous source code itself -- original compiler/interpreter produced error message excluded. We utilized various strategies to make more effective error explanations, including one-shot prompting and fine-tuning. We report the baseline results of how effective the error explanations are at providing feedback, as well as how various prompting strategies might improve the explanations' effectiveness. Our results can help educators by understanding how LLMs respond to such prompts that novices are bound to make, and hopefully lead to more effective use of Generative AI in the classroom.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T04:32:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05706v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05706v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bowen Wang, Jiuyang Chang, Yiming Qian, Guoxin Chen, Junhao Chen, Zhouqiang Jiang, Jiahao Zhang, Yuta Nakashima, Hajime Nagahara
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 511 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability and that of human doctors, highlighting the critical need for models that can reason effectively in real-world clinical scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T04:09:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.01933v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.01933v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Human-In-the-Loop Software Development Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wannita Takerngsaksiri, Jirat Pasuksmit, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Ruixiong Zhang, Fan Jiang, Jing Li, Evan Cook, Kun Chen, Ming Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, Large Language Models (LLMs)-based multi-agent paradigms for software engineering are introduced to automatically resolve software development tasks (e.g., from a given issue to source code). However, existing work is evaluated based on historical benchmark datasets, rarely considers human feedback at each stage of the automated software development process, and has not been deployed in practice. In this paper, we introduce a Human-in-the-loop LLM-based Agents framework (HULA) for software development that allows software engineers to refine and guide LLMs when generating coding plans and source code for a given task. We design, implement, and deploy the HULA framework into Atlassian JIRA for internal uses. Through a multi-stage evaluation of the HULA framework, Atlassian software engineers perceive that HULA can minimize the overall development time and effort, especially in initiating a coding plan and writing code for straightforward tasks. On the other hand, challenges around code quality remain a concern in some cases. We draw lessons learned and discuss opportunities for future work, which will pave the way for the advancement of LLM-based agents in software development.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T03:55:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.HC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.12924v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.12924v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Facilitate Collaboration between Large Language Model and Task-specific
  Model for Time Series Anomaly Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Feiyi Chen, Leilei Zhang, Guansong Pang, Roger Zimmermann, Shuiguang Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In anomaly detection, methods based on large language models (LLMs) can incorporate expert knowledge, while task-specific smaller models excel at extracting normal patterns and detecting value fluctuations. Inspired by the human nervous system, where the brain stores expert knowledge and the peripheral nervous system and spinal cord handle specific tasks like withdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to facilitate collaboration between LLMs and task-specific models, leveraging the strengths of both.   In this work, we first formulate the collaboration process and identify two key challenges in the collaboration between LLMs and task-specific models: (1) the misalignment between the expression domains of LLMs and smaller models, and (2) error accumulation arising from the predictions of both models.   To address these challenges, we introduce two key components in CoLLaTe: the alignment module and the collaborative loss function. Through theoretical analysis and experimental validation, we demonstrate that these components effectively mitigate the identified challenges and achieve better performance than LLM based methods and task-specific smaller model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T02:57:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05675v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05675v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Targeted Adversarial Denoising Autoencoders (TADA) for Neural Time
  Series Filtration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin J. Choi, Griffin Milsap, Clara A. Scholl, Francesco Tenore, Mattson Ogg
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current machine learning (ML)-based algorithms for filtering electroencephalography (EEG) time series data face challenges related to cumbersome training times, regularization, and accurate reconstruction. To address these shortcomings, we present an ML filtration algorithm driven by a logistic covariance-targeted adversarial denoising autoencoder (TADA). We hypothesize that the expressivity of a targeted, correlation-driven convolutional autoencoder will enable effective time series filtration while minimizing compute requirements (e.g., runtime, model size). Furthermore, we expect that adversarial training with covariance rescaling will minimize signal degradation. To test this hypothesis, a TADA system prototype was trained and evaluated on the task of removing electromyographic (EMG) noise from EEG data in the EEGdenoiseNet dataset, which includes EMG and EEG data from 67 subjects. The TADA filter surpasses conventional signal filtration algorithms across quantitative metrics (Correlation Coefficient, Temporal RRMSE, Spectral RRMSE), and performs competitively against other deep learning architectures at a reduced model size of less than 400,000 trainable parameters. Further experimentation will be necessary to assess the viability of TADA on a wider range of deployment cases.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T02:54:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04967v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04967v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 The Oscars of AI Theater: A Survey on Role-Playing with Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nuo Chen, Yan Wang, Yang Deng, Jia Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially confined to simple persona consistency due to limited model capabilities, role-playing tasks have now expanded to embrace complex character portrayals involving character consistency, behavioral alignment, and overall attractiveness. We provide a comprehensive taxonomy of the critical components in designing these systems, including data, models and alignment, agent architecture and evaluation. This survey not only outlines the current methodologies and challenges, such as managing dynamic personal profiles and achieving high-level persona consistency but also suggests avenues for future research in improving the depth and realism of role-playing applications. The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement. Related resources and papers are available at https://github.com/nuochenpku/Awesome-Role-Play-Papers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T02:18:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.11484v9' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.11484v9' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 A Practical Cross-Layer Approach for ML-Driven Storage Placement in
  Warehouse-Scale Computers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenxi Yang, Yan Li, Martin Maas, Mustafa Uysal, Ubaid Ullah Hafeez, Arif Merchant, Richard McDougall
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Storage systems account for a major portion of the total cost of ownership (TCO) of warehouse-scale computers, and thus have a major impact on the overall system's efficiency. Machine learning (ML)-based methods for solving key problems in storage system efficiency, such as data placement, have shown significant promise. However, there are few known practical deployments of such methods. Studying this problem in the context of real-world hyperscale data center deployments at Google, we identify a number of challenges that we believe cause this lack of practical adoption. Specifically, prior work assumes a monolithic model that resides entirely within the storage layer, an unrealistic assumption in real-world data center deployments. We propose a cross-layer approach that moves ML out of the storage system and performs it in the application running on top of it, co-designed with a scheduling algorithm at the storage layer that consumes predictions from these application-level models. This approach combines small, interpretable models with a co-designed heuristic that adapts to different online environments. We build a proof-of-concept of this approach in a production distributed computation framework at Google. Evaluations in a test deployment and large-scale simulation studies using production traces show improvements of as much as 3.47x in TCO savings compared to state of the art baselines. We believe this work represents a significant step towards more practical ML-driven storage placement in warehouse-scale computers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T01:42:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05651v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05651v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Collaboration of Large Language Models and Small Recommendation Models
  for Device-Cloud Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheqi Lv, Tianyu Zhan, Wenjie Wang, Xinyu Lin, Shengyu Zhang, Wenqiao Zhang, Jiwei Li, Kun Kuang, Fei Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising research direction that has demonstrated exceptional performance in this field. However, its inability to capture real-time user preferences greatly limits the practical application of LLM4Rec because (i) LLMs are costly to train and infer frequently, and (ii) LLMs struggle to access real-time data (its large number of parameters poses an obstacle to deployment on devices). Fortunately, small recommendation models (SRMs) can effectively supplement these shortcomings of LLM4Rec diagrams by consuming minimal resources for frequent training and inference, and by conveniently accessing real-time data on devices.   In light of this, we designed the Device-Cloud LLM-SRM Collaborative Recommendation Framework (LSC4Rec) under a device-cloud collaboration setting. LSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the benefits of cloud and edge computing, achieving a complementary synergy. We enhance the practicability of LSC4Rec by designing three strategies: collaborative training, collaborative inference, and intelligent request. During training, LLM generates candidate lists to enhance the ranking ability of SRM in collaborative scenarios and enables SRM to update adaptively to capture real-time user interests. During inference, LLM and SRM are deployed on the cloud and on the device, respectively. LLM generates candidate lists and initial ranking results based on user behavior, and SRM get reranking results based on the candidate list, with final results integrating both LLM's and SRM's scores. The device determines whether a new candidate list is needed by comparing the consistency of the LLM's and SRM's sorted lists. Our comprehensive and extensive experimental analysis validates the effectiveness of each strategy in LSC4Rec.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T01:27:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CL</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3690624.3709335' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.05647v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05647v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Consistency Checks for Language Model Forecasters</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters instantaneously? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on arbitrage: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60% probability of winning the 2024 US presidential election, an arbitrageur can trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T01:06:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.18544v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.18544v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Iconicity in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anna Marklová, Jiří Milička, Leonid Ryvkin, Ľudmila Lacková Bennet, Libuše Kormaníková
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Lexical iconicity, a direct relation between a word's meaning and its form, is an important aspect of every natural language, most commonly manifesting through sound-meaning associations. Since Large language models' (LLMs') access to both meaning and sound of text is only mediated (meaning through textual context, sound through written representation, further complicated by tokenization), we might expect that the encoding of iconicity in LLMs would be either insufficient or significantly different from human processing. This study addresses this hypothesis by having GPT-4 generate highly iconic pseudowords in artificial languages. To verify that these words actually carry iconicity, we had their meanings guessed by Czech and German participants (n=672) and subsequently by LLM-based participants (generated by GPT-4 and Claude 3.5 Sonnet). The results revealed that humans can guess the meanings of pseudowords in the generated iconic language more accurately than words in distant natural languages and that LLM-based participants are even more successful than humans in this task. This core finding is accompanied by several additional analyses concerning the universality of the generated language and the cues that both human and LLM-based participants utilize.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T01:00:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05643v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05643v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 The Impact of Model Scaling on Seen and Unseen Language Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rhitabrat Pokharel, Sina Bagheri Nezhad, Ameeta Agrawal, Suresh Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of Large Language Models (LLMs), particularly those trained on multilingual corpora, has intensified the need for a deeper understanding of their performance across a diverse range of languages and model sizes. Our research addresses this critical need by studying the performance and scaling behavior of multilingual LLMs in text classification and machine translation tasks across 204 languages. We systematically examine both seen and unseen languages across three model families of varying sizes in zero-shot and few-shot settings. Our findings show significant differences in scaling behavior between zero-shot and two-shot scenarios, with striking disparities in performance between seen and unseen languages. Model scale has little effect on zero-shot performance, which remains mostly flat. However, in two-shot settings, larger models show clear linear improvements in multilingual text classification. For translation tasks, however, only the instruction-tuned model showed clear benefits from scaling. Our analysis also suggests that overall resource levels, not just the proportions of pretraining languages, are better predictors of model performance, shedding light on what drives multilingual LLM effectiveness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T00:10:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05629v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05629v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Harnessing Large Language Model for Virtual Reality Exploration Testing:
  A Case Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhenyu Qi, Haotang Li, Hao Qin, Kebin Peng, Sen He, Xue Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As the Virtual Reality (VR) industry expands, the need for automated GUI testing is growing rapidly. Large Language Models (LLMs), capable of retaining information long-term and analyzing both visual and textual data, are emerging as a potential key to deciphering the complexities of VR's evolving user interfaces. In this paper, we conduct a case study to investigate the capability of using LLMs, particularly GPT-4o, for field of view (FOV) analysis in VR exploration testing. Specifically, we validate that LLMs can identify test entities in FOVs and that prompt engineering can effectively enhance the accuracy of test entity identification from 41.67% to 71.30%. Our study also shows that LLMs can accurately describe identified entities' features with at least a 90% correction rate. We further find out that the core features that effectively represent an entity are color, placement, and shape. Furthermore, the combination of the three features can especially be used to improve the accuracy of determining identical entities in multiple FOVs with the highest F1-score of 0.70. Additionally, our study demonstrates that LLMs are capable of scene recognition and spatial understanding in VR with precisely designed structured prompts. Finally, we find that LLMs fail to label the identified test entities, and we discuss potential solutions as future research directions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T23:48:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05625v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05625v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian
  Thermodynamic Approach to Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rithvik Prakki
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces a novel approach to creating adaptive language agents by integrating active inference with large language models (LLMs). While LLMs demonstrate remarkable capabilities, their reliance on static prompts limits adaptation to new information and changing environments. We address this by implementing an active inference framework that acts as a cognitive layer above an LLM-based agent, dynamically adjusting prompts and search strategies through principled information-seeking behavior. Our framework models the environment using three state factors (prompt, search, and information states) with seven observation modalities capturing quality metrics. By framing the agent's learning through the free energy principle, we enable systematic exploration of prompt combinations and search strategies. Experimental results demonstrate the effectiveness of this approach, with the agent developing accurate models of environment dynamics evidenced by emergent structure in observation matrices. Action selection patterns reveal sophisticated exploration-exploitation behavior, transitioning from initial information-gathering to targeted prompt testing. The integration of thermodynamic principles with language model capabilities provides a principled framework for creating robust, adaptable agents, extending active inference beyond traditional low-dimensional control problems to high-dimensional, language-driven environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T22:46:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.10425v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.10425v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024 and August 30, 2024, respectively) across 56 embedding tasks, demonstrating the sustained effectiveness of the proposed methods over time. Additionally, it achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T22:27:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.17428v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.17428v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 KTWIN: A Serverless Kubernetes-based Digital Twin Platform</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alexandre Gustavo Wermann, Juliano Araujo Wickboldt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Digital Twins (DTs) systems are virtual representations of physical assets allowing organizations to gain insights and improve existing processes. In practice, DTs require proper modeling, coherent development and seamless deployment along cloud and edge landscapes relying on established patterns to reduce operational costs. In this work, we propose KTWIN a Kubernetes-based Serverless Platform for Digital Twins. KTWIN was developed using the state-of-the-art open-source Cloud Native tools, allowing DT operators to easily define models through open standards and configure details of the underlying services and infrastructure. The experiments carried out with the developed prototype show that KTWIN can provide a higher level of abstraction to model and deploy a Digital Twin use case without compromising the solution scalability. The tests performed also show cost savings ranging between 60% and 80% compared to overprovisioned scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T22:27:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.01635v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.01635v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Arcee's MergeKit: A Toolkit for Merging Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, Jacob Solawetz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pretrained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multitask learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of model merging strategies. MergeKit offers an extensible framework to efficiently merge models on any hardware, providing utility to researchers and practitioners. To date, thousands of models have been merged by the open-source community, leading to the creation of some of the worlds most powerful open-source model checkpoints, as assessed by the Open LLM Leaderboard. The library is accessible at https://github.com/arcee-ai/MergeKit.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T22:21:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.13257v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.13257v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Exploring Large Language Models for Translating Romanian Computational
  Problems into English</h2>
                <div class="authors">
                    <strong>Authors:</strong> Adrian Marius Dumitran, Adrian-Catalin Badea, Stefan-Gabriel Muscalu, Angela-Liliana Dumitran, Stefan-Cosmin Dascalescu, Radu-Sebastian Amarie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent studies have suggested that large language models (LLMs) underperform on mathematical and computer science tasks when these problems are translated from Romanian into English, compared to their original Romanian format. Accurate translation is critical for applications ranging from automatic translations in programming competitions to the creation of high-quality educational materials, as well as minimizing errors or fraud in human translations. This study shows that robust large language models (LLMs) can maintain or even enhance their performance in translating less common languages when given well-structured prompts. Our findings suggest that LLMs, with appropriate supervision, can be reliably used for the automatic translation of IOI (International Olympiad in Informatics)-style tasks. We evaluate several translation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B, Llama 3.2 3B and GPT-4o, assessing their translation accuracy and performance stability through repeated runs. Additionally, we augment the OJI (Romanian County-Level Informatics Olympiad) Romanian dataset with accurate English translations, enhancing its utility for future LLM training and evaluation. Through detailed syntactic and semantic analyses, we confirm that with human oversight, LLMs can serve as a viable solution for multilingual problem-solving. We also compare the translation quality of LLMs against human translators, as evaluated by a certified expert, underscoring the potential of LLMs in realworld scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T22:17:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05601v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05601v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Separating Tongue from Thought: Activation Patching Reveals
  Language-Agnostic Concept Representations in Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Clément Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing latent representations (latents) during a word translation task in transformer-based LLMs. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Building on this insight, we conduct two key experiments. First, we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone. Second, we show that patching with the mean over latents across different languages does not impair and instead improves the models' performance in translating the concept. Our results provide evidence for the existence of language-agnostic concept representations within the investigated models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T21:53:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.08745v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.08745v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Exploring Large Language Models (LLMs) through interactive Python
  activities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eugenio Tufino
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents an approach to introduce physics students to the basic concepts of Large Language Models (LLMs) using Python-based activities in Google Colab. The teaching strategy integrates active learning strategies and combines theoretical ideas with practical, physics-related examples. Students engage with key technical concepts, such as word embeddings, through hands-on exploration of the Word2Vec neural network and GPT-2 - an LLM that gained a lot of attention in 2019 for its ability to generate coherent and plausible text from simple prompts.   The activities highlight how words acquire meaning and how LLMs predict subsequent tokens by simulating simplified scenarios related to physics. By focusing on Word2Vec and GPT-2, the exercises illustrate fundamental principles underlying modern LLMs, such as semantic representation and contextual prediction. Through interactive experimenting in Google Colab, students observe the relationship between model parameters (such as temperature) in GPT-2 and output behaviour, understand scaling laws relating data quantity to model performance, and gain practical insights into the predictive capabilities of LLMs. This approach allows students to begin to understand how these systems work by linking them to physics concepts - systems that will shape their academic studies, professional careers and roles in society.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T21:09:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.ed-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05577v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05577v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Approximate Supervised Object Distance Estimation on Unmanned Surface
  Vehicles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin Kiefer, Yitong Quan, Andreas Zell
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unmanned surface vehicles (USVs) and boats are increasingly important in maritime operations, yet their deployment is limited due to costly sensors and complexity. LiDAR, radar, and depth cameras are either costly, yield sparse point clouds or are noisy, and require extensive calibration. Here, we introduce a novel approach for approximate distance estimation in USVs using supervised object detection. We collected a dataset comprising images with manually annotated bounding boxes and corresponding distance measurements. Leveraging this data, we propose a specialized branch of an object detection model, not only to detect objects but also to predict their distances from the USV. This method offers a cost-efficient and intuitive alternative to conventional distance measurement techniques, aligning more closely with human estimation capabilities. We demonstrate its application in a marine assistance system that alerts operators to nearby objects such as boats, buoys, or other waterborne hazards.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T20:34:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05567v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05567v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene
  Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammed Elhenawy, Huthaifa I. Ashqar, Andry Rakotonirainy, Taqwa I. Alhadidi, Ahmed Jaber, Mohammad Abu Tami
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scene understanding is essential for enhancing driver safety, generating human-centric explanations for Automated Vehicle (AV) decisions, and leveraging Artificial Intelligence (AI) for retrospective driving video analysis. This study developed a dynamic scene retrieval system using Contrastive Language-Image Pretraining (CLIP) models, which can be optimized for real-time deployment on edge devices. The proposed system outperforms state-of-the-art in-context learning methods, including the zero-shot capabilities of GPT-4o, particularly in complex scenarios. By conducting frame-level analysis on the Honda Scenes Dataset, which contains a collection of about 80 hours of annotated driving videos capturing diverse real-world road and weather conditions, our study highlights the robustness of CLIP models in learning visual concepts from natural language supervision. Results also showed that fine-tuning the CLIP models, such as ViT-L/14 and ViT-B/32, significantly improved scene classification, achieving a top F1 score of 91.1%. These results demonstrate the ability of the system to deliver rapid and precise scene recognition, which can be used to meet the critical requirements of Advanced Driver Assistance Systems (ADAS). This study shows the potential of CLIP models to provide scalable and efficient frameworks for dynamic scene understanding and classification. Furthermore, this work lays the groundwork for advanced autonomous vehicle technologies by fostering a deeper understanding of driver behavior, road conditions, and safety-critical scenarios, marking a significant step toward smarter, safer, and more context-aware autonomous driving systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T20:29:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05566v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05566v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Impacts of EPA's Finalized Power Plant Greenhouse Gas Standards</h2>
                <div class="authors">
                    <strong>Authors:</strong> John Bistline, Aaron Bergman, Geoffrey Blanford, Maxwell Brown, Dallas Burtraw, Maya Domeshek, Allen Fawcett, Anne Hamilton, Gokul Iyer, Jesse Jenkins, Ben King, Hannah Kolus, Amanda Levin, Qian Luo, Kevin Rennert, Molly Robertson, Nicholas Roy, Ethan Russell, Daniel Shawhan, Daniel Steinberg, Anna van Brummen, Grace Van Horn, Aranya Venkatesh, John Weyant, Ryan Wiser, Alicia Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Inflation Reduction Act subsidizes the deployment of clean electricity, hydrogen production, and carbon capture and storage, which could enable additional actions by other federal, state, and local policymakers to reduce emissions. Power plant rules finalized by the Environmental Protection Agency (EPA) in 2024 are one such example of complementary policies. The rules establish emissions intensity standards, not technology mandates, meaning power plant owners can choose from a range of technologies and control options provided that emissions standards are met. This flexibility makes electricity systems modeling important to understand the potential effects of these regulations. We report below a multi-model analysis of the EPA power plant rules that can provide timely information, including for other countries and states, on emissions impacts, policy design for electricity decarbonization, power sector investments and retirements, cost impacts, and load growth. We also discuss related technical, political, and legal uncertainties.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T20:14:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.soc-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1126/science.adt5665' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.05561v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05561v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 SepsisCalc: Integrating Clinical Calculators into Early Sepsis
  Prediction via Dynamic Temporal Graph Construction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Changchang Yin, Shihan Fu, Bingsheng Yao, Thai-Hoang Pham, Weidan Cao, Dakuo Wang, Jeffrey Caterino, Ping Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sepsis is an organ dysfunction caused by a deregulated immune response to an infection. Early sepsis prediction and identification allow for timely intervention, leading to improved clinical outcomes. Clinical calculators (e.g., the six-organ dysfunction assessment of SOFA) play a vital role in sepsis identification within clinicians' workflow, providing evidence-based risk assessments essential for sepsis diagnosis. However, artificial intelligence (AI) sepsis prediction models typically generate a single sepsis risk score without incorporating clinical calculators for assessing organ dysfunctions, making the models less convincing and transparent to clinicians. To bridge the gap, we propose to mimic clinicians' workflow with a novel framework SepsisCalc to integrate clinical calculators into the predictive model, yielding a clinically transparent and precise model for utilization in clinical settings. Practically, clinical calculators usually combine information from multiple component variables in Electronic Health Records (EHR), and might not be applicable when the variables are (partially) missing. We mitigate this issue by representing EHRs as temporal graphs and integrating a learning module to dynamically add the accurately estimated calculator to the graphs. Experimental results on real-world datasets show that the proposed model outperforms state-of-the-art methods on sepsis prediction tasks. Moreover, we developed a system to identify organ dysfunctions and potential sepsis risks, providing a human-AI interaction tool for deployment, which can help clinicians understand the prediction outputs and prepare timely interventions for the corresponding dysfunctions, paving the way for actionable clinical decision-making support for early intervention.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T20:00:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00190v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00190v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 The dynamics of meaning through time: Assessment of Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohamed Taher Alrefaie, Fatty Salem, Nour Eldin Morsy, Nada Samir, Mohamed Medhat Gaber
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding how large language models (LLMs) grasp the historical context of concepts and their semantic evolution is essential in advancing artificial intelligence and linguistic studies. This study aims to evaluate the capabilities of various LLMs in capturing temporal dynamics of meaning, specifically how they interpret terms across different time periods. We analyze a diverse set of terms from multiple domains, using tailored prompts and measuring responses through both objective metrics (e.g., perplexity and word count) and subjective human expert evaluations. Our comparative analysis includes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama. Findings reveal marked differences in each model's handling of historical context and semantic shifts, highlighting both strengths and limitations in temporal semantic understanding. These insights offer a foundation for refining LLMs to better address the evolving nature of language, with implications for historical text analysis, AI design, and applications in digital humanities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T19:56:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05552v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05552v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 GUTS: Generalized Uncertainty-Aware Thompson Sampling for Multi-Agent
  Active Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nikhil Angad Bakshi, Tejus Gupta, Ramina Ghods, Jeff Schneider
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Robotic solutions for quick disaster response are essential to ensure minimal loss of life, especially when the search area is too dangerous or too vast for human rescuers. We model this problem as an asynchronous multi-agent active-search task where each robot aims to efficiently seek objects of interest (OOIs) in an unknown environment. This formulation addresses the requirement that search missions should focus on quick recovery of OOIs rather than full coverage of the search region. Previous approaches fail to accurately model sensing uncertainty, account for occlusions due to foliage or terrain, or consider the requirement for heterogeneous search teams and robustness to hardware and communication failures. We present the Generalized Uncertainty-aware Thompson Sampling (GUTS) algorithm, which addresses these issues and is suitable for deployment on heterogeneous multi-robot systems for active search in large unstructured environments. We show through simulation experiments that GUTS consistently outperforms existing methods such as parallelized Thompson Sampling and exhaustive search, recovering all OOIs in 80% of all runs. In contrast, existing approaches recover all OOIs in less than 40% of all runs. We conduct field tests using our multi-robot system in an unstructured environment with a search area of approximately 75,000 sq. m. Our system demonstrates robustness to various failure modes, achieving full recovery of OOIs (where feasible) in every field run, and significantly outperforming our baseline.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T19:54:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/ICRA48891.2023.10160597' target='_blank'>doi</a><a href='http://arxiv.org/abs/2304.02075v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2304.02075v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Infecting Generative AI With Viruses</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Noever, Forrest McKee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study demonstrates a novel approach to testing the security boundaries of Vision-Large Language Model (VLM/ LLM) using the EICAR test file embedded within JPEG images. We successfully executed four distinct protocols across multiple LLM platforms, including OpenAI GPT-4o, Microsoft Copilot, Google Gemini 1.5 Pro, and Anthropic Claude 3.5 Sonnet. The experiments validated that a modified JPEG containing the EICAR signature could be uploaded, manipulated, and potentially executed within LLM virtual workspaces. Key findings include: 1) consistent ability to mask the EICAR string in image metadata without detection, 2) successful extraction of the test file using Python-based manipulation within LLM environments, and 3) demonstration of multiple obfuscation techniques including base64 encoding and string reversal. This research extends Microsoft Research's "Penetration Testing Rules of Engagement" framework to evaluate cloud-based generative AI and LLM security boundaries, particularly focusing on file handling and execution capabilities within containerized environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T19:27:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05542v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05542v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 NSChat: A Chatbot System To Rule Them All</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zenon Lamprou, Yashar Moshfeghi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of artificial intelligence has resulted in the advent of large language models (LLMs) with the capacity to produce text that closely resembles human communication. These models have been seamlessly integrated into diverse applications, enabling interactive and responsive communication across multiple platforms. The potential utility of chatbots transcends these traditional applications, particularly in research contexts, wherein they can offer valuable insights and facilitate the design of innovative experiments. In this study, we present NSChat, a web-based chatbot system designed to assist in neuroscience research. The system is meticulously designed to function as an experimental instrument rather than a conventional chatbot, necessitating users to input a username and experiment code upon access. This setup facilitates precise data cross-referencing, thereby augmenting the integrity and applicability of the data collected for research purposes. It can be easily expanded to accommodate new basic events as needed; and it allows researchers to integrate their own logging events without the necessity of implementing a separate logging mechanism. It is worth noting that our system was built to assist primarily neuroscience research but is not limited to it, it can easily be adapted to assist information retrieval research or interacting with chat bot agents in general.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T19:27:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05541v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05541v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 The Norwegian-Polish CCS Network: A Case Study in Bilateral
  Collaboration for European Climate Action</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad Nooraiepour
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the face of escalating climate change, achieving significant reductions in greenhouse gas emissions from hard-to-abate industrial sectors is imperative. Carbon Capture and Storage (CCS) represents an essential technological advancement to achieve sustainable decarbonization. This manuscript reports on the bilateral CCS network between Norway and Poland, designed and implemented to leverage their capabilities to expedite technology deployment via mutual cooperation and accelerate CCS initiatives targeted to member states' challenges across Europe, aiming for a meaningful contribution to climate goals. Norway is renowned for its operational acumen, as demonstrated by landmark projects like Sleipner and Snohvit, and its forward-looking initiatives, such as the open-source cross-border Northern Lights project, which offers advanced infrastructure and expertise. Conversely, Poland, characterized by its coal-dependent economy and the challenge of decarbonizing extensive industrial emissions, presents a significant geological CO$_2$ storage potential, estimated at over 200 gigatonnes. This study delves into the potential synergies derived from collaborative endeavors in academic education, research and development, industrial implementation, regulatory coherence, and public engagement. By underscoring the reciprocal benefits of such partnership, the study underscores the indispensable role of bilateral cooperation in harnessing CCS's capabilities to meet ambitious climate objectives, paving the way toward a sustainable and low-carbon future. Additionally, it outlines a scalable model for fostering and supporting broader bi- and multi-lateral collaborations, emphasizing the pivotal role of interconnected networks in shaping effective global climate action strategies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T19:25:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.soc-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05539v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05539v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Neural Architecture Codesign for Fast Physics Applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jason Weitz, Dmitri Demler, Luke McDermott, Nhan Tran, Javier Duarte
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We develop a pipeline to streamline neural architecture codesign for physics applications to reduce the need for ML expertise when designing models for novel tasks. Our method employs neural architecture search and network compression in a two-stage approach to discover hardware efficient models. This approach consists of a global search stage that explores a wide range of architectures while considering hardware constraints, followed by a local search stage that fine-tunes and compresses the most promising candidates. We exceed performance on various tasks and show further speedup through model compression techniques such as quantization-aware-training and neural network pruning. We synthesize the optimal models to high level synthesis code for FPGA deployment with the hls4ml library. Additionally, our hierarchical search space provides greater flexibility in optimization, which can easily extend to other tasks and domains. We demonstrate this with two case studies: Bragg peak finding in materials science and jet classification in high energy physics, achieving models with improved accuracy, smaller latencies, or reduced resource utilization relative to the baseline models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T19:00:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cond-mat.mtrl-sci</span><span>hep-ex</span><span>physics.ins-det</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05515v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05515v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video
  Understanding?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Temporal Awareness, the ability to reason dynamically based on the timestamp when a question is raised, is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks. To fill this gap, we present OVO-Bench (Online-VideO-Benchmark), a novel video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) Backward tracing: trace back to past events to answer the question. (2) Real-time understanding: understand and respond to events as they unfold at the current timestamp. (3) Forward active responding: delay the response until sufficient future information becomes available to answer the question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately human-curated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline. Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing a significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T19:00:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05510v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05510v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 ReFocus: Visual Editing as a Chain of Thought for Structured Image
  Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, Cha Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer. However, current multimodal large language models (LLMs) lack this multihop selective attention capability. In this work, we introduce ReFocus, a simple yet effective framework that equips multimodal LLMs with the ability to generate "visual thoughts" by performing visual editing on the input image through code, shifting and refining their visual focuses. Specifically, ReFocus enables multimodal LLMs to generate Python codes to call tools and modify the input image, sequentially drawing boxes, highlighting sections, and masking out areas, thereby enhancing the visual reasoning process. We experiment upon a wide range of structured image understanding tasks involving tables and charts. ReFocus largely improves performance on all tasks over GPT-4o without visual editing, yielding an average gain of 11.0% on table tasks and 6.8% on chart tasks. We present an in-depth analysis of the effects of different visual edits, and reasons why ReFocus can improve the performance without introducing additional information. Further, we collect a 14k training set using ReFocus, and prove that such visual chain-of-thought with intermediate information offers a better supervision than standard VQA data, reaching a 8.0% average gain over the same model trained with QA pairs and 2.6% over CoT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T18:59:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05452v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05452v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 A survey of textual cyber abuse detection using cutting-edge language
  models and large language models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jose A. Diaz-Garcia, Joao Paulo Carvalho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The success of social media platforms has facilitated the emergence of various forms of online abuse within digital communities. This abuse manifests in multiple ways, including hate speech, cyberbullying, emotional abuse, grooming, and sexting. In this paper, we present a comprehensive analysis of the different forms of abuse prevalent in social media, with a particular focus on how emerging technologies, such as Language Models (LMs) and Large Language Models (LLMs), are reshaping both the detection and generation of abusive content within these networks. We delve into the mechanisms through which social media abuse is perpetuated, exploring the psychological and social impact. Additionally, we examine the dual role of advanced language models-highlighting their potential to enhance automated detection systems for abusive behavior while also acknowledging their capacity to generate harmful content. This paper aims to contribute to the ongoing discourse on online safety and ethics, offering insights into the evolving landscape of cyberabuse and the technological innovations that both mitigate and exacerbate it.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T18:55:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05443v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05443v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Muhammad Awais, Ali Husain Salem Abdulla Alharthi, Amandeep Kumar, Hisham Cholakkal, Rao Muhammad Anwer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Significant progress has been made in advancing large multimodal conversational models (LMMs), capitalizing on vast repositories of image-text data available online. Despite this progress, these models often encounter substantial domain gaps, hindering their ability to engage in complex conversations across new domains. Recent efforts have aimed to mitigate this issue, albeit relying on domain-specific image-text data to curate instruction-tuning data. However, many domains, such as agriculture, lack such vision-language data. In this work, we propose an approach to construct instruction-tuning data that harnesses vision-only data for the agriculture domain. We utilize diverse agricultural datasets spanning multiple domains, curate class-specific information, and employ large language models (LLMs) to construct an expert-tuning set, resulting in a 70k expert-tuning dataset called AgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient LMM that can hold complex agriculture-related conversations and provide useful insights. We also develop AgroEvals for evaluation and compare {AgroGPT's} performance with large open and closed-source models. {AgroGPT} excels at identifying fine-grained agricultural concepts, can act as an agriculture expert, and provides helpful information for multimodal agriculture questions. The code, datasets, and models are available at https://github.com/awaisrauf/agroGPT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T18:43:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.08405v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.08405v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Authorship Attribution in the Era of LLMs: Problems, Methodologies, and
  Challenges</h2>
                <div class="authors">
                    <strong>Authors:</strong> Baixiang Huang, Canyu Chen, Kai Shu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate attribution of authorship is crucial for maintaining the integrity of digital content, improving forensic investigations, and mitigating the risks of misinformation and plagiarism. Addressing the imperative need for proper authorship attribution is essential to uphold the credibility and accountability of authentic authorship. The rapid advancements of Large Language Models (LLMs) have blurred the lines between human and machine authorship, posing significant challenges for traditional methods. We presents a comprehensive literature review that examines the latest research on authorship attribution in the era of LLMs. This survey systematically explores the landscape of this field by categorizing four representative problems: (1) Human-written Text Attribution; (2) LLM-generated Text Detection; (3) LLM-generated Text Attribution; and (4) Human-LLM Co-authored Text Attribution. We also discuss the challenges related to ensuring the generalization and explainability of authorship attribution methods. Generalization requires the ability to generalize across various domains, while explainability emphasizes providing transparent and understandable insights into the decisions made by these models. By evaluating the strengths and limitations of existing methods and benchmarks, we identify key open problems and future research directions in this field. This literature review serves a roadmap for researchers and practitioners interested in understanding the state of the art in this rapidly evolving field. Additional resources and a curated list of papers are available and regularly updated at https://llm-authorship.github.io
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T18:39:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.08946v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.08946v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese
  Micro-bloggers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jerry Chongyi Hu, Mohammed Shahid Modi, Boleslaw K. Szymanski
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Studying public sentiment during crises is crucial for understanding how opinions and sentiments shift, resulting in polarized societies. We study Weibo, the most popular microblogging site in China, using posts made during the outbreak of the COVID-19 crisis. The study period includes the pre-COVID-19 stage, the outbreak stage, and the early stage of epidemic prevention. We use Llama 3 8B, a Large Language Model, to analyze users' sentiments on the platform by classifying them into positive, negative, sarcastic, and neutral categories. Analyzing sentiment shifts on Weibo provides insights into how social events and government actions influence public opinion. This study contributes to understanding the dynamics of social sentiments during health crises, fulfilling a gap in sentiment analysis for Chinese platforms. By examining these dynamics, we aim to offer valuable perspectives on digital communication's role in shaping society's responses during unprecedented global challenges.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T18:30:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05423v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05423v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 FairCode: Evaluating Social Bias of LLMs in Code Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongkang Du, Jen-tse Huang, Jieyu Zhao, Lu Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated significant capability in code generation, drawing increasing attention to the evaluation of the quality and safety of their outputs. However, research on bias in code generation remains limited. Existing studies typically assess bias by applying malicious prompts or reapply tasks and dataset for discriminative models. Given that LLMs are often aligned with human values and that prior datasets are not fully optimized for code-related tasks, there is a pressing need for benchmarks specifically designed for evaluating code models. In this study, we introduce FairCode, a novel benchmark for evaluating bias in code generation. FairCode comprises two tasks: function implementation and test case generation, each evaluating social bias through diverse scenarios. Additionally, we propose a new metric, FairScore, to assess model performance on this benchmark. We conduct experiments on widely used LLMs and provide a comprehensive analysis of the results. The findings reveal that all tested LLMs exhibit bias. The code is available at https://github.com/YongkDu/FairCode.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T17:42:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05396v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05396v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While current large language models (LLMs) perform well on many knowledge-related tasks, they are limited by relying on their parameters as an implicit storage mechanism. As a result, they struggle with memorizing rare events and with updating their memory as facts change over time. In addition, the uninterpretable nature of parametric memory makes it challenging to prevent hallucination. Model editing and augmenting LLMs with parameters specialized for memory are only partial solutions. In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLM's capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLM's performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T17:18:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.11672v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.11672v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Large Physics Models: Towards a collaborative approach with Large
  Language Models and Foundation Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kristian G. Barman, Sascha Caron, Emily Sullivan, Henk W. de Regt, Roberto Ruiz de Austri, Mieke Boon, Michael Färber, Stefan Fröse, Faegheh Hasibi, Andreas Ipp, Rukshak Kapoor, Gregor Kasieczka, Daniel Kostić, Michael Krämer, Tobias Golling, Luis G. Lopez, Jesus Marco, Sydney Otten, Pawel Pawlowski, Pietro Vischia, Erik Weber, Christoph Weniger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper explores ideas and provides a potential roadmap for the development and evaluation of physics-specific large-scale AI models, which we call Large Physics Models (LPMs). These models, based on foundation models such as Large Language Models (LLMs) - trained on broad data - are tailored to address the demands of physics research. LPMs can function independently or as part of an integrated framework. This framework can incorporate specialized tools, including symbolic reasoning modules for mathematical manipulations, frameworks to analyse specific experimental and simulated data, and mechanisms for synthesizing theories and scientific literature. We begin by examining whether the physics community should actively develop and refine dedicated models, rather than relying solely on commercial LLMs. We then outline how LPMs can be realized through interdisciplinary collaboration among experts in physics, computer science, and philosophy of science. To integrate these models effectively, we identify three key pillars: Development, Evaluation, and Philosophical Reflection. Development focuses on constructing models capable of processing physics texts, mathematical formulations, and diverse physical data. Evaluation assesses accuracy and reliability by testing and benchmarking. Finally, Philosophical Reflection encompasses the analysis of broader implications of LLMs in physics, including their potential to generate new scientific understanding and what novel collaboration dynamics might arise in research. Inspired by the organizational structure of experimental collaborations in particle physics, we propose a similarly interdisciplinary and collaborative approach to building and refining Large Physics Models. This roadmap provides specific objectives, defines pathways to achieve them, and identifies challenges that must be addressed to realise physics-specific large scale AI models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T17:11:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.data-an</span><span>cs.AI</span><span>hep-ph</span><span>physics.comp-ph</span><span>physics.hist-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05382v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05382v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Occupation-aware planning method for robotic monitoring missions in
  dynamic environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yaroslav Marchukov, Luis Montano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a method for robotic monitoring missions in the presence of moving obstacles. Although the scenario map is known, the robot lacks information about the movement of dynamic obstacles during the monitoring mission. Numerous local planners have been developed in recent years for navigating highly dynamic environments. However, the absence of a global planner for these environments can result in unavoidable collisions or the inability to successfully complete missions in densely populated areas, such as a scenario monitoring in our case. This work addresses the development and evaluation of a global planner, $MADA$ (Monitoring Avoiding Dynamic Areas), aimed at enhancing the deployment of robots in such challenging conditions. The robot plans and executes the mission using the proposed two-step approach. The first step involves selecting the observation goal based on the environment's distribution and estimated monitoring costs. In the second step, the robot identifies areas with moving obstacles and obtains paths avoiding densely occupied dynamic regions based on their occupation. Quantitative and qualitative results based on simulations and on real-world experimentation, confirm that the proposed method allows the robot to effectively monitor most of the environment while avoiding densely occupied dynamic areas.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T16:55:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.00846v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.00846v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Extracting Affect Aggregates from Longitudinal Social Media Data with
  Temporal Adapters for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Georg Ahnert, Max Pellert, David Garcia, Markus Strohmaier
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper proposes temporally aligned Large Language Models (LLMs) as a tool for longitudinal analysis of social media data. We fine-tune Temporal Adapters for Llama 3 8B on full timelines from a panel of British Twitter users, and extract longitudinal aggregates of emotions and attitudes with established questionnaires. We focus our analysis on the beginning of the COVID-19 pandemic that had a strong impact on public opinion and collective emotions. We validate our estimates against representative British survey data and find strong positive, significant correlations for several collective emotions. The obtained estimates are robust across multiple training seeds and prompt formulations, and in line with collective emotions extracted using a traditional classification model trained on labeled data. We demonstrate the flexibility of our method on questions of public opinion for which no pre-trained classifier is available. Our work extends the analysis of affect in LLMs to a longitudinal setting through Temporal Adapters. It enables flexible, new approaches towards the longitudinal analysis of social media data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T16:47:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.17990v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.17990v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 No-Regret Linear Bandits under Gap-Adjusted Misspecification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chong Liu, Dan Qiao, Ming Yin, Ilija Bogunovic, Yu-Xiang Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work studies linear bandits under a new notion of gap-adjusted misspecification and is an extension of Liu et al. (2023). When the underlying reward function is not linear, existing linear bandits work usually relies on a uniform misspecification parameter $\epsilon$ that measures the sup-norm error of the best linear approximation. This results in an unavoidable linear regret whenever $\epsilon > 0$. We propose a more natural model of misspecification which only requires the approximation error at each input $x$ to be proportional to the suboptimality gap at $x$. It captures the intuition that, for optimization problems, near-optimal regions should matter more and we can tolerate larger approximation errors in suboptimal regions.   Quite surprisingly, we show that the classical LinUCB algorithm -- designed for the realizable case -- is automatically robust against such $\rho$-gap-adjusted misspecification with parameter $\rho$ diminishing at $O(1/(d \sqrt{\log T}))$. It achieves a near-optimal $O(\sqrt{T})$ regret for problems that the best-known regret is almost linear in time horizon $T$. We further advance this frontier by presenting a novel phased elimination-based algorithm whose gap-adjusted misspecification parameter $\rho = O(1/\sqrt{d})$ does not scale with $T$. This algorithm attains optimal $O(\sqrt{T})$ regret and is deployment-efficient, requiring only $\log T$ batches of exploration. It also enjoys an adaptive $O(\log T)$ regret when a constant suboptimality gap exists. Technically, our proof relies on a novel self-bounding argument that bounds the part of the regret due to misspecification by the regret itself, and a new inductive lemma that limits the misspecification error within the suboptimality gap for all valid actions in each batch selected by G-optimal design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T16:44:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05361v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05361v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 TradingAgents: Multi-Agents LLM Financial Trading Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yijia Xiao, Edward Sun, Di Luo, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs). In finance, efforts have largely focused on single-agent systems handling specific tasks or multi-agent frameworks independently gathering data. However, multi-agent systems' potential to replicate real-world trading firms' collaborative dynamics remains underexplored. TradingAgents proposes a novel stock trading framework inspired by trading firms, featuring LLM-powered agents in specialized roles such as fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents assessing market conditions, a risk management team monitoring exposure, and traders synthesizing insights from debates and historical data to make informed decisions. By simulating a dynamic, collaborative trading environment, this framework aims to improve trading performance. Detailed architecture and extensive experiments reveal its superiority over baseline models, with notable improvements in cumulative returns, Sharpe ratio, and maximum drawdown, highlighting the potential of multi-agent LLM frameworks in financial trading. More details on TradingAgents are available at https://TradingAgents-AI.github.io.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T16:36:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-fin.TR</span><span>cs.AI</span><span>cs.CE</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.20138v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.20138v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Unsupervised representation learning with Hebbian synaptic and
  structural plasticity in brain-like feedforward neural networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Naresh Ravichandran, Anders Lansner, Pawel Herman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Neural networks that can capture key principles underlying brain computation offer exciting new opportunities for developing artificial intelligence and brain-like computing algorithms. Such networks remain biologically plausible while leveraging localized forms of synaptic learning rules and modular network architecture found in the neocortex. Compared to backprop-driven deep learning approches, they provide more suitable models for deployment of neuromorphic hardware and have greater potential for scalability on large-scale computing clusters. The development of such brain-like neural networks depends on having a learning procedure that can build effective internal representations from data. In this work, we introduce and evaluate a brain-like neural network model capable of unsupervised representation learning. It builds on the Bayesian Confidence Propagation Neural Network (BCPNN), which has earlier been implemented as abstract as well as biophyscially detailed recurrent attractor neural networks explaining various cortical associative memory phenomena. Here we developed a feedforward BCPNN model to perform representation learning by incorporating a range of brain-like attributes derived from neocortical circuits such as cortical columns, divisive normalization, Hebbian synaptic plasticity, structural plasticity, sparse activity, and sparse patchy connectivity. The model was tested on a diverse set of popular machine learning benchmarks: grayscale images (MNIST, F-MNIST), RGB natural images (SVHN, CIFAR-10), QSAR (MUV, HIV), and malware detection (EMBER). The performance of the model when using a linear classifier to predict the class labels fared competitively with conventional multi-layer perceptrons and other state-of-the-art brain-like neural networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T16:30:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NE</span><span>q-bio.NC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.04733v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.04733v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with
  Hardware-Software Co-Exploration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingzi Wang, Yuan Meng, Chen Tang, Weixiang Zhang, Yijian Qin, Yang Yao, Yingxin Li, Tongtong Feng, Xin Wang, Xun Guan, Zhi Wang, Wenwu Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The co-design of neural network architectures, quantization precisions, and hardware accelerators offers a promising approach to achieving an optimal balance between performance and efficiency, particularly for model deployment on resource-constrained edge devices. In this work, we propose the JAQ Framework, which jointly optimizes the three critical dimensions. However, effectively automating the design process across the vast search space of those three dimensions poses significant challenges, especially when pursuing extremely low-bit quantization. Specifical, the primary challenges include: (1) Memory overhead in software-side: Low-precision quantization-aware training can lead to significant memory usage due to storing large intermediate features and latent weights for back-propagation, potentially causing memory exhaustion. (2) Search time-consuming in hardware-side: The discrete nature of hardware parameters and the complex interplay between compiler optimizations and individual operators make the accelerator search time-consuming. To address these issues, JAQ mitigates the memory overhead through a channel-wise sparse quantization (CSQ) scheme, selectively applying quantization to the most sensitive components of the model during optimization. Additionally, JAQ designs BatchTile, which employs a hardware generation network to encode all possible tiling modes, thereby speeding up the search for the optimal compiler mapping strategy. Extensive experiments demonstrate the effectiveness of JAQ, achieving approximately 7% higher Top-1 accuracy on ImageNet compared to previous methods and reducing the hardware search time per iteration to 0.15 seconds.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T16:10:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05339v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05339v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Stream Aligner: Efficient Sentence-Level Alignment via Distribution
  Induction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hantao Lou, Jiaming Ji, Kaile Wang, Yaodong Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of large language models (LLMs) has led to significant improvements in their capabilities, but also to increased concerns about their alignment with human values and intentions. Current alignment strategies, including adaptive training and inference-time methods, have demonstrated potential in this area. However, these approaches still struggle to balance deployment complexity and capability across various tasks and difficulties. In this work, we introduce the Streaming Distribution Induce Aligner (Stream Aligner), a novel alignment paradigm that combines efficiency with enhanced performance in various tasks throughout the generation process. Stream Aligner achieves dynamic sentence-level correction by using a small model to learn the preferences of the suffix sentence, iteratively correcting the suffix sentence output by the upstream model, and then using the corrected sentence to replace the suffix sentence in subsequent generations. Compared to Aligner, our experiments demonstrate that Stream Aligner reduces reliance on the capabilities of additional models, enhances the reasoning abilities of LLMs, and decreases latency during user interaction. Specifically, Stream Aligner-2B model has achieved an improvement of 76.1% in helpfulness, 36.0% in harmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has achieved an improvement of 3.5% on the math ability of the tested Llama3-70B-Instruct model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T16:02:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05336v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05336v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Knowledge Transfer in Model-Based Reinforcement Learning Agents for
  Efficient Multi-Task Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dmytro Kuzmenko, Nadiya Shvai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose an efficient knowledge transfer approach for model-based reinforcement learning, addressing the challenge of deploying large world models in resource-constrained environments. Our method distills a high-capacity multi-task agent (317M parameters) into a compact 1M parameter model, achieving state-of-the-art performance on the MT30 benchmark with a normalized score of 28.45, a substantial improvement over the original 1M parameter model's score of 18.93. This demonstrates the ability of our distillation technique to consolidate complex multi-task knowledge effectively. Additionally, we apply FP16 post-training quantization, reducing the model size by 50% while maintaining performance. Our work bridges the gap between the power of large models and practical deployment constraints, offering a scalable solution for efficient and accessible multi-task reinforcement learning in robotics and other resource-limited domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T15:55:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.RO</span><span>I.2.9; I.2.11</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05329v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05329v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 "What's Happening"- A Human-centered Multimodal Interpreter Explaining
  the Actions of Autonomous Vehicles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xuewen Luo, Fan Ding, Ruiqi Chen, Rishikesh Panda, Junnyong Loo, Shuyun Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Public distrust of self-driving cars is growing. Studies emphasize the need for interpreting the behavior of these vehicles to passengers to promote trust in autonomous systems. Interpreters can enhance trust by improving transparency and reducing perceived risk. However, current solutions often lack a human-centric approach to integrating multimodal interpretations. This paper introduces a novel Human-centered Multimodal Interpreter (HMI) system that leverages human preferences to provide visual, textual, and auditory feedback. The system combines a visual interface with Bird's Eye View (BEV), map, and text display, along with voice interaction using a fine-tuned large language model (LLM). Our user study, involving diverse participants, demonstrated that the HMI system significantly boosts passenger trust in AVs, increasing average trust levels by over 8%, with trust in ordinary environments rising by up to 30%. These results underscore the potential of the HMI system to improve the acceptance and reliability of autonomous vehicles by providing clear, real-time, and context-sensitive explanations of vehicle actions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T15:45:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05322v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05322v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Optimizing Distributed Deployment of Mixture-of-Experts Model Inference
  in Serverless Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengfan Liu, Wei Wang, Chuan Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the advancement of serverless computing, running machine learning (ML) inference services over a serverless platform has been advocated, given its labor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models have been a dominant type of model architectures to enable large models nowadays, with parallel expert networks. Serving large MoE models on serverless computing is potentially beneficial, but has been underexplored due to substantial challenges in handling the skewed expert popularity and scatter-gather communication bottleneck in MoE model execution, for cost-efficient serverless MoE deployment and performance guarantee. We study optimized MoE model deployment and distributed inference serving on a serverless platform, that effectively predict expert selection, pipeline communication with model execution, and minimize the overall billed cost of serving MoE models. Especially, we propose a Bayesian optimization framework with multi-dimensional epsilon-greedy search to learn expert selections and optimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian decision-making method for predicting expert popularity; 2) flexibly pipelined scatter-gather communication; and 3) an optimal model deployment algorithm for distributed MoE serving. Extensive experiments on AWS Lambda show that our designs reduce the billed cost of all MoE layers by at least 75.67% compared to CPU clusters while maintaining satisfactory inference throughput. As compared to LambdaML in serverless computing, our designs achieves 43.41% lower cost with a throughput decrease of at most 18.76%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T15:29:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05313v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05313v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Handover_Management_in_UAV_Networks_with_Blockages</h2>
                <div class="authors">
                    <strong>Authors:</strong> Neetu R R, Gourab Ghatak, Vivek Ashok Bohara
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate the performance of unmanned aerial vehicle (UAV)-based networks in urban environments characterized by blockages, focusing on their capability to support the service demands of mobile users. The UAV-base stations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson point process (MPPP), where the marks represent the altitude of each UAV-BS. Leveraging stochastic geometry, we analyze the impact of blockages on network reliability by studying the meta distribution (MD) of the signal-to-interference noise ratio (SINR) for a specific reliability threshold and the association probabilities for both line-of-sight (LoS) and non line-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile users, we propose a novel cache-based handover management strategy that dynamically selects the cell search time and delays the received signal strength (RSS)-based base station (BS) associations. This strategy aims to minimize unnecessary handovers (HOs) experienced by users by leveraging caching capabilities at user equipment (UE), thus reducing latency, ensuring seamless connectivity, and maintaining the quality of service (QoS). This study provides valuable insights into optimizing UAV network deployments to support the stringent requirements in the network, ensuring reliable, low-latency, and high-throughput communication for next-generation smart cities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T15:14:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.20433v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.20433v2' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    