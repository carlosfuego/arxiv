
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoyu Ma, Zhengqing Yuan, Zheyuan Zhang, Kaiwen Shi, Lichao Sun, Yanfang Ye
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:30:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11769v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11769v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Zhang, Zhiheng Chen, Ye Qiao, Sitao Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:35:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11550v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11550v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 xGR: Efficient Generative Recommendation Serving at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qingxiao Sun, Tongxuan Liu, Shen Zhang, Siyu Wu, Peijun Yang, Haotian Liang, Menxin Li, Xiaolong Ma, Zhiwei Liang, Ziyi Ren, Minchao Zhang, Xinyu Liu, Ke Zhang, Depei Qian, Hailong Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:59:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11529v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11529v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Drinfeld associators and Kashiwara-Vergne associators in higher genera</h2>
                <div class="authors">
                    <strong>Authors:</strong> Toyo Taniguchi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> For $g\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:32:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.QA</span><span>math.AT</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.00473v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.00473v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingmin Zhu, Anqi Zhu, Hossein Rahmani, Jun Liu, Mohammed Bennamoun, Qiuhong Ke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:53:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11458v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11458v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qifan Zhang, Zilin Shen, Imtiaz Karim, Elisa Bertino, Zhou Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional "break-and-fix" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:12:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.FL</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11431v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11431v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chaochao Li, Ruikui Wang, Liangbo Zhou, Jinheng Feng, Huaishao Luo, Huan Zhang, Youzheng Wu, Xiaodong He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:06:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11423v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11423v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianyuan Wu, Lunxi Cao, Yining Wei, Wei Gao, Yuheng Zhao, Dakai An, Shaopan Xiong, Zhiqiang Lv, Ju Huang, Siran Yang, Yinghao Yu, Jiamang Wang, Lin Qu, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T06:03:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11306v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11306v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangyang Luo, Qingyu Li, Xiaokun Liu, Wenyu Qin, Miao Yang, Meng Wang, Pengfei Wan, Di Zhang, Kun Gai, Shao-Lun Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:34:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11274v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11274v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Electrical Stability of Cr2O3/\b{eta}-Ga2O3 and NiOx/\b{eta}-Ga2O3 Heterojunction Diodes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yizheng Liu, Haochen Wang, Carl Peterson, Chinmoy Nath Saha, Chris G. Van de Walle, Sriram Krishnamoorthy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work reports the electrical characteristics comparison study between Cr2O3 and NiOx based heterojunction diodes (HJD) on halide vapor phase epitaxy (HVPE) grown \b{eta}-Ga2O3 epitaxial layers. Both as-fabricated Cr2O3 and NiOx HJDs exhibited forward current density in a range of 130-150 A/cm^2 at 5 V with rectifying ratios >10^10 and a reverse leakage current density at 10^-8 A/cm^2 at -5 V. The differential specific on-resistance of Cr2O3 and NiOx HJDs was 12.01 mΩ*cm^2 and 12.05 mΩ*cm^2, respectively. Breakdown voltages of Cr2O3 HJDs ranged from 1.4-1.9 kV and 1.5-2.3 kV for NiOx HJDs. Theoretical band alignment between Cr2O3 and \b{eta}-Ga2O3 was calculated from first principles. The ambient exposed NiOx/HVPE \b{eta}-Ga2O3 HJDs forward current density degraded after 10 days while that of Cr2O3/HVPE \b{eta}-Ga2O3 HJDs remained nearly unchanged after the same amount of time. It was later confirmed that the ambient exposed sputtered NiOx sheet resistance (Rsh) degradation gave rise to the reduction of the forward current density of the NiOx based HJDs, and water (H2O) was qualitatively determined to be the agent attributed to the forward conduction degradation by measuring the Rsh of NiOx-on-sapphire reference wafer after exposing it to different environments. The Cr2O3/HVPE \b{eta}-Ga2O3 HJD also exhibited enhanced thermal stability compared to the NiOx/\b{eta}-Ga2O3 heterostructures at elevated temperatures. Interfacial nickel gallate (Ga2NiO4) phase formation expected from phase diagrams can explain the reduced thermal stability of NiOx/\b{eta}-Ga2O3 HJDs. This study indicates that Cr2O3 is a stable p-type oxide for the realization of robust multi-kV \b{eta}-Ga2O3 HJDs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T03:57:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.app-ph</span><span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11264v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11264v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haotian Wang, Yuzhe Weng, Xinyi Yu, Jun Du, Haoran Xu, Xiaoyan Wu, Shan He, Bing Yin, Cong Liu, Qingfeng Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T02:28:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11229v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11229v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Wang, Junda Wu, Yu Xia, Tong Yu, Ruiyi Zhang, Ryan Rossi, Subrata Mitra, Lina Yao, Julian McAuley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T02:25:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2504.21228v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2504.21228v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Adilet Metinov, Gulida M. Kudakeeva, Bolotbek uulu Nursultan, Gulnara D. Kabaeva
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T02:02:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11221v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11221v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhengyang Yu, Akio Hayakawa, Masato Ishii, Qingtao Yu, Takashi Shibuya, Jing Zhang, Yuki Mitsufuji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T01:28:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11203v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11203v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huawei Zhang, Chunwei Xia, Zheng Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-11T23:35:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.11907v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.11907v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinhang Chen, Chao Zhang, Jiahuan He, Wei Liu, Jianming Zhang, Wenlong Zhou, Xiao Li, Pai Zeng, Shiyong Li, Yuanpan Qian, Dong Li, Zhaogeng Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.   To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.   Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-11T12:06:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10576v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10576v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qingsen Ma, Dianyun Wang, Jiaming Lyu, Yaoye Wang, Lechen Ning, Sujie Zhu, Zhenbo Xu, Liuyu Xiang, Huining Li, Huijia Wu, Zhaofeng He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-11T11:23:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10547v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10547v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Electric-Field-Controlled Altermagnetic Transition for Neuromorphic Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiyuan Duan, Peixin Qin, Chengyan Zhong, Shaoxuan Zhang, Li Liu, Guojian Zhao, Xiaoning Wang, Hongyu Chen, Ziang Meng, Jingyu Li, Sixu Jiang, Xiaoyang Tan, Qiong Wu, Yu Liu, Zhiqi Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Altermagnets represent a novel magnetic phase with transformative potential for ultrafast spintronics, yet efficient control of their magnetic states remains challenging. We demonstrate an ultra-low-power electric-field control of altermagnetism in MnTe through strain-mediated coupling in MnTe/PMN-PT heterostructures with negligible Joule heating. Application of +6 kV/cm electric fields induces piezoelectric strain in PMN-PT, modulating the Néel temperature from 310 to 328 K. As a result, around the magnetic phase transition, the altermagnetic spin splitting of MnTe is reversibly switched "on" and "off" by the electric fields. Meanwhile, the piezoelectric strain generates lattice distortions and magnetic structure changes in MnTe, enabling up to 9.7% resistance modulation around the magnetic phase transition temperature. Leveraging this effect, we implement programmable resistance states in a Hopfield neuromorphic network, achieving 100% pattern recognition accuracy at <=40% noise levels. This approach establishes the electric-field control as a low-power strategy for altermagnetic manipulation while demonstrating the viability of altermagnetic materials for energy-efficient neuromorphic computing beyond conventional charge-based architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-11T08:14:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span><span>cond-mat.mes-hall</span><span>physics.app-ph</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10405v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10405v1' target='_blank'>pdf</a><a href='https://doi.org/10.1021/jacs.5c15276' target='_blank'>doi</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huanlin Gao, Ping Chen, Fuyuan Shi, Chao Tan, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-11T08:10:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.00090v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.00090v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Efficient-VLN: A Training-Efficient Vision-Language Navigation Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-11T05:57:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10310v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10310v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings</h2>
                <div class="authors">
                    <strong>Authors:</strong> Karthikeya KV, Narendra Bandaru
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-11T05:20:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10293v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10293v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Mixture of Lookup Key-Value Experts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zongcheng Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-10T15:05:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09723v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09723v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingcong Liang, Siyuan Wang, Miren Tian, Yitong Li, Duyu Tang, Zhongyu Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the hit rate of an expert cache utilizing a length of future information under a cache limit. We analyze 20 MoE LLMs with diverse sizes and architectures and use toy models to verify key factors related to local routing consistency. We find a strong trade-off between local routing consistency and *local* load balance, while showing that *global* load balance can coexist with local routing consistency. Meanwhile, settings like shared experts that decrease expert combination space can lead to low local routing consistency. We further reveal that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models balance between cache effectiveness and efficiency with cache sizes approximately twice the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-10T14:34:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2505.16056v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2505.16056v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Matrix-free algorithms for fast ab initio calculations on distributed CPU architectures using finite-element discretization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gourab Panigrahi, Phani Motamarri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Finite-element (FE) discretisations have emerged as a powerful real-space alternative to large-scale Kohn-Sham density functional theory (DFT) calculations, offering systematic convergence, excellent parallel scalability, while accommodating generic boundary conditions. However, the dominant computational bottleneck in FE-based DFT arises from the repeated application of the discretised sparse Hamiltonian to large blocks of trial vectors during iterations in an iterative eigensolver. Traditional sparse matrix-vector multiplications and FE cell-matrix approaches encounter memory limitations and high data-movement overheads, particularly at higher polynomial orders, typically used in DFT calculations. To overcome these challenges, this work develops matrix-free algorithms for FE-discretised DFT that substantially accelerate these products by doing on-the-fly operations that utilize structured tensor contractions over 1D basis functions and quadrature data. A unified multilevel batched data layout that handles both real and complex-valued operators is introduced to maximise cache reuse and SIMD utilisation on Frontier (AVX2), Param Pravega (AVX512) and Fugaku (SVE). We also combine terms for optimal cache reuse, even-odd decomposition to reduce FLOP, and mixed-precision intrinsics. Extensive benchmarks show that for large multivector pseudopotential DFT calculations, the matrix-free kernels deliver 1.5-4x speedups over the state-of-the-art cell-matrix approach baselines. For all-electron DFT calculations, the matrix-free operator achieves gains of up to 5.8x due to its efficient implementation and superior arithmetic intensity. When integrated with an error-tolerant Chebyshev-filtered subspace iteration eigensolver, the matrix-free formalism yields substantial reductions in end-to-end time-to-solution using FE meshes that deliver desired accuracies in ground-state properties.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-10T13:19:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.comp-ph</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.08571v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.08571v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Supporting Dynamic Agentic Workloads: How Data and Agents Interact</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ioana Giurgiu, Michael E. Nidd
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-10T11:38:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09548v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09548v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Wang, Xi Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-10T08:35:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.01802v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.01802v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xun Li, Qiong Wu, Pingyi Fan, Kezhi Wang, Wen Chen, Khaled B. Letaief
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-10T07:19:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09378v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09378v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jinyu Chen, Long Shi, Taotao Wang, Jiaheng Wang, Wei Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-10T02:52:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09961v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09961v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Training-free Context-adaptive Attention for Efficient Long Context Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeng You, Yaofo Chen, Shuhai Zhang, Zhijie Qiu, Tingyu Wu, Yingjian Li, Yaowei Wang, Mingkui Tan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-10T01:54:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09238v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09238v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Häggström, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Håkan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-10T00:29:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.AR</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.03092v5' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.03092v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 StreamingThinker: Large Language Models Can Think While Reading</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junlong Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\% reduction in token waiting before the onset of reasoning and a more than 60\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-09T17:34:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.17238v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.17238v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongyuan Tao, Bencheng Liao, Shaoyu Chen, Haoran Yin, Qian Zhang, Wenyu Liu, Xinggang Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-09T17:18:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.08829v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.08829v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 StarDist: A Code Generator for Distributed Graph Algorithms</h2>
                <div class="authors">
                    <strong>Authors:</strong> Barenya Kumar Nandy, Rupesh Nasre
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-09T15:15:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.01646v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.01646v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR</h2>
                <div class="authors">
                    <strong>Authors:</strong> Agrim Bari, Gustavo de Veciana, Yuqi Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.   In this paper, we introduce the \textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.   We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-09T14:10:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.08626v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.08626v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dahyeon Kye, Jeahun Sung, Mingyu Jeon, Jihyong Oh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-09T07:35:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.07155v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.07155v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiayi Tian, Seyedarmin Azizi, Yequan Zhao, Erfan Baghaei Potraghloo, Sean McPherson, Sharath Nittur Sridhar, Zhengyang Wang, Zheng Zhang, Massoud Pedram, Souvik Kundu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-08T19:32:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.07993v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.07993v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vivek Chari, Benjamin Van Durme
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. We present Compactor, a training-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 20% fewer tokens in both synthetic and real-world context tasks, while being more task-robust. We further introduce a procedure for context-calibrated compression: inferring the maximum compression a given context supports before significant performance loss. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 68%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families. Finally, we release compactor-vllm, an inference engine and suite of optimized Triton kernels designed to efficiently support the sparse, non-contiguous memory access patterns inherent to compressed KV caches. This work demonstrates that Compactor offers a practical, high-performance solution for alleviating the memory bottleneck in modern LLM deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-08T19:02:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2507.08143v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2507.08143v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zizhuo Fu, Xiaotian Guo, Wenxuan Zeng, Shuzhang Zhong, Yadong Zhang, Peiyu Chen, Runsheng Wang, Le Ye, Meng Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications. However, the high energy and latency overhead induced by the KV cache limits the edge deployment, especially for long contexts. Emerging hybrid bonding (HB) technology has been proposed as a promising alternative to conventional near-memory processing (NMP) architectures, offering improved bandwidth efficiency and lower power consumption while exhibiting characteristics of distributed memory. In this paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse attention algorithm-hardware co-design for efficient LLM inference at the edge. At the algorithm level, we propose a hybrid sparse attention scheme with static and dynamic sparsity for different heads to fully leverage the sparsity with high accuracy. At the hardware level, we co-design the hardware to support hybrid sparse attention and propose memory-compute co-placement to address the distributed memory bottleneck. Since different attention heads exhibit different sparse patterns and the attention structure often mismatches the HB architecture, we further develop a load-balancing scheduler with parallel tiled attention to address workload imbalance and optimize the mapping strategy. Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and 6.22~73.48x energy efficiency improvement over baseline HB implementation, with a negligible average accuracy drop of 0.87% on multiple benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-08T13:48:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2508.16653v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2508.16653v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhongchun Zhou, Chengtao Lai, Yuhang Gu, Wei Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.   We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.   Finally, we implement the design in RTL and the area of our design is $\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-08T08:56:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.07312v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.07312v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jucheng Shen, Gaurav Sarkar, Yeonju Ro, Sharath Nittur Sridhar, Zhangyang Wang, Aditya Akella, Souvik Kundu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-08T05:15:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.07173v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.07173v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhifan Luo, Shuo Shao, Su Zhang, Lijing Zhou, Yuke Hu, Chenxu Zhao, Zhihao Liu, Zhan Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-08T02:23:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2508.09442v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2508.09442v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Leveraging KV Similarity for Online Structured Pruning in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jungmin Lee, Gwangeun Byeon, Yulhwa Kim, Seokin Hong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-08T01:56:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.07090v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.07090v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 AutoNeural: Co-Designing Vision-Language Models for NPU Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Chen, Liangmin Wu, Yunhai Hu, Zhiyuan Li, Zhiyuan Cheng, Yicheng Qian, Lingyue Zhu, Zhipeng Hu, Luoyi Liang, Qiang Tang, Zhen Liu, Han Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-08T00:15:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.02924v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.02924v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 MotionStream: Real-Time Video Generation with Interactive Motion Controls</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Shechtman, Xun Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons -- (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-08T00:05:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.01266v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.01266v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 ELANA: A Simple Energy and Latency Analyzer for LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hung-Yueh Chiang, Bokun Wang, Diana Marculescu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-07T18:43:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09946v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09946v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Spatial Retrieval Augmented Autonomous Driving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaosong Jia, Chenhe Zhang, Yule Jiang, Songbur Wong, Zhiyuan Zhang, Chen Chen, Shaofeng Zhang, Xuanhe Zhou, Xue Yang, Junchi Yan, Yu-Gang Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.   For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-07T14:40:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.06865v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.06865v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 EVCtrl: Efficient Control Adapter for Visual Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zixiang Yang, Yue Ma, Yinhan Zhang, Shanhui Mo, Dongrui Liu, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-07T14:21:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2508.10963v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2508.10963v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziran Qin, Youru Lv, Mingbao Lin, Hang Guo, Zeren Zhang, Danping Zou, Weiyao Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual Autoregressive (VAR) models adopt a next-scale prediction paradigm, offering high-quality content generation with substantially fewer decoding steps. However, existing VAR models suffer from significant attention complexity and severe memory overhead due to the accumulation of key-value (KV) caches across scales. In this paper, we tackle this challenge by introducing KV cache compression into the next-scale generation paradigm. We begin with a crucial observation: attention heads in VAR models can be divided into two functionally distinct categories: Contextual Heads focus on maintaining semantic consistency, while Structural Heads are responsible for preserving spatial coherence. This structural divergence causes existing one-size-fits-all compression methods to perform poorly on VAR models. To address this, we propose HACK, a training-free Head-Aware KV cache Compression frameworK. HACK utilizes an offline classification scheme to separate head types, enabling it to apply pattern-specific compression strategies with asymmetric cache budgets for each category. By doing so, HACK effectively constrains the average KV cache length within a fixed budget $B$, reducing the theoretical attention complexity from $\mathcal{O}(n^4)$ to $\mathcal{O}(Bn^2)$. Extensive experiments on multiple VAR models across text-to-image and class-conditional tasks validate the effectiveness and generalizability of HACK. It achieves up to 70% KV cache compression without degrading output quality, resulting in memory savings and faster inference. For example, HACK provides a $1.75\times$ memory reduction and a $1.57\times$ speedup on Infinity-8B.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-07T12:44:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2504.09261v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2504.09261v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sourjya Roy, Shrihari Sridharan, Surya Selvam, Anand Raghunathan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-07T08:40:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.06727v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.06727v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei-Bin Kou, Guangxu Zhu, Jingreng Lei, Chen Zhang, Yik-Chung Wu, Jianping Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-07T05:28:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.06664v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.06664v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Solving larger Travelling Salesman Problem networks with a penalty-free Variational Quantum Algorithm</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Goldsmith, Xing Liang, Dimitrios Makris, Hongwei Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Travelling Salesman Problem (TSP) is a well-known NP-Hard combinatorial optimisation problem, with industrial use cases such as last-mile delivery. Although TSP has been studied extensively on quantum computers, it is rare to find quantum solutions of TSP network with more than a dozen locations. In this paper, we present high quality solutions in noise-free Qiskit simulations of networks with up to twelve locations using a hybrid penalty-free, circuit-model, Variational Quantum Algorithm (VQA). Noisy qubits are also simulated. To our knowledge, this is the first successful VQA simulation of a twelve-location TSP on circuit-model devices. Multiple encoding strategies, including factorial, non-factorial, and Gray encoding are evaluated. Our formulation scales as $\mathcal{O}(nlog_2(n))$ qubits, requiring only 29 qubits for twelve locations, compared with over 100 qubits for conventional approaches scaling as $\mathcal{O}(n^2)$. Computational time is further reduced by almost two orders of magnitude through the use of Simultaneous Perturbation Stochastic Approximation (SPSA) gradient estimation and cost-function caching. We also introduce a novel machine-learning model, and benchmark both quantum and classical approaches against a Monte Carlo baseline. The VQA outperforms the classical machine-learning approach, and performs similarly to Monte Carlo for the small networks simulated. Additionally, the results indicate a trend toward improved performance with problem size, outlining a pathway to solving larger TSP instances on quantum devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-06T18:21:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.06523v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.06523v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Convolution operators preserving the set of totally positive sequences</h2>
                <div class="authors">
                    <strong>Authors:</strong> Olga Katkova, Anna Vishnyakova
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A real sequence $(a_k)_{k=0}^\infty$ is called {\it totally positive} if all minors of the infinite Toeplitz matrix $ \left\| a_{j-i} \right\|_{i, j =0}^\infty$ are nonnegative (here $a_k=0$ for $k<0$). In this paper, which continues our earlier work \cite{kv}, we investigate the set of real sequences $(b_k)_{k=0}^\infty$ with the property that for every totally positive sequence $(a_k)_{k=0}^\infty,$ the sequense of termwise products $(a_k b_k)_{k=0}^\infty$ is also totally positive. In particular, we show that for every totally positive sequence $(a_k)_{k=0}^\infty$ the sequence $\left(a_k a^{-k (k-1)}\right)_{k=0}^\infty$ is totally positive whenever $a^2\geq 3{.}503.$ We also propose several open problems concerning convolution operators that preserve total positivity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-06T15:13:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.CV</span><span>math.FA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.06468v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.06468v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangyu Li, Chengyu Yin, Weijun Wang, Jianyu Wei, Ting Cao, Yunxin Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.   However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.   To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-06T14:14:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.06443v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.06443v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yize Wu, Ke Gao, Ling Li, Yanjun Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. We observe that such inefficiency stems from the sequential execution of layers, which is seemingly natural but actually unnecessary. Therefore, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the inter-layer data dependencies in the draft model, enabling multiple layers to run simultaneously across multiple devices as 'fuzzy' speculation. After each drafting-and-verification iteration, the draft model's key-value cache is calibrated in a single forward pass, preventing long-term fuzzy-error accumulation at minimal additional latency. EasySpec is a training-free and plug-in method. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distributions of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum speculation accuracy drop of only 7%. The code is available at https://github.com/Yize-Wu/EasySpec.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-06T08:55:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2502.02493v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2502.02493v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Damien Lesens, Beheshteh T. Rakhshan, Guillaume Rabusseau
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-05T17:51:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.05916v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.05916v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Morphling: Fast, Fused, and Flexible GNN Training at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anubhab, Rupesh Nasre
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-05T16:07:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.01678v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.01678v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yubo Huang, Hailong Guo, Fangtai Wu, Shifeng Zhang, Shijie Huang, Qijun Gan, Lin Liu, Sirui Zhao, Enhong Chen, Jiaming Liu, Steven Hoi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-05T06:32:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.04677v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.04677v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhan Liu, Yihua Cheng, Jiayi Yao, Yuwei An, Xiaokun Chen, Shaoting Feng, Yuyang Huang, Samuel Shen, Rui Zhang, Kuntai Du, Junchen Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-05T04:52:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.09665v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.09665v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiyan Zhai, Bintang Dwi Marthen, Sarath Balivada, Vamsi Sudhakar Bojji, Eric Knauft, Jitender Rohilla, Jiaqi Zuo, Quanxing Liu, Maxime Austruy, Wenguang Wang, Juncheng Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-05T02:13:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.21958v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.21958v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Cloud-Native Vector Search: A Comprehensive Performance Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhaoheng Li, Wei Ding, Silu Huang, Zikang Wang, Yuanjin Lin, Ke Wu, Yongjoo Park, Jianjun Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.   This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-05T00:04:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.14748v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.14748v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jung Yi, Wooseok Jang, Paul Hyunbin Cho, Jisu Nam, Heeji Yoon, Seungryong Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-04T18:46:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.05081v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.05081v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 A Survey on Diffusion Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-04T17:57:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2508.10875v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2508.10875v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhijian Shu, Cheng Lin, Tao Xie, Wei Yin, Ben Li, Zhiyuan Pu, Weize Li, Yao Yao, Xun Cao, Xiaoyang Guo, Xiao-Xiao Long
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-04T16:07:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.04939v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.04939v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziran Qin, Youru Lv, Mingbao Lin, Zeren Zhang, Chanfan Gan, Tieyuan Chen, Weiyao Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-04T14:41:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.04857v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.04857v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seungwon Choi, Dong-Gyu Park, Seo-Yeon Hwang, Tae-Wan Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-04T12:53:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.03397v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.03397v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Semantic-Aware Caching for Efficient Image Generation in Edge Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanshuai Cui, Zhiqing Tang, Zhi Yao, Weijia Jia, Wei Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-04T07:11:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.22421v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.22421v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 EgoLCD: Egocentric Video Generation with Long Context Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liuzhou Zhang, Jiarui Ye, Yuanlei Wang, Ming Zhong, Mingju Cao, Wanke Xia, Bowen Zeng, Zeyu Zhang, Hao Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-04T06:53:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.04515v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.04515v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jialong Qin, Xin Zou, Di Lu, Yibo Yan, Xuming Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-04T06:19:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.08003v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.08003v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ryan Swann, Muhammad Osama, Xiaohu Guo, Bryant Nelson, Lixun Zhang, Alex Brown, Yen Ong, Ali Yazdani, Sean Siddens, Ganesh Dasika, Alex Underwood
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T19:46:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.04226v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.04226v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 RELIC: Interactive Video World Model with Long-Horizon Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yicong Hong, Yiqun Mei, Chongjian Ge, Yiran Xu, Yang Zhou, Sai Bi, Yannick Hold-Geoffroy, Mike Roberts, Matthew Fisher, Eli Shechtman, Kalyan Sunkavalli, Feng Liu, Zhengqi Li, Hao Tan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T18:29:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.04040v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.04040v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Demonstration of KV-Class \b{eta}-Ga2O3 Trench Junction Barrier Schottky Diodes with SpaceModulated Junction Termination Extension</h2>
                <div class="authors">
                    <strong>Authors:</strong> Advait Gilankar, Julian Gervassi-Saga, Martha R. McCartney, Nabasindhu Das, David Malcolm McComas, David J. Smith, Nidhin Kurian Kalarickal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we report on the design and fabrication of p-NiO/Ga2O3 trench junction barrier schottky diodes (JBSD) integrated with space-modulated junction termination extension (SM-JTE) and compare the performance with planar Ni/Ga2O3 schottky diodes (SBDs) and p-NiO/Ga2O3 heterojunction diodes (HJDs). The JBSDs achieved breakdown voltages exceeding 1.8 kV along with low leakage currents (<10-2 A/cm2), while displaying low turn on voltage (VON) of ~1V, which is similar to that of planar Ni/Ga2O3 SBDs. The fabricated devices showed excellent forward characteristics with low differential on-resistance (Ron,sp) ranging from 4-10.5 mΩ-cm2, for fin width between 0.6- 1.25 microns. Best performing device with fin width of 0.85μm showed a unipolar figure of merit (FOM) of ~0.7GW/cm2. This work showcases the benefits of trench JBS design along with SM-JTE edge-termination for efficient high-performance kilovolt-class \b{eta}- Ga2O3 diodes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T18:17:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.app-ph</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.04033v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.04033v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaolong Li, Youping Gu, Xi Lin, Weijie Wang, Bohan Zhuang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T18:02:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.04025v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.04025v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 OOPredictor: Predicting Object-Oriented Accesses using Static Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hassan Arafat, David Bremner, Kenneth B. Kent, Julian Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T17:05:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.03972v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.03972v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liujianfu Wang, Yuyang Du, Yuchen Pan, Soung Chang Liew, Jiacheng Liu, Kexin Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T16:27:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.03927v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.03927v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andreas Kosmas Kakolyris, Dimosthenis Masouros, Petros Vavaroutsos, Sotirios Xydis, Dimitrios Soudris
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \textit{throttLL'eM} achieves up to 43.8\% lower energy consumption and an energy efficiency improvement of at least $1.71\times$ under SLOs, when compared to NVIDIA's Triton server.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T16:21:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.AR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2408.05235v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2408.05235v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongzhan Lin, Zhiqi Bai, Xinmiao Zhang, Sen Yang, Xiang Li, Siran Yang, Yunlong Xu, Jiaheng Liu, Yongchi Zhao, Jiamang Wang, Yuchi Xu, Wenbo Su, Bo Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T15:22:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.03870v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.03870v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lishuo Deng, Shaojie Xu, Jinwu Chen, Changwei Yan, Jiajie Wang, Zhe Jiang, Weiwei Shan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.   We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\(\times\)/1.94\(\times\)/2.05\(\times\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T09:41:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.03608v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.03608v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Fletch: File-System Metadata Caching in Programmable Switches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qingxiu Liu, Jiazhen Cai, Siyuan Sheng, Yuhui Chen, Lu Tang, Zhirong Shen, Patrick P. C. Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fast and scalable metadata management across multiple metadata servers is crucial for distributed file systems to handle numerous files and directories. Client-side caching of frequently accessed metadata can mitigate server loads, but incurs significant overhead and complexity in maintaining cache consistency when the number of clients increases. We explore caching in programmable switches by serving file-system metadata requests from multiple clients on the switch data plane. Despite prior efforts on in-switch key-value caching, they fail to address the path dependencies specific to file-system semantics. We propose Fletch, an in-switch file-system metadata caching framework that leverages programmable switches to serve file-system metadata requests from multiple clients directly in the switch data plane. Unlike prior in-switch key-value caching approaches, Fletch addresses file-system-specific path dependencies under stringent switch resource constraints. We implement Fletch atop Hadoop HDFS and evaluate it on a Tofino-switch testbed using real-world file-system metadata workloads. Fletch achieves up to 181.6% higher throughput than vanilla HDFS and complements client-side caching with additional throughput gains of up to 139.6%. It also incurs low latencies and limited switch resource usage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T09:23:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.08351v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.08351v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Decentralized Fairness Aware Multi Task Federated Learning for VR Network</h2>
                <div class="authors">
                    <strong>Authors:</strong> Krishnendu S. Tharakan, Carlo Fischione
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T08:13:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.IT</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.02513v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.02513v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ngoc Bui, Shubham Sharma, Simran Lamba, Saumitra Mishra, Rex Ying
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-03T00:20:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.03324v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.03324v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Generation of strong ultralow-phase-noise microwave fields with tunable ellipticity for ultracold polar molecules</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shrestha Biswas, Sebastian Eppelt, Christian Buchberger, Xing-Yan Chen, Andreas Schindewolf, Michael Hani, Erwin Biebl, Immanuel Bloch, Xin-Yu Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Microwave(MW) fields with strong field strength, ultralow phase-noise and tunable polarization are crucial for stabilizing and manipulating ultracold polar molecules, which have emerged as a promising platform for quantum sciences. In this letter, we present the design, characterization, and performance of a robust MW setup tailored for precise control of molecular states. This setup achieves a high electric field intensity of 6.9 kV/m in the near-field from a dual-feed waveguide antenna, enabling a Rabi frequency as high as 71 MHz for the rotational transition of sodium-potassium molecules. In addition, the low noise signal source and controlled electronics provide ultralow phase-noise and dynamically tunable polarization. Narrow-band filters within the MW circuitry further reduce phase-noise by more than 20 dB at 20 MHz offset frequency, ensuring prolonged one-body molecular lifetimes up to 10 seconds. We also show practical methods to measure the MW field strength and polarization using a simple homemade dipole probe, and to characterize phase-noise down to -170 dBc/Hz with a commercial spectrum analyser and a notch filter. Those capabilities allowed us to evaporatively cool our molecular sample to deep quantum degeneracy. Furthermore, the polarization tunability enabled the observation of field-linked resonances and facilitated the creation of field-linked tetramers.These techniques advance the study of ultracold polar molecules and broaden the potential applications of MW tools in other platforms of quantum sciences.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-02T18:32:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.atom-ph</span><span>cond-mat.quant-gas</span><span>physics.ins-det</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.03007v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.03007v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Computational Model for Photoionization in Pure SF6 Streamer at 1-15 atm</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zihao Feng, Liyang Zhang, Xiaobing Zou, Haiyun Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Photoionization plays a crucial role in achieving accurate quantitative predictions in SF6 streamer simulations, but accurate models for SF6 photoionization remains limited, motivating this paper. First, we develop a computational model for SF6 photoionization and provide the detailed theoretical modeling process, as well as comparison between experiment and simulation. A concise summary of model parameters within the comprehensive pressure range of 1 - 15 atm is provided for direct reference. Then, we perform comparative studies against simplified approaches. The results demonstrate that the proposed model effectively captures the non-local effects of SF6 photoionization, enhancing both the spatial numerical convergence and the accuracy of the streamer structure. Finally, we perform comparative studies by artificially increasing the photoionization intensity through multiplying the photoionization source term Sph by a factor of 50 (50*Sph) relative to the baseline intensity. Regarding breakdown voltage prediction, 50*Sph leads to a significant underestimation of the breakdown voltage for positive streamers, introducing errors greater than 0.5 kV, while exerting a small impact on negative streamers. Regarding streamer propagation dynamics, the radius of the positive streamer head exhibits pronounced shrinking, and 50*Sph reduces this shrinking and significantly lowers the head field by more than 700 Td. In contrast, 50*Sph has little impact on the morphology of the negative streamers and slightly enhances the head field by less than 30 Td.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-02T07:05:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.plasm-ph</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2505.04216v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2505.04216v2' target='_blank'>pdf</a><a href='https://doi.org/10.1088/1361-6595/ae259e' target='_blank'>doi</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ning Wang, Sainyam Galhotra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-02T06:05:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.02444v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.02444v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhendong Tan, Xingjun Zhang, Chaoyi Hu, Junjie Peng, Kun Xia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-02T02:15:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.02337v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.02337v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mani Tofigh, Edward Guo, Weiwei Jia, Xiaoning Ding, Zirui Neil Zhao, Jianchen Shan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-02T01:24:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.OS</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.09956v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.09956v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kitsuya Azuma, Takayuki Nishio, Yuichi Kitagawa, Wakako Nakano, Takahito Tanimura
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels, i.e., normalized probability distributions) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining competitive accuracy. Enhanced ERA resolves the fundamental instability of conventional temperature-based aggregation, ensuring robust control and high performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-02T00:43:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2504.19602v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2504.19602v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Liu, Chen Qian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T23:53:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.02281v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.02281v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Unleashing Hour-Scale Video Training for Long Video-Language Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Xiaodong Yu, Hao Chen, Jiebo Luo, Zicheng Liu, Emad Barsoum
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T22:47:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2506.05332v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2506.05332v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ching Fang, Kanaka Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T21:56:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2506.19686v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2506.19686v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aaron Jarmusch, Sunita Chandrasekaran
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.   Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T20:31:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.02189v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.02189v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sai Gokhale, Devleena Das, Rajeev Patwari, Ashish Sirasao, Elliott Delaye
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T18:03:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.01953v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.01953v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in Cold Xenon Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> M. Adrover, L. Baudis, A. Bismark, A. P. Colijn, J. J. Cuenca-García, M. P. Decowski, M. Flierman, T. den Hollander
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Hamamatsu R12699-406-M2 is a $2\times2$ multi-anode 2-inch photomultiplier tube that offers a compact form factor, low intrinsic radioactivity, and high photocathode coverage. These characteristics make it a promising candidate for next-generation xenon-based direct detection dark matter experiments, such as XLZD and PandaX-xT. We present a detailed characterization of this photosensor operated in cold xenon environments, focusing on its single photoelectron response, dark count rate, light emission, and afterpulsing behavior. The device demonstrated a gain exceeding $2\cdot 10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of $(0.4\pm0.2)\;\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited short delay times, resulting in some cases in an overlap with the light-induced signal. To evaluate its applicability in a realistic detector environment, two R12699-406-M2 units were deployed in a small-scale dual-phase xenon time projection chamber. The segmented $2\times2$ anode structure enabled lateral position reconstruction using a single photomultiplier tube, highlighting the potential of the sensor for effective event localization in future detectors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T17:42:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.ins-det</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2506.04844v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2506.04844v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 A Low-Cost Reliable Racetrack Cache Based on Data Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Elham Cheshmikhani, Fateme Shokouhinia, Hamed Farbeh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> SRAM-based cache memory faces several scalability limitations in deep nanoscale technologies, e.g., high leakage current, low cell stability, and low density. Emerging Non-Volatile Memory (NVM) technologies have received lots of attention in recent years, where Racetrack Memory (RTM) is among the most promising ones. RTM has the highest density among all NVMs and its access performance is comparable to SRAM technology. Therefore, RTM is a suitable alternative for SRAM in the Last-Level Caches (LLCs). Despite all its benefits, RTM confronts different reliability challenges due to the stochastic behavior of its storage element and highly error-prone data shifting, leading to a high probability of multiple-bit errors. Conventional Error-Correcting Codes (ECCs) are either incapable of tolerating multiple-bit errors or require a large amount of extra storage for check bits. This paper proposes taking advantage of value locality for compressing data blocks and freeing up a large fraction of cache blocks for storing data redundancy of strong ECCs. Utilizing the proposed scheme, a large majority of cache blocks are protected by strong ECCs to tolerate multiple-bit errors without any storage overhead. The evaluation using gem5 full-system simulator demonstrates that the proposed scheme enhances the mean-time-to-failure of the cache by an average of 11.3x with less than 1% hardware and performance overhead.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T17:32:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.ET</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.01915v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.01915v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Efficient Low Rank Attention for Long-Context Inference in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tenghui Li, Guoxu Zhou, Xuyang Zhao, Yuning Qiu, Qibin Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\(r\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \(\mathcal{O}(lr)\) time at each decode step. By selecting only the top-\(k\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T12:51:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.23649v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.23649v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 RoMe: Row Granularity Access Memory System for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hwayong Nam, Seungmin Baek, Jumin Kim, Michael Jaemin Kim, Jung Ho Ahn
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.   To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T11:14:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.01541v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.01541v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zipeng Wang, Dan Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T11:12:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.01540v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.01540v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenbin Zhu, Zhaoyan Shen, Zili Shao, Hongjun Dai, Feng Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T07:10:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.01357v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.01357v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunfeng Wu, Jiayi Song, Zhenxiong Tan, Zihao He, Songhua Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T06:11:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.14712v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.14712v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilong Zhao, Jiaming Tang, Kan Zhu, Zihao Ye, Chi-Chih Chang, Chaofan Lin, Jongseok Park, Guangxuan Xiao, Mohamed S. Abdelfattah, Mingyu Gao, Baris Kasikci, Song Han, Ion Stoica
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.   To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-01T04:50:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.01278v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.01278v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 EPLKG: Efficient Prompt Learning with Knowledge Graph</h2>
                <div class="authors">
                    <strong>Authors:</strong> YongTaek Lim, Suho Kang, Yewon Kim, Dokyung Yoon, KyungWoo Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large-scale pre-trained models such as CLIP excel in transferability and robust generalization across diverse datasets. However, adapting these models to new datasets or domains is computationally costly, especially in low-resource or few-shot settings, and existing prompt-learning methods often lack interpretability. We introduce Efficient Prompt Learning with Knowledge Graph (EPLKG), which uses a knowledge graph to curate diverse, interpretable prompts and, where KG coverage is limited, augments this bank with LLM-generated human-readable visual descriptions. EPLKG operates entirely on cached CLIP image and text embeddings and employs a lightweight Gumbel-Softmax module to select a single prompt per image-class pair, enabling low-memory, fast training. Across 11 benchmarks, EPLKG reduces per-image training time by up to 45 percent and peak GPU memory by around 30 to 40 percent compared to strong prompt-learning baselines, while keeping the average base-new harmonic-mean accuracy within 2 percentage points, thereby improving the efficiency of model adaptation without sacrificing competitive performance or interpretability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-11-30T14:24:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2304.10805v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2304.10805v3' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 Particulate: Feed-Forward 3D Object Articulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruining Li, Yuxin Yao, Chuanxia Zheng, Christian Rupprecht, Joan Lasenby, Shangzhe Wu, Andrea Vedaldi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:59:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.GR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11798v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11798v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fabrizio Falasca
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green's function experiments. To explore the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and to probe causal mechanisms. We argue that the ability of emulators of multiscale systems to reproduce perturbed statistics depends critically on (i) the choice of an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. These insights highlight reduced-order models, tailored to specific goals, processes, and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application by developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system's probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. Overall, these results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory providing a principled framework to guide model design and enhance causal understanding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:57:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>nlin.CD</span><span>cond-mat.stat-mech</span><span>cs.LG</span><span>physics.ao-ph</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2506.22552v6' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2506.22552v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrew Adiletta, Kathryn Adiletta, Kemal Derya, Berk Sunar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.   Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:52:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11783v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11783v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoyu Ma, Zhengqing Yuan, Zheyuan Zhang, Kaiwen Shi, Lichao Sun, Yanfang Ye
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:30:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11769v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11769v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Covariate-assisted graph matching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Trisha Dawn, Jesús Arroyo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Data integration is essential across diverse domains, from historical records to biomedical research, facilitating joint statistical inference. A crucial initial step in this process involves merging multiple data sources based on matching individual records, often in the absence of unique identifiers. When the datasets are networks, this problem is typically addressed through graph matching methodologies. For such cases, auxiliary features or covariates associated with nodes or edges can be instrumental in achieving improved accuracy. However, most existing graph matching techniques do not incorporate this information, limiting their performance against non-identifiable and erroneous matches. To overcome these limitations, we propose two novel covariate-assisted seeded graph matching methods, where a partial alignment for a set of nodes, called seeds, is known. The first one solves a quadratic assignment problem (QAP) over the whole graph, while the second one only leverages the local neighborhood structure of seed nodes for computational scalability. Both methods are grounded in a conditional modeling framework, where elements of one graph's adjacency matrix are modeled using a generalized linear model (GLM), given the other graph and the available covariates. We establish theoretical guarantees for model estimation error and exact recovery of the solution of the QAP. The effectiveness of our methods is demonstrated through numerical experiments and in an application to matching the statistics academic genealogy and the collaboration networks. By leveraging additional covariates, we achieve improved alignment accuracy. Our work highlights the power of integrating covariate information in the classical graph matching setup, offering a practical and improved framework for combining network data with wide-ranging applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:12:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11761v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11761v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 The Effect of a Non-universal Extinction Curve on the Wesenheit Function and Cepheid Distances</h2>
                <div class="authors">
                    <strong>Authors:</strong> D. M. Skowron, M. L. Fouesneau, R. Drimmel, S. Khanna
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Wesenheit function is widely used to reduce the effects of interstellar reddening in distance measurements. Its construction, however, relies on the assumption of a universal extinction curve and on fixed values of the total-to-selective extinction ratio, Rv. Recent studies have shown that Rv varies significantly across the Milky Way and between different galaxies, raising concerns about systematic biases in Wesenheit magnitudes and period-Wesenheit relations. In this work, we discuss the impact of non-universal extinction on Wesenheit indices by combining the Rv-dependent extinction curve with a grid of stellar atmosphere models. We compute the integrated extinction in optical and near-infrared passbands, derive Rv-dependent R coefficients for multiple Wesenheit indices, and examine how changes in Rv propagate into Wesenheit magnitudes and Cepheid distances in our Galaxy. We find that the R coefficients in the Wesenheit functions vary strongly with Rv. For classical Cepheids in the Milky Way disk, variations of Rv within the typical observed range (2.6-3.6) can lead to substantial differences in the Wesenheit function, reaching +-0.7 mag from the mean for the Gaia-based Wesenheit index W_G and resulting in distance errors of almost 40%. Near-infrared Wesenheit indices are much less sensitive to Rv changes. Our results clearly show that accounting for variable Rv is essential when applying period-Wesenheit relations, particularly in the optical regime, or that near or mid infrared based distances should be used. While we present this effect for classical Cepheids, it applies to all pulsating stars for which period-Wesenheit relations are used to infer distances.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:08:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span><span>astro-ph.SR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11758v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11758v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuming Feng, Xinrui Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Online product reviews contain rich but noisy signals that overwhelm users and hinder effective decision-making. Existing LLM-based summarizers remain generic and fail to account for individual preferences, limiting their practical utility. We propose SUMFORU, a steerable review summarization framework that aligns outputs with explicit user personas to support personalized purchase decisions. Our approach integrates a high-quality data pipeline built from the Amazon 2023 Review Dataset with a two-stage alignment procedure: (1) persona-aware Supervised Fine-Tuning (SFT) via asymmetric knowledge distillation, and (2) Reinforcement Learning with AI Feedback (RLAIF) using a preference estimator to capture fine-grained, persona-relevant signals. We evaluate the model across rule-based, LLM-based, and human-centered metrics, demonstrating consistent improvements in consistency, grounding, and preference alignment. Our framework achieves the highest performance across all evaluation settings and generalizes effectively to unseen product categories. Our results highlight the promise of steerable pluralistic alignment for building next-generation personalized decision-support systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:05:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11755v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11755v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 More powerful multiple testing under dependence via randomization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyu Xu, Aaditya Ramdas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We develop a technique to improve the power of any e-value by a simple randomization involving one independent uniform random variable. Using this framework, we show that two procedures for false discovery rate (FDR) control -- the Benjamini-Yekutieli procedure for dependent p-values, and the e-Benjamini-Hochberg procedure for dependent e-values -- can be improved through randomization. We also improve the Hommel test under dependence, and post-selection inference procedures for confidence intervals with false coverage rate (FCR) that allow for arbitrary selection rules and dependence. Importantly, our randomized improvements are never worse than the originals and are typically strictly more powerful, with marked improvements in simulations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:00:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2305.11126v4' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2305.11126v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ci Zhang, Huayu Li, Changdi Yang, Jiangnan Xia, Yanzhi Wang, Xiaolong Ma, Jin Lu, Ao Li, Geng Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent studies show that using diffusion models for time series signal reconstruction holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adaptively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency overhead. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:57:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.07873v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.07873v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Forest Kernel Balancing Weights: Outcome-Guided Features for Causal Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andy A. Shen, Eli Ben-Michael, Avi Feller, Luke Keele, Jared Murray
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While balancing covariates between groups is central for observational causal inference, selecting which features to balance remains a challenging problem. Kernel balancing is a promising approach that first estimates a kernel that captures similarity across units and then balances a (possibly low-dimensional) summary of that kernel, indirectly learning important features to balance. In this paper, we propose forest kernel balancing, which leverages the underappreciated fact that tree-based machine learning models, namely random forests and Bayesian additive regression trees (BART), implicitly estimate a kernel based on the co-occurrence of observations in the same terminal leaf node. Thus, even though the resulting kernel is solely a function of baseline features, the selected nonlinearities and other interactions are important for predicting the outcome -- and therefore are important for addressing confounding. Through simulations and applied illustrations, we show that forest kernel balancing leads to meaningful computational and statistical improvement relative to standard kernel methods, which do not incorporate outcome information when learning features.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:52:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11751v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11751v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Darvin Yi, Teng Liu, Mattie Terzolo, Lance Hasson, Ayan Sinha, Pablo Mendes, Andrew Rabinovich
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:51:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.12306v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.12306v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Bias-Reduced Estimation of Structural Equation Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haziq Jamil, Yves Rosseel, Oliver Kemp, Ioannis Kosmidis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Finite-sample bias is a pervasive challenge in the estimation of structural equation models (SEMs), especially when sample sizes are small or measurement reliability is low. A range of methods have been proposed to improve finite-sample bias in the SEM literature, ranging from analytic bias corrections to resampling-based techniques, with each carrying trade-offs in scope, computational burden, and statistical performance. We apply the reduced-bias M-estimation framework (RBM, Kosmidis & Lunardon, 2024, J. R. Stat. Soc. Series B Stat. Methodol.) to SEMs. The RBM framework is attractive as it requires only first- and second-order derivatives of the log-likelihood, which renders it both straightforward to implement, and computationally more efficient compared to resampling-based alternatives such as bootstrap and jackknife. It is also robust to departures from modelling assumptions. Using the same simulation setup as in Dhaene and Rosseel (2022), we illustrate that RBM estimators consistently reduce mean bias in the estimation of SEMs without inflating mean squared error. They also deliver improvements in both median bias and inference relative to maximum likelihood estimators, while maintaining robustness under non-normality. Our findings suggest that RBM offers a promising, practical, and broadly applicable tool for mitigating bias in the estimation of SEMs, particularly in small-sample research contexts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:51:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.CO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2509.25419v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2509.25419v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minglei Shi, Haolin Wang, Borui Zhang, Wenzhao Zheng, Bohan Zeng, Ziyang Yuan, Xiaoshi Wu, Yuanxing Zhang, Huan Yang, Xintao Wang, Pengfei Wan, Kun Gai, Jie Zhou, Jiwen Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:45:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11749v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11749v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 The low-mass and structured stellar halo of M83 argues against a merger origin for its starburst and extended neutral hydrogen disk</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eric F. Bell, Benjamin Harmsen, Matthew Cosby, Paul A. Price, Sarah Pearson, Antonela Monachesi, Roelof S. de Jong, Richard D'Souza, Katya Gozman, Jacob Nibauer, Michael P. Busch, Jeremy Bailin, Benne W. Holwerda, In Sung Jang, Adam Smercina
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A merger origin has been suggested for M83's massive, metal-rich extended HI disk and nuclear starburst. We observe M83's stellar halo to test this idea. We train nearest-neighbor star-galaxy separation on wide-area Subaru imaging with Hubble Space Telescope data to map M83's halo in resolved stars. We find that M83 has an extended, very low density smooth stellar halo of old and metal-poor [M/H]$\sim -1.15$ RGB stars with a mass between 15 and 40 kpc of $\log_{10}M_{*,15-40,maj}/M_{\odot}=8.02\pm0.10$. In addition to M83's well-known Northern Stream, our ground-based Subaru imaging reveals a new stream to M83's south, which modeling suggests could be its trailing arm. The combined stream masses are $\log_{10}M_{stream}/M_{\odot}=7.93\pm0.10$, with metallicity [M/H]$= -1.0\pm0.2$. The stream progenitor was only recently accreted, as its stellar populations suggest that it formed stars until $2.1\pm1.3$ Gyr ago. M83 lies on the stellar halo mass-metallicity correlation seen for other Milky Way mass galaxies, albeit with low stellar halo mass. We infer a total accreted mass of $\log_{10}M_{*,accreted}/M_{\odot}=8.78^{+0.22}_{-0.28}$, with the most massive past merger having $\log_{10}M_{*,dom}/M_{\odot}=8.5\pm0.3$. We identify plausible M83 analogs in TNG-50 with similar stellar halos, finding that while a recent accretion can create a prominent stellar stream, such accretions do not trigger starburst activity, nor do they deliver enough gas to form M83's extended Hi disk. We conclude that other non-merger mechanisms, such as secular evolution or accretion of gas from the IGM, are likely to be responsible for M83's remarkable properties.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:38:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11744v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11744v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Annabelle Sujun Tang, Christopher Priebe, Rohan Mahapatra, Lianhui Qin, Hadi Esmaeilzadeh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:38:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2506.01374v4' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2506.01374v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Data-driven dust inference at mid-to-high Galactic latitudes using probabilistic machine learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matthew O'Callaghan, Kaisey S. Mandel, Gerry Gilmore
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a method for accurately and precisely inferring photometric dust extinction towards stars at mid-to-high Galactic latitudes using probabilistic machine learning to model the colour-magnitude distribution of zero-extinction stars in these regions. Photometric dust maps rely on a robust method for inferring stellar reddening. At high Galactic latitudes, where extinction is low, such inferences are particularly susceptible to contamination from modelling errors and prior assumptions, potentially introducing artificial structure into dust maps. In this work, we demonstrate the use of normalising flows to learn the conditional probability distribution of the photometric colour-magnitude relations of zero-extinction stars, conditioned on Galactic cylindrical coordinates for stars at mid-to-high Galactic latitudes. By using the normalising flow to model the colour-magnitude diagram, we infer the posterior distribution of dust extinction towards stars along different lines of sight by marginalising over the colour-magnitude flow. We validate our method using data from Gaia, Pan-STARRS, and 2MASS, showing that we can recover unbiased posteriors and successfully detect dust along the line-of-sight in two calibration regions at mid-Galactic latitude that have been extensively studied in the context of polarisation surveys.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:35:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2508.05781v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2508.05781v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting</h2>
                <div class="authors">
                    <strong>Authors:</strong> James Luther, Donald Brown
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Culture is a core component of human-to-human interaction and plays a vital role in how we perceive and interact with others. Advancements in the effectiveness of Large Language Models (LLMs) in generating human-sounding text have greatly increased the amount of human-to-computer interaction. As this field grows, the cultural alignment of these human-like agents becomes an important field of study. Our work uses Hofstede's VSM13 international surveys to understand the cultural alignment of the following models: DeepSeek-V3, V3.1, GPT-4, GPT-4.1, GPT-4o, and GPT-5. We use a combination of prompt language and cultural prompting, a strategy that uses a system prompt to shift a model's alignment to reflect a specific country, to align these LLMs with the United States and China. Our results show that DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language. We also find that GPT-4 exhibits an alignment closer to China when prompted in English, but cultural prompting is effective in shifting this alignment closer to the United States. Other low-cost models, GPT-4o and GPT-4.1, respond to the prompt language used (i.e., English or Simplified Chinese) and cultural prompting strategies to create acceptable alignments with both the United States and China.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:25:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09772v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09772v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Misspecification-robust amortised simulation-based inference using variational methods</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matthew O'Callaghan, Kaisey S. Mandel, Gerry Gilmore
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in neural density estimation have enabled powerful simulation-based inference (SBI) methods that can flexibly approximate Bayesian inference for intractable stochastic models. Although these methods have demonstrated reliable posterior estimation when the simulator accurately represents the underlying data generative process (DGP), recent work has shown that they perform poorly in the presence of model misspecification. This poses a significant issue for their use in real-world problems, due to simulators always misrepresenting the true DGP to a certain degree. In this paper, we introduce robust variational neural posterior estimation (RVNP), a method which addresses the problem of misspecification in amortised SBI by bridging the simulation-to-reality gap using variational inference and error modelling. We test RVNP on multiple benchmark tasks, including using real data from astronomy, and show that it can recover robust posterior inference in a data-driven manner without adopting hyperparameters or priors governing the misspecification influence.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:22:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>astro-ph.GA</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2509.05724v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2509.05724v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Model Error Resonance: The Geometric Nature of Error Dynamics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuntao Dai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces a geometric theory of model error, treating true and model dynamics as geodesic flows generated by distinct affine connections on a smooth manifold. When these connections differ, the resulting trajectory discrepancy--termed the Latent Error Dynamic Response (LEDR)--acquires an intrinsic dynamical structure governed by curvature. We show that the LEDR satisfies a Jacobi-type equation, where curvature mismatch acts as an explicit forcing term. In the important case of a flat model connection, the LEDR reduces to a classical Jacobi field on the true manifold, causing Model Error Resonance (MER) to emerge under positive sectional curvature. The theory is extended to a discrete-time analogue, establishing that this geometric structure and its resonant behavior persist in sampled systems. A closed-form analysis of a sphere--plane example demonstrates that curvature can be inferred directly from the LEDR evolution. This framework provides a unified geometric interpretation of structured error dynamics and offers foundational tools for curvature-informed model validation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:21:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>math.DS</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11734v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11734v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Spatially Varying Gene Regulatory Networks via Bayesian Nonparametric Covariate-Dependent Directed Cyclic Graphical Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Trisha Dawn, Yang Ni
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Spatial transcriptomics technologies enable the measurement of gene expression with spatial context, providing opportunities to understand how gene regulatory networks vary across tissue regions. However, existing graphical models focus primarily on undirected graphs or directed acyclic graphs, limiting their ability to capture feedback loops that are prevalent in gene regulation. Moreover, ensuring the so-called stability condition of cyclic graphs, while allowing graph structures to vary continuously with spatial covariates, presents significant statistical and computational challenges. We propose BNP-DCGx, a Bayesian nonparametric approach for learning spatially varying gene regulatory networks via covariate-dependent directed cyclic graphical models. Our method introduces a covariate-dependent random partition as an intermediary layer in a hierarchical model, which discretizes the covariate space into clusters with cluster-specific stable directed cyclic graphs. Through partition averaging, we obtain smoothly varying graph structures over space while maintaining theoretical guarantees of stability. We develop an efficient parallel tempered Markov chain Monte Carlo algorithm for posterior inference and demonstrate through simulations that our method accurately recovers both piecewise constant and continuously varying graph structures. Application to spatial transcriptomics data from human dorsolateral prefrontal cortex reveals spatially varying regulatory networks with feedback loops, identifies potential cell subtypes within established cell types based on distinct regulatory mechanisms, and provides new insights into spatial organization of gene regulation in brain tissue.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:16:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11732v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11732v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sergey Pankratov, Dan Alistarh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speculative generation has emerged as a promising technique to accelerate inference in large language models (LLMs) by leveraging parallelism to verify multiple draft tokens simultaneously. However, the fundamental limits on the achievable speedup remain poorly understood. In this work, we establish the first ``tight'' lower bounds on the runtime of any deterministic speculative generation algorithm. This is achieved by drawing a parallel between the token generation process and branching random walks, which allows us to analyze the optimal draft tree selection problem. We prove, under basic assumptions, that the expected number of tokens successfully predicted per speculative iteration is bounded as $\mathbb{E}[X] \leq (μ+ μ_{(2)})\log(P )/μ^2 + O(1)$, where $P$ is the verifier's capacity, $μ$ is the expected entropy of the verifier's output distribution, and $μ_{(2)}$ is the expected second log-moment. This result provides new insights into the limits of parallel token generation, and could guide the design of future speculative decoding systems. Empirical evaluations on Llama models validate our theoretical predictions, confirming the tightness of our bounds in practical settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:54:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11718v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11718v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Text2Graph: Combining Lightweight LLMs and GNNs for Efficient Text Classification in Label-Scarce Scenarios</h2>
                <div class="authors">
                    <strong>Authors:</strong> João Lucas Luz Lima Sarcinelli, Ricardo Marcondes Marcacini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have become effective zero-shot classifiers, but their high computational requirements and environmental costs limit their practicality for large-scale annotation in high-performance computing (HPC) environments. To support more sustainable workflows, we present Text2Graph, an open-source Python package that provides a modular implementation of existing text-to-graph classification approaches. The framework enables users to combine LLM-based partial annotation with Graph Neural Network (GNN) label propagation in a flexible manner, making it straightforward to swap components such as feature extractors, edge construction methods, and sampling strategies. We benchmark Text2Graph on a zero-shot setting using five datasets spanning topic classification and sentiment analysis tasks, comparing multiple variants against other zero-shot approaches for text classification. In addition to reporting performance, we provide detailed estimates of energy consumption and carbon emissions, showing that graph-based propagation achieves competitive results at a fraction of the energy and environmental cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:45:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10061v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10061v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 The Domain Adaptation problem in photometric redshift estimation: a solution applied to the HSC Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> M. Treyer, R. Ait-Ouahmed, S. Arnouts, J. Pasquet, E. Bertin, G. Desprez, V. Picouet, M. Sawicki
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The multi-band HSC-CLAUDS survey comprises several sky regions with varying observing conditions, only one of which, the COSMOS Ultra Deep Field (UDF), offers extensive redshift coverage. We aim to exploit a complete sample of labeled galaxies from the COSMOS UDF at i<25 (z<~5) to train a convolutional neural network (CNN) and infer more accurate photometric redshifts in the other regions than those currently available from SED-fitting methods. To address the severe domain mismatch problem we observed when applying the trained CNN to regions other than the COSMOS UDF, we developed an unsupervised adversarial domain adaptation network that we grafted onto the CNN. The method is validated by three tests: the predicted redshifts are compared to the spectroscopic redshifts that are available for limited samples of mostly bright galaxies; the predicted redshift distributions of the entire galaxy population of a given field in several intervals of magnitude are compared to those of the COSMOS UDF, assumed to be representative; the redshifts predicted for a sample of galaxies selected by narrow-band filter observations sensitive to [OII] emitters at z~1.47 are compared to those of confirmed [OII] emission line galaxies. The results show successful domain adaptation: the network is able to transfer its redshift classification capability learnt from the COSMOS UDF to other regions of HSC-CLAUDS. Accuracy varies depending on magnitude and redshift, following that of the labels we used, but far exceeds that of currently available photometric redshifts. The catalogs of CNN redshifts we inferred for the XMM, DEEP2 and ELAIS fields and for the remaining COSMOS region (~4 million sources in total at i<25) are made public.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:31:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11700v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11700v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sophia Hager, David Mueller, Kevin Duh, Nicholas Andrews
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) are increasingly used for factual question-answering, it becomes more important for LLMs to have the capability to communicate the likelihood that their answer is correct. For these verbalized expressions of uncertainty to be meaningful, they should reflect the error rates at the expressed level of confidence. However, when prompted to express confidence, the error rates of current LLMs are inconsistent with their communicated confidences, highlighting the need for uncertainty quantification methods. Many prior methods calculate lexical uncertainty, estimating a model's confidence in the specific string it generated. In some cases, however, it may be more useful to estimate semantic uncertainty, or the model's confidence in the answer regardless of how it is verbalized. We propose a simple procedure, uncertainty distillation, to teach an LLM to verbalize calibrated semantic confidences. Using held-out data to map initial uncertainty estimates to meaningful probabilities, we create examples annotated with verbalized probabilities for supervised fine-tuning. We find that our method yields verbalized confidences that correlate well with observed error rates, even when compared to strong baselines, some of which are more than twenty times slower at inference time. Additionally, we demonstrate that our method can be applied to black-box models that allow API-based fine-tuning, resulting in estimates of uncertainty that are both more effective and more efficient than any of our baselines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:31:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2503.14749v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2503.14749v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Non-reciprocal interactions and high-dimensional chaos: comparing dynamics and statistics of equilibria in a solvable class of models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Samantha J. Fournier, Alessandro Pacco, Valentina Ros, Pierfrancesco Urbani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate a model of high-dimensional dynamical variables with all-to-all interactions that are random and non-reciprocal. We characterize its phase diagram and show that the model can exhibit chaotic dynamics. We show that the equations describing the system's dynamics exhibit a number of equilibria that is exponentially large in the dimensionality of the system, and these equilibria are all linearly unstable in the chaotic phase. Solving the effective equations governing the dynamics in the infinite-dimensional limit, we determine the typical properties (magnetization, overlap) of the configurations belonging to the attractor manifold. We show that these properties cannot be inferred from those of the equilibria, challenging the expectation that chaos can be understood purely in terms of the numerous unstable equilibria of the dynamical equations. We discuss the dependence of this scenario on the strength of non-reciprocity in the interactions. These results are obtained through a combination of analytical methods such as Dynamical Mean-Field Theory and the Kac-Rice formalism.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:27:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.dis-nn</span><span>cond-mat.stat-mech</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2503.20908v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2503.20908v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Particle Image Velocimetry Refinement via Consensus ADMM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alan Bonomi, Francesco Banelli, Antonio Terpin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Particle Image Velocimetry (PIV) is an imaging technique in experimental fluid dynamics that quantifies flow fields around bluff bodies by analyzing the displacement of neutrally buoyant tracer particles immersed in the fluid. Traditional PIV approaches typically depend on tuning parameters specific to the imaging setup, making the performance sensitive to variations in illumination, flow conditions, and seeding density. On the other hand, even state-of-the-art machine learning methods for flow quantification are fragile outside their training set. In our experiments, we observed that flow quantification would improve if different tunings (or algorithms) were applied to different regions of the same image pair. In this work, we parallelize the instantaneous flow quantification with multiple algorithms and adopt a consensus framework based on the alternating direction method of multipliers, seamlessly incorporating priors such as smoothness and incompressibility. We perform several numerical experiments to demonstrate the benefits of this approach. For instance, we achieve a decrease in end-point-error of up to 20% of a dense-inverse-search estimator at an inference rate of 60Hz, and we show how this performance boost can be increased further with outlier rejection. Our method is implemented in JAX, effectively exploiting hardware acceleration, and integrated in Flow Gym, enabling (i) reproducible comparisons with the state-of-the-art, (ii) testing different base algorithms, (iii) straightforward deployment for active fluids control applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:20:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.flu-dyn</span><span>cs.CV</span><span>eess.IV</span><span>math.OC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11695v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11695v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Algorithms for Reconstructing B Cell Lineages in the Presence of Context-Dependent Somatic Hypermutation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongkang Li, Kevin J. Wiehe, Scott C. Schmidler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce a method for approximating posterior probabilities of phylogenetic trees and reconstructing ancestral sequences under models of sequence evolution with site-dependence, where standard phylogenetic likelihood computations (pruning) fail. Our approach uses a combined data-augmentation and importance sampling scheme. A key advantage of our approach is the ability to leverage existing highly optimized phylogenetic software. We apply our approach to the reconstruction of B cell receptor affinity maturation lineages from high-throughput repertoire sequencing data and evaluate the impact of incorporating site-dependence on the reconstruction accuracy of both trees and ancestral sequences. We show that accounting for context-dependence during inference always improves the estimates of both ancestral sequences and lineage trees on simulated datasets. We also examine the impact of incorporating priors based on VDJ recombination models, and find that they significantly improve ancestral sequence reconstruction in germline-encoded regions, but increase errors in non-templated nucleotides. We propose a modified, piecewise prior to address this demonstrate that it improves empirical reconstruction accuracy. We apply our approach to the analysis of the HIV broadly neutralizing antibodies DH270 and CH235 which are important targets of current vaccine design efforts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:18:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.PE</span><span>stat.CO</span><span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11693v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11693v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Crystal Su, Kuai Yu, Jingrui Zhang, Mingyuan Shao, Daniel Bauer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work presents an ontology-integrated large language model (LLM) framework for chemical engineering that unites structured domain knowledge with generative reasoning. The proposed pipeline aligns model training and inference with the COPE ontology through a sequence of data acquisition, semantic preprocessing, information extraction, and ontology mapping steps, producing templated question-answer pairs that guide fine-tuning. A control-focused decoding stage and citation gate enforce syntactic and factual grounding by constraining outputs to ontology-linked terms, while evaluation metrics quantify both linguistic quality and ontological accuracy. Feedback and future extensions, including semantic retrieval and iterative validation, further enhance the system's interpretability and reliability. This integration of symbolic structure and neural generation provides a transparent, auditable approach for applying LLMs to process control, safety analysis, and other critical engineering contexts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:14:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.26898v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.26898v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Evaluating Cooperative Resilience in Multiagent Systems: A Comparison Between Humans and LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manuela Chacon-Chamorro, Juan Sebastián Pinzón, Rubén Manrique, Luis Felipe Giraldo, Nicanor Quijano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a comparative analysis of cooperative resilience in multi-agent systems, defined as the ability to anticipate, resist, recover from, and transform to disruptive events that affect collective well-being. We focus on mixed-motive social dilemmas instantiated as a \textit{Tragedy of the Commons} environment from the Melting Pot suite, where we systematically compare human groups and Large Language Model (LLM)-based agents, each evaluated with and without explicit communication. Cooperative resilience is assessed under a continuously disruptive condition induced by a persistent unsustainable consumption bot, together with intermittent environmental shocks implemented as stochastic removal of shared resources across scenarios. This experimental design establishes a benchmark for cooperative resilience across agent architectures and interaction modalities, constituting a key step toward systematically comparing humans and LLM-based agents. Using this framework, we find that human groups with communication achieve the highest cooperative resilience compared to all other groups. Communication also improves the resilience of LLM agents, but their performance remains below human levels. Motivated by the performance of humans, we further examine a long-horizon setting with harsher environmental conditions, where humans sustain the shared resource and maintain high resilience in diverse disruption scenarios. Together, these results suggest that human decision-making under adverse social conditions can inform the design of artificial agents that promote prosocial and resilient behaviors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:11:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11689v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11689v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Transformer Embeddings for Fast Microlensing Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nolan Smyth, Laurence Perreault-Levasseur, Yashar Hezaveh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The search for free-floating planets (FFPs) is a key science driver for upcoming microlensing surveys like the Nancy Grace Roman Galactic Exoplanet Survey. These rogue worlds are typically detected via short-duration microlensing events, the characterization of which often requires analyzing noisy, irregularly-sampled observations. We present a pipeline for this task using simulation-based inference. We use a Transformer encoder to learn a compressed summary representation of the raw time-series data, which in turn conditions a neural posterior estimator. We demonstrate that our method produces accurate and well-calibrated posteriors over three orders of magnitude faster than traditional methods. We also demonstrate its performance on KMT-BLG-2019-2073, a short-duration FFP candidate event.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:10:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.IM</span><span>astro-ph.EP</span><span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11687v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11687v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Hierarchical Bayesian Model for Gene Deconvolution and Functional Analysis in Human Endometrium Across the Menstrual Cycle</h2>
                <div class="authors">
                    <strong>Authors:</strong> Crystal Su, Kuai Yu, Mingyuan Shao, Daniel Bauer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Bulk tissue RNA sequencing of heterogeneous samples provides averaged gene expression profiles, obscuring cell type-specific dynamics. To address this, we present a probabilistic hierarchical Bayesian model that deconvolves bulk RNA-seq data into constituent cell-type expression profiles and proportions, leveraging a high-resolution single-cell reference. We apply our model to human endometrial tissue across the menstrual cycle, a context characterized by dramatic hormone-driven cellular composition changes. Our extended framework provides a principled inference of cell type proportions and cell-specific gene expression changes across cycle phases. We demonstrate the model's structure, priors, and inference strategy in detail, and we validate its performance with simulations and comparisons to existing methods. The results reveal dynamic shifts in epithelial, stromal, and immune cell fractions between menstrual phases, and identify cell-type-specific differential gene expression associated with endometrial function (e.g., decidualization markers in stromal cells during the secretory phase). We further conduct robustness tests and show that our Bayesian approach is resilient to reference mismatches and noise. Finally, we discuss the biological significance of our findings, potential clinical implications for fertility and endometrial disorders, and future directions, including integration of spatial transcriptomics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:09:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>q-bio.GN</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.27097v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.27097v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Assumption-Lean Post-Integrated Inference with Surrogate Control Outcomes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jin-Hong Du, Kathryn Roeder, Larry Wasserman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference method that accounts for latent heterogeneity by utilizing control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects using negative control outcomes. By utilizing surrogate control outcomes as an extension of negative control outcomes, we develop semiparametric inference on projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated using random forests through simulations and analysis of single-cell CRISPR perturbed datasets, which may contain potential unmeasured confounders.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:09:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>cs.LG</span><span>q-bio.GN</span><span>stat.AP</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2410.04996v5' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2410.04996v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Crystal Su
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:08:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.12963v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.12963v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier</h2>
                <div class="authors">
                    <strong>Authors:</strong> Crystal Su
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) often produce fluent reasoning steps while violating simple mathematical or logical constraints. We introduce MedRule-KG, a compact typed knowledge graph coupled with a symbolic verifier, designed to enforce mathematically interpretable rules in reasoning tasks. MedRule-KG encodes entities, relations, and three domain-inspired rules, while the verifier checks predictions and applies minimal corrections to guarantee consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields 1.000 EM while eliminating rule violations entirely. We demonstrate how MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss ablations, and release code and data to encourage reproducibility.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:08:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.16309v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.16309v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Machine-learned global glacier ice volumes</h2>
                <div class="authors">
                    <strong>Authors:</strong> N. Maffezzoli, E. Rignot, C. Barbante, M. Morlighem, T. Petersen, S. Vascon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a global dataset of glacier ice thickness modeled with IceBoost v2.0, a machine learning model trained on 7 million ice thickness measurements and informed by physical and geometrical predictors. We model the distributed ice thickness for every glacier in the two latest Randolph Glacier Inventory releases (v6.0 and v7.0), totaling 215,547 and 274,531 glacier outlines, respectively, plus 955 ice masses contiguous with the Greenland Ice Sheet. We find a global glacier volume of $(149 \pm 38)\times 10^3$ km$^3$, consistent with the previous ensemble estimate of $(147 \pm 28)\times 10^3$ km$^3$. The corresponding sea-level equivalent, $323 \pm 91$ mm, is likewise consistent with the earlier value of $315 \pm 63$ mm. Compared to measurements, IceBoost error is 20-45% lower than the other solutions in the high Arctic, highlighting the value of machine-learning approaches. Confidence in our solution is highest at higher latitudes. Over mountainous terrain, small glaciers, and under-represented lower-latitude regions, confidence is lower. IceBoost v2.0 demonstrates strong generalization at ice sheet margins. On the Geikie Plateau (East Greenland), we find nearly twice as much ice as previously reported, highlighting the method's potential to infer bed topography in parts of the ice sheets. The quality of the solutions depends on the accuracy of the training data, the Digital Elevation Model, ice velocity fields, and glacier geometries, including nunataks. Using the Jensen Gap, we probe the model's curvature with respect to input errors and find it is strongly concave over low-slope, thick-ice regions, implying a potential downward bias in predicted thickness under input uncertainty. The released dataset can be used to model future glacier evolution and sea-level rise, inform the design of glaciological surveys and field campaigns, as well as guide policies on freshwater management.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:07:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.geo-ph</span><span>physics.app-ph</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11685v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11685v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Stochastics of shapes and Kunita flows</h2>
                <div class="authors">
                    <strong>Authors:</strong> Stefan Sommer, Gefan Yang, Elizabeth Louise Baker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Stochastic processes of evolving shapes are used in applications including evolutionary biology, where morphology changes stochastically as a function of evolutionary processes. Due to the non-linear and often infinite-dimensional nature of shape spaces, the mathematical construction of suitable stochastic shape processes is far from immediate. We define and formalize properties that stochastic shape processes should ideally satisfy to be compatible with the shape structure, and we link this to Kunita flows that, when acting on shape spaces, induce stochastic processes that satisfy these criteria by their construction. We couple this with a survey of other relevant shape stochastic processes and show how bridge sampling techniques can be used to condition shape stochastic processes on observed data thereby allowing for statistical inference of parameters of the stochastic dynamics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:54:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.PR</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11676v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11676v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liangliang Zhang, Nandana Mihindukulasooriya, Niharika S. D'Souza, Sola Shirai, Sarthak Dash, Yao Ma, Horst Samulowitz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Data products are reusable, self-contained assets designed for specific business use cases. Automating their discovery and generation is of great industry interest, as it enables discovery in large data lakes and supports analytical Data Product Requests (DPRs). Currently, there is no benchmark established specifically for data product discovery. Existing datasets focus on answering single factoid questions over individual tables rather than collecting multiple data assets for broader, coherent products. To address this gap, we introduce Data Product Benchmark, the first user-request-driven data product benchmark over hybrid table-text corpora.   Our framework systematically repurposes existing table-text QA datasets by clustering related tables and passages into coherent data products, generating professional-level analytical requests that span both data sources, and validating benchmark quality through multi-LLM evaluation. Data Product Benchmark preserves full provenance while producing actionable, analyst-like data product requests. Baseline experiments with hybrid retrieval methods establish the feasibility of DPR evaluation, reveal current limitations, and point to new opportunities for automatic data product discovery research.   Code and datasets are available at: Dataset: https://huggingface.co/datasets/ibm-research/data-product-benchmark Code: https://github.com/ibm/data-product-benchmark
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:52:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.21737v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.21737v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews</h2>
                <div class="authors">
                    <strong>Authors:</strong> Brenda Nogueira, Werner Geyer, Andrew Anderson, Toby Jia-Jun Li, Dongwhi Kim, Nuno Moniz, Nitesh V. Chawla
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:38:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11661v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11661v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Causal Inference in Energy Demand Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chutian Ma, Grigorii Pomazkin, Giacinto Paolo Saggese, Paul Smith
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Energy demand prediction is critical for grid operators, industrial energy   consumers, and service providers. Energy demand is influenced by multiple   factors, including weather conditions (e.g. temperature, humidity, wind   speed, solar radiation), and calendar information (e.g. hour of day and   month of year), which further affect daily work and life schedules. These   factors are causally interdependent, making the problem more complex than   simple correlation-based learning techniques satisfactorily allow for. We   propose a structural causal model that explains the causal relationship   between these variables. A full analysis is performed to validate our causal   beliefs, also revealing important insights consistent with prior studies.   For example, our causal model reveals that energy demand responds to   temperature fluctuations with season-dependent sensitivity. Additionally, we   find that energy demand exhibits lower variance in winter due to the   decoupling effect between temperature changes and daily activity patterns.   We then build a Bayesian model, which takes advantage of the causal insights   we learned as prior knowledge. The model is trained and tested on unseen   data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on   the test set. The model also demonstrates strong robustness, as the   cross-validation across two years of data yields an average MAPE of 3.88 percent.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:30:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11653v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11653v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 The Luminosity Function and Clustering of H$α$ Emitting Galaxies at $z\approx4-6$ from a Complete NIRCam Grism Redshift Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaojing Lin, Eiichi Egami, Fengwu Sun, Haowen Zhang, Xiaohui Fan, Jakob M. Helton, Feige Wang, Andrew J. Bunker, Zheng Cai, Daniel J. Eisenstein, Daniel T. Jaffe, Zhiyuan Ji, Xiangyu Jin, Maria Anne Pudoka, Sandro Tacchella, Wei Leong Tee, Pierluigi Rinaldi, Brant Robertson, Yang Sun, Christopher N. A. Willmer, Chris Willott, Junyu Zhang, Yongda Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We study the luminosity function (LF) and clustering properties of 888 H$α$ emitters (HAEs) at $3.75 < z < 6$ in the GOODS-N field. The sample, built from JWST CONGRESS and FRESCO NIRCam grism surveys using a novel redshift assignment algorithm, spans $\sim$62 arcmin$^2$ and reaches $L_{\rm Hα} \sim 10^{41.2} {\rm erg s^{-1}}$. We identify two prominent filamentary protoclusters at $z \approx 4.41$ and $z \approx 5.19$, hosting 98 and 144 HAEs, respectively. The observed H$α$ LFs show similar shallow faint-end slopes for both protocluster and field galaxies at $z=3.75-5$, and for the protocluster at $z=5-6$ ($α\approx 1.2$ to $-1.3$). In contrast, the field LF at $z=5-6$ is much steeper ($α=-1.87_{-0.23}^{+0.30}$), suggesting that protocluster galaxies at $z > 5$ are more evolved, resembling those at $z=3.75-5$. The observed star formation rate density from H$α$, integrated down to 0.45 ${\rm M_\odot yr^{-1}}$, is $0.050^{+0.002}_{-0.003}$ and $0.046^{+0.006}_{-0.004} M_\odot {\rm yr}^{-1} {\rm Mpc}^{-3}$ at $z=3.75-5$ and $z=5-6$, with protoclusters contributing $\sim$25% and 55%, respectively. This implies that a large fraction of star formation at $z > 4$ occurs in protoclusters. We conduct the first star-formation-rate-limited 3D clustering analysis at $z > 4$. We find the filamentary protocluster geometry flattens the power-law shape of the HAE auto-correlation functions, with slopes much shallower than typically assumed. The auto-correlation function of field HAEs have correlation lengths of $r_0 = 4.61^{+1.00}_{-0.68} h^{-1}{\rm Mpc}$ at $z \approx 4-5$ and $r_0 = 6.23^{+1.68}_{-1.13} h^{-1}{\rm Mpc}$ at $z=5-6$. Comparing the observed correlation functions with the UniverseMachine simulation, we infer the dark matter (sub-)halo masses of HAEs to be $\log (M_h/M_\odot)=11.0-11.2$ at $z\approx 4-6$, with a scatter of 0.4 dex.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:10:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span><span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2504.08028v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2504.08028v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Estimating the Effects of Heatwaves on Health: A Causal Inference Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giulio Grossi, Leo Vanciu, Veronica Ballerini, Danielle Braun, Falco J. Bargagli Stoffi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The harmful relationship between heatwaves and health has been extensively documented in medical and epidemiological literature. However, most evidence is associational and cannot be interpreted causally unless strong assumptions are made. In this paper, we first make explicit the assumptions underlying the statistical methods frequently used in the heatwave literature and demonstrate when these assumptions might break down in heatwave contexts. To address these shortcomings, we propose a causal inference framework that transparently elicits causal identification assumptions. Within this new framework, we first introduce synthetic controls (SC) for estimating heatwave effects, then propose a spatially augmented Bayesian synthetic control (SA-SC) method that accounts for spatial dependence and spillovers. Empirical Monte Carlo simulations show both methods perform well, with SA-SC reducing root mean squared error and improving posterior interval coverage under spillovers and spatial dependence. Finally, we apply the proposed methods to estimate the causal effects of heatwaves on Medicare heat-related hospitalizations among 13,753,273 beneficiaries residing in Northeastern U.S. from 2000 to 2019. This causal inference framework provides spatially coherent counterfactual outcomes and robust, interpretable, and transparent causal estimates while explicitly addressing the unexamined assumptions in existing methods that pervade the heatwave effect literature.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:02:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.11433v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.11433v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 LLM tools in the prediction of the stability of perovskite solar cells</h2>
                <div class="authors">
                    <strong>Authors:</strong> S. Frenkel, V. Zakharov, E. A. Katz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate whether tools based on large language models (LLMs) can be effectively used by a developer of new perovskite solar cells (PSCs) to predict both the "lifetime" of the device and the degree of its degradation at specific time intervals. We demonstrate the ability of common LLM tools (ChatGPT, DeepSeek, and even a simplified free version of ChatGPT) to suggest and justify prediction methods in a dialogue with the user under conditions of incomplete information about the physical models of PSC degradation and the influence of the environment. One of the results covers LLM ChatGPT's ontology of the specific subject domain of PSCs. It allows the formation of time series of efficiency with a given architecture, calculated using various available models together with environmental characteristics archived in various meteorological databases (illumination, temperature, humidity, UV level). We conclude that ChatGPT currently has sufficient access to training samples, can find various models in the literature, and has adequate solutions for predicting degradation trends.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:50:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11615v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11615v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</h2>
                <div class="authors">
                    <strong>Authors:</strong> Björn Deiseroth, Max Henning Höth, Kristian Kersting, Letitia Parcalabescu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:50:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11614v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11614v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Euclidean Ideal Point Estimation From Roll-Call Data via Distance-Based Bipartite Network Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seungju Lee, In Kyun Kim, Jong Hee Park, Ick Hoon Jin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Conventional ideal point models rely on Gaussian or quadratic utility functions that violate the triangle inequality, producing non-metric distances that complicate geometric interpretation and undermine clustering and dispersion-based analyses. We introduce a distance-based alternative that adapts the Latent Space Item Response Model (LSIRM) to roll-call data, treating legislators and bills as nodes in a bipartite network jointly embedded in a Euclidean metric space. Through controlled simulations, Euclidean LSIRM consistently recovers latent coalition structure with superior cluster separation relative to existing methods. Applied to the 118th U.S. House, the model improves vote prediction and yields bill embeddings that clarify cross-cutting issue alignments. The results show that restoring metric structure to ideal point estimation provides a clearer and more coherent inference about party cohesion, factional divisions, and multidimensional legislative behavior.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:48:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11610v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11610v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Exploring Diffusion with Test-Time Training on Efficient Image Restoration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rongchang Lu, Tianduo Luo, Yunzhi Jiang, Conghan Yue, Pei Yang, Guibao Liu, Changyang Gu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:42:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2506.14541v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2506.14541v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:42:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2507.05178v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2507.05178v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifu Guo, Zishan Xu, Zhiyuan Yao, Yuquan Lu, Jiaye Lin, Sen Hu, Zhenheng Tang, Huacan Wang, Ronghao Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:34:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.15351v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.15351v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hyungrok Do, Yuyan Wang, Mengling Liu, Myeonggyun Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\texttt{https://github.com/hyungrok-do/NeuralPLSI}).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:28:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.AP</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11593v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11593v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Meta-Statistical Learning: Supervised Learning of Statistical Estimators</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maxime Peyrard, Kyunghyun Cho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Statistical inference, a central tool of science, revolves around the study and the usage of statistical estimators: functions that map finite samples to predictions about unknown distribution parameters. In the frequentist framework, estimators are evaluated based on properties such as bias, variance (for parameter estimation), accuracy, power, and calibration (for hypothesis testing). However, crafting estimators with desirable properties is often analytically challenging, and sometimes impossible, e.g., there exists no universally unbiased estimator for the standard deviation. In this work, we introduce meta-statistical learning, an amortized learning framework that recasts estimator design as an optimization problem via supervised learning. This takes a fully empirical approach to discovering statistical estimators; entire datasets are input to permutation-invariant neural networks, such as Set Transformers, trained to predict the target statistical property. The trained model is the estimator, and can be analyzed through the classical frequentist lens. We demonstrate the approach on two tasks: learning a normality test (classification) and estimating mutual information (regression), achieving strong results even with small models. Looking ahead, this paradigm opens a path to automate the discovery of generalizable and flexible statistical estimators.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:27:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2502.12088v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2502.12088v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 A Study of Library Usage in Agent-Authored Pull Requests</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lukas Twist
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited "library preferences" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:21:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11589v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11589v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengjiao Ma, Qi Ma, Yue Li, Jiahuan Cheng, Runyi Yang, Bin Ren, Nikola Popovic, Mingqiang Wei, Nicu Sebe, Luc Van Gool, Theo Gevers, Martin R. Oswald, Danda Pani Paudel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets are released at https://scenesplatpp.gaussianworld.ai/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:15:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2506.08710v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2506.08710v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 In-Context Learning for Seismic Data Processing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fabian Fuchs, Mario Ruben Fernandez, Norman Ettrich, Janis Keuper
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:03:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11575v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11575v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Visualizing token importance for black-box language models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paulius Rauba, Qiyao Wei, Mihaela van der Schaar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:01:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11573v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11573v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Designing Efficient Hybrid and Single-Arm Trials: External Control Borrowing and Sample Size Calculation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yujing Gao, Xiang Zhang, Shu Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> External controls (ECs) from historical trials or real-world data have gained increasing attention as a way to augment hybrid and single-arm trials, especially when balanced randomization is infeasible. While most existing work has focused on post-trial inference using ECs, their role in prospective trial design remains less explored. We address this gap by focusing on the sample size determination and power analysis for an experimental design problem that encompasses standard randomized controlled trials (RCTs), hybrid trials, and single-arm trials. Building on estimators derived from the efficient influence function, we develop hybrid and single-arm design strategies that leverage comparable EC data to reduce the required sample size of the current study. We derive asymptotic variance expressions for these estimators in terms of interpretable, population-level quantities and introduce a pre-experimental variance estimation procedure to guide sample size calculation, ensuring prespecified type I error and power for the relevant hypothesis test. Simulation studies demonstrate that the proposed hybrid and single-arm designs maintain valid type I error and achieve target power across diverse scenarios while requiring substantially fewer subjects in the current study than RCT designs. A real data application further illustrates the practical utility and advantages of the proposed hybrid and single-arm designs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:56:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.09353v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.09353v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 ReCode: Unify Plan and Action for Universal Granularity Control</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yuyu Luo, Bang Liu, Chenglin Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:55:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.23564v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.23564v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Fully Inductive Node Representation Learning via Graph View Transformation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dooho Lee, Myeong Kong, Minho Jeong, Jaemin Yoo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:45:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11561v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11561v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haiteng Zhao, Junhao Shen, Yiming Zhang, Songyang Gao, Kuikun Liu, Tianyou Ma, Fan Zheng, Dahua Lin, Wenwei Zhang, Kai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:43:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10534v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10534v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Zhang, Zhiheng Chen, Ye Qiao, Sitao Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:35:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11550v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11550v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan Shen, Xiaojun Wu, Linghua Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:25:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11544v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11544v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chong Tang, Hao Dai, Jagmohan Chauhan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:07:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11532v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11532v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 xGR: Efficient Generative Recommendation Serving at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qingxiao Sun, Tongxuan Liu, Shen Zhang, Siyu Wu, Peijun Yang, Haotian Liang, Menxin Li, Xiaolong Ma, Zhiwei Liang, Ziyi Ren, Minchao Zhang, Xinyu Liu, Ke Zhang, Depei Qian, Hailong Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:59:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11529v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11529v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Approaches to biological species delimitation based on genetic and spatial dissimilarity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gabriele d'Angella, Christian Hennig
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. We investigate the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Existing methods such as the partial Mantel test and jackknife-based distance-distance regression are considered. New approaches, i.e., an adaptation of a mixed effects model, a bootstrap approach, and a jackknife version of partial Mantel, are proposed. All these methods address the issue that distance data violate the independence assumption for standard inference regarding correlation and regression; a standard linear regression is also considered. The approaches are compared on simulated meta-populations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Simulations show that the new jackknife version of the partial Mantel test provides a good compromise between power and respecting the nominal type I error rate. Mixed-effects models have larger power than jackknife-based methods, but tend to display type I error rates slightly above the significance level. An application on brassy ringlets concludes the paper.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:54:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.PE</span><span>stat.AP</span><span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2401.12126v5' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2401.12126v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Learning Unmasking Policies for Diffusion Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Metod Jazbec, Theo X. Olausson, Louis Béthune, Pierre Ablin, Michael Kirchhof, João Monteiro, Victor Turrisi, Jason Ramapuram, Marco Cuturi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:48:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09106v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09106v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shibing Mo, Haoyang Ruan, Kai Wu, Jing Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:35:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.06682v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.06682v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohor Banerjee, Nadya Yuki Wangsajaya, Syed Ali Redha Alsagoff, Min Sen Tan, Zachary Choy Kit Chun, Alvin Chan Guo Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:14:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11509v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11509v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Fast LISA Likelihood Approximations by Downsampling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jethro Linley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Laser Interferometer Space Antenna (LISA) is due to launch in the mid-2030s. A key challenge for LISA data analysis is efficient Bayesian inference with parametrised gravitational-wave models, particularly for early inspirals of low- and intermediate-mass black-hole binaries, where time series can contain $\sim 10^8$-$10^9$ samples and naive likelihood evaluations become prohibitively expensive. For purely simulated studies, we present a simple time-domain likelihood-approximation scheme for such signals. The method retains only a small subset of samples and defines a modified noise-weighted inner product on this reduced set that closely reproduces the original inner product on the waveform manifold. Because this alters the effective noise model, the scheme is intended for simulated LISA data rather than direct analysis of real LISA data. In our examples, the resulting posteriors agree to high accuracy with those obtained using much denser sampling, while retaining only $10^3$-$10^4$ samples. The computational cost then scales linearly with the number of retained samples $N_s$, so the speed-up is roughly $N/N_s$, where $N$ is the original data length; for realistic LISA-like datasets with $N\sim10^8$-$10^9$ this would correspond to gains of order $10^4$-$10^6$ over straightforward frequency-domain likelihood evaluations of the full dataset. The time-domain formulation is particularly convenient for incorporating waveform modifications from non-trivial astrophysical environments or alternative-gravity effects. The results presented here provide the theoretical basis for the software package \texttt{Dolfen}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:12:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span><span>astro-ph.HE</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2402.01819v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2402.01819v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 On Geometric Understanding and Learned Data Priors in VGGT</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jelena Bratulić, Sudhanshu Mittal, Thomas Brox, Christian Rupprecht
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:11:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11508v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11508v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild</h2>
                <div class="authors">
                    <strong>Authors:</strong> Binquan Zhang, Li Zhang, Haoyuan Zhang, Fang Liu, Song Wang, Bo Shen, An Fu, Lin Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:10:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10493v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10493v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Georgios Kaoukis, Ioannis Aris Koufopoulos, Psaroudaki Eleni, Danae Pla Karidi, Evaggelia Pitoura, George Papastefanatos, Panayiotis Tsaparas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:06:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11506v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11506v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanan Liu, Jun Liu, Hao Zhang, Dan Xu, Hossein Rahmani, Mohammed Bennamoun, Qiuhong Ke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:55:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11503v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11503v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Spatially resolved broad line region in a quasar at z=4: Dynamical black hole mass and prominent outflow</h2>
                <div class="authors">
                    <strong>Authors:</strong> GRAVITY+ Collaboration, K. Abd El Dayem, N. Aimar, A. Berdeu, J. -P. Berger, G. Bourdarot, P. Bourget, W. Brandner, Y. Cao, C. Correia, S. Cuevas Cardona, R. Davies, D. Defrère, A. Drescher, A. Eckart, F. Eisenhauer, M. Fabricius, A. Farah, H. Feuchtgruber, N. M. Förster Schreiber, A. Foschi, P. Garcia, R. Garcia Lopez, R. Genzel, S. Gillessen, T. Gomes, F. Gonté, V. Gopinath, J. Graf, M. Hartl, X. Haubois, F. Haußmann, L. C. Ho, S. Hönig, M. Houllé, S. Joharle, C. Keiman, P. Kervella, J. Kolb, L. Kreidberg, A. Labdon, S. Lacour, O. Lai, S. Lai, R. Laugier, J. -B. Le Bouquin, J. Leftley, R. Li, B. Lopez, D. Lutz, F. Mang, A. Mérand, F. Millour, M. Montargès, N. More, N. Morujão, H. Nowacki, M. Nowak, S. Oberti, C. Onken, J. Osorno, T. Ott, T. Paumard, K. Perraut, G. Perrin, R. Petrov, P. -O. Petrucci, N. Pourré, S. Rabien, C. Rau, D. Ribeiro, S. Robbe-Dubois, M. Sadun Bordoni, M. Salman, J. Sanchez-Bermudez, D. Santos, J. Sauter, M. Scialpi, J. Scigliuto, J. Shangguan, P. Shchekaturov, T. Shimizu, F. Soulez, C. Straubmeier, E. Sturm, M. Subroweit, C. Sykes, L. J. Tacconi, H. Übler, G. Ulbricht, F. Vincent, R. Webster, E. Wieprecht, J. Woillez, C. Wolf
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present the first near-infrared interferometric data of a QSO at z=4. The K-band observations were performed with GRAVITY+ on the VLTI using all four UTs, detecting a differential phase signal that traces the spatially resolved kinematics for both the \hb\ and \hg\ lines in the broad line region. We fit the two lines simultaneously with an updated model that includes distinct rotating and conical outflowing components. For the best fit model, more than 80\% of the \hi\ line emission from the BLR originates in an outflow with a velocity up to $10^4$ km s$^{-1}$. This is oriented so that our line of sight is along an edge of the conical structure, which produces the prominent blue wing on the line profile. A combination of anisotropic line emission and mid-plane opacity lead to the single-sided phase signal. The model is able to qualitatively match both the outflowing \civ\ line profile and the systemic \oi\ fluorescent emission. The derived black hole mass of $8\times10^8$ M$_\odot$ is the highest redshift black hole mass measurement to date obtained directly from BLR dynamics. It is an order of magnitude lower than that inferred from various single epoch scaling relations, and implies that the accretion is highly super-Eddington. With reference to recent simulations, the data suggest that this QSO is emitting close to its radiative limit in a regime where strong outflows are expected around a polar conical region.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:41:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2509.13911v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2509.13911v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Mistake Notebook Learning: Selective Batch-Wise Context Optimization for In-Context Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xuanbo Su, Yingfang Zhang, Hao Luo, Xiaoteng Liu, Leo Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) adapt to tasks via gradient fine-tuning (heavy computation, catastrophic forgetting) or In-Context Learning (ICL: low robustness, poor mistake learning). To fix this, we introduce Mistake Notebook Learning (MNL), a training-free framework with a persistent knowledge base of abstracted error patterns. Unlike prior instance/single-trajectory memory methods, MNL uses batch-wise error abstraction: it extracts generalizable guidance from multiple failures, stores insights in a dynamic notebook, and retains only baseline-outperforming guidance via hold-out validation (ensuring monotonic improvement). We show MNL nearly matches Supervised Fine-Tuning (93.9% vs 94.3% on GSM8K) and outperforms training-free alternatives on GSM8K, Spider, AIME, and KaggleDBQA. On KaggleDBQA (Qwen3-8B), MNL hits 28% accuracy (47% relative gain), outperforming Memento (15.1%) and Training-Free GRPO (22.1) - proving it's a strong training-free alternative for complex reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:33:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11485v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11485v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 When Large Language Models Meet UAV Projects: An Empirical Study from Developers' Perspective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yihua Chen, Xingle Que, Jiashuo Zhang, Jiachi Chen, Ting Cui, Guangshun Li, Ting Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In recent years, unmanned aerial vehicles (UAVs) have become increasingly popular in our daily lives and have attracted significant research interest in software engineering. At the same time, large language models (LLMs) have made notable advancements in language understanding, reasoning, and generation, making LLM applications in UAVs a promising research direction. However, existing studies have largely remained in preliminary exploration with a limited understanding of real-world practice, which causes an academia-industry gap and hinders the application of LLMs in UAVs. To address this, we conducted the first empirical study to investigate how LLMs support UAVs. To characterize common tasks and application scenarios of real-world UAV-LLM practices, we conducted a large-scale empirical study involving 997 research papers and 1,509 GitHub projects. The results classified nine common tasks (e.g., Natural Language Command Parsing) in four UAV workflows (e.g., Information Input) undertaken by LLMs in real-world UAV projects and revealed a large difference in the task distribution of research efforts and industry practices. To gain deeper insight into these differences and understand developers' perspectives on the application of LLMs in UAVs, we conducted a survey of practitioners, receiving 52 valid responses from 15 countries. The results revealed that while 40.4% of developers have attempted to apply LLMs to UAV tasks, 59.6% still face challenges integrating their UAV projects with advanced LLM capabilities. Their feedback attributes these challenges to five factors, including technological maturity, performance, safety, cost, and others, and provides practical implications for researchers and developers in conducting UAV-LLM practices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:30:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2509.12795v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2509.12795v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weijian Ma, Shizhao Sun, Ruiyu Wang, Jiang Bian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:29:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11480v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11480v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ying Cheng, Yu-Ho Lin, Min-Hung Chen, Fu-En Yang, Shang-Hong Lai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:18:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.07299v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.07299v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 General-purpose AI models can generate actionable knowledge on agroecological crop protection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kris A. G. Wyckhuys
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:17:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CY</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11474v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11474v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Rethinking Expert Trajectory Utilization in LLM Post-training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bowen Ding, Yuhan Chen, Jiayang Lv, Jiyao Yuan, Qi Zhu, Shuangshuang Tian, Dantong Zhu, Futing Wang, Heyuan Deng, Fei Mi, Lifeng Shang, Tao Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:13:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11470v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11470v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 An Input-Output Data-Driven Dissipativity Approach for Compositional Stability Certification of Interconnected LTI MIMO Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alejandra Sandoval-Carranza, Juan E. Machado, Johannes Schiffer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose an input-output data-driven framework for certifying the stability of interconnected multiple-input-multiple-output linear time-invariant discrete-time systems via QSR-dissipativity. That is, by using measured input-output trajectories of each subsystem, we verify dissipative properties and extract local passivity indices without requiring an explicit model identification.These passivity indices are then used to derive conditions under which the equilibrium of the interconnected system is stable. In particular, the framework identifies how the lack of passivity in some subsystems can be compensated by surpluses in others. The proposed approach enables a compositional stability analysis by combining subsystem-level conditions into a criterion valid for the overall interconnected system. We illustrate via a numerical case study, how to compute channel-wise passivity indices and infer stability guarantees directly from data with the proposed method.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:12:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11468v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11468v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Fast Multi-Party Open-Ended Conversation with a Social Robot</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giulio Antonio Abbo, Maria Jose Pinto-Bernal, Martijn Catrycke, Tony Belpaeme
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-party open-ended conversation remains a major challenge in human-robot interaction, particularly when robots must recognise speakers, allocate turns, and respond coherently under overlapping or rapidly shifting dialogue. This paper presents a multi-party conversational system that combines multimodal perception (voice direction of arrival, speaker diarisation, face recognition) with a large language model for response generation. Implemented on the Furhat robot, the system was evaluated with 30 participants across two scenarios: (i) parallel, separate conversations and (ii) shared group discussion. Results show that the system maintains coherent and engaging conversations, achieving high addressee accuracy in parallel settings (92.6%) and strong face recognition reliability (80-94%). Participants reported clear social presence and positive engagement, although technical barriers such as audio-based speaker recognition errors and response latency affected the fluidity of group interactions. The results highlight both the promise and limitations of LLM-based multi-party interaction and outline concrete directions for improving multimodal cue integration and responsiveness in future social robots.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:10:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2503.15496v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2503.15496v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Exploring MLLM-Diffusion Information Transfer with MetaCanvas</h2>
                <div class="authors">
                    <strong>Authors:</strong> Han Lin, Xichen Pan, Ziqi Huang, Ji Hou, Jialiang Wang, Weifeng Chen, Zecheng He, Felix Juefei-Xu, Junzhe Sun, Zhipeng Fan, Ali Thabet, Mohit Bansal, Chu Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:07:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11464v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11464v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingmin Zhu, Anqi Zhu, Hossein Rahmani, Jun Liu, Mohammed Bennamoun, Qiuhong Ke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:53:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11458v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11458v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Advances in Ontology--Based Mining of Adverse Drug Reactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kenenisa Tadesse Dame, Pietro Belloni, Ugo Moretti, Fabio Scapini, Marco Tuccori, Alessandra R. Brazzale
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Post--marketing pharmacovigilance is essential for identifying adverse drug reactions (ADRs) that elude detection during pre--marketing clinical trials. This study explores a novel approach that integrates an adverse event (AE) ontology into a zero--inflated negative binomial model to improve ADR detection. By accounting for the biological similarities among correlated AEs and addressing the excess of zero counts, this method more effectively disentangles AE associations. Statistical significance is evaluated using a permutation--based maximum statistic that preserves AE correlations within individual reports. Simulations and an application to real data from the Veneto drug safety database demonstrate that the ontology--based model consistently outperforms classical models such as the Gamma--Poisson Shrinker (GPS). For post--selection inference, we furthermore explore a data thinning technique for convolution--closed families, enabling the creation of independent training and validation datasets while retaining all drug--AE pairs. This approach is compared with conventional random train/test splitting, which may leave some drugs or AEs absent from one subset, and stratified splitting, which requires expanding aggregated counts into individual instances. The data--thinning technique and stratified splitting yield very similar results, with stratified splitting showing a slight benefit, and both clearly outperform random splitting in ensuring reliable and consistent model evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:45:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11452v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11452v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 The Large-scale Environments of Low-luminosity AGNs at $3.9 < z < 6$ and Implications for Their Host Dark Matter Halos from a Complete NIRCam Grism Redshift Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaojing Lin, Xiaohui Fan, Fengwu Sun, Junyu Zhang, Eiichi Egami, Jakob M. Helton, Feige Wang, Haowen Zhang, Andrew J. Bunker, Zheng Cai, Zhiyuan Ji, Xiangyu Jin, Roberto Maiolino, Maria Anne Pudoka, Pierluigi Rinaldi, Brant Robertson, Sandro Tacchella, Wei Leong Tee, Yang Sun, Christopher N. A. Willmer, Chris Willott, Yongda Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We study the large-scale environments and clustering properties of 28 low-luminosity AGNs at $z=3.9-6$ in the GOODS-N field. Our sample, identified from the JWST NIRCam Imaging and WFSS data in CONGRESS and FRESCO surveys with either broad H$α$ emission lines or V-shape continua, are compared to 782 H$α$ emitters (HAEs) selected from the same data. These AGNs are located in diverse large-scale environments and do not preferentially reside in denser environments compared to HAEs. Their overdensity field, $δ$, averaged over (15 $h^{-1}$cMpc)$^3$, ranges from $-0.56$ to 10.56, and shows no clear correlation with broad-line luminosity, black hole (BH) masses, or the AGN fraction. It suggests that $> 10$ cMpc structures do not significantly influence BH growth. We measure the two-point cross-correlation function of AGNs with HAEs, finding a comparable amplitude to that of the HAE auto-correlation. This indicates similar bias parameters and host dark matter halo masses for AGNs and HAEs. The correlation length of field AGNs is 4.26 $h^{-1}$cMpc, and 7.66 $h^{-1}$cMpc at $3.9 < z < 5$ and $5 < z < 6$, respectively. We infer a median host dark matter halo mass of $\log (M_h/M_\odot)\approx 11.0-11.2$ and host stellar masses of $\log (M_\star/M_\odot) \approx 8.4-8.6$ by comparing with the UniverseMachine simulation. Our clustering analysis suggests that low-luminosity AGNs at high redshift reside in normal star-forming galaxies with overmassive BHs. They represent an intrinsically distinct population from luminous quasars and could be a common phase in galaxy evolution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:24:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span><span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2505.02896v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2505.02896v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuowei Cai, Yansong Ning, Hao Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:08:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11426v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11426v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chaochao Li, Ruikui Wang, Liangbo Zhou, Jinheng Feng, Huaishao Luo, Huan Zhang, Youzheng Wu, Xiaodong He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:06:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11423v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11423v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gonca Gürsun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.   The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:03:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11421v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11421v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hugo Math, Rainer Lienhart
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:54:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2509.19112v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2509.19112v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Assessment of Simulation-based Inference Methods for Stochastic Compartmental Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vincent Wieland, Nils Wassmuth, Lorenzo Contento, Martin Kühn, Jan Hasenauer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Global pandemics, such as the recent COVID-19 crisis, highlight the need for stochastic epidemic models that can capture the randomness inherent in the spread of disease. Such models must be accompanied by methods for estimating parameters in order to generate fast nowcasts and short-term forecasts that can inform public health decisions. This paper presents a comparison of two advanced Bayesian inference methods: 1) pseudo-marginal particle Markov chain Monte Carlo, short Particle Filters (PF), and 2) Conditional Normalizing Flows (CNF). We investigate their performance on two commonly used compartmental models: a classical Susceptible-Infected-Recovered (SIR) model and a two-variant Susceptible-Exposed-Infected-Recovered (SEIR) model, complemented by an observation model that maps latent trajectories to empirical data. Addressing the challenges of intractable likelihoods for parameter inference in stochastic settings, our analysis highlights how these likelihood-free methods provide accurate and robust inference capabilities. The results of our simulation study further underscore the effectiveness of these approaches in capturing the stochastic dynamics of epidemics, providing prediction capabilities for the control of epidemic outbreaks. Results on an Ethiopian cohort study demonstrate operational robustness under real-world noise and irregular data sampling. To facilitate reuse and to enable building pipelines that ultimately contribute to better informed decision making in public health, we make code and synthetic datasets publicly available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:40:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.QM</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.02528v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.02528v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qiming Bao, Xiaoxuan Fu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) achieve strong performance on many natural language tasks, yet their generalisation under structured perturbations of logical rule systems remains insufficiently characterised. We present a controlled evaluation framework that probes reasoning reliability through four stress tests: (1) rule deletion, removing redundant versus essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites based on equivalence laws (contraposition, double negation, implication-to-disjunction, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that composes 2--5 transformations. Across three representative model families -- BERT, Qwen2, and LLaMA-like models -- all models attain Acc$=1.0000$ on the base split and show no degradation under redundant rule deletion. In contrast, essential rule deletion yields a pronounced decrease to near-chance performance, and injecting explicit contradictions reduces accuracy to 0.0000. Under logic-preserving rewrites, accuracy is largely preserved for single-law transformations with only small degradations in a few cases, whereas multi-law stacking exposes model-dependent sensitivity: BERT matches the base condition, TinyLlama shows only marginal degradation, and Qwen2 exhibits a substantial drop.   Overall, the results indicate that contemporary LLMs are generally stable under semantic-preserving reformulations, yet remain brittle to missing or inconsistent evidence and may degrade under composed logical transformations depending on the model family. The proposed framework provides a concise diagnostic tool for isolating these failure modes and for evaluating logical generalisation beyond surface-form variation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:31:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.LG</span><span>cs.LO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.06393v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.06393v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yousouf Taghzouti, Franck Michel, Tao Jiang, Louis-Félix Nothias, Fabien Gandon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The SPARQL query language is the standard method to access knowledge graphs (KGs). However, formulating SPARQL queries is a significant challenge for non-expert users, and remains time-consuming for the experienced ones. Best practices recommend to document KGs with competency questions and example queries to contextualise the knowledge they contain and illustrate their potential applications. In practice, however, this is either not the case or the examples are provided in limited numbers. Large Language Models (LLMs) are being used in conversational agents and are proving to be an attractive solution with a wide range of applications, from simple question-answering about common knowledge to generating code in a targeted programming language. However, training and testing these models to produce high quality SPARQL queries from natural language questions requires substantial datasets of question-query pairs. In this paper, we present Q${}^2$Forge that addresses the challenge of generating new competency questions for a KG and corresponding SPARQL queries. It iteratively validates those queries with human feedback and LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular, meaning that the different modules of the application (CQ generation, query generation and query refinement) can be used separately, as an integrated pipeline, or replaced by alternative services. The result is a complete pipeline from competency question formulation to query evaluation, supporting the creation of reference query sets for any target KG.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:31:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2505.13572v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2505.13572v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Network Estimation for Stationary Time Series</h2>
                <div class="authors">
                    <strong>Authors:</strong> Madeline A. Shelley, Chiara Boetti, Marina I. Knight, Matthew A. Nunes
                </div>
                <div class="summary">
                    <strong>Summary:</strong> High-dimensional multivariate time series are common in many scientific and industrial applications, where the interest lies in identifying key dependence structure within the data for subsequent analysis tasks, such as forecasting. An important avenue to achieve this is through the estimation of the conditional independence graph via graphical models, although for time series data settings the underpinning temporal dependence can make this task challenging. In this article, we propose a novel wavelet domain technique that allows the data-driven inference of the (sparse) conditional independence graph of a high-dimensional stationary multivariate time series. By adopting the locally stationary wavelet modelling framework, we repose the estimation problem as a well-principled wavelet domain graphical lasso formulation. Theoretical results establish that our associated estimation scheme enjoys good consistency properties when determining sparse dependence structure in input time series data. The performance of the proposed method is illustrated using extensive simulations and we demonstrate its applicability on a real-world dataset representing hospitalisations of COVID-19 patients.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:31:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11406v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11406v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 REMODEL-LLM: Transforming C code to Java using LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aryan Gupta, Y. Raghu Reddy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:25:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11402v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11402v1' target='_blank'>pdf</a><a href='https://doi.org/10.13140/RG.2.2.15123.54564' target='_blank'>doi</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research</h2>
                <div class="authors">
                    <strong>Authors:</strong> Simon Chung, Colby J. Vorland, Donna L. Maney, Andrew W. Brown
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:24:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.08371v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.08371v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Galann Pennec, Zhengyuan Liu, Nicholas Asher, Philippe Muller, Nancy F. Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:19:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11399v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11399v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuting Hu, Chenhui Xu, Ruiyang Qin, Dancheng Liu, Amir Nassereldine, Yiyu Shi, Jinjun Xiong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Existing protocols typically default to entirely risk-avoidant actions such as immediate stops, which are detrimental to navigation goals and lack flexibility for rare driving scenarios. Yet, in cases of minor risk, halting the vehicle may be unnecessary, and more adaptive responses are preferable. In this paper, we propose LLM-RCO, a risk-averse framework leveraging large language models (LLMs) to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules interacting with the dynamic driving environment: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator, enabling proactive and context-aware actions in such challenging conditions. To enhance the driving decision-making of LLMs, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, annotated for LLM fine-tuning in hazard detection and motion planning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that LLM-RCO promotes proactive maneuvers over purely risk-averse actions in perception deficit scenarios, underscoring its value for boosting autonomous driving resilience against perception loss challenges.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:19:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2503.07020v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2503.07020v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 AutoFSM: A Multi-agent Framework for FSM Code Generation with IR and SystemC-Based Testing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qiuming Luo, Yanming Lei, Kunzhong Wu, Yixuan Cao, Chengjian Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapid advancement of large language models (LLMs) in code generation, their applications in hardware design are receiving growing attention. However, existing LLMs face several challenges when generating Verilog code for finite state machine (FSM) control logic, including frequent syntax errors, low debugging efficiency, and heavy reliance on test benchmarks. To address these challenges, this paper proposes AutoFSM, a multi-agent collaborative framework designed for FSM code generation tasks. AutoFSM introduces a structurally clear intermediate representation (IR) to reduce syntax error rate during code generation and provides a supporting toolchain to enable automatic translation from IR to Verilog. Furthermore, AutoFSM is the first to integrate SystemC-based modeling with automatic testbench generation, thereby improving debugging efficiency and feedback quality. To systematically evaluate the framework's performance, we construct SKT-FSM, the first hierarchical FSM benchmark in the field, comprising 67 FSM samples across different complexity levels. Experimental results show that, under the same base LLM, AutoFSM consistently outperforms the open-source framework MAGE on the SKT-FSM benchmark, achieving up to an 11.94% improvement in pass rate and up to a 17.62% reduction in syntax error rate. These results demonstrate the potential of combining LLMs with structured IR and automated testing to improve the reliability and scalability of register-transfer level (RTL) code generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:15:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11398v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11398v1' target='_blank'>pdf</a><a href='https://doi.org/10.1007/978-981-95-4088-4_29' target='_blank'>doi</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Niu, Han Xiao, Dongyi Liu, Nuo Chen, Jia Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:01:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11391v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11391v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Felipe Ribeiro Fujita de Mello, Hideyuki Takada
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigated the impact of data selection on machine translation fine-tuning for open LLMs. Using Japanese-English corpora, we compare five selectors: TF-IDF, COMET Kiwi, QuRate, FD-Score, and random selection, under controlled training conditions. We observed that semantic selectors consistently outperform lexical and geometry-based heuristics, and that even when the selected data differ by less than 3%, the impact on model performance is substantial, underscoring the sensitivity of fine-tuning to data quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:59:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11388v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11388v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Adversarial Signed Graph Learning with Differential Privacy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haobin Ke, Sen Zhang, Qingqing Ye, Xun Ran, Haibo Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:57:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.00307v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.00307v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lim Chien Her, Ming Yan, Yunshu Bai, Ruihao Li, Hao Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:48:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10501v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10501v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrew Adiletta, Kathryn Adiletta, Kemal Derya, Berk Sunar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.   Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:52:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11783v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11783v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Osprey: Production-Ready Agentic AI for Safety-Critical Control Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thorsten Hellert, João Montenegro, Antonin Sulc
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Operating large-scale scientific facilities requires coordinating diverse subsystems, translating operator intent into precise hardware actions, and maintaining strict safety oversight. Language model-driven agents offer a natural interface for these tasks, but most existing approaches are not yet reliable or safe enough for production use. In this paper, we introduce Osprey, a framework for using agentic AI in large, safety-critical facility operations. Osprey is built around the needs of control rooms and addresses these challenges in four ways. First, it uses a plan-first orchestrator that generates complete execution plans, including all dependencies, for human review before any hardware is touched. Second, a coordination layer manages complex data flows, keeps data types consistent, and automatically downsamples large datasets when needed. Third, a classifier dynamically selects only the tools required for a given task, keeping prompts compact as facilities add capabilities. Fourth, connector abstractions and deployment patterns work across different control systems and are ready for day-to-day use. We demonstrate the framework through two case studies: a control-assistant tutorial showing semantic channel mapping and historical data integration, and a production deployment at the Advanced Light Source, where Osprey manages real-time operations across hundreds of thousands of control channels. These results establish Osprey as a production-ready framework for deploying agentic AI in complex, safety-critical environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:30:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2508.15066v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2508.15066v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuming Feng, Xinrui Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Online product reviews contain rich but noisy signals that overwhelm users and hinder effective decision-making. Existing LLM-based summarizers remain generic and fail to account for individual preferences, limiting their practical utility. We propose SUMFORU, a steerable review summarization framework that aligns outputs with explicit user personas to support personalized purchase decisions. Our approach integrates a high-quality data pipeline built from the Amazon 2023 Review Dataset with a two-stage alignment procedure: (1) persona-aware Supervised Fine-Tuning (SFT) via asymmetric knowledge distillation, and (2) Reinforcement Learning with AI Feedback (RLAIF) using a preference estimator to capture fine-grained, persona-relevant signals. We evaluate the model across rule-based, LLM-based, and human-centered metrics, demonstrating consistent improvements in consistency, grounding, and preference alignment. Our framework achieves the highest performance across all evaluation settings and generalizes effectively to unseen product categories. Our results highlight the promise of steerable pluralistic alignment for building next-generation personalized decision-support systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T18:05:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11755v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11755v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Darvin Yi, Teng Liu, Mattie Terzolo, Lance Hasson, Ayan Sinha, Pablo Mendes, Andrew Rabinovich
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:51:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.12306v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.12306v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Annabelle Sujun Tang, Christopher Priebe, Rohan Mahapatra, Lianhui Qin, Hadi Esmaeilzadeh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:38:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2506.01374v4' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2506.01374v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongsheng Huang, Peibo Duan, Yujie Wu, Kai Sun, Zhipeng Liu, Changsheng Zhang, Bin Zhang, Mingkun Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:36:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11743v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11743v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting</h2>
                <div class="authors">
                    <strong>Authors:</strong> James Luther, Donald Brown
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Culture is a core component of human-to-human interaction and plays a vital role in how we perceive and interact with others. Advancements in the effectiveness of Large Language Models (LLMs) in generating human-sounding text have greatly increased the amount of human-to-computer interaction. As this field grows, the cultural alignment of these human-like agents becomes an important field of study. Our work uses Hofstede's VSM13 international surveys to understand the cultural alignment of the following models: DeepSeek-V3, V3.1, GPT-4, GPT-4.1, GPT-4o, and GPT-5. We use a combination of prompt language and cultural prompting, a strategy that uses a system prompt to shift a model's alignment to reflect a specific country, to align these LLMs with the United States and China. Our results show that DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language. We also find that GPT-4 exhibits an alignment closer to China when prompted in English, but cultural prompting is effective in shifting this alignment closer to the United States. Other low-cost models, GPT-4o and GPT-4.1, respond to the prompt language used (i.e., English or Simplified Chinese) and cultural prompting strategies to create acceptable alignments with both the United States and China.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T17:25:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09772v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09772v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhaodong Wang, Zhenting Qi, Sherman Wong, Nathan Hu, Samuel Lin, Jun Ge, Erwin Gao, Yining Yang, Ben Maurer, Wenlin Chen, David Recordon, Yilun Du, Minlan Yu, Ying Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:59:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10398v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10398v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sergey Pankratov, Dan Alistarh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speculative generation has emerged as a promising technique to accelerate inference in large language models (LLMs) by leveraging parallelism to verify multiple draft tokens simultaneously. However, the fundamental limits on the achievable speedup remain poorly understood. In this work, we establish the first ``tight'' lower bounds on the runtime of any deterministic speculative generation algorithm. This is achieved by drawing a parallel between the token generation process and branching random walks, which allows us to analyze the optimal draft tree selection problem. We prove, under basic assumptions, that the expected number of tokens successfully predicted per speculative iteration is bounded as $\mathbb{E}[X] \leq (μ+ μ_{(2)})\log(P )/μ^2 + O(1)$, where $P$ is the verifier's capacity, $μ$ is the expected entropy of the verifier's output distribution, and $μ_{(2)}$ is the expected second log-moment. This result provides new insights into the limits of parallel token generation, and could guide the design of future speculative decoding systems. Empirical evaluations on Llama models validate our theoretical predictions, confirming the tightness of our bounds in practical settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:54:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11718v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11718v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Text2Graph: Combining Lightweight LLMs and GNNs for Efficient Text Classification in Label-Scarce Scenarios</h2>
                <div class="authors">
                    <strong>Authors:</strong> João Lucas Luz Lima Sarcinelli, Ricardo Marcondes Marcacini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have become effective zero-shot classifiers, but their high computational requirements and environmental costs limit their practicality for large-scale annotation in high-performance computing (HPC) environments. To support more sustainable workflows, we present Text2Graph, an open-source Python package that provides a modular implementation of existing text-to-graph classification approaches. The framework enables users to combine LLM-based partial annotation with Graph Neural Network (GNN) label propagation in a flexible manner, making it straightforward to swap components such as feature extractors, edge construction methods, and sampling strategies. We benchmark Text2Graph on a zero-shot setting using five datasets spanning topic classification and sentiment analysis tasks, comparing multiple variants against other zero-shot approaches for text classification. In addition to reporting performance, we provide detailed estimates of energy consumption and carbon emissions, showing that graph-based propagation achieves competitive results at a fraction of the energy and environmental cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:45:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10061v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10061v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sophia Hager, David Mueller, Kevin Duh, Nicholas Andrews
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) are increasingly used for factual question-answering, it becomes more important for LLMs to have the capability to communicate the likelihood that their answer is correct. For these verbalized expressions of uncertainty to be meaningful, they should reflect the error rates at the expressed level of confidence. However, when prompted to express confidence, the error rates of current LLMs are inconsistent with their communicated confidences, highlighting the need for uncertainty quantification methods. Many prior methods calculate lexical uncertainty, estimating a model's confidence in the specific string it generated. In some cases, however, it may be more useful to estimate semantic uncertainty, or the model's confidence in the answer regardless of how it is verbalized. We propose a simple procedure, uncertainty distillation, to teach an LLM to verbalize calibrated semantic confidences. Using held-out data to map initial uncertainty estimates to meaningful probabilities, we create examples annotated with verbalized probabilities for supervised fine-tuning. We find that our method yields verbalized confidences that correlate well with observed error rates, even when compared to strong baselines, some of which are more than twenty times slower at inference time. Additionally, we demonstrate that our method can be applied to black-box models that allow API-based fine-tuning, resulting in estimates of uncertainty that are both more effective and more efficient than any of our baselines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:31:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2503.14749v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2503.14749v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Particle Image Velocimetry Refinement via Consensus ADMM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alan Bonomi, Francesco Banelli, Antonio Terpin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Particle Image Velocimetry (PIV) is an imaging technique in experimental fluid dynamics that quantifies flow fields around bluff bodies by analyzing the displacement of neutrally buoyant tracer particles immersed in the fluid. Traditional PIV approaches typically depend on tuning parameters specific to the imaging setup, making the performance sensitive to variations in illumination, flow conditions, and seeding density. On the other hand, even state-of-the-art machine learning methods for flow quantification are fragile outside their training set. In our experiments, we observed that flow quantification would improve if different tunings (or algorithms) were applied to different regions of the same image pair. In this work, we parallelize the instantaneous flow quantification with multiple algorithms and adopt a consensus framework based on the alternating direction method of multipliers, seamlessly incorporating priors such as smoothness and incompressibility. We perform several numerical experiments to demonstrate the benefits of this approach. For instance, we achieve a decrease in end-point-error of up to 20% of a dense-inverse-search estimator at an inference rate of 60Hz, and we show how this performance boost can be increased further with outlier rejection. Our method is implemented in JAX, effectively exploiting hardware acceleration, and integrated in Flow Gym, enabling (i) reproducible comparisons with the state-of-the-art, (ii) testing different base algorithms, (iii) straightforward deployment for active fluids control applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:20:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.flu-dyn</span><span>cs.CV</span><span>eess.IV</span><span>math.OC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11695v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11695v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Crystal Su, Kuai Yu, Jingrui Zhang, Mingyuan Shao, Daniel Bauer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work presents an ontology-integrated large language model (LLM) framework for chemical engineering that unites structured domain knowledge with generative reasoning. The proposed pipeline aligns model training and inference with the COPE ontology through a sequence of data acquisition, semantic preprocessing, information extraction, and ontology mapping steps, producing templated question-answer pairs that guide fine-tuning. A control-focused decoding stage and citation gate enforce syntactic and factual grounding by constraining outputs to ontology-linked terms, while evaluation metrics quantify both linguistic quality and ontological accuracy. Feedback and future extensions, including semantic retrieval and iterative validation, further enhance the system's interpretability and reliability. This integration of symbolic structure and neural generation provides a transparent, auditable approach for applying LLMs to process control, safety analysis, and other critical engineering contexts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:14:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.26898v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.26898v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Evaluating Cooperative Resilience in Multiagent Systems: A Comparison Between Humans and LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manuela Chacon-Chamorro, Juan Sebastián Pinzón, Rubén Manrique, Luis Felipe Giraldo, Nicanor Quijano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a comparative analysis of cooperative resilience in multi-agent systems, defined as the ability to anticipate, resist, recover from, and transform to disruptive events that affect collective well-being. We focus on mixed-motive social dilemmas instantiated as a \textit{Tragedy of the Commons} environment from the Melting Pot suite, where we systematically compare human groups and Large Language Model (LLM)-based agents, each evaluated with and without explicit communication. Cooperative resilience is assessed under a continuously disruptive condition induced by a persistent unsustainable consumption bot, together with intermittent environmental shocks implemented as stochastic removal of shared resources across scenarios. This experimental design establishes a benchmark for cooperative resilience across agent architectures and interaction modalities, constituting a key step toward systematically comparing humans and LLM-based agents. Using this framework, we find that human groups with communication achieve the highest cooperative resilience compared to all other groups. Communication also improves the resilience of LLM agents, but their performance remains below human levels. Motivated by the performance of humans, we further examine a long-horizon setting with harsher environmental conditions, where humans sustain the shared resource and maintain high resilience in diverse disruption scenarios. Together, these results suggest that human decision-making under adverse social conditions can inform the design of artificial agents that promote prosocial and resilient behaviors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:11:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11689v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11689v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Crystal Su
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:08:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.12963v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.12963v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier</h2>
                <div class="authors">
                    <strong>Authors:</strong> Crystal Su
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) often produce fluent reasoning steps while violating simple mathematical or logical constraints. We introduce MedRule-KG, a compact typed knowledge graph coupled with a symbolic verifier, designed to enforce mathematically interpretable rules in reasoning tasks. MedRule-KG encodes entities, relations, and three domain-inspired rules, while the verifier checks predictions and applies minimal corrections to guarantee consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields 1.000 EM while eliminating rule violations entirely. We demonstrate how MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss ablations, and release code and data to encourage reproducibility.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T16:08:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.16309v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.16309v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liangliang Zhang, Nandana Mihindukulasooriya, Niharika S. D'Souza, Sola Shirai, Sarthak Dash, Yao Ma, Horst Samulowitz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Data products are reusable, self-contained assets designed for specific business use cases. Automating their discovery and generation is of great industry interest, as it enables discovery in large data lakes and supports analytical Data Product Requests (DPRs). Currently, there is no benchmark established specifically for data product discovery. Existing datasets focus on answering single factoid questions over individual tables rather than collecting multiple data assets for broader, coherent products. To address this gap, we introduce Data Product Benchmark, the first user-request-driven data product benchmark over hybrid table-text corpora.   Our framework systematically repurposes existing table-text QA datasets by clustering related tables and passages into coherent data products, generating professional-level analytical requests that span both data sources, and validating benchmark quality through multi-LLM evaluation. Data Product Benchmark preserves full provenance while producing actionable, analyst-like data product requests. Baseline experiments with hybrid retrieval methods establish the feasibility of DPR evaluation, reveal current limitations, and point to new opportunities for automatic data product discovery research.   Code and datasets are available at: Dataset: https://huggingface.co/datasets/ibm-research/data-product-benchmark Code: https://github.com/ibm/data-product-benchmark
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:52:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.21737v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.21737v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Toward Scalable VR-Cloud Gaming: An Attention-aware Adaptive Resource Allocation Framework for 6G Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gabriel Almeida, João Paulo Esper, Cleverson Nahum, Audebaro Klautau, Kleber Vieira Cardoso
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Virtual Reality Cloud Gaming (VR-CG) represents a demanding class of immersive applications, requiring high bandwidth, ultra-low latency, and intelligent resource management to ensure optimal user experience. In this paper, we propose a scalable and QoE-aware multi-stage optimization framework for resource allocation in VR-CG over 6G networks. Our solution decomposes the joint resource allocation problem into three interdependent stages: (i) user association and communication resource allocation; (ii) VR-CG game engine placement with adaptive multipath routing; and (iii) attention-aware scheduling and wireless resource allocation based on motion-to-photon latency. For each stage, we design specialized heuristic algorithms that achieve near-optimal performance while significantly reducing computational time. We introduce a novel user-centric QoE model based on visual attention to virtual objects, guiding adaptive resolution and frame rate selection. A dataset-driven evaluation demonstrates that, when compared against state-of-the-art approaches, our framework improves QoE by up to 50\%, reduces communication resource usage by 75\%, and achieves up to 35\% cost savings, while maintaining an average optimality gap of 5\%. Our proposed heuristics solve large-scale scenarios in under 0.1 seconds, highlighting their potential for real-time deployment in next-generation mobile networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:46:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11667v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11667v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews</h2>
                <div class="authors">
                    <strong>Authors:</strong> Brenda Nogueira, Werner Geyer, Andrew Anderson, Toby Jia-Jun Li, Dongwhi Kim, Nuno Moniz, Nitesh V. Chawla
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:38:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11661v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11661v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manideep Reddy Chinthareddy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices.   This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:21:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11643v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11643v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Wave energy converters as offshore wind farm guardians: a pathway to resilient ocean systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Olivia Vitale, Maha Haji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Maximizing the durability and reliability of offshore wind farms is essential for the clean energy transition. In this work, we demonstrate how wave energy converter (WEC) farms can shelter offshore wind farms from cyclic wave loading, resulting in significant reductions in turbine fatigue damage. Using experimentally validated hydrodynamic models, we show that WEC arrays can reduce wave-induced fatigue damage on the turbines by up to 25%, potentially lowering required monopile diameters and extending turbine lifetimes. This damage reduction propagates to the levelized cost of energy (LCOE) of the wind farm, targeting cost reductions in nearly 50% of the total system costs. Additionally, WEC farms can benefit from this co-location by sharing siting costs, operation and maintenance teams, and mooring and transmission cables with the offshore wind farm. This work supports resilient, cost-effective offshore renewables for global deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T15:14:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.flu-dyn</span><span>physics.ao-ph</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2410.13898v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2410.13898v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Complexity at Scale: A Quantitative Analysis of an Alibaba Microservice Deployment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giles Winchester, George Parisis, Guoyao Xu, Luc Berthouze
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Microservice management and testbed research often rests on assumptions about deployments that have rarely been validated at production scale. While recent studies have begun to characterise production microservice deployments, they are often limited in breadth, do not compare findings across deployments, and lack consideration of the implications of findings for commonly held assumptions. We analyse a distributed tracing dataset from Alibaba's production microservice deployment to examine its scale, heterogeneity, and dynamicity. By comparing our findings to prior measurements of Meta's MSA we illustrate both convergent and divergent properties, clarifying which patterns may generalise. Our study reveals extreme architectural scale, long-tail distributions of workloads and dependencies, highly diverse functionality, substantial call graph variability, and pronounced time-varying behaviour which diverge from assumptions underlying research models and testbeds. We summarise how these observations challenge common assumptions in research on fault management, scaling, and testbed design, and outline recommendations for more realistic future approaches and evaluations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:55:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2504.13141v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2504.13141v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 LLM tools in the prediction of the stability of perovskite solar cells</h2>
                <div class="authors">
                    <strong>Authors:</strong> S. Frenkel, V. Zakharov, E. A. Katz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate whether tools based on large language models (LLMs) can be effectively used by a developer of new perovskite solar cells (PSCs) to predict both the "lifetime" of the device and the degree of its degradation at specific time intervals. We demonstrate the ability of common LLM tools (ChatGPT, DeepSeek, and even a simplified free version of ChatGPT) to suggest and justify prediction methods in a dialogue with the user under conditions of incomplete information about the physical models of PSC degradation and the influence of the environment. One of the results covers LLM ChatGPT's ontology of the specific subject domain of PSCs. It allows the formation of time series of efficiency with a given architecture, calculated using various available models together with environmental characteristics archived in various meteorological databases (illumination, temperature, humidity, UV level). We conclude that ChatGPT currently has sufficient access to training samples, can find various models in the literature, and has adequate solutions for predicting degradation trends.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:50:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11615v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11615v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</h2>
                <div class="authors">
                    <strong>Authors:</strong> Björn Deiseroth, Max Henning Höth, Kristian Kersting, Letitia Parcalabescu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:50:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11614v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11614v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Embodied Image Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chunyi Li, Rui Qing, Jianbo Zhang, Yuan Tian, Xiangyang Zhu, Zicheng Zhang, Xiaohong Liu, Weisi Lin, Guangtao Zhai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:49:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11612v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11612v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 A Modeling and Optimization Framework for Fostering Modal Shift through the Integration of Tradable Credits and Demand-Responsive Autonomous Shuttles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zenghao Hou, Ludovic Leclercq
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tradable Credit Schemes (TCS) promote the use of public and shared transport by capping private car usage while maintaining fair welfare outcomes by allowing credit trading. However, most existing studies assume unlimited public transit capacity or a fixed occupancy of shared modes, often neglecting waiting time and oversimplifying time-based costs by depending solely on in-vehicle travel time. These assumptions can overstate the system's performance with TCS regulation, especially when there are insufficient public or shared transport supplies.   To address this, we develop a dynamic multimodal equilibrium model to capture operation constraints and induced waiting times under TCS regulation. The model integrates travelers' mode choices, credit trading, traffic dynamics, and waiting time, which depend on key operational features of service vehicles such as fleet size and capacity.   Besides, most TCS studies assume fixed transport supply, overlooking supply-side responses triggered by demand shifts. Therefore, we further propose integrating adaptive supply management through the deployment of Demand-Responsive Autonomous Shuttles (DRAS) and developing a bi-level optimization framework that incorporates the equilibrium model to jointly optimize TCS design and operational strategies for the DRAS.   We apply the framework to a section of the A10 highway near Paris, France, to examine demand-supply interactions and assess the potential benefits of jointly implementing TCS and DRAS. Numerical results demonstrate the importance of modeling operational features within multimodal equilibrium and incorporating flexible supply in TCS policies for mitigating overall generalized cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:44:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11607v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11607v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:42:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2507.05178v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2507.05178v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Granite: Granular Runtime Enforcement for GitHub Actions Permissions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mojtaba Moazen, Amir. M Ahmadian, Musard Balliu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern software projects use automated CI/CD pipelines to streamline their development, build, and deployment processes. GitHub Actions is a popular CI/CD platform that enables project maintainers to create custom workflows -- collections of jobs composed of sequential steps -- using reusable components known as actions. Wary of the security risks introduced by fully-privileged actions, GitHub provides a job-level permission model for controlling workflow access to repository resources. Unfortunately, this model is too coarse-grained to reduce the attack surface pertaining to permission misuse attacks: All actions within a job share the same permissions granted to the job. This violates the principle of least privilege and can lead to broader software supply chain attacks, whenever a compromised action exploits the granted permissions to compromise the repository resources. In this paper, we present Granite, a runtime proxy-based system that enforces fine-grained permissions for GitHub Actions at the step-level granularity within a job. Granite transparently monitors requests made by JavaScript and composite actions during workflow execution and checks them against predefined step-level policies at runtime. We evaluate Granite in terms of compatibility, security, and performance overhead using a dataset of 500 workflows comprising 12,916 jobs from the most-starred GitHub repositories that use GitHub Actions. Our analysis reveals that 52.7% of the jobs can be protected by Granite against permission misuse attacks. We evaluate Granite on 20 top-starred repositories (63 actions, 58 workflows), validate attack prevention using 10 permission misuse attacks across 42 overprivileged jobs, and measure an average overhead of 55% (3.67 seconds) per job, concluding that Granite effectively reduces CI/CD attack surfaces.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:38:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11602v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11602v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 A Study of Library Usage in Agent-Authored Pull Requests</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lukas Twist
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited "library preferences" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:21:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11589v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11589v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 AI Benchmark Democratization and Carpentry</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gregor von Laszewski, Wesley Brewer, Jeyan Thiyagalingam, Juri Papay, Armstrong Foundjem, Piotr Luszczek, Murali Emani, Shirley V. Moore, Vijay Janapa Reddi, Matthew D. Sinclair, Sebastian Lobentanzer, Sujata Goswami, Benjamin Hawks, Marco Colombo, Nhan Tran, Christine R. Kirkpatrick, Abdulkareem Alsudais, Gregg Barrett, Tianhao Li, Kirsten Morehouse, Shivaram Venkataraman, Rutwik Jain, Kartik Mathur, Victor Lu, Tejinder Singh, Khojasteh Z. Mirza, Kongtao Chen, Sasidhar Kunapuli, Gavin Farrell, Renato Umeton, Geoffrey C. Fox
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.   Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.   Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:20:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11588v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11588v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Visualizing token importance for black-box language models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paulius Rauba, Qiyao Wei, Mihaela van der Schaar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T14:01:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11573v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11573v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 ReCode: Unify Plan and Action for Universal Granularity Control</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yuyu Luo, Bang Liu, Chenglin Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:55:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.23564v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.23564v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haiteng Zhao, Junhao Shen, Yiming Zhang, Songyang Gao, Kuikun Liu, Tianyou Ma, Fan Zheng, Dahua Lin, Wenwei Zhang, Kai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:43:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10534v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10534v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Zhang, Zhiheng Chen, Ye Qiao, Sitao Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:35:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11550v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11550v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan Shen, Xiaojun Wu, Linghua Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:25:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11544v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11544v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 All-in-One ASR: Unifying Encoder-Decoder Models of CTC, Attention, and Transducer in Dual-Mode ASR</h2>
                <div class="authors">
                    <strong>Authors:</strong> Takafumi Moriya, Masato Mimura, Tomohiro Tanaka, Hiroshi Sato, Ryo Masumura, Atsunori Ogawa
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper proposes a unified framework, All-in-One ASR, that allows a single model to support multiple automatic speech recognition (ASR) paradigms, including connectionist temporal classification (CTC), attention-based encoder-decoder (AED), and Transducer, in both offline and streaming modes. While each ASR architecture offers distinct advantages and trade-offs depending on the application, maintaining separate models for each scenario incurs substantial development and deployment costs. To address this issue, we introduce a multi-mode joiner that enables seamless integration of various ASR modes within a single unified model. Experiments show that All-in-One ASR significantly reduces the total model footprint while matching or even surpassing the recognition performance of individually optimized ASR models. Furthermore, joint decoding leverages the complementary strengths of different ASR modes, yielding additional improvements in recognition accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T13:23:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11543v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11543v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 xGR: Efficient Generative Recommendation Serving at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qingxiao Sun, Tongxuan Liu, Shen Zhang, Siyu Wu, Peijun Yang, Haotian Liang, Menxin Li, Xiaolong Ma, Zhiwei Liang, Ziyi Ren, Minchao Zhang, Xinyu Liu, Ke Zhang, Depei Qian, Hailong Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:59:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11529v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11529v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Mini-SFC: A Comprehensive Simulation Framework for Orchestration and Management of Service Function Chains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xi Wang, Shuo Shi, Chenyu Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the continuously evolving cloud computing and network environment, service function chain (SFC) plays a crucial role in implementing complex services in the network with its flexible deployment capabilities. To address the limitations of existing SFC simulation tools, this paper introduces Mini-SFC, a modular simulation framework that supports both numerical and container-based virtual simulations, while also supporting online dynamic topology adjustments. As an open-source platform emphasizing user-friendliness, Mini-SFC facilitates rapid algorithm verification and realistic service deployment validation. By simplifying module design and providing standardized solver interfaces, Mini-SFC significantly shortens the learning curve for researchers and enhances the flexibility and scalability required for advanced SFC management and optimization. For readers interested in exploring or utilizing Mini-SFC, more information is available on the official project page.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:54:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11527v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11527v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shibing Mo, Haoyang Ruan, Kai Wu, Jing Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:35:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.06682v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.06682v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohor Banerjee, Nadya Yuki Wangsajaya, Syed Ali Redha Alsagoff, Min Sen Tan, Zachary Choy Kit Chun, Alvin Chan Guo Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:14:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11509v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11509v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild</h2>
                <div class="authors">
                    <strong>Authors:</strong> Binquan Zhang, Li Zhang, Haoyuan Zhang, Fang Liu, Song Wang, Bo Shen, An Fu, Lin Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:10:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10493v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10493v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Diffusion-based Surrogate Model for Time-varying Underwater Acoustic Channels</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kexin Li, Mandar Chitre
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate modeling of time-varying underwater acoustic channels is essential for the design, evaluation, and deployment of reliable underwater communication systems. Conventional physics models require detailed environmental knowledge, while stochastic replay methods are constrained by the limited diversity of measured channels and often fail to generalize to unseen scenarios, reducing their practical applicability. To address these challenges, we propose StableUASim, a pre-trained conditional latent diffusion surrogate model that captures the stochastic dynamics of underwater acoustic communication channels. Leveraging generative modeling, StableUASim produces diverse and statistically realistic channel realizations, while supporting conditional generation from specific measurement samples. Pre-training enables rapid adaptation to new environments using minimal additional data, and the autoencoder latent representation facilitates efficient channel analysis and compression. Experimental results demonstrate that StableUASim accurately reproduces key channel characteristics and communication performance, providing a scalable, data-efficient, and physically consistent surrogate model for both system design and machine learning-driven underwater applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:07:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.18078v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.18078v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Georgios Kaoukis, Ioannis Aris Koufopoulos, Psaroudaki Eleni, Danae Pla Karidi, Evaggelia Pitoura, George Papastefanatos, Panayiotis Tsaparas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T12:06:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11506v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11506v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Mistake Notebook Learning: Selective Batch-Wise Context Optimization for In-Context Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xuanbo Su, Yingfang Zhang, Hao Luo, Xiaoteng Liu, Leo Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) adapt to tasks via gradient fine-tuning (heavy computation, catastrophic forgetting) or In-Context Learning (ICL: low robustness, poor mistake learning). To fix this, we introduce Mistake Notebook Learning (MNL), a training-free framework with a persistent knowledge base of abstracted error patterns. Unlike prior instance/single-trajectory memory methods, MNL uses batch-wise error abstraction: it extracts generalizable guidance from multiple failures, stores insights in a dynamic notebook, and retains only baseline-outperforming guidance via hold-out validation (ensuring monotonic improvement). We show MNL nearly matches Supervised Fine-Tuning (93.9% vs 94.3% on GSM8K) and outperforms training-free alternatives on GSM8K, Spider, AIME, and KaggleDBQA. On KaggleDBQA (Qwen3-8B), MNL hits 28% accuracy (47% relative gain), outperforming Memento (15.1%) and Training-Free GRPO (22.1) - proving it's a strong training-free alternative for complex reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:33:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11485v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11485v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Melih Catal, Pooja Rani, Harald C. Gall
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:31:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11482v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11482v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 When Large Language Models Meet UAV Projects: An Empirical Study from Developers' Perspective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yihua Chen, Xingle Que, Jiashuo Zhang, Jiachi Chen, Ting Cui, Guangshun Li, Ting Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In recent years, unmanned aerial vehicles (UAVs) have become increasingly popular in our daily lives and have attracted significant research interest in software engineering. At the same time, large language models (LLMs) have made notable advancements in language understanding, reasoning, and generation, making LLM applications in UAVs a promising research direction. However, existing studies have largely remained in preliminary exploration with a limited understanding of real-world practice, which causes an academia-industry gap and hinders the application of LLMs in UAVs. To address this, we conducted the first empirical study to investigate how LLMs support UAVs. To characterize common tasks and application scenarios of real-world UAV-LLM practices, we conducted a large-scale empirical study involving 997 research papers and 1,509 GitHub projects. The results classified nine common tasks (e.g., Natural Language Command Parsing) in four UAV workflows (e.g., Information Input) undertaken by LLMs in real-world UAV projects and revealed a large difference in the task distribution of research efforts and industry practices. To gain deeper insight into these differences and understand developers' perspectives on the application of LLMs in UAVs, we conducted a survey of practitioners, receiving 52 valid responses from 15 countries. The results revealed that while 40.4% of developers have attempted to apply LLMs to UAV tasks, 59.6% still face challenges integrating their UAV projects with advanced LLM capabilities. Their feedback attributes these challenges to five factors, including technological maturity, performance, safety, cost, and others, and provides practical implications for researchers and developers in conducting UAV-LLM practices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:30:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2509.12795v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2509.12795v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Integrated Sensing and Communications Over the Years: An Evolution Perspective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Di Zhang, Yuanhao Cui, Xiaowen Cao, Nanchi Su, Yi Gong, Fan Liu, Weijie Yuan, Xiaojun Jing, J. Andrew Zhang, Jie Xu, Christos Masouros, Dusit Niyato, Marco Di Renzo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Integrated Sensing and Communications (ISAC) enables efficient spectrum utilization and reduces hardware costs for beyond 5G (B5G) and 6G networks, facilitating intelligent applications that require both high-performance communication and precise sensing capabilities. This survey provides a comprehensive review of the evolution of ISAC over the years. We examine the expansion of the spectrum across RF and optical ISAC, highlighting the role of advanced technologies, along with key challenges and synergies. We further discuss the advancements in network architecture from single-cell to multi-cell systems, emphasizing the integration of collaborative sensing and interference mitigation strategies. Moreover, we analyze the progress from single-modal to multi-modal sensing, with a focus on the integration of edge intelligence to enable real-time data processing, reduce latency, and enhance decision-making. Finally, we extensively review standardization efforts by 3GPP, IEEE, and ITU, examining the transition of ISAC-related technologies and their implications for the deployment of 6G networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:24:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2504.06830v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2504.06830v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ying Cheng, Yu-Ho Lin, Min-Hung Chen, Fu-En Yang, Shang-Hong Lai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:18:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2511.07299v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2511.07299v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 General-purpose AI models can generate actionable knowledge on agroecological crop protection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kris A. G. Wyckhuys
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:17:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CY</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11474v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11474v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Rethinking Expert Trajectory Utilization in LLM Post-training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bowen Ding, Yuhan Chen, Jiayang Lv, Jiyao Yuan, Qi Zhu, Shuangshuang Tian, Dantong Zhu, Futing Wang, Heyuan Deng, Fei Mi, Lifeng Shang, Tao Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:13:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11470v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11470v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Fast Multi-Party Open-Ended Conversation with a Social Robot</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giulio Antonio Abbo, Maria Jose Pinto-Bernal, Martijn Catrycke, Tony Belpaeme
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-party open-ended conversation remains a major challenge in human-robot interaction, particularly when robots must recognise speakers, allocate turns, and respond coherently under overlapping or rapidly shifting dialogue. This paper presents a multi-party conversational system that combines multimodal perception (voice direction of arrival, speaker diarisation, face recognition) with a large language model for response generation. Implemented on the Furhat robot, the system was evaluated with 30 participants across two scenarios: (i) parallel, separate conversations and (ii) shared group discussion. Results show that the system maintains coherent and engaging conversations, achieving high addressee accuracy in parallel settings (92.6%) and strong face recognition reliability (80-94%). Participants reported clear social presence and positive engagement, although technical barriers such as audio-based speaker recognition errors and response latency affected the fluidity of group interactions. The results highlight both the promise and limitations of LLM-based multi-party interaction and outline concrete directions for improving multimodal cue integration and responsiveness in future social robots.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:10:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2503.15496v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2503.15496v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Exploring MLLM-Diffusion Information Transfer with MetaCanvas</h2>
                <div class="authors">
                    <strong>Authors:</strong> Han Lin, Xichen Pan, Ziqi Huang, Ji Hou, Jialiang Wang, Weifeng Chen, Zecheng He, Felix Juefei-Xu, Junzhe Sun, Zhipeng Fan, Ali Thabet, Mohit Bansal, Chu Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T11:07:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11464v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11464v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 STAR-RIS-Aided Secure Communications:Analytical Insights and Performance Comparison</h2>
                <div class="authors">
                    <strong>Authors:</strong> Taissir Y. Elganimi, Mahmoud Aldababsa, Ali A. Nasir, Khaled M. Rabie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs) have emerged as a promising technology for enabling full-space signal manipulation and enhancing wireless network coverage and capacity. In this article, we present a comprehensive analytical comparison of STAR-RIS-assisted systems with single-input single-output (SISO), conventional RISs, and decode-and-forward (DF) relaying schemes, including both half-duplex (HD) and full-duplex (FD) modes. Closed-form expressions are derived for the achievable secrecy rates of STAR-RIS-aided communications under both the absence and presence of eavesdroppers. Unlike most existing works, the direct source destination link is incorporated in all considered schemes, and optimal transmit power allocation is investigated for HD and FD-DF relaying. Furthermore, we provide the conditions under which STAR-RIS outperforms HD- and FD-DF relaying and quantify the minimum number of STAR-RIS elements required to achieve superior rates. The impacts of key system parameters including transmit power, number of elements, reflection-to-transmission power ratio, element-splitting factor, and deployment positions on both achievable and secrecy performance are investigated. The results reveal that STAR-RIS systems can achieve superior rates and secrecy rates compared to all benchmark schemes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:59:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11461v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11461v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingmin Zhu, Anqi Zhu, Hossein Rahmani, Jun Liu, Mohammed Bennamoun, Qiuhong Ke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:53:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11458v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11458v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuowei Cai, Yansong Ning, Hao Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:08:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11426v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11426v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gonca Gürsun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.   The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T10:03:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11421v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11421v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Real-Time QP Solvers: A Concise Review and Practical Guide Towards Legged Robots</h2>
                <div class="authors">
                    <strong>Authors:</strong> Van Nam Dinh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quadratic programming (QP) underpins real-time robotics by enabling efficient, constrained optimization in state estimation, motion planning, and control. In legged locomotion and manipulation, essential modules like inverse dynamics, Model Predictive Control (MPC), and Whole-Body Control (WBC) are inherently QP-based, demanding reliable solutions amid tight timing, energy, and computational resources on embedded platforms. This paper presents a comprehensive analysis and benchmarking study of QP solvers for legged robotics. We begin by formulating the standard convex QP and classify solvers into principal algorithmic approaches: interior-point methods, active-set strategies, operator-splitting schemes, and augmented Lagrangian/proximal approaches, while also discussing solver code generation for fixed-structure QPs. Each solver is examined in terms of algorithmic structure, computational characteristics, and its ability to exploit problem structure and warm-starting. Performance is reviewed using publicly available benchmarks, with a focus on metrics such as computation time, constraint satisfaction, and robustness under perturbations. Unified comparison tables yield practical guidance for solver selection, underscoring trade-offs in speed, accuracy, and energy efficiency. Our findings emphasize the synergy between solvers, tasks, and hardware -- e.g., sparse structured IPMs for long-horizon MPC and dense active-set for high-frequency WBC to advance agile, autonomous legged systems, with emerging trends toward ill-conditioned, conic, and code-generated deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:48:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.21773v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.21773v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qiming Bao, Xiaoxuan Fu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) achieve strong performance on many natural language tasks, yet their generalisation under structured perturbations of logical rule systems remains insufficiently characterised. We present a controlled evaluation framework that probes reasoning reliability through four stress tests: (1) rule deletion, removing redundant versus essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites based on equivalence laws (contraposition, double negation, implication-to-disjunction, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that composes 2--5 transformations. Across three representative model families -- BERT, Qwen2, and LLaMA-like models -- all models attain Acc$=1.0000$ on the base split and show no degradation under redundant rule deletion. In contrast, essential rule deletion yields a pronounced decrease to near-chance performance, and injecting explicit contradictions reduces accuracy to 0.0000. Under logic-preserving rewrites, accuracy is largely preserved for single-law transformations with only small degradations in a few cases, whereas multi-law stacking exposes model-dependent sensitivity: BERT matches the base condition, TinyLlama shows only marginal degradation, and Qwen2 exhibits a substantial drop.   Overall, the results indicate that contemporary LLMs are generally stable under semantic-preserving reformulations, yet remain brittle to missing or inconsistent evidence and may degrade under composed logical transformations depending on the model family. The proposed framework provides a concise diagnostic tool for isolating these failure modes and for evaluating logical generalisation beyond surface-form variation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:31:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.LG</span><span>cs.LO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.06393v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.06393v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yousouf Taghzouti, Franck Michel, Tao Jiang, Louis-Félix Nothias, Fabien Gandon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The SPARQL query language is the standard method to access knowledge graphs (KGs). However, formulating SPARQL queries is a significant challenge for non-expert users, and remains time-consuming for the experienced ones. Best practices recommend to document KGs with competency questions and example queries to contextualise the knowledge they contain and illustrate their potential applications. In practice, however, this is either not the case or the examples are provided in limited numbers. Large Language Models (LLMs) are being used in conversational agents and are proving to be an attractive solution with a wide range of applications, from simple question-answering about common knowledge to generating code in a targeted programming language. However, training and testing these models to produce high quality SPARQL queries from natural language questions requires substantial datasets of question-query pairs. In this paper, we present Q${}^2$Forge that addresses the challenge of generating new competency questions for a KG and corresponding SPARQL queries. It iteratively validates those queries with human feedback and LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular, meaning that the different modules of the application (CQ generation, query generation and query refinement) can be used separately, as an integrated pipeline, or replaced by alternative services. The result is a complete pipeline from competency question formulation to query evaluation, supporting the creation of reference query sets for any target KG.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:31:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2505.13572v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2505.13572v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 REMODEL-LLM: Transforming C code to Java using LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aryan Gupta, Y. Raghu Reddy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:25:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11402v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11402v1' target='_blank'>pdf</a><a href='https://doi.org/10.13140/RG.2.2.15123.54564' target='_blank'>doi</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Galann Pennec, Zhengyuan Liu, Nicholas Asher, Philippe Muller, Nancy F. Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:19:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11399v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11399v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuting Hu, Chenhui Xu, Ruiyang Qin, Dancheng Liu, Amir Nassereldine, Yiyu Shi, Jinjun Xiong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Existing protocols typically default to entirely risk-avoidant actions such as immediate stops, which are detrimental to navigation goals and lack flexibility for rare driving scenarios. Yet, in cases of minor risk, halting the vehicle may be unnecessary, and more adaptive responses are preferable. In this paper, we propose LLM-RCO, a risk-averse framework leveraging large language models (LLMs) to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules interacting with the dynamic driving environment: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator, enabling proactive and context-aware actions in such challenging conditions. To enhance the driving decision-making of LLMs, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, annotated for LLM fine-tuning in hazard detection and motion planning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that LLM-RCO promotes proactive maneuvers over purely risk-averse actions in perception deficit scenarios, underscoring its value for boosting autonomous driving resilience against perception loss challenges.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:19:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2503.07020v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2503.07020v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 AutoFSM: A Multi-agent Framework for FSM Code Generation with IR and SystemC-Based Testing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qiuming Luo, Yanming Lei, Kunzhong Wu, Yixuan Cao, Chengjian Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapid advancement of large language models (LLMs) in code generation, their applications in hardware design are receiving growing attention. However, existing LLMs face several challenges when generating Verilog code for finite state machine (FSM) control logic, including frequent syntax errors, low debugging efficiency, and heavy reliance on test benchmarks. To address these challenges, this paper proposes AutoFSM, a multi-agent collaborative framework designed for FSM code generation tasks. AutoFSM introduces a structurally clear intermediate representation (IR) to reduce syntax error rate during code generation and provides a supporting toolchain to enable automatic translation from IR to Verilog. Furthermore, AutoFSM is the first to integrate SystemC-based modeling with automatic testbench generation, thereby improving debugging efficiency and feedback quality. To systematically evaluate the framework's performance, we construct SKT-FSM, the first hierarchical FSM benchmark in the field, comprising 67 FSM samples across different complexity levels. Experimental results show that, under the same base LLM, AutoFSM consistently outperforms the open-source framework MAGE on the SKT-FSM benchmark, achieving up to an 11.94% improvement in pass rate and up to a 17.62% reduction in syntax error rate. These results demonstrate the potential of combining LLMs with structured IR and automated testing to improve the reliability and scalability of register-transfer level (RTL) code generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:15:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11398v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11398v1' target='_blank'>pdf</a><a href='https://doi.org/10.1007/978-981-95-4088-4_29' target='_blank'>doi</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Niu, Han Xiao, Dongyi Liu, Nuo Chen, Jia Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T09:01:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11391v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11391v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Felipe Ribeiro Fujita de Mello, Hideyuki Takada
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigated the impact of data selection on machine translation fine-tuning for open LLMs. Using Japanese-English corpora, we compare five selectors: TF-IDF, COMET Kiwi, QuRate, FD-Score, and random selection, under controlled training conditions. We observed that semantic selectors consistently outperform lexical and geometry-based heuristics, and that even when the selected data differ by less than 3%, the impact on model performance is substantial, underscoring the sensitivity of fine-tuning to data quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:59:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11388v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11388v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lim Chien Her, Ming Yan, Yunshu Bai, Ruihao Li, Hao Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:48:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10501v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10501v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 RECAP: REwriting Conversations for Intent Understanding in Agentic Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kushan Mitra, Dan Zhang, Hannah Kim, Estevam Hruschka
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding user intent is essential for effective planning in conversational assistants, particularly those powered by large language models (LLMs) coordinating multiple agents. However, real-world dialogues are often ambiguous, underspecified, or dynamic, making intent detection a persistent challenge. Traditional classification-based approaches struggle to generalize in open-ended settings, leading to brittle interpretations and poor downstream planning. We propose RECAP (REwriting Conversations for Agent Planning), a new benchmark designed to evaluate and advance intent rewriting, reframing user-agent dialogues into concise representations of user goals. RECAP captures diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. Alongside the dataset, we introduce an LLM-based evaluator that assesses planning utility given the rewritten intent. Using RECAP, we develop a prompt-based rewriting approach that outperforms baselines, in terms of plan preference. We further demonstrate that fine-tuning two DPO-based rewriters yields additional utility gains. Our results highlight intent rewriting as a critical and tractable component for improving agentic planning in open-domain dialogue systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:45:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2509.04472v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2509.04472v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Mining Legal Arguments to Study Judicial Formalism</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tomáš Koref, Lena Held, Mahammad Namazov, Harun Kumru, Yassine Thlija, Christoph Burchard, Ivan Habernal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Courts must justify their decisions, but systematically analyzing judicial reasoning at scale remains difficult. This study refutes claims about formalistic judging in Central and Eastern Europe (CEE) by developing automated methods to detect and classify judicial reasoning in Czech Supreme Courts' decisions using state-of-the-art natural language processing methods. We create the MADON dataset of 272 decisions from two Czech Supreme Courts with expert annotations of 9,183 paragraphs with eight argument types and holistic formalism labels for supervised training and evaluation. Using a corpus of 300k Czech court decisions, we adapt transformer LLMs for Czech legal domain by continued pretraining and experiment with methods to address dataset imbalance including asymmetric loss and class weighting. The best models successfully detect argumentative paragraphs (82.6\% macro-F1), classify traditional types of legal argument (77.5\% macro-F1), and classify decisions as formalistic/non-formalistic (83.2\% macro-F1). Our three-stage pipeline combining ModernBERT, Llama 3.1, and traditional feature-based machine learning achieves promising results for decision classification while reducing computational costs and increasing explainability. Empirically, we challenge prevailing narratives about CEE formalism. This work shows that legal argument mining enables reliable judicial philosophy classification and shows the potential of legal argument mining for other important tasks in computational legal studies. Our methodology is easily replicable across jurisdictions, and our entire pipeline, datasets, guidelines, models, and source codes are available at https://github.com/trusthlt/madon.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:37:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11374v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11374v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shreya Shukla, Aditya Sriram, Milinda Kuppur Narayanaswamy, Hiteshi Jain
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The deployment of large language models for specialized tasks often requires domain-specific parameter-efficient finetuning through Low-Rank Adaptation (LoRA) modules. However, effectively fusing these adapters to handle complex, multi-domain composite queries remains a critical challenge. Existing LoRA fusion approaches either use static weights, which assign equal relevance to each participating LoRA, or require data-intensive supervised training for every possible LoRA combination to obtain respective optimal fusion weights. We propose qa-FLoRA, a novel query-adaptive data-and-training-free method for LoRA fusion that dynamically computes layer-level fusion weights by measuring distributional divergence between the base model and respective adapters. Our approach eliminates the need for composite training data or domain-representative samples, making it readily applicable to existing adapter collections. Extensive experiments across nine multilingual composite tasks spanning mathematics, coding, and medical domains, show that qa-FLoRA outperforms static fusion by ~5% with LLaMA-2 and ~6% with LLaMA-3, and the training-free baselines by ~7% with LLaMA-2 and ~10% with LLaMA-3, while significantly closing the gap with supervised baselines. Further, layer-level analysis of our fusion weights reveals interpretable fusion patterns, demonstrating the effectiveness of our approach for robust multi-domain adaptation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:27:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11366v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11366v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 A Cross-Chain Event-Driven Data Infrastructure for Aave Protocol Analytics and Applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junyi Fan, Li Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Decentralized lending protocols, exemplified by Aave V3, have transformed financial intermediation by enabling permissionless, multi-chain borrowing and lending without intermediaries. Despite managing over $10 billion in total value locked, empirical research remains severely constrained by the lack of standardized, cross-chain event-level datasets.   This paper introduces the first comprehensive, event-driven data infrastructure for Aave V3 spanning six major EVM-compatible chains (Ethereum, Arbitrum, Optimism, Polygon, Avalanche, and Base) from respective deployment blocks through October 2025. We collect and fully decode eight core event types -- Supply, Borrow, Withdraw, Repay, LiquidationCall, FlashLoan, ReserveDataUpdated, and MintedToTreasury -- producing over 50 million structured records enriched with block metadata and USD valuations.   Using an open-source Python pipeline with dynamic batch sizing and automatic sharding (each file less than or equal to 1 million rows), we ensure strict chronological ordering and full reproducibility. The resulting publicly available dataset enables granular analysis of capital flows, interest rate dynamics, liquidation cascades, and cross-chain user behavior, providing a foundational resource for future studies on decentralized lending markets and systemic risk.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:22:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11363v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11363v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chao Xu, Suyu Zhang, Yang Liu, Baigui Sun, Weihong Chen, Bo Xu, Qi Liu, Juncheng Wang, Shujun Wang, Shan Luo, Jan Peters, Athanasios V. Vasilakos, Stefanos Zafeiriou, Jiankang Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \href{https://suyuz1.github.io/Survery/}{project page}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:22:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11362v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11362v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Channel Knowledge Map Enabled Low-Altitude ISAC Networks: Joint Air Corridor Planning and Base Station Deployment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiaxuan Li, Yilong Chen, Fan Liu, Jie Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This letter addresses the joint air corridor planning and base station (BS) deployment problem for low-altitude integrated sensing and communication (ISAC) networks. In the considered system, unmanned aerial vehicles (UAVs) operate within a structured air corridor composed of connected cubic segments, and multiple BSs need to be selectively deployed at a set of candidate locations to ensure both sensing and communication coverage throughout the corridor. In particular, we leverage the channel knowledge map (CKM) to characterize wireless channels for candidate BS sites prior to deployment, thereby facilitating the offline planning. Under this setup, we minimize the system cost in terms of the weighted sum of the air corridor length and the number of deployed BSs, subject to the constraints on both sensing and communication performance across the corridor. To solve the formulated large-scale nonconvex integer programming problem, we develop a hierarchical coarse-to-fine grid decomposition algorithm. Simulation results demonstrate the benefit of the proposed joint design in reducing the overall deployment cost while ensuring the coverage of the low-altitude ISAC networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:10:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.02464v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.02464v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Textual Data Bias Detection and Mitigation -- An Extensible Pipeline with Experimental Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rebekka Görge, Sujan Sai Gannamaneni, Tabea Naeven, Hammam Abdelwahab, Héctor Allende-Cid, Armin B. Cremers, Lennard Helmer, Michael Mock, Anna Schmitz, Songkai Xue, Elif Yildirir, Maximilian Poretschkin, Stefan Wrobel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T08:06:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.10734v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.10734v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zihang Zeng, Shu Sun, Meixia Tao, Yin Xu, Xianghao Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Wireless channel modeling in complex environments is crucial for wireless communication system design and deployment. Traditional channel modeling approaches face challenges in balancing accuracy, efficiency, and scalability, while recent neural approaches such as neural radiance field (NeRF) suffer from long training and slow inference. To tackle these challenges, we propose voxelized radiance field (VoxelRF), a novel neural representation for wireless channel modeling that enables fast and accurate synthesis of spatial spectra. VoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based methods with trilinear interpolation of voxel grid-based representation, and two shallow MLPs to model both propagation and transmitter-dependent effects. To further accelerate training and improve generalization, we introduce progressive learning, empty space skipping, and an additional background entropy loss function. Experimental results demonstrate that VoxelRF achieves competitive accuracy with significantly reduced computation and limited training data, making it more practical for real-time and resource-constrained wireless applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T07:47:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2507.09987v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2507.09987v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hewen Pan, Cong Wei, Dashuang Liang, Zepeng Huang, Pengfei Gao, Ziqi Zhou, Lulu Xue, Pengfei Yan, Xiaoming Wei, Minghui Li, Shengshan Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T07:17:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11336v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11336v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qingyao Ai, Yichen Tang, Changyue Wang, Jianming Long, Weihang Su, Yiqun Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling up data, parameters, and test-time computation has been the mainstream methods to improve LLM systems (LLMsys), but their upper bounds are almost reached due to the gradual depletion of high-quality data and marginal gains obtained from larger computational resource consumption. Inspired by the abilities of human and traditional AI systems in learning from practice, constructing memory and continual learning frameworks for LLMsys has become an important and popular research direction in recent literature. Yet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time. Therefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys. Experiments show that the effectiveness and efficiency of state-of-the-art baselines are far from satisfying, and we hope this benchmark could pave the way for future studies on LLM memory and optimization algorithms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T07:08:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.17281v4' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.17281v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 NeurIDA: Dynamic Modeling for Effective In-Database Analytics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lingze Zeng, Naili Xing, Shaofeng Cai, Peng Lu, Gang Chen, Jian Pei, Beng Chin Ooi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics.   We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically "tweaks" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improvement in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T07:08:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.08483v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.08483v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Pace: Physics-Aware Attentive Temporal Convolutional Network for Battery Health Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sara Sameer, Wei Zhang, Kannan Dhivya Dharshini, Xin Lou, Yulin Gao, Terence Goh, Qingyu Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Batteries are critical components in modern energy systems such as electric vehicles and power grid energy storage. Effective battery health management is essential for battery system safety, cost-efficiency, and sustainability. In this paper, we propose Pace, a physics-aware attentive temporal convolutional network for battery health estimation. Pace integrates raw sensor measurements with battery physics features derived from the equivalent circuit model. We develop three battery-specific modules, including dilated temporal blocks for efficient temporal encoding, chunked attention blocks for context modeling, and a dual-head output block for fusing short- and long-term battery degradation patterns. Together, the modules enable Pace to predict battery health accurately and efficiently in various battery usage conditions. In a large public dataset, Pace performs much better than existing models, achieving an average performance improvement of 6.5 and 2.0x compared to two best-performing baseline models. We further demonstrate its practical viability with a real-time edge deployment on a Raspberry Pi. These results establish Pace as a practical and high-performance solution for battery health analytics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T07:04:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11332v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11332v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Zhang, Zhenjia Li, Runfeng Bao, Yifan Gao, Xi Xiao, Heng Zhang, Shuyang Zhang, Bo Huang, Yuhang Wu, Tianyang Wang, Hao Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), has emerged as a promising approach to fine-tuning large language models(LLMs) while reducing computational and memory overhead. However, LoRA assumes a uniform rank \textit{r} for each incremental matrix, not accounting for the varying significance of weight matrices across different modules and layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize updates and employs pruning of singular values to introduce dynamic rank allocation, thereby enhancing adaptability. However, during the training process, it often encounters issues of slow convergence speed and high computational overhead. To address this issue, we propose HyperAdaLoRA, a novel framework that accelerates the convergence of AdaLoRA by leveraging a hypernetwork. Instead of directly optimizing the components of Singular Value Decomposition $(P, Λ, Q)$, HyperAdaLoRA employs a hypernetwork based on attention mechanisms to dynamically generate these parameters. By pruning the outputs of the hypernetwork that generates the singular values, dynamic rank allocation is achieved. Comprehensive experiments on various datasets and models demonstrate that our method achieves faster convergence without sacrificing performance. Additionally, further extension experiments on other LoRA-based approaches validate the broad applicability of our method.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T07:03:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.02630v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.02630v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kelin Fu, Tianyu Liu, Zeyu Shang, Yingwei Ma, Jian Yang, Jiaheng Liu, Kaigui Bian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T06:54:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.06915v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.06915v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 MLLM Machine Unlearning via Visual Knowledge Distillation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhang Wang, Zhenxing Niu, Haoxuan Ji, Guangyu He, Haichang Gao, Gang Hua
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T06:51:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11325v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11325v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 KeyframeFace: From Text to Expressive Facial Keyframes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingchao Wu, Zejian Kang, Haibo Liu, Yuanchen Fei, Xiangru Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T06:45:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11321v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11321v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Scalable Best-of-N Selection for Large Language Models via Self-Certainty</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhewei Kang, Xuandong Zhao, Dawn Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size N, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T06:33:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2502.18581v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2502.18581v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alexander Xiong, Xuandong Zhao, Aneesh Pappu, Dawn Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they also exhibit memorization of their training data. This phenomenon raises critical questions about model behavior, privacy risks, and the boundary between learning and memorization. Addressing these concerns, this paper synthesizes recent studies and investigates the landscape of memorization, the factors influencing it, and methods for its detection and mitigation. We explore key drivers, including training data duplication, training dynamics, and fine-tuning procedures that influence data memorization. In addition, we examine methodologies such as prefix-based extraction, membership inference, and adversarial prompting, assessing their effectiveness in detecting and measuring memorized content. Beyond technical analysis, we also explore the broader implications of memorization, including the legal and ethical implications. Finally, we discuss mitigation strategies, including data cleaning, differential privacy, and post-training unlearning, while highlighting open challenges in balancing the need to minimize harmful memorization with model utility. This paper provides a comprehensive overview of the current state of research on LLM memorization across technical, privacy, and performance dimensions, identifying critical directions for future work.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T06:27:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2507.05578v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2507.05578v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 LegalRikai: Open Benchmark -- A Benchmark for Complex Japanese Corporate Legal Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shogo Fujita, Yuji Naraki, Yiqing Zhu, Shinsuke Mori
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces LegalRikai: Open Benchmark, a new benchmark comprising four complex tasks that emulate Japanese corporate legal practices. The benchmark was created by legal professionals under the supervision of an attorney. This benchmark has 100 samples that require long-form, structured outputs, and we evaluated them against multiple practical criteria. We conducted both human and automated evaluations using leading LLMs, including GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1. Our human evaluation revealed that abstract instructions prompted unnecessary modifications, highlighting model weaknesses in document-level editing that were missed by conventional short-text tasks. Furthermore, our analysis reveals that automated evaluation aligns well with human judgment on criteria with clear linguistic grounding, and assessing structural consistency remains a challenge. The result demonstrates the utility of automated evaluation as a screening tool when expert availability is limited. We propose a dataset evaluation framework to promote more practice-oriented research in the legal domain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T05:47:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11297v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11297v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huanyu Liu, Ge Li, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.   To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.   We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T05:42:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2505.16368v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2505.16368v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yasaman Hashem Pour, Nazanin Mahjourian, Vinh Nguyen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T05:42:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11296v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11296v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 AI Autonomy or Human Dependency? Defining the Boundary in Responsible AI with the $α$-Coefficient</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nattaya Mairittha, Gabriel Phorncharoenmusikul, Sorawit Worapradidth
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integrity of contemporary AI systems is undermined by a critical design flaw: the misappropriation of Human-in-the-Loop (HITL) models to mask systems that are fundamentally reliant on human labor. We term this structural reliance Human-Instead-of-AI (HISOAI). HISOAI systems represent an ethical failure and an unsustainable economic dependency, where human workers function as hidden operational fallbacks rather than strategic collaborators. To rectify this, we propose the AI-First, Human-Empowered (AFHE) paradigm. AFHE mandates a technological design where the AI component must achieve a minimum, quantifiable level of functional independence prior to deployment. This standard is formalized through the AI Autonomy Coefficient (alpha), a metric that determines the proportion of tasks that the AI successfully processes without mandatory human substitution. We introduce the AFHE Deployment Algorithm, an algorithmic gate that requires the system to meet a specified alpha threshold across both offline and shadow testing. By enforcing this structural separation, the AFHE framework redefines the human's role to focus exclusively on high-value tasks, including ethical oversight, boundary pushing, and strategic model tuning, thereby ensuring true system transparency and operational independence. This work advocates for a critical shift toward metric-driven, structurally sound AI architecture, moving the industry beyond deceptive human dependency toward verifiable autonomy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T05:41:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11295v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11295v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kesheng Chen, Wenjian Luo, Zhenqian Zhu, Yamin Hu, Yiya Xi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the "curse of dimensionality," rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T05:23:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span><span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.09972v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.09972v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Recent Advances in Discrete Speech Tokens: A Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiwei Guo, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang, Chenpeng Du, Xie Chen, Shujie Liu, Kai Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T05:18:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.AI</span><span>cs.MM</span><span>cs.SD</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2502.06490v4' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2502.06490v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have achieved remarkable performance across a wide range of tasks, but their increasing parameter sizes significantly slow down inference. Speculative decoding mitigates this issue by leveraging a smaller draft model to predict candidate tokens, which are then verified by a larger target model. However, existing approaches often require additional training, extensive hyperparameter tuning, or prior analysis of models and tasks before deployment. In this paper, we propose Adaptive Speculative Decoding (AdaSD), a hyperparameter-free decoding scheme that dynamically adjusts generation length and acceptance criteria during inference. AdaSD introduces two adaptive thresholds: one to determine when to stop candidate token generation and another to decide token acceptance, both updated in real time based on token entropy and Jensen-Shannon distance. This approach eliminates the need for pre-analysis or fine-tuning and is compatible with off-the-shelf models. Experiments on benchmark datasets demonstrate that AdaSD achieves up to 49\% speedup over standard speculative decoding while limiting accuracy degradation to under 2\%, making it a practical solution for efficient and adaptive LLM inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:56:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11280v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11280v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mrinal Rawat, Arkajyoti Chakraborty, Neha Gupta, Roberto Pieraccini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:44:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11277v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11277v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Suchang Chen, Daqiang Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing pipelines for vision-language models (VLMs) in robotic manipulation prioritize broad semantic generalization from images and language, but typically omit execution-critical parameters required for contact-rich actions in manufacturing cells. We formalize an object-centric manipulation-logic schema, serialized as an eight-field tuple τ, which exposes object, interface, trajectory, tolerance, and force/impedance information as a first-class knowledge signal between human operators, VLM-based assistants, and robot controllers. We instantiate τ and a small knowledge base (KB) on a 3D-printer spool-removal task in a collaborative cell, and analyze τ-conditioned VLM planning using plan-quality metrics adapted from recent VLM/LLM planning benchmarks, while demonstrating how the same schema supports taxonomy-tagged data augmentation at training time and logic-aware retrieval-augmented prompting at test time as a building block for assistant systems in smart manufacturing enterprises.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:36:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11275v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11275v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 ProEx: A Unified Framework Leveraging Large Language Model with Profile Extrapolation for Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Zhang, Yiwen Zhang, Yu Wang, Tong Chen, Hongzhi Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The powerful text understanding and generation capabilities of large language models (LLMs) have brought new vitality to general recommendation with implicit feedback. One possible strategy involves generating a unique user (or item) profile from historical interaction data, which is then mapped to a semantic representation in the language space. However, a single-instance profile may be insufficient to comprehensively capture the complex intentions behind a user's interacted items. Moreover, due to the inherent instability of LLMs, a biased or misinterpreted profile could even undermine the original recommendation performance. Consequently, an intuitive solution is to generate multiple profiles for each user (or item), each reflecting a distinct aspect of their characteristics. In light of this, we propose a unified recommendation framework with multi-faceted profile extrapolation (ProEx) in this paper. By leveraging chain-of-thought reasoning, we construct multiple distinct profiles for each user and item. These new profiles are subsequently mapped into semantic vectors, extrapolating from the position of the original profile to explore a broader region of the language space. Subsequently, we introduce the concept of environments, where each environment represents a possible linear combination of all profiles. The differences across environments are minimized to reveal the inherent invariance of user preferences. We apply ProEx to three discriminative methods and three generative methods, and conduct extensive experiments on three datasets. The experimental results demonstrate that ProEx significantly enhances the performance of these base recommendation models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:33:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.00679v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.00679v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxing Chen, Basem Suleiman, Qifan Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:27:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11271v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11271v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hong Je-Gal, Chan-Bin Yi, Hyun-Suk Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:21:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11270v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11270v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 A Unified Model for Multi-Task Drone Routing in Post-Disaster Road Assessment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huatian Gong, Jiuh-Biing Sheu, Zheng Wang, Xiaoguang Yang, Ran Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Post-disaster road assessment (PDRA) is essential for emergency response, enabling rapid evaluation of infrastructure conditions and efficient allocation of resources. Although drones provide a flexible and effective tool for PDRA, routing them in large-scale networks remains challenging. Exact and heuristic optimization methods scale poorly and demand domain expertise, while existing deep reinforcement learning (DRL) approaches adopt a single-task paradigm, requiring separate models for each problem variant and lacking adaptability to evolving operational needs. This study proposes a unified model (UM) for drone routing that simultaneously addresses eight PDRA variants. By training a single neural network across multiple problem configurations, UM captures shared structural knowledge while adapting to variant-specific constraints through a modern transformer encoder-decoder architecture. A lightweight adapter mechanism further enables efficient finetuning to unseen attributes without retraining, enhancing deployment flexibility in dynamic disaster scenarios. Extensive experiments demonstrate that the UM reduces training time and parameters by a factor of eight compared with training separate models, while consistently outperforming single-task DRL methods by 6-14%, heuristic algorithms by 22-42%, and commercial solvers by 24-82% in terms of solution quality (total collected information value). The model achieves rapid solutions (1-10 seconds) across networks of up to 1,000 nodes, with robustness confirmed through sensitivity analyses. Moreover, finetuning experiments show that unseen attributes can be effectively incorporated with minimal cost while retaining high solution quality. The source code for UM is publicly available at https://github.com/PJ-HTU/UM_PDRA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:16:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2510.21525v2' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2510.21525v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 A Scalable Multi-GPU Framework for Encrypted Large-Model Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Siddharth Jayashankar, Joshua Kim, Michael B. Sullivan, Wenting Zheng, Dimitrios Skarlatos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:15:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2512.11269v1' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2512.11269v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junha Song, Yongsik Jo, So Yeon Min, Quanting Xie, Taehwan Kim, Yonatan Bisk, Jaegul Choo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Systems such as video chatbots and navigation robots often depend on streaming image captioning to interpret visual inputs. Existing approaches typically employ large multimodal language models (MLLMs) for this purpose, but their substantial computational cost hinders practical application. This limitation motivates our development of a lightweight captioning model. Our investigation begins by replacing the large-scale language component in MLLMs with a compact 125M-parameter model. Surprisingly, this compact model, despite a 93x reduction in size, achieves comparable performance to MLLMs, suggesting that factual image captioning does not significantly require the complex reasoning abilities of LLMs. Despite this promising result, our lightweight model still lacks reliability. To address this, we draw inspiration from the human visual process: perceiving a global and coarse understanding of the scene before attending to finer details. Accordingly, we propose a multimodal self-refinement framework that guides the model to utilize features from salient regions, identified by referencing the previous coarse caption, and to produce a refined description. Experimental results demonstrate the superiority of our model in both single-sentence and detailed captioning, extending even to long-range video QA tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:06:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2508.21451v4' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2508.21451v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nasim Soltani, Michael Loehning, Kaushik Chowdhury
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artificial Intelligence (AI)-native receivers prove significant performance improvement in high noise regimes and can potentially reduce communication overhead compared to the traditional receiver. However, their performance highly depends on the representativeness of the training dataset. A major issue is the uncertainty of whether the training dataset covers all test environments and waveform configurations, and thus, whether the trained model is robust in practical deployment conditions. To this end, we propose a joint measurement-recovery framework for AI-native transceivers post deployment, called VERITAS, that continuously looks for distribution shifts in the received signals and triggers finite re-training spurts. VERITAS monitors the wireless channel using 5G pilots fed to an auxiliary neural network that detects out-of-distribution channel profile, transmitter speed, and delay spread. As soon as such a change is detected, a traditional (reference) receiver is activated, which runs for a period of time in parallel to the AI-native receiver. Finally, VERTIAS compares the bit probabilities of the AI-native and the reference receivers for the same received data inputs, and decides whether or not a retraining process needs to be initiated. Our evaluations reveal that VERITAS can detect changes in the channel profile, transmitter speed, and delay spread with 99%, 97%, and 69% accuracies, respectively, followed by timely initiation of retraining for 86%, 93.3%, and 94.8% of inputs in channel profile, transmitter speed, and delay spread test sets, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-12-12T04:02:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='https://arxiv.org/abs/2501.09761v3' target='_blank'>HTML</a><a href='https://arxiv.org/pdf/2501.09761v3' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    